

\usepackage{amsmath,amssymb,amsthm}

\setlength{\textheight}{9.5in}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color,bbm}
\usepackage[normalem]{ulem}

\definecolor{violet}{rgb}{1.00,0.00,1.00}	

\graphicspath{{./Figures/}}

 
  

 

\begin{document}

\title{Topological and Dynamical Complexity of Random Neural Networks}

\author{Gilles Wainrib}\email[]{wainrib@math.univ-paris13.fr}
\affiliation{LAGA, Universit\'e Paris XIII}
\author{Jonathan Touboul}
\email[]{jonathan.touboul@college-de-france.fr}
\affiliation{The Mathematical Neuroscience Laboratory, CIRB / Coll\`ege de France (CNRS UMR 7241, INSERM U1050, UPMC ED 158, MEMOLIFE PSL*)}
\affiliation{BANG Laboratory, INRIA Paris}

\date{\today}
\begin{abstract}
	Random neural networks are dynamical descriptions of randomly interconnected neural units. These show a phase transition to chaos as a disorder parameter is increased. The microscopic mechanisms underlying this phase transition are unknown, and similarly to spin-glasses, shall be fundamentally related to the  behavior of the system. In this Letter we investigate the explosion of complexity arising near that phase transition. We show that the mean number of equilibria undergoes a sharp transition from one equilibrium to a very large number scaling exponentially with the dimension on the system. Near criticality, we compute the exponential rate of divergence, called topological complexity. Strikingly, we show that it behaves exactly as the maximal Lyapunov exponent, a classical measure of dynamical complexity. This relationship unravels a microscopic mechanism leading to chaos which we further demonstrate on a simplerÂ® disordered systems, suggesting a deep and underexplored link between topological and dynamical complexity. 
\end{abstract}

\pacs{
87.18.Tt, 
05.10.-a 
87.19.ll, 
87.18.Sn, 
87.18.Nq 
}
\keywords{Randomly connected neural networks, complexity, Lyapunov exponents, phase transitions, random matrix}
\maketitle

Heterogeneity of interconnections is a crucial property to understand the behavior of realistic networks that arise in the modeling of physical, biological or social complex systems. Among these, a paramount example is given by neuronal networks of the brain. In these systems, synaptic connections display characteristic disorder properties~\cite{parker:03,marder-goaillard:06} resulting from development and learning. 
Taking into account this heterogeneity seems now essential, as experimental studies of neuronal tissues have shown that the degree of disorder in the connections significantly impacts the input-output function, rhythmicity and synchrony, effects that can be related to transitions between physiological and pathological behaviors~\cite{aradi-soltesz:02,santhakumar2004plasticity,soltesz2005diversity}. As an example, Aradi and Soltesz~\cite{aradi-soltesz:02} have shown that rats subject to febrile seizure present the same average synaptic properties but increased variance. 

These properties are reminiscent of disordered physical systems such as spin-glasses. The relationship between disorder and qualitative behaviors has been thoroughly studied within theoretical frameworks such as the Sherrington-Kirkpatrick model~\cite{sherrington-kirkpatrick:75} describing the behavior of binary variables interacting through a random connectivity matrix. In these models, estimating the number of metastable states, deemed to be deeply related to the transition to chaos, remains an important endeavor~\cite{parisi2006course}. In the context of nervous system modeling, random neural networks \cite{sompolinsky-crisanti-etal:88, amari:72} constitute a prominent class of models, which describe the evolution of the activity of a neuron $i$ in a $n$-neurons network through the randomly coupled system of ordinary differential equations: 
\begin{equation}
	\label{eq:sompo}
	\dot{x_i} = -x_i + \sum_{j=1}^n J_{ij}S(x_j)
\end{equation}
where $J_{ij}$ are independent centered Gaussian random variables of variance $\sigma^2/n$ representing the synaptic connectivity coefficients between neuron $i$ and $j$~\footnote{{This classical model does not satisfy Dale's principle: a neuron can here be both excitatory and inhibitory.}}, and $S$ is an odd sigmoid function with maximal unit slope at the origin ($S'(x)\leq S'(0)=1$) representing the synaptic nonlinearity. 

The behavior of system \eqref{eq:sompo} has been analyzed in the asymptotic regime of infinite population size $n\to \infty$~\cite{sompolinsky-crisanti-etal:88} and displays a phase transition for a critical value of the disorder at $\sigma=1$: for $\sigma<1$ all the trajectories are attracted to the trivial equilibrium ${\mathbf{x}}=0$ and for $\sigma>1$ the trajectories have chaotic dynamics. This generic phase transition has been numerically observed in a number of situations in more realistic models involving multiple populations and excitable dynamics~\cite{hermann-touboul:12}. That phase transition, and the chaotic regime beyond the edge of chaos, appear particularly relevant to understand the computational capabilities of neuronal networks. In particular, information processing capacity was characterized as optimal at the edge of criticality~\cite{abbott-toyoizumi:11, sussillo-abbott:09}, and such random neural networks are in particular used in recent machine learning algorithms \cite{jaeger:04}. Moreover, the question of criticality has been widely debated in the theoretical neuroscience community and beyond \cite{bak:96, beggs:08, kitzbichler:09} and it seems that through a number of different mechanisms among which plasticity of synapses, networks may tend to be naturally poised near criticality. In the random neural network, the microscopic mechanisms underpinning this phase transition remains unclear.
It is hence of great interest to dissect precisely the behavior of such systems at the edge of chaos. 

Characterizing the topological modifications of the phase space arising at the edge of chaos is precisely the question we shall address in this Letter. More precisely, we will estimate the averaged number of equilibria in the random neural network~\eqref{eq:sompo}.
To this end, we will develop upon the theories of random matrices~\cite{tao2012topics}, random geometry and Gaussian fields~\cite{adler-taylor:07}. 

We denote by $A_n(\sigma)$ the random number of equilibria (depending on the realization of the matrix ${\mathbf{J}}=(J_{ij})$). These are the solutions of the system:
\begin{equation}\label{eq:FP}
	x_i = \sum_{j=1}^n J_{ij}S(x_j) 
\end{equation}
For $\sigma<1$, consistently with the mean-field analysis, we first show that ${\mathbb{E}\left [ {A_n(\sigma)} \right]} \to 1$ when $n\to \infty$. The proof proceeds by showing that the system is contracting on the whole space ${\mathbb{R}}^n$. To this purpose, one needs to characterize the eigenvalues of the random Jacobian matrix $-{\mathbf{I}}+{\mathbf{J}}.\Delta (S'({\mathbf{x}}))$ where ${\mathbf{I}}$ is the identity matrix, and $\Delta(S'({\mathbf{x}}))$ is the diagonal matrix with elements $S'(x_i)$. The matrix ${\mathbf{J}}.\Delta (S'({\mathbf{x}}))$ is a centered Gaussian matrix with independent components, and each column has a distinct standard deviation given by $\sigma^2 S'(x_i)^2/n$. Rajan and Abbott~\cite{rajan-abbott:06} provide the system of equations satisfied by the squared modulus of the eigenvalues of such random matrices, and solve these when considering only two different variances. Their methodology readily generalizes to our problem, and elementary algebraic manipulations (see~\cite{garcia:masterthesis,yiwei}) show that the spectral density has support in the disc of radius $\frac {\sigma^2} n \sum_{i=1}^n{S'(x_i)^2}$ for large $n$. In our case, $S'(x)\leq 1$, and hence all eigenvalues of the matrix $-{\mathbf{I}}+{\mathbf{J}}\cdot \Delta(S'({\mathbf{x}}))$ have a negative real part in the limit $n\to\infty$.  This implies global contraction of the dynamics, ensuring the fact that the trivial equilibrium ${\mathbf{x}}=0$ is the unique equilibrium for $\sigma<1$ (Banach fixed point theorem) and its global stability. 

For $\sigma>1$ the situation is more complex. Similarly to the phase transition in spin glasses, the behavior of ${\mathbb{E}\left [ {A_n(\sigma)} \right]}$ is likely to scale as $\exp(n\,C(\sigma))$ where $C(\sigma)$ is the {topological complexity}. Our starting point is to observe that the fixed point equations~\eqref{eq:FP} can be viewed as zero crossings of a Gaussian field indexed by ${\mathbf{x}}\in{\mathbb{R}}^n$. Moreover, regularity and measurability of that Gaussian field ensure that the Kac-Rice~\footnote{This formula was initially introduced by Kac~\cite{kac:43} to evaluate the average number of real roots of a random polynomial, and a modern account can be found in~\cite{azais:09,adler-taylor:07}} formula can be applied in order to estimate the number of solutions:
\begin{multline*}
	{\mathbb{E}\left [ {A_n(\sigma)} \right]} = \int_{{\mathbb{R}}^n} \textrm{d}{\mathbf{x}} \;\mathbbm{E}\bigg[ |\det (-{\mathbf{I}} + {\mathbf{J}}.\Delta (S'({\mathbf{x}})))| \\
	\times \delta_0(-{\mathbf{x}}+{\mathbf{J}}.S({\mathbf{x}}))\bigg]. 
\end{multline*}
Recent studies \cite{fyodorov:04,fyodorov-williams:07, auffinger-ben-arous:11} have used that formula to estimate the mean number of critical points in random energy landscapes arising from Hamiltonian systems with strong symmetry properties (spin-glass with spherical symmetry in~\cite{auffinger-ben-arous:11} and translation invariant potential in~\cite{fyodorov:04,fyodorov-williams:07}). In our case, the situation is substantially different: (i) there is no underlying energy landscape since the system \eqref{eq:sompo} is not Hamiltonian and (ii) symmetry properties of the vector field do not enable the same kind of reduction method developed in \cite{auffinger-ben-arous:11,fyodorov-williams:07,fyodorov:04}. A major technical difficulty is the fact that, because the lack of symmetry in our system, one needs to deal with determinant of random matrices with columns of non-identical variances, for which the spectral density is unknown. 

However, near criticality, for $\sigma=1+\varepsilon$ with $0<\varepsilon \ll 1$, we can obtain a first order estimate of the number of equilibria taking advantage of the fact that all equilibria remain close to ${\mathbf{x}}=0$. More precisely, we first show that with an arbitrarily high probability $1-\xi(n,\varepsilon)$ with $\xi(n,\varepsilon)\to 0$ as $n \to \infty$, all equilibria belong to a ball $\mathcal{B}_{\rho(\varepsilon)}$ centered at ${\mathbf{x}}=0$ of radius $\rho(\varepsilon)$ which tends to $0$ as $\varepsilon \to 0$. This is a consequence of the spectral analysis of the random matrix $-{\mathbf{I}}+{\mathbf{J}}\cdot \Delta(S'({\mathbf{x}}))$, whose eigenvalues all have negative real parts outside of $\mathcal{B}_{\rho(\varepsilon)}$ for large $n$. The property that fixed points remain in a small ball around zero is non-trivial. The proof proceeds by defining $\rho(\varepsilon)$ the unique positive solution of the scalar equation $x/\sigma=S(x)$, which is clearly arbitrarily small when $\varepsilon\to 0$, and the smooth modified sigmoid function $S_{\varepsilon,\eta}$ 
\[S_{\varepsilon,\eta}(x)=\begin{cases}
	x/\sigma & \vert x\vert <\rho(\varepsilon)\\
	S(x) & \vert x\vert >\rho(\varepsilon)+\eta
\end{cases}\] 
(the small interval $[\rho(\varepsilon),\rho(\varepsilon)+\eta]$ allows to define a smooth continuation). Because of the properties of the sigmoidal function, and in particular the fact that the differential is decreasing for $x>0$ (and increasing for $x<0$), the same argument as used in the case $\sigma<1$ applied to system~\eqref{eq:sompo} defined with the sigmoid $S_{\varepsilon,\eta}$ (termed \emph{modified system}) ensures that all eigenvalues of $-{\mathbf{I}}+{\mathbf{J}}\cdot \Delta(S_{\varepsilon,\eta}'({\mathbf{x}}))$ have negative real part and hence that the unique fixed point of the modified system is $0$. In particular, there is no fixed point outside the ball of radius $\rho(\varepsilon)$. In that region the original and modified system are identical, implying that the only possible fixed points of the original system are contained in the ball of radius $\rho(\varepsilon)$.

Therefore, one can split the expectation according to whether $|{\mathbf{x}}|<\rho(\varepsilon)$ or not, yielding:
\begin{align*}
	{\mathbb{E}\left [ {A_n(\sigma)} \right]} =\int_{\mathcal{B}_{\rho(\varepsilon)}} \textrm{d}{\mathbf{x}}\;\mathbbm{E}\Big[ |\det (-{\mathbf{I}} + {\mathbf{J}}.\Delta (S'({\mathbf{x}})))| \\
	\times \delta_0(-{\mathbf{x}}+{\mathbf{J}}.S({\mathbf{x}}))\Big] + O(\xi(n,\varepsilon))
\end{align*}
Moreover, thanks to the differentiability of the determinant operator, we know that within the ball $\mathcal{B}_{\rho(\varepsilon)}$, the integrand is equal to $|\det (-{\mathbf{I}} + {\mathbf{J}})| + O(\rho(\varepsilon))$, eventually yielding:
\begin{align*}
	{\mathbb{E}\left [ {A_n(\sigma)} \right]} ={\mathbb{E}\left [ {|\det (-{\mathbf{I}} + {\mathbf{J}})|} \right]} + O(\rho(\varepsilon)+\xi(n,\varepsilon))
\end{align*}
To evaluate this formula, we first compute the logarithm of the determinant:
\begin{align*}
	\frac{1}{n} \log |\det (-{\mathbf{I}} + {\mathbf{J}})| &= \frac 1 n \sum_{\lambda \in sp({\mathbf{J}})} \log |\lambda -1|
	\end{align*} 
where $sp({\mathbf{J}})$ denotes the spectrum of ${\mathbf{J}}$, which in the large $n$ limit is uniformly distributed in the disc of radius $\sigma$ \cite{girko:85}. Using this property one obtains in the large $n$ limit
\begin{align*}
	\frac{1}{n} \log |\det (-{\mathbf{I}} + {\mathbf{J}})| &= c(\sigma) + R(n)
	\end{align*}
 and  $c(\sigma):=\int_{\mathbb{C}} \log |z -1| \mu_{\sigma}(dz)$ with $\mu_{\sigma}(dz)$ the uniform distribution on the disk of radius $\sigma=1+\varepsilon$ and $R(n)$ is the finite-size error associated to the convergence to the circular law. We have:
\begin{equation}
	\frac 1 n \log {\mathbb{E}\left [ {|\det(-{\mathbf{I}}+{\mathbf{J}})|} \right]} = c(\sigma) + \frac 1 n \log {\mathbb{E}\left [ {e^{nR(n)}} \right]}.
\end{equation}
This ensures that $\frac 1 n \log {\mathbb{E}\left [ {A_n(\sigma)} \right]}$ is arbitrarily close $c(\sigma)$ when $\varepsilon \to 0$ and $n\to\infty$. 

We are hence left computing $c(\sigma)$. Since $\log \vert z\vert$ is harmonic, we can show that for $a,b>0$ :
\[\int_0^{2\pi} \log\vert a - be^{\mathbf{i}\theta}\vert d\theta = 2\pi \log(\min (a, b))\]
ensuring that $c(\sigma)=0$ for $\sigma<1$, which is consistent with our previous analysis of this case, and 
\begin{eqnarray}
	\nonumber c(\sigma)&=&\frac 1 {\pi\sigma^2} \int_1^{\sigma} 2\pi r\log(r)\,dr \\
	&=& \log(\sigma) + \frac 1 2 \left(\frac 1 {\sigma^2}-1\right)\label{eq:CofSigma}
\end{eqnarray}
for $\sigma>1$. For $\sigma$ close to $1^+$, we conclude $c(\sigma)\sim (\sigma-1)^2$. 
Therefore we have shown that:
\begin{equation}\label{eq:result}
	\mathbbm{E}[A_n(\sigma)]\sim e^{n(\sigma-1)^2}
\end{equation}
up to multiplicative polynomial factors.

This combinatorial explosion of the number of equilibria is the hallmark of the accumulation of bifurcations \cite{cessac:95} in a the neighborhood of the critical parameter value $\sigma=1$. Moreover, this result provides a possible topological explanation for the emergence of chaos. For $\sigma>1$, the phase space is heavily mined with equilibrium points, most of which are saddles due to the spectral properties of the Jacobian matrices as discussed above. Typical trajectories evolving in this landscape will hence wander from the vicinity of the different unstable equilibria and attractors appearing, inducing a very high sensitivity to perturbations, distinctive feature of chaos. 

The classical characterization of chaos relies on the evaluation of the maximal Lyapunov exponent of the trajectories quantifying the dynamical complexity. This quantity is defined as follows. Applying
 an infinitesimal external perturbation $x_i\to x_i+\delta x_i^0$ on neuron $i$ at time $t_0$ induces a change on all neurons at subsequent times $x_j(t)\to
 x_j(t)+\delta x_{ji}(t)$ defining a susceptibility matrix $\Psi_{ij}(t_0+\tau,t_0)=\delta x_{ji}(t_0+\tau)/\delta x_i^0$. From the trace of this matrix we shall define an averaged susceptibility $\Psi^2(\tau)=\lim_{t_0\to\infty} \frac 1 n {\mathbb{E}\left [ {\sum_{i,j} \Psi_{ij}^2(t_0+\tau,t_0)} \right]}$ and eventually obtain the maximal Lyapunov exponent:
\[\lambda = \lim_{t\to\infty} \frac {\log(\Psi^2(t))} {t}.\]  
This quantity was analyzed in~\cite{sompolinsky-crisanti-etal:88} using a spectral decomposition based on the one-dimensional Schr\"odinger equation. Near criticality, the decomposition dramatically simplifies and yields for $\sigma\sim 1^+$:
\[\lambda \sim {(\sigma-1)^2}.\]
We hence conclude that the topological and dynamical complexities have the same behavior at the edge of chaos.
More surprising is that using the spectral decomposition of the Schr\"odinger equation in the limit $\sigma \gg 1$, Sompolinsky and collaborators show that $\lambda$ diverges as $\log(\sigma)$, precisely as the complexity of the system given in formula~\eqref{eq:CofSigma}, although our analysis rigorously only applies for $\sigma$ close to $1$. 
This further strengthens our microscopic interpretation of the emergence of chaos in relationship with the number of saddles. 

If this interpretation is the actual phenomenon arising in random neural networks at the edge of chaos, then the same phenomenon may hold in simpler, lower-dimensional dynamical systems with a large number of unstable fixed points, and these shall display a similar relationship between the number of unstable equilibria and the Lyapunov exponent. Probably the simplest low-dimensional system with easily controllable number of fixed points is the \emph{fakir bed} dynamics, corresponding to the movement of a particle in a two-dimensional complex landscape with $k$ unstable fixed points. More precisely, we now consider a particle confined in a compact subset of ${\mathbb{R}}^2$, with close to the origin a fixed number $k$ of Gaussian hills (corresponding to the presence of $k$ unstable fixed points) randomly chosen in space. Trajectories of particles soon get chaotic as $k$ increases (see Fig.~\ref{fig:Fakir}(b)), and we numerically compute the maximal Lyapunov exponent of the trajectories. Since the landscape is probabilistic due to the choice of the location of hills, we compute the averaged maximal Lyapunov exponent across $100$ independent realizations of the process, and plot it against the logarithm of the number of equilibria. The corresponding curve (Fig.~\ref{fig:Fakir}(a)), indeed displays an increasing profile well approximated by a linear curve: a clear relationship again emerges between number of unstable fixed points and Lyapunov exponents, supporting our interpretation related to the random neural network.

\begin{figure}[htbp]
	\centering
		\subfigure[Lyapunov exponent vs number of fixed point]{\includegraphics[width=.2\textwidth]{LyapunovsemilogxEdt}}
		\subfigure[Fakir bed trakectories]{\includegraphics[width=.25\textwidth]{3dTrajectories.pdf}}
	\caption{The Fakir bed dynamics. (a): blue: Lyapunov exponent averaged over 100 realizations of the Fakir's dynamics vs number of fixed points (in semilogarithmic axes) and linear regression (slope $0.36$). (b): Red and Green: two trajectories with very close initial conditions.}
	\label{fig:Fakir}
\end{figure}

Thus, motivated by the analysis of fine microscopic phenomena arising at the edge of chaos in random neural networks, we have shown that the average number of equilibria scales exponentially with the system size, with an exponential coefficient proportional to the Lyapunov exponent. This property, relying in part on the spectral theory of random matrices, readily inherits universality properties of the circular law~\cite{tao2008random}, and therefore is valid for a large class of independent couplings beyond Gaussian. Matrices satisfying Dale's principle usually do not fall in this universality class, and do not necessarily present the type of phase transition under consideration. Indeed, Dale's principle typically requires to consider correlated or non-centered synaptic weights, generally modifying the spectrum of the connectivity matrix and the network dynamics. Combining recent results on the spectrum of such matrices~\cite{rajan-abbott:06,tao2011outliers} with the Kac-Rice formula should provide new insight into the complexity of these networks. 

Moreover, our result shows that the complexity smoothly with the disorder parameter with a critical exponent $2$, larger than $1$, which corresponds to second order phase transitions. This property ensures a form of structural robustness in the neighborhood of the phase transition in the sense that the complexity-related properties of the critical state would hold beyond the edge of chaos, which may have several implications in information processing capabilities~\cite{abbott-toyoizumi:11}. Moreover, the result obtained~\eqref{eq:result} reveals a particular scaling $\sigma=1+O(n^{-\frac 1 2 })$ characterizing the typical thickness of the edge of chaos for large finite-size networks. This study is the first application to random neural networks of recent methods used for counting the number of metastable equilibria in spin glasses. From the theoretical viewpoint, we extended that approach to out of equilibrium, non-Hamiltonian systems at zero temperature (singular points of a vector field). This identity incidentally found between topological and dynamical complexity highlights what we conjecture to be a deep correspondence in large complex systems. Numerous questions and perspectives have emerged from this study, among which the estimation of the distribution of equilibria and their number beyond the edge of criticality, requiring significant advances in the analysis of random matrix determinants, or pursuing the exploration, in line with \cite{ledrappier1985metric,frederickson1983liapunov,chatterjee:08}, of the relationship between topological and dynamical complexity with other measures such as the fractal dimensions of chaotic attractors.
\medskip

\begin{thebibliography}{35}\makeatletter
\providecommand \@ifxundefined [1]{ \@ifx{#1\undefined}
}\providecommand \@ifnum [1]{ \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}\providecommand \@ifx [1]{ \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}\providecommand \natexlab [1]{#1}\providecommand \enquote  [1]{``#1''}\providecommand \bibnamefont  [1]{#1}\providecommand \bibfnamefont [1]{#1}\providecommand \citenamefont [1]{#1}\providecommand \href@noop [0]{\@secondoftwo}\providecommand \href [0]{\begingroup \@sanitize@url \@href}\providecommand \@href[1]{\@@startlink{#1}\@@href}\providecommand \@@href[1]{\endgroup#1\@@endlink}\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode
  `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}\providecommand \@@startlink[1]{}\providecommand \@@endlink[0]{}\providecommand \url  [0]{\begingroup\@sanitize@url \@url }\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}\providecommand \urlprefix  [0]{URL }\providecommand \Eprint [0]{\href }\providecommand \doibase [0]{http://dx.doi.org/}\providecommand \selectlanguage [0]{\@gobble}\providecommand \bibinfo  [0]{\@secondoftwo}\providecommand \bibfield  [0]{\@secondoftwo}\providecommand \translation [1]{[#1]}\providecommand \BibitemOpen [0]{}\providecommand \bibitemStop [0]{}\providecommand \bibitemNoStop [0]{.\EOS\space}\providecommand \EOS [0]{\spacefactor3000\relax}\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}\let\auto@bib@innerbib\@empty
\bibitem [{\citenamefont {Parker}(2003)}]{parker:03}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Parker}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {The
  Journal of Neuroscience}\ }\textbf {\bibinfo {volume} {23}},\ \bibinfo
  {pages} {3154} (\bibinfo {year} {2003})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Marder}\ and\ \citenamefont
  {Goaillard}(2006)}]{marder-goaillard:06}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Marder}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Goaillard}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Reviews Neuroscience}\ }\textbf {\bibinfo {volume} {7}},\ \bibinfo
  {pages} {563} (\bibinfo {year} {2006})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Aradi}\ and\ \citenamefont
  {Soltesz}(2002)}]{aradi-soltesz:02}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Aradi}}\ and\ \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Soltesz}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {The
  Journal of Physiology}\ }\textbf {\bibinfo {volume} {538}},\ \bibinfo {pages}
  {227} (\bibinfo {year} {2002})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Santhakumar}\ and\ \citenamefont
  {Soltesz}(2004)}]{santhakumar2004plasticity}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Santhakumar}}\ and\ \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Soltesz}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {TRENDS in Neurosciences}\ }\textbf {\bibinfo {volume} {27}},\ \bibinfo
  {pages} {504} (\bibinfo {year} {2004})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Soltesz}(2005)}]{soltesz2005diversity}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Soltesz}},\ }\href@noop {} {\emph {\bibinfo {title} {Diversity in the
  neuronal machine: order and variability in interneuronal microcircuits}}}\
  (\bibinfo  {publisher} {Oxford University Press, USA},\ \bibinfo {year}
  {2005})\BibitemShut {NoStop}\bibitem [{\citenamefont {Sherrington}\ and\ \citenamefont
  {Kirkpatrick}(1975)}]{sherrington-kirkpatrick:75}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Sherrington}}\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Kirkpatrick}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {35}},\ \bibinfo
  {pages} {1792} (\bibinfo {year} {1975})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Parisi}(2006)}]{parisi2006course}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Parisi}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Les
  Houches}\ }\textbf {\bibinfo {volume} {83}},\ \bibinfo {pages} {295}
  (\bibinfo {year} {2006})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Sompolinsky}\ \emph {et~al.}(1988)\citenamefont
  {Sompolinsky}, \citenamefont {Crisanti},\ and\ \citenamefont
  {Sommers}}]{sompolinsky-crisanti-etal:88}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Sompolinsky}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Crisanti}}, \ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Sommers}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {61}},\ \bibinfo
  {pages} {259} (\bibinfo {year} {1988})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Amari}(1972)}]{amari:72}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Amari}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Syst.
  Man Cybernet. SMC-2}\ } (\bibinfo {year} {1972})}\BibitemShut {NoStop}\bibitem [{Note1()}]{Note1}  \BibitemOpen
  \bibinfo {note} {{This classical model does not satisfy Dale's principle: a
  neuron can here be both excitatory and inhibitory.}}\BibitemShut {Stop}\bibitem [{\citenamefont {Hermann}\ and\ \citenamefont
  {Touboul}(2012)}]{hermann-touboul:12}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Hermann}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Touboul}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {109}},\ \bibinfo
  {pages} {018702} (\bibinfo {year} {2012})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Toyoizumi}\ and\ \citenamefont
  {Abbott}(2011)}]{abbott-toyoizumi:11}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Toyoizumi}}\ and\ \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont
  {Abbott}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review E}\ }\textbf {\bibinfo {volume} {84}},\ \bibinfo {pages}
  {051908} (\bibinfo {year} {2011})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Sussillo}\ and\ \citenamefont
  {Abbott}(2009)}]{sussillo-abbott:09}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Sussillo}}\ and\ \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont
  {Abbott}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Neuron}\ }\textbf {\bibinfo {volume} {63}},\ \bibinfo {pages} {544}
  (\bibinfo {year} {2009})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Jaeger}\ and\ \citenamefont
  {Haas}(2004)}]{jaeger:04}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.}~\bibnamefont
  {Jaeger}}\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Haas}},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Science}\ }\textbf
  {\bibinfo {volume} {304}},\ \bibinfo {pages} {78} (\bibinfo {year}
  {2004})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Bak}(1996)}]{bak:96}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Bak}},\ }\href@noop {} {\emph {\bibinfo {title} {How nature works: the
  science of self-organized criticality}}},\ Vol.\ \bibinfo {volume} {212}\
  (\bibinfo  {publisher} {Copernicus New York},\ \bibinfo {year}
  {1996})\BibitemShut {NoStop}\bibitem [{\citenamefont {Beggs}(2008)}]{beggs:08}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Beggs}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Philosophical Transactions of the Royal Society A: Mathematical, Physical
  and Engineering Sciences}\ }\textbf {\bibinfo {volume} {366}},\ \bibinfo
  {pages} {329} (\bibinfo {year} {2008})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Kitzbichler}\ \emph {et~al.}(2009)\citenamefont
  {Kitzbichler}, \citenamefont {Smith}, \citenamefont {Christensen},\ and\
  \citenamefont {Bullmore}}]{kitzbichler:09}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Kitzbichler}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Smith}},
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Christensen}}, \ and\
  \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Bullmore}},\ }\href@noop
  {} {\bibfield  {journal} {\bibinfo  {journal} {PLoS Computational Biology}\
  }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages} {e1000314} (\bibinfo
  {year} {2009})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Tao}(2012)}]{tao2012topics}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Tao}},\ }\href@noop {} {\emph {\bibinfo {title} {Topics in random matrix
  theory}}},\ Vol.\ \bibinfo {volume} {132}\ (\bibinfo  {publisher} {Amer
  Mathematical Society},\ \bibinfo {year} {2012})\BibitemShut {NoStop}\bibitem [{\citenamefont {Adler}\ and\ \citenamefont
  {Taylor}(2007)}]{adler-taylor:07}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Adler}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Taylor}},\
  }\href@noop {} {\emph {\bibinfo {title} {Random fields and geometry}}},\
  Vol.\ \bibinfo {volume} {115}\ (\bibinfo  {publisher} {Springer Verlag},\
  \bibinfo {year} {2007})\BibitemShut {NoStop}\bibitem [{\citenamefont {Rajan}\ and\ \citenamefont
  {Abbott}(2006)}]{rajan-abbott:06}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont
  {Rajan}}\ and\ \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Abbott}},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Physical Review
  Letters}\ }\textbf {\bibinfo {volume} {97}},\ \bibinfo {pages} {188104}
  (\bibinfo {year} {2006})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Garcia~del Molino}(2012)}]{garcia:masterthesis}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont
  {Garcia~del Molino}},\ }\emph {\bibinfo {title} {Dynamics of randomly
  connected systems with applications to neural networks}},\ \href@noop {}
  {Master's thesis},\ \bibinfo  {school} {\'Ecole Polytechnique} (\bibinfo
  {year} {2012})\BibitemShut {NoStop}\bibitem [{\citenamefont {Wei}(2012)}]{yiwei}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Wei}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Physical
  Review E}\ }\textbf {\bibinfo {volume} {85}},\ \bibinfo {pages} {066116}
  (\bibinfo {year} {2012})}\BibitemShut {NoStop}\bibitem [{Note2()}]{Note2}  \BibitemOpen
  \bibinfo {note} {This formula was initially introduced by Kac~\cite {kac:43}
  to evaluate the average number of real roots of a random polynomial, and a
  modern account can be found in~\cite {azais:09,adler-taylor:07}}\BibitemShut
  {NoStop}\bibitem [{\citenamefont {Fyodorov}(2004)}]{fyodorov:04}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Fyodorov}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {92}},\ \bibinfo
  {pages} {240601} (\bibinfo {year} {2004})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Fyodorov}\ and\ \citenamefont
  {Williams}(2007)}]{fyodorov-williams:07}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont
  {Fyodorov}}\ and\ \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont
  {Williams}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Journal of Statistical Physics}\ }\textbf {\bibinfo {volume} {129}},\
  \bibinfo {pages} {1081} (\bibinfo {year} {2007})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Auffinger}\ and\ \citenamefont
  {Arous}(2011)}]{auffinger-ben-arous:11}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Auffinger}}\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {Arous}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Arxiv
  preprint arXiv:1110.5872}\ } (\bibinfo {year} {2011})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Girko}(1985)}]{girko:85}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Girko}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Theory
  of Probability and its Applications}\ }\textbf {\bibinfo {volume} {29}},\
  \bibinfo {pages} {694} (\bibinfo {year} {1985})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Cessac}(1995)}]{cessac:95}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont
  {Cessac}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Journal de Physique I}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages}
  {409} (\bibinfo {year} {1995})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Tao}\ and\ \citenamefont
  {VAN}(2008)}]{tao2008random}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Tao}}\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {VAN}},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Communications in
  Contemporary Mathematics}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo
  {pages} {261} (\bibinfo {year} {2008})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Tao}(2011)}]{tao2011outliers}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {Tao}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Probability Theory and Related Fields}\ ,\ \bibinfo {pages} {1}} (\bibinfo
  {year} {2011})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Ledrappier}(1985)}]{ledrappier1985metric}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Ledrappier}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {The Annals of Mathematics}\ }\textbf {\bibinfo {volume} {122}},\ \bibinfo
  {pages} {540} (\bibinfo {year} {1985})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Frederickson}\ \emph {et~al.}(1983)\citenamefont
  {Frederickson}, \citenamefont {Kaplan}, \citenamefont {Yorke},\ and\
  \citenamefont {Yorke}}]{frederickson1983liapunov}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Frederickson}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Kaplan}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Yorke}}, \
  and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Yorke}},\
  }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Journal of
  Differential Equations}\ }\textbf {\bibinfo {volume} {49}},\ \bibinfo {pages}
  {185} (\bibinfo {year} {1983})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Chatterjee}()}]{chatterjee:08}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Chatterjee}},\ }\href@noop {} {\bibinfo  {journal} {arXiv:0810.4221}\
  }\BibitemShut {NoStop}\bibitem [{\citenamefont {Kac}(1943)}]{kac:43}  \BibitemOpen
\bibfield  {journal} {  }\bibfield  {author} {\bibinfo {author} {\bibfnamefont
  {M.}~\bibnamefont {Kac}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo
  {journal} {Bull. Amer. Math. Soc}\ }\textbf {\bibinfo {volume} {49}},\
  \bibinfo {pages} {314} (\bibinfo {year} {1943})}\BibitemShut {NoStop}\bibitem [{\citenamefont {Aza{\"\i}s}\ and\ \citenamefont
  {Wschebor}(2009)}]{azais:09}  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Aza{\"\i}s}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Wschebor}},\ }\href@noop {} {\emph {\bibinfo {title} {Level sets and extrema
  of random processes and fields}}}\ (\bibinfo  {publisher} {Wiley},\ \bibinfo
  {year} {2009})\BibitemShut {NoStop}\end{thebibliography}

\end{document}

