\documentclass[a4paper,10pt]{amsart}
\usepackage[latin1]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{enumitem}

\usepackage[T1]{fontenc}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows,decorations.pathmorphing}

\usepackage[left=2.3cm, right=2.3cm]{geometry}

\theoremstyle{plain}
\newtheorem{conj}{Conjecture}
\newtheorem{theo}{Theorem}[section]
\newtheorem{prop}[theo]{Proposition}
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}

\theoremstyle{definition}
\newtheorem{defi}{Definition}
\newtheorem{hyp}{Hypothesis}
\newtheorem{Rk}{Remark}
\newtheorem{ex}{Example}

\begin{document}

\title{Chebyshev's bias for analytic $L$-functions}
\author{Lucile Devin}
\address{Laboratoire de Math{\'e}matiques d'Orsay \\ Univ. Paris-Sud \\ CNRS \\ Universit{\'e} Paris-Saclay \\ 91405 Orsay  \\ France}
\email{lucile.devin@math.u-psud.fr}
\keywords{}
\subjclass[2010]{Primary 11N45, 11F30, 11S40; Secondary 11G40}
\date\today

\begin{abstract}
We give a general framework for the study of prime number races and Chebyshev's bias attached to general $L$-functions satisfying natural analytic hypotheses.
This extends the cases previously considered by several authors and involving Dirichlet $L$-functions, Hasse--Weil $L$-functions of elliptic curves over $\mathbf{Q}$,... 
We put the emphasis on weakening the required hypotheses such as GRH or linear independence properties of zeros of $L$-functions.
In particular we prove unconditionally the existence of the limiting distribution of the relevant function.
We also establish the existence of the logarithmic density of the set 
$\lbrace x\geq 2 : \sum_{p\leq x} \lambda_{f}(p) \geq 0 \rbrace$ 
 for coefficients $(\lambda_{f}(p))$ of general $L$-functions conditionally on a much weaker hypothesis than was previously known. We also include applications to new Chebyshev's bias phenomena that were beyond the reach of the previously known cases.
\end{abstract}

\maketitle

\section{Introduction}

\subsection{Context}

In 1853 Chebyshev noticed in a letter to Fuss that 
there is a bias in the distribution of primes modulo $4$.
In initial intervals of the integers, 
there seems to be more primes congruent to $3\ [\bmod\ 4]$ than congruent to $1\ [\bmod\ 4]$.
Over the years the synonymous expression ``prime number races" has emerged to describe problems of Cheyshev's type.
Since then, it has been quite investigated and generalized in other number theoretical contexts.

In \cite{RS} Rubinstein and Sarnak gave a framework for the quantification of Chebyshev's bias in prime number races in arithmetic progressions.
Following an observation of Wintner \cite{Wintner} used for the race between $\pi(x)$ and $\operatorname{Li}(x)$, 
they studied the logarithmic density of the set
$ \lbrace x \geq 2 : \pi(x; q,a) >\pi(x;q, b) \rbrace $
where $\pi(x; q,a)$ is the number of primes $\leq x$ that are congruent to $a\ [\bmod\ q]$.
The authors use explicit formul\ae\ for the Dirichlet $L$-functions of the primitive characters modulo $q$
and prove conditionally the existence of a 
limiting logarithmic distribution for the vector valued function encoding the prime number race. 
This logarithmic distribution plays a crucial role in the analysis of the bias and  has since then been generalized greatly to answer this type of questions.
For many instances of applications, one can read the introduction of \cite{ANS}.

Under the Generalised Riemann Hypothesis (GRH) and Linear Independence (LI for short) of the zeros, 
Rubinstein and Sarnak answered Chebyshev's original question.
Precisely they showed that the logarithmic density $\delta(4;3,1)$ of the set of $x\geq2$ for which $\pi(x; 4,3)>\pi(x;4, 1)$
exists and is about $0.9959$.
In their paper about the Shanks--Renyi prime number race \cite{FiorilliMartin}, 
Fiorilli and Martin gave a more precise estimation for $\delta(q;a,b)$ in general under the same hypotheses.
For more details on prime number races, we refer to the expository article of Granville and Martin \cite{GranvilleMartin}, see also \cite{FordKonyagin}.

In the recent years some variants of Chebyshev's question coming from a broad variety of arithmetic contexts have been studied. 
Let us quickly review some of them. 

A first generalization of Chebyshev's bias consists in studying the sign of 
\begin{align*}
x \mapsto \frac{1}{\lvert A\rvert}\pi(x; q,A)- \frac{1}{\lvert B\rvert}\pi(x;q, B)
\end{align*} 
where $A$ and $B$ are subsets of $(\mathbf{Z}/q\mathbf{Z})^{*}$
and where $\pi(x; q,A)$ is the number of primes $\leq x$ that reduce modulo $q$ to an element of $A$.
In \cite{RS} Rubinstein and Sarnak performed this study in the case $A=\rm{NR}$ the set of non-residues modulo $q$ and
$B=\rm{R}$ the set of quadratic residues modulo $q$,
showing that there is always a bias towards non-squares.
Following this idea, Fiorilli \cite{Fiorilli_HighlyBiased} showed one could tweak the prime number race so that it exhibits an  arbitrarily large bias.

In his thesis \cite{NgThesis}, Ng has generalized this question to that
of biases in the distribution of Frobenius substitutions in conjugacy classes of Galois groups of number fields. 
Here the underlying equidistribution property is Chebotarev's density Theorem.
Ng's results are conditional on GRH, LI and Artin's Conjecture for Artin $L$-functions.

In his expository paper about error terms in arithmetic, 
Mazur \cite{MazurErrorTerm} raised the question of prime number races for elliptic curves, 
or more generally for the Fourier coefficients of a modular form.
For example he plotted graphs of functions:
\begin{align*}
x\mapsto \lvert\lbrace p\leq x : a_{p}(E) >0 \rbrace\rvert -  \lvert\lbrace p\leq x : a_{p}(E) <0 \rbrace\rvert
\end{align*}
where $a_{p}(E) = p+1 - \lvert E(\mathbf{F}_{p})\rvert$, 
for some elliptic curve $E$ defined over $\mathbf{Q}$.
He observed that the race between the primes such that $a_{p}(E) >0 $ 
and the primes such that $a_{p}(E) <0 $
tends to be biased towards negative values when the algebraic rank of the elliptic curve is large.
In a letter to Mazur \cite{SarnakLetter}, 
Sarnak gave an effective framework for Mazur's question.
Under GRH and LI, 
he explained the prime number race using the zeros of all the symmetric powers $L(\operatorname{Sym}^{n}E,s)$ of the Hasse--Weil $L$-function of $E/\mathbf{Q}$.
Sarnak also introduced a related (simpler) race: study the sign of the function 
$\sum_{p\leq x}\frac{a_{p}(E)}{\sqrt{p}}$.
For this prime number race, which only involves $L(E,s)$, Sarnak noticed the link between the analytic rank of $E$ and the bias.
In \cite{FioEC}, Fiorilli developed Sarnak's idea
and gave sufficient conditions to get highly biased prime number races in the context of elliptic curves conditionally on versions of GRH and LI.

In this paper, we generalize the questions above to prime number races for the coefficients of analytic $L$-functions. 
We prove unconditionally in Section~\ref{Sec_ProofExist} the existence of the limiting logarithmic distribution associated to the prime number race for a wide variety of usual $L$-functions (Theorem \ref{Th_DistLim}) including Dirichlet $L$-functions and Hasse--Weil $L$-functions.
In particular we obtain unconditional proofs of some of the results of \cite{RS}.
Section~\ref{Sec_Applications} is also devoted to applications of our general framework to new instances of Chebyshev's bias phenomena.
For example we prove unconditionally that, after suitable scaling the function
$$x\mapsto\sum_{p=a^{2} + 4b^{2}\leq x}\frac{a^{2} - 4b^{2}}{a^{2} + 4b^{2}}$$
admits a limiting logarithmic distribution of negative mean (see Theorem~\ref{Th_sum2squares} for the precise statement). 
The fact that the mean (which we compute explicitly) is negative gives evidence that when writing $p=a^{2}+4b^{2}$ there is a bias: the even square tends to be more often larger than the odd square.
In Section~\ref{Sec_AddHyp} we prove results conditionally on weak forms of GRH and LI.
We study minimal conditions to ensure that the distribution has nice properties such as regularity, symmetry, and concentration.

\subsection{Setting}\label{subsec_setting}

In this paper we use a custom-made definition of ``analytic" $L$-function inspired by \cite[Chap. 5]{IK} and Selberg's class.
We will only use analytic properties of the function to study the associated prime number race.

\begin{defi}[Analytic $L$-function]\label{Def_Lfunc}
Let $L(f,s)$ be a complex-valued function of the variable $s\in\mathbf{C}$ attached to an auxiliary parameter $f$ to which one can attach an integer $q(f)$ (usually $f$ is of arithmetic origin and $q(f)$ is its conductor). 
We say that $L(f,s)$ is an analytic $L$-function if we have the following data and conditions:
\begin{enumerate}
\item\label{Hyp_Euler+RamanujanPetersson} A Dirichlet series factorizing as an Euler product of degree $d\geq 1$ that coincides with $L(f,s)$ for $\operatorname{Re}(s)>1$:
\begin{align*}
L(f,s) = \sum_{n\geq 1} \lambda_{f}(n)n^{-s} = \prod_{p}\prod_{j=1}^{d}\left(1-\alpha_{f,j}(p)p^{-s}\right)^{-1}
\end{align*}
with $\lambda_{f}(1)=1$ and $\alpha_{f,j}(p) \in \mathbf{C}$,
satisfying $\lvert \alpha_{f,j}(p)\rvert=1$ for all $j$ and $p\nmid q(f)$.
In particular the series and Euler product are absolutely convergent for $\operatorname{Re}(s)>1$.
\item\label{Hyp_FunctEquation} A gamma factor with local parameters $\kappa_{j}\in\mathbf{C}$, $\operatorname{Re}(\kappa_{j})>-1$:
\begin{align*}
\gamma(f,s) = \pi^{-ds/2}\prod_{j=1}^{d}\Gamma\left(\frac{s + \kappa_{j}}{2}\right).
\end{align*}
The \emph{analytic conductor} of $f$ is then defined by: 
\begin{align*}
\mathfrak{q}(f) = q(f)\prod_{j=1}^{d}\left( \lvert \kappa_{j}\rvert + 3\right),
\end{align*}
and we can define the completed $L$-function
\begin{align*}
\Lambda(f,s) =  q(f)^{s/2}\gamma(f,s)L(f,s).
\end{align*}
It admits an analytic continuation to an meromorphic function of order $1$, with at most poles at $s=0$ and $s=1$.
Moreover it satisfies a functional equation $\Lambda(f,s)=\epsilon(f)\Lambda(\overline{f},1-s)$, 
with $\epsilon(f)=\pm 1$.
Here $\Lambda(\overline{f},1-s)$ is the completed $L$-function associated to 
$L(\overline{f},s) := \sum_{n\geq 1} \overline{\lambda_{f}(n)}n^{-s}$.
\item\label{Hyp_L2isLfunct} The function
$$L(f^{(2)},s) = \prod_{p}\prod_{j=1}^{d}\left(1-\alpha_{j}(p)^{2}p^{-s}\right)^{-1}$$
is defined for $\operatorname{Re}(s)>1$.
We assume that there exists an open subset 
$U \supset \lbrace\operatorname{Re}(s)\geq 1\rbrace$ such that
$L(f^{(2)},\cdot)$ can be analytically continued to a meromorphic function for $s\in U$,
and on $U-\lbrace 1\rbrace$ there is neither a zero nor a pole of $L(f^{(2)},\cdot)$.
\end{enumerate}

\end{defi}

\begin{Rk}[Order of a meromorphic function]
An entire function $\Lambda$ is said to be of order $1$, if for every $\beta>1$ (and no $\beta<1$) one has
$$\lvert \Lambda(s) \rvert\ll \exp(\lvert s\rvert^{\beta}).$$
A meromorphic function on $\mathbf{C}$ is said to be of order $1$ if it can be written as the quotient of two entire functions of order $1$.
For $L$-functions defined by a Dirichlet series converging on a half-plane and the completed $L$-function denoted $\Lambda$,
using Stirling formula and the absolute convergence of the Dirichlet series on the half-plane, one gets for all $s$, $\operatorname{Re}(s) > 1+\epsilon$, for every $\beta>1$ (and no $\beta<1$):
$$\lvert \Lambda(s) \rvert\ll \exp(\lvert s\rvert^{\beta}).$$
By the functional equation, the same inequaltity holds for $\operatorname{Re}(s)< -\epsilon$.
To see that such $\Lambda$ functions are meromorphic of order $1$, it suffices to prove that $\Lambda$ is bounded in vertical strip: 
$a\leq \operatorname{Re}(s)\leq b$, $\lvert\operatorname{Im}(s)\rvert \geq 1$. 
Usually this property is obtained in the proof of the functional equation using the method of zeta-integrals 
(see e.g. \cite[Cor. 13.8, Prop. 13.9]{GodementJacquet} for the general case of automorphic $L$-functions).

In particular by Jensen's formula (see \cite[15.20]{Rudin}) one can prove that the sum over the zeros of $\Lambda$:
$$\sum_{\Lambda(\rho) =0}\frac{1}{\lvert \rho\rvert^{1+\epsilon}}$$
converges for every $\epsilon>0$.
From the Hadamard factorisation of $\Lambda$ one can also deduce bounds for its logarithmic derivative, 
see section \ref{Sec_boundlogder} for the bound in our case.
\end{Rk}

\begin{ex}\label{Ex_Lfunc}
	\begin{enumerate}
	    \item\label{Ex_Lfunc_zeta} Riemman's zeta function is a classical example of $L$-function, it satisfies all the conditions of Defintion~\ref{Def_Lfunc}.
		\item\label{Ex_Lfunc_Dirichlet} Dirichlet $L$-functions are analytic $L$-functions. Hence some of the results of \cite{RS} can be given unconditionally, see Theorem \ref{Th_RS_raceModq}.
		
		\item\label{Ex_Lfunc_Modular2} Modular $L$-functions of degree $2$ are analytic $L$-functions.
		It is a consequence of \cite[Th. 8.2]{Deligne_Weil1} and \cite{DeligneSerre}.
		In particular, following results on modularity (\cite{Wiles_ModEll}, \cite{TaylorWiles_Modularity}, \cite{BCDT_Modularity}), 
		if $E/\mathbf{Q}$ is an elliptic curve, $L(E,s)$ is an analytic $L$-function.
		This property was already used by Fiorilli in \cite{FioEC}, see Proposition \ref{Prop_Fi_EC}.
		
		
		\item\label{Ex_Lfunc_cusp} Under the Ramanujan--Petersson Conjecture, general automorphic $L$-functions associated to cusp forms on $GL(m)$ for $m\geq 1$, are analytic $L$-functions in the sense of Definition \ref{Def_Lfunc}.
		Indeed (\ref{Hyp_Euler+RamanujanPetersson}) precisely says that the Ramanujan--Petersson Conjecture is satisfied
		and (\ref{Hyp_FunctEquation}) is known for such $L$-functions (\cite{GodementJacquet}, \cite{Cogdell}).
		One has
		\begin{align*}
		L(f^{(2)},s) = L(\operatorname{Sym}^{2}f,s)L(\Lambda^{2}f,s)^{-1}.
		\end{align*}
		The two factors are automorphic $L$-functions,
		hence (\ref{Hyp_L2isLfunct}) is satisfied. 
		(As F. Brumley pointed out to us one could ask for the third hypothesis above to be about $L(f\otimes\overline{f},s)$ or rather about $L(\operatorname{Sym}^{2}f,s)$ and $L(\Lambda^{2}f,s)$.)
		
		In \cite[Cor. 1.5]{ANS}, the existence of the limiting logarithmic distribution 
		for the function $x^{-1/2}\psi(f,x)$ associated to an automorphic $L$-function $L(f,s)$ is proved under GRH 
		and does not depend on the Ramanujan--Petersson Conjecture.
		In this paper we need to assume the Ramanujan--Petersson Conjecture but not GRH to prove that 
		the function $x^{-\beta_{f,0}}\pi(f,x)$ 
		has a limiting logarithmic distribution (see Theorem~\ref{Th_Aut_bias}). 
		
		\item\label{Ex_Lfunc_RankinSelberg} If $L(f,s)$ and $L(g,s)$ are two modular $L$-functions of degree $2$, such that $g\neq \overline{f}$, 
		then the  Rankin-Selberg product $L(f\otimes g,s)$ is an analytic $L$-function.
		The conditions (\ref{Hyp_Euler+RamanujanPetersson}) and (\ref{Hyp_FunctEquation}) are satisfied (see e.g. \cite[Th.2.3]{CogdellPS}).
		One has 
		\begin{align*}
        L((f\otimes g)^{(2)},s) = \frac{L(\operatorname{Sym}^{2}f\otimes \operatorname{Sym}^{2}g,s) 
        L(\chi_{f}\chi_{g},s)}{L((\operatorname{Sym}^{2}f)\otimes      \chi_{g},s)L((\operatorname{Sym}^{2}g)\otimes\chi_{f},s)}
        \end{align*}
        where $\chi_{f}$, $\chi_{g}$ are the nebentypus respectively associated to the modular forms $f$ and $g$.
		We deduce that Condition (\ref{Hyp_L2isLfunct}) of Definition~\ref{Def_Lfunc} is satisfied.
		We use this property in Theorem \ref{Prop_CorrEllcurves} in the case $f$ and $g$ are associated to two elliptic curves over $\mathbf{Q}$ that are non-isogeneous in a strong sense.
	\end{enumerate}
	
\end{ex}

The parameters in Definition~\ref{Def_Lfunc} satisfy
for every prime $p$, $\lambda_{f}(p)=\sum_{j=1}^{d}\alpha_{f,j}(p)$.
In case $L(f,\cdot)=L(\overline{f},\cdot)$ is real, one has $\lambda_{f}(p)\in\mathbf{R}$.
The Generalized Sato--Tate conjecture states that in this case the $(\lambda_{f}(p))_{p}$ should equidistribute 
according to a certain probability measure on $[-d,d]$.
In case the $L$-function is entire and does not vanish on the line $\operatorname{Re}(s) =1$, 
a general Prime Number Theorem for the $L$-function implies 
that the Sato--Tate law has mean equal to $0$.
We expect this to hold more generally.

\begin{conj}\label{Sato-Tate} 
Let $\mathcal{S}$ be a finite set of entire analytic $L$-functions stable by conjugation (such that $\overline{\mathcal{S}}=\mathcal{S}$), 
and $(a_{f})_{f\in \mathcal{S}}$ a set of complex numbers satisfying $a_{\overline{f}}= \overline{a_{f}}$.
Then the sequence $(\sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p))_{p}$ is equidistributed in an interval of $\mathbf{R}$ 
according to a Sato--Tate law with mean $0$.
\end{conj}

In particular for $L$-functions associated with an elliptic curve over $\mathbf{Q}$, 
Conjecture~\ref{Sato-Tate} is a (very) weak version of the Sato--Tate conjecture, and is known to hold (\cite{CHT_SatoTate}, \cite{HSBT_SatoTate}).

\subsection{Chebyshev type questions for analytic $L$-functions}

The Chebyshev type question this paper primarily focuses on is the following.
Let $\mathcal{S}$ be a finite set of entire analytic $L$-functions such that $\overline{\mathcal{S}}=\mathcal{S}$, 
and $(a_{f})_{f\in \mathcal{S}}$ a set of complex numbers satisfying $a_{\overline{f}}= \overline{a_{f}}$.
Under Conjecture \ref{Sato-Tate} one has 
$$\frac{1}{\pi(x)}\sum_{p\leq x}\sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p) \rightarrow 0$$ as $x\rightarrow +\infty$.
It is natural to study the sign of the summatory function
\begin{align}\label{Expr_Sum_function1}
x\mapsto \sum_{p\leq x}\sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p)
\end{align}
for $x>0$.

\begin{Rk}
In the case of elliptic curve $L$-functions (where the Sato--Tate law is known to be symmetric),
Mazur (\cite{MazurErrorTerm}) was first interested in studying the function
\begin{align*}
x \mapsto \lvert\lbrace p\leq x : \sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p) >0\rbrace\rvert - \lvert\lbrace p\leq x : \sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p) <0\rbrace\rvert
\end{align*}
for $x>0$.
But as Sarnak showed in \cite{SarnakLetter}, 
for this study, we need information about all symmetric powers of the $L$-functions involved.
It may yield non-converging infinite sums.
Hence following Sarnak's idea, we focus on the summatory function as in (\ref{Expr_Sum_function1}).
\end{Rk}

More generally if $L(f,\cdot)$ has a pole of order $r_{f}$ at $s=1$,
we study the sign of the summatory function
$$S:x\mapsto \sum_{p\leq x}\sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p)  - \sum_{f\in\mathcal{S}}a_{f}r_{f} \operatorname{Li}(x).$$
In \cite{SarnakLetter}, Sarnak gave a method to deal with this question in the case $\mathcal{S}$ is a singleton.
Precisely he considers the cases of the $L$-function associated to Ramanujan's $\tau$ function and of $L$-functions associated with an elliptic curve over $\mathbf{Q}$. 
The case of elliptic curve $L$-functions over $\mathbf{Q}$ was completed by Fiorilli in \cite{FioEC}
conditionally on RH for the Hasse--Weil $L$-function $L(E,s)$ and a bounded multiplicity hypothesis on the non-real zeros of $L(E,s)$.

Building on work of these authors, we wish to understand the set of $x$ for which $S(x)\geq0$.
As was first pointed by Wintner \cite{Wintner} we use the logarihmic density to measure this sets.

\begin{defi}\label{Def_logdens}
\begin{enumerate}
\item Define $$\overline{\delta}(\mathcal{S}) = \limsup\frac{1}{Y}\int_{2}^{Y}\mathbf{1}_{\geq 0}(S(e^{y})){\mathop{}\!\mathrm{d}} y \ \text{  and }
\ \underline{\delta}(\mathcal{S}) = \liminf\frac{1}{Y}\int_{2}^{Y}\mathbf{1}_{\geq0}(S(e^{y})){\mathop{}\!\mathrm{d}} y.$$
If these two densities are equal, we denote $\delta(\mathcal{S})$ their common value. These quantities measure the bias of $S(x)$ towards positive values. 
\item If $\delta(\mathcal{S})$ exists and is $> \frac{1}{2}$ 
we say that there is a bias towards positive values. 
If it is $<\frac{1}{2}$ we say that there is a bias towards negative values. 
\end{enumerate}
\end{defi}

Under GRH and LI, 
Sarnak showed that for an $L$-function of degree $2$, 
the bias exists and always differs from $\frac{1}{2}$.
One of the main results of this article is that the bias exists without assuming GRH and under a hypothesis weaker than LI on the independence of the zeros of the $L$-functions involved (see Theorem \ref{Th_withLI}(\ref{Th1_Tselfsuff})).
To state the existence of the bias we first need to prove that 
a suitable normalization of the function $S(x)$ admits a limiting logarithmic distribution.

\begin{defi}
Let $F:\mathbf{R}\rightarrow\mathbf{R}$ be a real function, 
we say that $F$ admits a limiting logarithmic distribution $\mu$ if
for any bounded Lipschitz continuous function $g$, we have
\begin{align*}
\lim_{Y\rightarrow\infty}\frac{1}{Y}\int_{2}^{Y}g(F(e^{y})){\mathop{}\!\mathrm{d}} y = 
\int_{\mathbf{R}}g(t){\mathop{}\!\mathrm{d}}\mu(t).
\end{align*}
\end{defi}

Before stating our main result we need to set some notation.
Since we do not assume the Riemann Hypothesis for our $L$-functions,
we denote
$$\beta_{\mathcal{S},0} =\sup\lbrace \operatorname{Re}(\rho) : \exists f\in\mathcal{S},  L(f,\rho)=0\rbrace.$$ 
One has $\beta_{\mathcal{S},0}\geq \frac{1}{2}$ and equality is equivalent to the Riemann Hypothesis
for all the $L$-functions $L(f,\cdot)$, $f\in\mathcal{S}$.
Define
\begin{align*}
&\mathcal{Z}_{\mathcal{S}} = \lbrace \gamma>0 : \exists f \in \mathcal{S}, L(f,\beta_{\mathcal{S},0} + i\gamma)=0\rbrace,
&\mathcal{Z}_{\mathcal{S}}(T) = \mathcal{Z}_{\mathcal{S}}\cap(0,T],
\end{align*}
seen as multi-sets of zeros of largest real part (i.e. we count the zeros with multiplicities).
We denote $\mathcal{Z}_{\mathcal{S}}^{*}$ and $\mathcal{Z}_{\mathcal{S}}^{*}(T)$ the corresponding sets (i.e. repetitions are not allowed).

If it does not lead to confusion we may omit the subscript $\mathcal{S}$. 
In the case the set $\mathcal{S}$ is a singleton $\lbrace f \rbrace$ and $a_{f}=1$, we will write $f$ in subscript instead of $\lbrace f\rbrace$.

For $L$ a meromorphic function in a neighbourhood of a point $\rho\in\mathbf{C}$,
let $m(L,\rho)$ be the multiplicity of the zero of $L$ at $s=\rho$.
(One has $m(L,\rho)=0$ if $L(\rho)\neq 0$, $m(L,\rho)>0$ if $L(\rho)= 0$ and $m(L,\rho)<0$ if $L$ has a pole at $s=\rho$.)

\section{Statement of the theoretical results}
\subsection{Limiting distribution}

Our first result is the existence of the limiting logarithmic distribution for the prime number races associated to analytic $L$-functions.
\begin{theo}\label{Th_DistLim}
Let $\lbrace L(f,\cdot) : f\in\mathcal{S} \rbrace$ be a finite set of analytic $L$-functions such that $\overline{\mathcal{S}}=\mathcal{S}$, 
and $(a_{f})_{f\in \mathcal{S}}$ a set of complex numbers satisfying $a_{\overline{f}}= \overline{a_{f}}$.
Define
$$E_{\mathcal{S}}(x)=\frac{\log x}{x^{\beta_{\mathcal{S},0}}}\left(
\sum_{p\leq x}\sum_{f\in\mathcal{S}}a_{f}\lambda_{f}(p) + \sum_{f\in\mathcal{S}}a_{f}m(L(f,\cdot),1) \operatorname{Li}(x) \right).$$

The function $E_{\mathcal{S}}(x)$ admits a limiting logarithmic distribution $\mu_{\mathcal{S}}$.
There exists a positive constant $C$ (depending on $\mathcal{S}$) such that one has 
$$\mu_{\mathcal{S}}(\mathbf{R}-[-A,A]) \ll \exp(-C\sqrt{A}).$$
Moreover let $X_{\mathcal{S}}$ be a random variable of law $\mu_{\mathcal{S}}$,
then the expected value of $X_{\mathcal{S}}$ is
	$$\mathbb{E}(X_{\mathcal{S}}) = m_{\mathcal{S}} := \sum_{f\in\mathcal{S}}a_{f}
	\left(m(L(f^{(2)},\cdot),1)\delta_{\beta_{\mathcal{S},0}=1/2} 
	-\beta_{\mathcal{S},0}^{-1}m(L(f,\cdot),\beta_{\mathcal{S},0})\right),$$ 
and its variance is
	$$\operatorname{Var}(X_{\mathcal{S}}) = 2\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{\beta_{\mathcal{S},0}^{2}+\gamma^{2}}$$
	where for $\gamma$ in $\mathcal{Z}_{\mathcal{S}}^{*}$, 
	$M(\gamma) = \sum_{f\in\mathcal{S}}a_{f}m(L(f,\cdot),\beta_{\mathcal{S},0}+i\gamma)$.
\end{theo}

This result generalizes \cite[Th 1.1]{RS} and \cite[Th. 5.1.2]{NgThesis} which are conditional on GRH 
and respectively deal with the cases of sets of Dirichlet $L$-functions and sets of Artin $L$-functions under Artin's Conjecture. 
Similar results are obtained under GRH in \cite{ANS} for more general $L$-functions. 
An uncoditional proof is given in \cite{FioEC} in the case of a singleton composed of one Hasse--Weil $L$-function.
The proof of Theorem~\ref{Th_DistLim}, in section \ref{Sec_ProofExist}, is essentially an adaptation of Fiorilli's proof to more general $L$-functions.
It is important to note that there are no asumption about the set $\mathcal{Z}_{\mathcal{S}}$, 
in particular the limiting logarithmic distribution exists even if the set $\mathcal{Z}_{\mathcal{S}}$ is empty 
(see Remark~\ref{Rk_onProp_LimOfDist}(\ref{It_NoZero})).

\begin{Rk}\label{Rk_mean_bias}
Morally, the sign of the mean $\mathbb{E}(X_{\mathcal{S}})$ gives an idea of the kind of bias one should expect.
When the mean is non-zero, we conjecture that the bias is imposed by the sign of the mean.
Conditionally on additional hypotheses we can prove this conjecture 
(see Corollary~\ref{Cor_mean_bias}).
\end{Rk}

\subsection{Further properties under extra hypotheses}

Under additional hypotheses over the zeros of the $L$-functions, we can deduce properties of $\mu_{\mathcal{S}}$, and in turn results on the bias. 
This idea is developed in the following results.
A standard hypothesis about the set $\mathcal{Z}_{\mathcal{S}}$ is the Linear Independence hypothesis, 
one can generalize Rubinstein and Sarnak's result \cite[Rk. 1.3]{RS} with weaker hypotheses.
We use the concept of self-sufficient zeros introduced by Martin and Ng in \cite{MartinNg}.

\begin{defi}\label{Def_selfsuff}\begin{enumerate}
\item An ordinate $\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}$ is self-sufficient if it is not in the $\mathbf{Q}$-span of $\mathcal{Z}_{\mathcal{S}}^{*}-\lbrace \gamma\rbrace$.
\item For $U,V>0$, we say that an ordinate $\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(U)$ is $(U,V)$-self-sufficient if it is not in the $\mathbf{Q}$-span of $\mathcal{Z}_{\mathcal{S}}^{*}(V)-\lbrace \gamma\rbrace$.
\end{enumerate}
\end{defi}

We prove conditional results for the regularity of the distribution $\mu_{\mathcal{S}}$.

\begin{theo}\label{Th_withLI}
Suppose that the set $\mathcal{S}$ satisfies the conditions of Theorem~\ref{Th_DistLim}.
\begin{enumerate}
\item\label{Th1_Tselfsuff} Suppose that there exists $\epsilon>0$ such that for every $T$ large enough there exists $\gamma_{T}\in \mathcal{Z}_{\mathcal{S}}^{*}(T^{\frac{1}{2} - \epsilon})$ a $(T^{\frac{1}{2} - \epsilon},T)$-self-sufficient ordinate.
Then $\delta(\mathcal{S})$ exists.
\item\label{Th1_LI1} Suppose $\mathcal{Z}^{*}_{\mathcal{S}}$ contains at least one self-sufficient element, 
then the distribution $\mu_{\mathcal{S}}$ is continuous (\textit{i.e.} $\mu_{\mathcal{S}}$ assigns zero mass to finite sets).
\item\label{Th2_LI3} Suppose $\mathcal{Z}^{*}_{\mathcal{S}}$ contains three or more self-sufficient elements, 
then the distribution $\mu_{\mathcal{S}}$ admits a density $\phi\in L^{1}$ (\textit{i.e.} ${\mathop{}\!\mathrm{d}}\mu_{\mathcal{S}}(x) = \phi(x){\mathop{}\!\mathrm{d}} x$).
\item\label{Th3_enoughIndep} Suppose that the set 
$\lbrace\gamma\in\mathcal{Z}_{\mathcal{S}}^{*} : \gamma \text{ self-sufficient} \rbrace$ is infinite, 
then the distribution $\mu_{\mathcal{S}}$ admits a density $\phi$ which is in the Schwartz space of indefinitely differentiable and rapidly decreasing functions.
\end{enumerate}
\end{theo}

This theorem is proved in section \ref{sub_Indep}.
It improves some results of \cite{RS} which are obtained under the Grand Simplicity Hypothesis (also called LI).
In loc. cit. the authors obtain Theorem \ref{Th_withLI} under LI, 
i.e. assuming that $\mathcal{Z}_{\mathcal{S}}$ is linearly independent over $\mathbf{Q}$.
In \cite{NgThesis}, \cite{ANS} and \cite{FioEC} similar results are obtained under LI.
In Theorem~\ref{Th_withLI}, hypotheses in (\ref{Th1_Tselfsuff})--(\ref{Th3_enoughIndep}) are ordered by increasing strength 
but are all weaker than LI.
In particular Theorem~\ref{Th_withLI}(\ref{Th1_Tselfsuff}) gives a condition for the logarithmic density of the set 
$\lbrace x\geq 2: E_{\mathcal{S}}(x)\geq 0\rbrace$ to exist, 
where the function $E_{\mathcal{S}}$ is as defined in Theorem~\ref{Th_DistLim}.

We are also insterested in the symmetry of the distribution $\mu_{\mathcal{S}}$.
We prove the following result conditionally on a weak conjecture of linear independence of the zeros. 

\begin{theo}\label{Th_Indep_Sym}
Suppose that the set $\mathcal{S}$ satisfies the conditions of Theorem~\ref{Th_DistLim}.
Suppose that for every $(k_{\gamma})_{\gamma} \in \mathbf{Z}^{(\mathcal{Z}_{\mathcal{S}})}$ one has
$$ \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}k_{\gamma}\gamma =0 \Rightarrow \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}k_{\gamma}\equiv 0\ [\bmod\ 2].$$
Then the distribution $\mu_{\mathcal{S}}$ is symmetric with respect to $m_{\mathcal{S}}$.
\end{theo}

We prove Theorem~\ref{Th_Indep_Sym} in section~\ref{sub_Sym}.
This theorem improves again a result obtained in \cite{RS} under LI.

We have avoided so far the use of the Riemann Hypothesis, weakening the hypotheses made in previous works.
We call a Zero Density Theorem for $\mathcal{S}$ a result saying that if the Riemann Hypothesis is not satisfied, then there should not be too many zeros off the critical line.
Precisely we state the following conjecture.
\begin{conj}\label{Conj_ZeroDensThm}
	Suppose $\beta_{\mathcal{S},0}>\frac{1}{2}$,
	then for every $\epsilon>0$ one has
	$$\lvert\mathcal{Z}_{\mathcal{S}}(T) \rvert \ll T^{1-\epsilon}.$$
	In particular 
	the sum $\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}\frac{1}{\lvert \beta_{0} + i\gamma\rvert}$ converges.
\end{conj}  
There are results in this direction, see for example \cite[Chap. 10]{IK} for the Riemann zeta function and Dirichlet $L$-functions. 
In the case of the Selberg class (in particular in the case of analytic $L$-functions) Kaczorowski and Perelli have shown (\cite[Lem. 3]{KP}) that for an $L$-function  $L(f,s)$ of degree $d$ with $\beta_{f,0} \geq 1 - \frac{1}{4(d+3)}$, Conjecture~\ref{Conj_ZeroDensThm} holds.

We generalize Rubinstein and Sarnak's result \cite[Th. 1.2]{RS} by stating a dichotomy depending on the validity of the Riemann Hypothesis.

\begin{theo}\label{Th_withRH}
Suppose that the set $\mathcal{S}$ satisfies the conditions of Theorem~\ref{Th_DistLim}.
\begin{enumerate}
\item\label{Th1_RH} Suppose the Riemann Hypothesis is satisfied for every $L(f,s)$, $f\in\mathcal{S}$ 
(\textit{i.e.} $\beta_{\mathcal{S},0}=\frac{1}{2}$).
Suppose also that for every $f\in \mathcal{S}$, one has $\operatorname{Re}(a_{f})\geq 0$, and that there exists $f\in\mathcal{S}$ such that $\operatorname{Re}(a_{f})> 0$.
Then there exists a constant $c$ depending on $\mathcal{S}$ such that
$$\mu_{\mathcal{S}}(\mathbf{R}-[-A,A]) \gg \exp(-\exp(cA)).$$
In particular
$0< \underline{\delta}(\mathcal{S}) \leq \overline{\delta}(\mathcal{S}) <1$.
\item\label{Th2_nRH} Suppose $\beta_{\mathcal{S},0}>\frac{1}{2}$, 
and Conjecture \ref{Conj_ZeroDensThm} holds.
Then $\mu_{\mathcal{S}}$ has compact support.
\end{enumerate}
\end{theo}
This Theorem is proved in section \ref{sub_Support}.
The proof of Theorem~\ref{Th_withRH}(\ref{Th1_RH}) is an adaptation of Rubinstein and Sarnak's proof to a more general case.
One can also show that the race is inclusive --- i.e. that each contestant leads the race infinitely many times 
(this is implied by $0< \underline{\delta}(\mathcal{S}) \leq \overline{\delta}(\mathcal{S}) <1$) ---
 assuming GRH and LI and nothing on the $a_{f}$'s (see \cite{RS}).
In \cite{MartinNg}, Martin and Ng prove that the race is inclusive assuming GRH and a weaker hypothesis than LI based on self-sufficient zeros (see Definition~\ref{Def_selfsuff}).

We can now come back to Remark~\ref{Rk_mean_bias}. 
If the bias exists, it should be imposed by the sign of the mean of the limiting distribution.
We get the following result as a corollary of Theorem~\ref{Th_withLI}, \ref{Th_Indep_Sym} and \ref{Th_withRH}.

\begin{cor}\label{Cor_mean_bias}
Let $\lbrace L(f,\cdot) : f\in\mathcal{S} \rbrace$ be a finite set of analytic $L$-functions such that $\overline{\mathcal{S}}=\mathcal{S}$, 
and $(a_{f})_{f\in \mathcal{S}}$ a set of complex numbers satisfying $a_{\overline{f}}= \overline{a_{f}}$.
Suppose that:
\begin{enumerate}
\item\label{Hyp_biasExist} There exists $\epsilon>0$ such that for every $T$ large enough there exists $\gamma_{T}\in \mathcal{Z}_{\mathcal{S}}^{*}(T^{\frac{1}{2} - \epsilon})$ a $(T^{\frac{1}{2} - \epsilon},T)$-self-sufficient ordinate.
\item\label{Hyp_symm} For every $(k_{\gamma})_{\gamma} \in \mathbf{Z}^{(\mathcal{Z}_{\mathcal{S}})}$ one has
$$ \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}k_{\gamma}\gamma =0 \Rightarrow \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}k_{\gamma}\equiv 0\ [\bmod\ 2].$$
\item\label{Hyp_supp} The Riemann Hypothesis is satisfied for every $L(f,s)$, $f\in\mathcal{S}$,
and for every $f\in \mathcal{S}$, one has $\operatorname{Re}(a_{f})\geq 0$, and that for one $f\in\mathcal{S}$, one has $\operatorname{Re}(a_{f})> 0$.
\end{enumerate}
Then $\delta(\mathcal{S})$ exists
and $\delta(\mathcal{S}) - \frac{1}{2}$ has the same sign as $m_{\mathcal{S}}$. 
\end{cor}

\begin{Rk}
In the case $m_{\mathcal{S}}=0$, we only need (\ref{Hyp_biasExist}) and (\ref{Hyp_symm}) to get $\delta(\mathcal{S})=\frac{1}{2}$. 
\end{Rk}

We can also quantify the bias via concentration results.
In section~\ref{sub_ChebInequality} we use Chebyshev's inequality to give conditions for arbitrarily biased races.
In particular we deduce the following result.

\begin{theo}\label{Th_arbitraryBiased}
Let $(L(f_{n},\cdot))_{n}$ be a sequence of analytic $L$-functions.
Assume that each element $\gamma\in\mathcal{Z}_{f_{n}}$ has multiplicity 
$o\left(m_{f_{n}}^{2} \log(\gamma) / \log \mathfrak{q}(f_{n})\right)$ as $n\rightarrow\infty$,
where $m_{f_{n}}$ is as defined in Theorem~\ref{Th_DistLim}.
Then for any $\epsilon >0$, there exists $n$ such that:
	\begin{itemize}
		\item In the case $m_{f_{n}}<0$ one has:
		\begin{align*}
		\overline{\delta}(f_{n}) \leq \epsilon.
		\end{align*}
		\item In the case $m_{f_{n}}>0$ one has:
		\begin{align*}
		\underline{\delta}(f_{n}) \geq 1- \epsilon.
		\end{align*}
	\end{itemize}
\end{theo}

This Theorem is a generalization of \cite[Th. 1.2]{FioEC} which is conditional on the Bounded Multiplicity hypothesis.
One can find a similar result in \cite[Th. 1.2]{Fiorilli_HighlyBiased}.

\section{Applications to old and new instances of prime number races}\label{Sec_Applications}

In this section we present two kinds of applications.
We first find some results of the literature as special cases of our general result.
Most of them were conditional on GRH, they are now unconditional.
In a second part, we use the fact that analytic $L$-functions describe a wide range of $L$-functions to present new applications of Chebyshev's races.

\subsection{Proofs of old results under weaker assumptions}

\subsubsection{Sign of the second term in the Prime Number Theorem}

The first example of analytic $L$-function is Riemann's zeta function (see Example~\ref{Ex_Lfunc}(\ref{Ex_Lfunc_zeta})).
Adapting Theorem~\ref{Th_DistLim} to $\zeta$ yields an unconditional proof of the existence of the logarithmic limiting distribution for the race $\pi(x)$ versus $\operatorname{Li}(x)$ (see e.g. \cite{Wintner}, \cite[p. 175]{RS} for previous results under RH).
\begin{theo}\label{Th_race_pi_Li}
With the notations as in Section~\ref{subsec_setting},
the function 
$$E_{\zeta}(x) = \frac{\log x}{x^{\beta_{\zeta,0}}}(\pi(x) -\operatorname{Li}(x))$$
has a limiting logarithmic distribution $\mu_{\zeta}$ on $\mathbf{R}$.
There exists a positive constant $C$ such that one has 
$$\mu_{\zeta}(\mathbf{R}-[-A,A]) \ll \exp(-C\sqrt{A}).$$
Moreover, the mean of $\mu_{\zeta}$ is 
$$m_{\zeta} = -\delta_{\beta_{\zeta,0} = \frac{1}{2}}.$$
\end{theo}

\begin{proof}
This follows from the fact that the function with squared local roots associated to $\zeta$ is $\zeta$ itself, 
moreover $\zeta$ has a pole of order $1$ at $s=1$ and does not vanish over $(0,1)$.
\end{proof}

We deduce the already known idea that morally 
(e.g. under the conditions of Corollary~\ref{Cor_mean_bias}), 
assuming RH, the race $\pi(x) - \operatorname{Li}(x)$ should be biased towards negative values. 
Conversely a bias towards negative values in the race $\pi(x) - \operatorname{Li}(x)$  gives evidence that RH should hold.

\subsubsection{Prime number races between congruence classes modulo an integer}\label{subsubsec_ex_cong}

Rubinstein and Sarnak's result \cite[Th. 1.1, Th. 1.2]{RS} in the case of a prime number race with only two contestants $a$ and $b$ modulo $q$ is a particular case of Theorem \ref{Th_DistLim}.
Indeed take $$\mathcal{S}= \lbrace L(\chi,\cdot) : \chi \bmod\ q, \chi\neq \chi_{0}\rbrace$$
and $a_{\chi} = \overline{\chi}(a) - \overline{\chi}(b)$.
We obtain an uncondtional (i.e. without assuming GRH) proof of \cite[Th. 1.1, Th. 1.2]{RS}.

\begin{theo}\label{Th_RS_raceModq}
Let $q$ be an integer, $a \neq b\ [\bmod\ q]$ two invertible residue classes.
Define $$\beta_{q,0} = \sup\lbrace \operatorname{Re}(\rho) : \exists \chi \bmod\ q, \chi\neq \chi_{0},  L(\chi,\rho)=0\rbrace.$$
The function 
$$E_{q;a,b}(x) = \frac{\log x}{x^{\beta_{q,0}}}(\pi(x,q,a) -\pi(x,q,b))$$
has a limiting logarithmic distribution $\mu_{q;a,b}$ on $\mathbf{R}$.
There exists a positive constant $C$ (depending on $q$) such that one has 
$$\mu_{q;a,b}(\mathbf{R}-[-A,A]) \ll \exp(-C\sqrt{A}).$$
Moreover, suppose GRH is satisfied (i.e. $\beta_{q,0}=1/2$) and $L(\chi,1/2)\neq 0$ for every $\chi \bmod\ q$, $\chi\neq \chi_{0}$.
Then the mean of $\mu_{q;a,b}$ is 
$$m_{q;a,b} = \sum_{\substack{\chi \bmod\ q\\ \chi^{2} = \chi_{0}}} (\chi(b) - \chi(a)).$$
\end{theo}

\begin{Rk}
In particular under these hypotheses and the conditions of Corollary~\ref{Cor_mean_bias}.
\begin{enumerate}
\item If $ab^{-1}$ is a square, then there is no bias. 
\item Otherwise, the bias is in the direction of the non quadratic residue.
\end{enumerate}
\end{Rk}

Following the idea of \cite{RS} and \cite{Fiorilli_HighlyBiased}, 
we can also study the prime number race between the subsets of quadratic residues and non-residues modulo an integer $q$. 
For this, take
$$\mathcal{S}= \lbrace L(\chi,\cdot) : \chi \bmod\ q, \chi\neq \chi_{0}, \chi^{2} = \chi_{0}\rbrace,$$
and for each real character $\chi$ modulo $q$, take $a_{\chi}=\frac{1}{\rho(q)} := [(\mathbf{Z}/q\mathbf{Z})^{\times} : (\mathbf{Z}/q\mathbf{Z})^{\times (2)}]^{-1}$.
We apply Theorem~\ref{Th_DistLim} to this setting and get the following result.
\begin{theo}
Let $q\geq 3$ be an integer.
Define $$\beta^{(2)}_{q,0} = \sup\lbrace \operatorname{Re}(\rho) : \exists \chi \bmod\ q, \chi\neq \chi_{0}, \chi^{2} = \chi_{0},  L(\chi,\rho)=0\rbrace.$$
The function
$$E_{q;\rm{R},\rm{NR}}(x) = \frac{\log(x)}{\rho(q)x^{\beta^{(2)}_{q,0}}}((\rho(q) -1)\pi(x;q,\rm{R}) -\pi(x;q,\rm{NR}))$$
has a limiting logarithmic distribution $\mu_{q;\rm{R},\rm{NR}}$ on $\mathbf{R}$.

Moreover, suppose GRH is satisfied for all Dirichlet $L$-function of a real character modulo $q$
and that $L(\chi,1/2)\neq 0$ for every $\chi \bmod\ q$, $\chi\neq \chi_{0}$, $\chi^{2}=\chi_{0}$.
Then the mean of $\mu_{q;\rm{R},\rm{NR}}$ is
$$m_{q;a,b} = \frac{1 -\rho(q)}{\rho(q)}.$$
\end{theo}

Thus we have obtained an unconditional proof (without GRH) of \cite[Th. 1.1]{RS} in the case of the race  between the subsets of quadratic residues and non-residues modulo an integer $q$  (see also \cite[Lem. 2.2]{Fiorilli_HighlyBiased}).
Under GRH, the mean of the logarithmic limiting distribution is negative, 
hence morally we should find a bias towards negative values (i.e. towards non quadratic residues).
This result has already been used by Fiorilli in \cite{Fiorilli_HighlyBiased} to find arbitrarily biased races between residues and non-residues modulo integers having a lot of prime factors (so that the mean is as far from $0$ as possible).
In particular \cite[Th. 1.2]{Fiorilli_HighlyBiased} is Theorem~\ref{Th_arbitraryBiased} in this case.

\subsubsection{$L$-functions of automorphic forms on $GL(m)$}

By works of Godement and Jacquet \cite{GodementJacquet} we only miss the Ramanujan--Petersson conjecture to ensure that 
$L$-functions associated to automorphic irreducible cuspidal forms on $GL(m)$ 
are analytic $L$-functions in the sense of Definition~\ref{Def_Lfunc}.
We get a version of Theorem~\ref{Th_DistLim} for automorphic $L$-functions conditional to the Ramanujan--Petersson conjecture.
\begin{theo}\label{Th_Aut_bias}
Let $L(\pi,s)$ be a real $L$-function associated to an automorphic irreducible cuspidal representation of $GL(m)$ 
with $m\geq 2$.
Suppose the Ramanujan--Petersson conjecture holds for $L(\pi,s)$.
Then, following the notations of Section~\ref{subsec_setting}, the function
$E_{\pi}(x) = \frac{\log x}{x^{\beta_{\pi,0}}}\sum_{p\leq x} \lambda_{\pi}(p)$
has a limiting logarithmic distribution $\mu_{\pi}$.

Moreover under GRH for $L(\pi,s)$, the mean of $\mu_{\pi}$ is
$m_{\pi} = \pm 1 -2m(L(\pi,\cdot),1/2)\neq 0$.
\end{theo}

This result should be compared to \cite[Cor. 1.5]{ANS} where GRH is assumed but not the Ramanujan--Petersson Conjecture.
Morally, under GRH, since the mean is non zero, we expect that the prime number race associated to such an $L$-function has always a bias. 

\begin{proof}
Under GRH for $L(\pi,s)$, we study the behaviour of the function $L(\pi^{(2)},s) = L(\operatorname{Sym}^{2}\pi,s)L(\Lambda^{2}\pi,s)^{-1}$ around $s=1$.
By \cite[Th. 1.1]{Sha}, in the case $\pi$ is an irreducible non trivial representation,
neither functions  $L(\operatorname{Sym}^{2}\pi,s)$, $L(\Lambda^{2}\pi,s)$ vanishes at $s=1$.
Moreover one has $L(\pi\otimes\pi,s) = L(\operatorname{Sym}^{2}\pi,s)L(\Lambda^{2}\pi,s)$,
and (\cite[App.]{MW}) this function has a simple pole at $s=1$ when $\pi$ is self-dual (i.e. $L(\pi,s)$ is real).
Hence there are only two possibilities:
\begin{itemize}
\item either $L(\operatorname{Sym}^{2}\pi,s)$ has a simple pole at $s=1$ and $m(L(f^{(2)},\cdot),1) = -1$,
\item or $L(\Lambda^{2}\pi,s)$ has a simple pole at $s=1$ and $m(L(f^{(2)},\cdot),1) = 1$.
\end{itemize} 
Theorem~\ref{Th_Aut_bias} follows.
\end{proof}

In the case $m=2$, the Ramanujan--Petersson conjecture has been proved by works of Deligne and Deligne--Serre (\cite{Deligne_Weil1},\cite{DeligneSerre}).
In particular the normalized Hasse--Weil $L$-function associated to an elliptic curve defined over $\mathbf{Q}$ is an analytic $L$-function (see Example \ref{Ex_Lfunc}(\ref{Ex_Lfunc_Modular2})).
Hence we deduce \cite[Lem. 2.3, Lem. 2.6, Lem. 3.4]{FioEC} from Theorem \ref{Th_DistLim}.

\begin{prop}\label{Prop_Fi_EC}
Let $E/\mathbf{Q}$ be an elliptic curve, and $L(E,s)$ its normalized Hasse $L$-function.
The function 
$$E_{E}(x) = \frac{\log x}{x^{\beta_{E,0}}}\sum_{p\leq x}\frac{a_{p}(E)}{\sqrt{p}}$$
has a limiting logarithmic distribution $\mu_{E}$ on $\mathbf{R}$.
Moreover, suppose GRH is satisfied for $L(E,s)$.
Then the mean of $\mu_{E}$ is 
$$m_{E} = -2r_{an}(E) + 1$$
where $r_{an}(E)$ is the analytic rank of $E$.
\end{prop}

\begin{proof}
In the case of a Hasse--Weil $L$-function attached to an elliptic curve $E/\mathbf{Q}$, 
one has $L(\Lambda^{2}, E, s)=\zeta(s)$.
Proposition~\ref{Prop_Fi_EC} follows.
\end{proof}

We observe the two distinct cases pointed out by Mazur:
either $r_{an}(E) = 0$ and we should expect a bias towards positive values,
or $r_{an}(E) >0$ and we should expect a bias towards negative values.
As in \cite{FioEC} we can expect an arbitrarily large bias in the case the rank of the elliptic curve is arbitrarily large compared to the variance of the distribution.
Fiorilli obtain such a result assuming bounded multiplicity for the zeros of the Hasse--Weil $L$-function:
\cite[Th. 1.2]{FioEC} follows from Theorem~\ref{Th_arbitraryBiased} with the same hypotheses.

\subsection{New applications}

\subsubsection{Chebyshev's bias and prime numbers of the form $a^{2} + D b^{2}$}

In \cite{BS}, the authors give several examples of $L$-functions of degree $2$ related to $K3$ surfaces.
Precisely they define the three following functions:
$$L_{D}(s) = \prod_{p\nmid 2D}\left(1-a_{p}p^{-s} + \left(\frac{-D}{p}\right)p^{-2s}\right)^{-1}$$
for $D= 4$, $2$ and $3$,
where
$$a_{p} =\left\lbrace
\begin{array}{ll}
0 & \mbox{if $\left(\frac{-D}{p}\right)=0$ or $-1$,}\\
2\frac{a^{2} - Db^{2}}{p} & \mbox{if one can write $p= a^{2} + Db^{2}$ with $a,b\in \mathbf{N}$.}
\end{array}
\right.$$
By \cite[Th. 14.2]{BS} (and \cite{Schoeneberg} in the case $D=4$), those $L$-functions are associated to cusp forms of weight $3$ and level $4D$.
In particular they satisfy Definition \ref{Def_Lfunc}.

To these functions $L_{D}$ one can associate the prime number race that consists in understanding the sign of
$$E_{D}(x) = \frac{\log x}{x^{\beta_{0,D}}}\sum_{p=a^{2} + Db^{2}\leq x}2\frac{a^{2} - Db^{2}}{a^{2} + Db^{2}}.$$
The adaptation of Theorem~\ref{Th_DistLim} to this context is the following result.

\begin{theo}\label{Th_sum2squares}
	For $D= 4$, $2$ and $3$,
	$E_{D}$ has a limiting logarithmic distribution whose mean is 
	$$-\frac{m(L_{D},\beta_{0})}{\beta_{0}} -\delta_{\frac{1}{2},\beta_{0}}\leq 0.$$
\end{theo}

\begin{proof}
By \cite[Th. 14.2]{BS}, we are in the situation of Theorem~\ref{Th_Aut_bias}.
In particular the limiting logarithmic distribution exists. 

One can compute that, for each of the three cases $D=4,3$ and $2$, and for every $p$,
the products of the local roots are
$\alpha_{1}(p)\alpha_{2}(p) = \left(\frac{-D}{p}\right)$.
We deduce  
$$L\left(\Lambda^{2}f_{D},s\right)=\prod\left(1-\left(\frac{-D}{p}\right)p^{-s}\right)^{-1},$$ 
and in particular this function is entire.
Hence (by \cite[App.]{MW}) the function $L(\operatorname{Sym}^{2}f_{D},s)$ has a pole of multiplicity $1$ at $s=1$.

In conclusion the function $L(f_{D}^{(2)},s)$ has a pole of multiplicity $1$ at $s=1$. 
Using Theorem \ref{Th_DistLim}, we obtain Theorem \ref{Th_sum2squares}.
\end{proof}

We can interpret this result by saying that in the decomposition $p=a^{2} + Db^{2}$, the term $Db^{2}$ is often larger than $a^2$.
The Figures \ref{fig_CourseD4}, \ref{fig_CourseD2} and \ref{fig_CourseD3} represent respectively the races between $a^{2}$ and $-Db^{2}$ for $D=4,2,3$. We used \texttt{sage} and Cornacchia's algorithm to obtain the values of the functions $S_{D}(x):=\sum_{p=a^{2} + Db^{2}\leq x}\frac{a^{2} - Db^{2}}{a^{2} + Db^{2}}$ for $x$ between $0$ and $2.10^{7}$.
We see on these figures that it is natural to expect a bias towards the negative values.

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{CourseD4.png}
	
	\caption{Values of $S_{4}(x)$ in the range $(0,2.10^{7})$}
	\label{fig_CourseD4}
\end{figure}

\begin{figure}
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseD2.png}
      
      \caption{Values of $S_{2}(x)$ in the range $(0,2.10^{7})$}
      \label{fig_CourseD2}
   \end{minipage} \hfill
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseD3.png}
      
      	\caption{Values of $S_{3}(x)$ in the range $(0,2.10^{7})$}
      	\label{fig_CourseD3}
   \end{minipage}
\end{figure}

\begin{Rk}
As Z. Rudnick pointed out to us, in the case $D=4$, the previous prime number race is related to the question of the bias in the distribution of the angles of the Gaussian primes.
Let $\lambda$ be the Hecke character on $\mathbf{Z}[i]$ defined by $\lambda(z) = \left(\frac{z}{\bar{z}}\right)^{2}$.
The $L$-function $L(\lambda,s) = \prod_{p} \prod_{\mathfrak{p}\mid p} (1 - \lambda(\mathfrak{p})N\mathfrak{p}^{-s})^{-1}$
is an anlytic $L$-function in the sense of Definition~\ref{Def_Lfunc}.
In the case $p\equiv 1\ [\bmod\ 4]$ the local factor is $(1 - \cos(4\theta_{p})p^{-s} + p^{-2s})^{-1}$
where $\pm\theta_{p}$ are the angle of the Gaussian primes dividing $p$ (they are defined modulo $\frac{\pi}{2}$).
The prime number race associated with this situation consists in understanding the sign of the function
\begin{align*}
E_{\lambda}(x) = \sum_{\substack{p\leq x\\ p\equiv 1\ [\bmod\ 4]}} \cos(4\theta_{p}).
\end{align*}
This prime number race should be biased towards negative values.
We could study more general biases in the distribution of angles of Gaussian primes, this will be the object of another article. 
\end{Rk}

\subsubsection{Correlations for two elliptic curves}\label{Ex_2ellcurves}

As advertised in Example~\ref{Ex_Lfunc}(\ref{Ex_Lfunc_RankinSelberg}),
we can study the prime number race associated to a Rankin--Selberg product of $L$-functions.
Let $E_{1}$ and $E_{2}$ be two non-isogenous non-CM elliptic curves defined over $\mathbf{Q}$.
By works of Wiles, Taylor--Wiles, Breuil--Conrad--Diamond--Taylor,
there exists cuspidal modular forms $f_{1}\neq f_{2}$ associated to $E_{1}$ and $E_{2}$ respectively
(i.e. the corresponding normalized $L$-functions are the same). 
One has 
$$\lambda_{f_{i}}(p) = \frac{a(E_{i},p)}{\sqrt{p}} = \frac{p+1 - \lvert E_{i}(\mathbf{F}_{p})\rvert}{\sqrt{p}}.$$

In the case $E_{1}$ and $E_{2}$  do not become isogenous over a quadratic extension of $\mathbf{Q}$, by \cite{Ramakrishnan}
the Rankin--Selberg convolution $L(f_{E_{1}}\otimes f_{E_{2}},\cdot)$
is a real analytic $L$-function in the sense of Definition \ref{Def_Lfunc}.
Its coefficients are $\lambda(p) = a_{p}(E_{1})a_{p}(E_{2})/p$.
Moreover, if we assume that the curves $E_{1}$ and $E_{2}$  do not become isogenous over any abelian extension of $\mathbf{Q}$,
a strong version of Conjecture \ref{Sato-Tate} holds for these coefficients (see \cite[Th. 5.4]{HarrisAutEC}). 

Hence we can apply Theorem \ref{Th_DistLim}.
The function
$E(x)= \frac{\log x}{x^{\beta_{0}}} \sum_{p\leq x}\frac{a_{p}(E_{1})a_{p}(E_{2})}{p}$
admits a limiting logarithmic distribution,
and we can give its mean explicitly.
The term
$m(L(f_{E_{1}}\otimes f_{E_{2}},\cdot),1/2)$ may not be easy to evaluate,
but 
$m(L((f_{E_{1}}\otimes f_{E_{2}})^{(2)},\cdot),1)$ can be computed.
From these considerations we obtain the following result.

\begin{theo}\label{Prop_CorrEllcurves}
Let $E_{1}$ and $E_{2}$ be two non-CM elliptic curves defined over $\mathbf{Q}$.
Assume $E_{1}$ and $E_{2}$  do not become isogenous over a quadratic extension of $\mathbf{Q}$.
The function $$E(x)= \frac{\log x}{x^{\beta_{0}}} \sum_{p\leq x}\frac{a_{p}(E_{1})a_{p}(E_{2})}{p}$$
admits a limiting logarithmic distribution.
Assume the Riemann hypothesis holds then the logarithmic distribution has negative mean.
\end{theo}

This result is a consequence of the following lemma.

\begin{lem}\label{lm_productCE}
Let $E_{1}$ and $E_{2}$ be two non-CM elliptic curves defined over $\mathbf{Q}$.
Suppose that $E_{1}$ and $E_{2}$ do not become isogenous over any quadratic extension of $\mathbf{Q}$,
then
$$m(L((f_{E_{1}}\otimes f_{E_{2}})^{(2)},\cdot),1)=-1$$
\end{lem}
\begin{proof}
To fix the notation we write for  $i=1,2$,
\begin{align*}
L(f_{E_{i}},s) = \prod_{p}(1-\pi_{i}p^{-s})^{-1}(1-\overline{\pi_{i}}p^{-s})^{-1}.
\end{align*}
The local roots of $L(f_{E_{1}}\otimes f_{E_{2}},\cdot)$ at $p$ are $\pi_{1}\pi_{2}$, $\overline{\pi_{1}}\pi_{2}$, $\pi_{1}\overline{\pi_{2}}$ and $\overline{\pi_{1}\pi_{2}}$.
Hence
\begin{align*}
L(\Lambda^{2}(f_{E_{1}}\otimes f_{E_{2}}),s) &=
\prod_{p}\prod_{i=1,2}(1-\pi_{i}^{2}p^{-s})^{-1}(1-\overline{\pi_{i}}^{2}p^{-s})^{-1}(1-p^{-s})^{-1} \\
&= L(\operatorname{Sym}^{2}f_{E_{1}},s)L(\operatorname{Sym}^{2}f_{E_{2}},s),
\end{align*}
and
\begin{align*}
L(\operatorname{Sym}^{2}(f_{E_{1}}\otimes f_{E_{2}}),s) 
&= L(\operatorname{Sym}^{2}f_{E_{1}}\otimes\operatorname{Sym}^{2}f_{E_{2}},s)\zeta(s).
\end{align*}
For $i = 1,2$, the function $L(\operatorname{Sym}^{2}f_{E_{i}},\cdot)$ is holomorphic and does not vanish at $s=1$.
By \cite{Ramakrishnan} one can associate to $f_{E_{1}}\otimes f_{E_{2}}$ a cuspidal irreducible representation of $GL(4)$ with the same $L$-function, 
hence by \cite[App.]{MW} the function 
\begin{align*}
L((f_{E_{1}}\otimes f_{E_{2}})\otimes(f_{E_{1}}\otimes f_{E_{2}}) ,s) = L(\operatorname{Sym}^{2}(f_{E_{1}}\otimes f_{E_{2}}),s)L(\Lambda^{2}(f_{E_{1}}\otimes f_{E_{2}}),s)
\end{align*}
has a pole of multiplicity $1$ at $s=1$.
As a consequence:
\begin{align*}
L((f_{E_{1}}\otimes f_{E_{2}})^{(2)},s) = L(\operatorname{Sym}^{2}(f_{E_{1}}\otimes f_{E_{2}}),s)L(\Lambda^{2}(f_{E_{1}}\otimes f_{E_{2}}),s)^{-1}
\end{align*}
has a pole of multiplicity $1$ at $s=1$.
\end{proof}

The proof of Theorem \ref{Prop_CorrEllcurves}
then follows from Theorem \ref{Th_DistLim} and Lemma \ref{lm_productCE}.
Under the Riemann Hypothesis, 
the mean of the limiting logarithmic distribution is
$$-2m\left(L(f_{E_{1}}\otimes f_{E_{2}},\cdot),\frac{1}{2}\right) - 1 <0.$$

We may interpret this result by saying that given two non-isogenous elliptic curves (in the strong sense as above),
the coefficients $a_{p}(E_{1})$ and $a_{p}(E_{2})$ often have opposite signs.
The Figures \ref{fig_CourseE1E2}, \ref{fig_CourseE0E2}, \ref{fig_CourseE1E0} and \ref{fig_CourseE0E'0} represent various races for the correlations of the signs of the $a_{p}(E)$ for two elliptic curves. 
We used four elliptic curves that we can define by an affine model as follows:
$$\begin{array}{cc}
E_{1} : y^{2} + y = x^{3} - x , &  E_{2} : y^{2} + y = x^{3} + x^{2} - 2x ,\\
E_{0} : y^{2} + y = x^{3} - x^{2} , &  E'_{0} : y^{2} + y = x^{3} + x^{2} + x.
\end{array}$$
The elliptic curves have algebraic rank respectively equal to $1$, $2$ and $0$.
We used \texttt{sage} and the counting points algorithm for elliptic curves implemented in \texttt{pari} to obtain the values of the functions $S_{E_{i},E_{j}}(x):=\sum_{p\leq x}\frac{a_{p}(E_{i})a_{p}(E_{j})}{p}$ for $x$ between $0$ and $5.10^{6}$.
The bias towards negative values can be guessed from Figures \ref{fig_CourseE1E2} and \ref{fig_CourseE0E2}, 
it is less clear on Figures \ref{fig_CourseE1E0} and \ref{fig_CourseE0E'0}.
The bias may be smaller in the last two cases and appear only on a larger scale. 

\begin{figure}
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseCourbesEll1_2.png}
      
      \caption{Values of $S_{E_{1},E_{2}}(x)$ in the range $(0,5.10^{6})$}
      \label{fig_CourseE1E2}
   \end{minipage} \hfill
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseCourbeEll2_0.png}
      
      	\caption{Values of $S_{E_{0},E_{2}}(x)$ in the range $(0,5.10^{6})$}
      	\label{fig_CourseE0E2}
   \end{minipage}
\end{figure}

\begin{figure}
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseCourbesEll1_0.png}
      
      \caption{Values of $S_{E_{0},E_{1}}(x)$ in the range $(0,5.10^{6})$}
      \label{fig_CourseE1E0}
   \end{minipage} \hfill
   \begin{minipage}[c]{.46\linewidth}
      \includegraphics[scale=0.16]{CourseCourbesEll0_0.png}
      
      	\caption{Values of $S_{E_{0},E'_{0}}(x)$ in the range $(0,5.10^{6})$}
      	\label{fig_CourseE0E'0}
   \end{minipage}
\end{figure}

\subsubsection{Jacobian of modular curves}

Our last example is the prime number race for the $L$-functions of the modular curves.
Let $q$ be a prime number.
We study the prime number race for the sum of the coefficients of all $L$-functions of primitive weight two cusp forms of level $q$.
The $L$-function associated to this race is the finite product
\begin{align*}
\prod_{f\in S_{2}(q)^{*}}L(f,s) = L(J_{0}(q),s),
\end{align*} 
where $J_{0}(q)$ is the Jacobian of the modular curve $X_{0}(q)$ (this factorisation is due to Shimura \cite{Shimura}).
The function $L(J_{0}(q),\cdot)$ is an analytic $L$-function in the sense of Definition \ref{Def_Lfunc}
since it is a product of analytic $L$-functions.

Assuming the Riemann Hypothesis for $L(J_{0}(q),s)$, Theorem \ref{Th_DistLim} applies to the function
$$E_{J_{0}(q)}(x) = \frac{\log x}{\sqrt{x}}\sum_{p\leq x}\sum_{f\in S_{2}(q)^{*}} \lambda_{f}(p).$$
One can conjecture a value for the mean of the limiting logarithmic distribution.
\begin{conj}\label{Conj_largeRank}
One has:
$$m\left(L(J_{0}(q),\cdot),\frac{1}{2}\right) \sim \frac{1}{2}\lvert S_{2}(q)^{*} \rvert$$
as $q\rightarrow \infty$.
\end{conj}
In the articles \cite{KM_upperbound} and \cite{KM_lowerbound}, 
Kowalski and Michel showed that there exist two explicit constants $c<\frac{1}{2}<C$ such that
\begin{align*}
c\lvert S_{2}(q)^{*} \rvert\leq  m(L(J_{0}(q),\cdot),\frac{1}{2}) \leq C\lvert S_{2}(q)^{*} \rvert,
\end{align*}
for all sufficiently large $q$.

The large multiplicity given by Conjecture \ref{Conj_largeRank} may lead us to think that we could get a large bias, but considering all the primitive weight two forms of level $q$ at once, the biases towards positive or negative values should in fact cancel each other.
Precisely:
\begin{theo}\label{Prop_JacModCurve}
Assume the Riemann Hypothesis for $L(J_{0}(q),\cdot)$ (for all $q$)
and assume Conjecture \ref{Conj_largeRank} holds.
Then the function 
$$E_{J_{0}(q)}(x) = \frac{\log x}{\sqrt{x}}\sum_{p\leq x}\sum_{f\in S_{2}(q)^{*}} \lambda_{f}(p)$$
admits a limiting logarithmic distribution
with mean $o_{q\rightarrow\infty}(\lvert S_{2}(q)^{*} \rvert)$
and variance $\gg \lvert S_{2}(q)^{*} \rvert \log q$.
\end{theo}

\begin{Rk}
In this situation it is difficult to apply Chebyshev's inequality (see section \ref{sub_ChebInequality}).
As it is the case of the original work of \cite{RS} the bias probably dissipates as $q\rightarrow\infty$.
If we want to show this, we need a better error term in Conjecture \ref{Conj_largeRank}: we need that $\frac{m_{q}}{\sqrt{\operatorname{Var}_{q}}}\rightarrow 0$ as $q\rightarrow\infty$.
\end{Rk}

For the proof of Theorem~\ref{Prop_JacModCurve}, we compute again $m(L(J_{0}(q)^{2},\cdot),1)$. 
\begin{lem}\label{lm_L2Jac}
Let $q$ be an integer.
One has $m(L(J_{0}(q)^{(2)},\cdot),1)= \lvert S_{2}(q)^{*} \rvert $.
\end{lem}
For the record, one has $\lvert S_{2}(q)^{*} \rvert  \sim \frac{q}{12}$.

\begin{proof}
As in the proof of Lemma \ref{lm_productCE}, we use local roots to determine 
$L(\Lambda^{2}(J_{0}(q)),\cdot)$ and $L(\operatorname{Sym}^{2}(J_{0}(q)),\cdot)$.
For $f\in S_{2}(q)^{*}$, denote by $\alpha_{f}(p)$ and $\overline{\alpha_{f}(p)}$ its local roots.
They satisfy $\alpha_{f}(p)\overline{\alpha_{f}(p)}=1$ if $p\nmid q$.
One has
\begin{align*}
L(\Lambda^{2}(J_{0}(q)),s) = \zeta_{q}(s)^{\lvert S_{2}(q)^{*} \rvert}\prod_{f\neq f'}L(f\otimes f',s)
\end{align*}
and
\begin{align*}
L(\operatorname{Sym}^{2}(J_{0}(q)),s) = \prod_{f}L(\operatorname{Sym}^{2}f,s)\prod_{f\neq f'}L(f\otimes f',s).
\end{align*}
Hence $L(\Lambda^{2}(J_{0}(q)),\cdot)$ has a pole of multiplicity $\lvert S_{2}(q)^{*} \rvert$ at $s=1$,
and $L(\operatorname{Sym}^{2}(J_{0}(q)),\cdot)$ is holomorphic and does not vanish at $s=1$.
We conclude that $L(J_{0}(q)^{(2)},\cdot)$ has a zero of multiplicity $\lvert S_{2}(q)^{*} \rvert$ at $s=1$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{Prop_JacModCurve}]
It follows from Theorem \ref{Th_DistLim} and Lemma \ref{lm_L2Jac}, under the Riemann Hypothesis that
the mean of the limiting logarithmic distribution is
\begin{align*}
2m\left(L(J_{0}(q),\cdot),\frac{1}{2}\right) - \lvert S_{2}(q)^{*} \rvert.
\end{align*}
If we assume Conjecture \ref{Conj_largeRank} is satisfied, then the mean is $=o(\lvert S_{2}(q)^{*} \rvert)$.
The variance is
\begin{align*}
{\mathop{\sum \Bigl.^{*}}\limits}_{\substack{L(J_{0}(q),\frac{1}{2} +i\gamma)=0 \\ \gamma\neq 0}}\frac{m(L(J_{0}(q),\cdot),\frac{1}{2} +i\gamma)^{2}}{(\frac{1}{4}+\gamma^{2})}
&\gg \sum_{\substack{L(J_{0}(q),\frac{1}{2} +i\gamma)=0 \\ \gamma\neq 0}}\frac{1}{(\frac{1}{4}+\gamma^{2})} \\
&\gg \log(\mathfrak{q}(J_{0}(q)) \asymp \lvert S_{2}(q)^{*} \rvert \log q.
\end{align*}
\end{proof}

\section{Proof of Theorem \ref{Th_DistLim}}\label{Sec_ProofExist}

In this section we prove Theorem~\ref{Th_DistLim} as a consequence of the following result relating $\mu_{\mathcal{S}}$ with the zeros of the $L$-functions.
\begin{prop}\label{Prop_LimOfDist}
Let $\lbrace L(f,\cdot) : f\in\mathcal{S} \rbrace$ be a finite set of analytic $L$-functions such that $\overline{\mathcal{S}}=\mathcal{S}$, 
and $(a_{f})_{f\in \mathcal{S}}$ a set of complex numbers satisfying $a_{\overline{f}}= \overline{a_{f}}$.
Let $T>2$ and 
$$G_{\mathcal{S},T}(x) = m_{\mathcal{S}} -\sum_{\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)}2\operatorname{Re}\left(M(\gamma)\frac{x^{i\gamma}}{\beta_{\mathcal{S},0} + i\gamma}\right)$$
where as in Theorem~\ref{Th_DistLim}, for $\gamma$ in $\mathcal{Z}_{\mathcal{S}}^{*}$, one has
$M(\gamma) = \sum_{f\in\mathcal{S}}a_{f}m(L(f,\cdot),\beta_{\mathcal{S},0}+i\gamma)$.
	
The function $G_{\mathcal{S},T}(x)$ admits a limiting logarithmic distribution $\mu_{\mathcal{S},T}$.
Moreover for any bounded Lipschitz continuous function $g$, one has
\begin{align*}
\lim_{T\rightarrow\infty}\int_{\mathbf{R}}g(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T}(t) = 
\int_{\mathbf{R}}g(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S}}(t).
\end{align*}
\end{prop}

\begin{Rk}\label{Rk_onProp_LimOfDist}
\begin{enumerate}
\item\label{It_NoZero} In the case $\mathcal{Z}_{\mathcal{S}}$ is empty (it may happen if the Riemann Hypothesis is not satisfied), the functions $G_{\mathcal{S},T}(x)$ are constant, and do not depend of $T$. 
Hence the limiting logarithmic distributions $\mu_{\mathcal{S},T}$ and $\mu_{\mathcal{S}}$ exist and are equal to the Dirac delta function $\delta_{m_{\mathcal{S}}}$.
In particular in the case $\beta_{\mathcal{S},0}=1$, the set $\mathcal{Z}_{\mathcal{S}}$ is empty, and the limiting logarithmic distribution is $\delta_{0}$. 
Hence the only information we get from Theorem \ref{Th_DistLim} is that $S(x) = o\left(\frac{x}{\log x}\right)$.
\item Another approach for this result can be found in \cite{ANS}.
The function $G_{\mathcal{S},T}(e^{y})$ is a trigonometric polynomial, 
and as $T\rightarrow\infty$ it approximates the function $E_{\mathcal{S}}(e^{y})$.
The improvement in our result is that we do not need to assume that the Generalized Riemann Hypothesis holds.
\end{enumerate}
\end{Rk}

To obtain this result (except for the statement about $\operatorname{Var}(X_{\mathcal{S}})$) 
it is enough to consider the case where $\mathcal{S}$ is a singleton $\lbrace f\rbrace$ and $a_{f}=1$ (by linearity).
The proof follows Fiorilli's ideas \cite[Lem. 3.4]{FioEC},
hence we only give the necessary extra details.
The proof is decomposed in the following way.
Subsections~\ref{Subsec_Approx1} to \ref{Subsec_ApproxFin} are dedicated to the proof that 
the functions $E_{f}(x)$ and $G_{f,T}(x)$ are close to each other.
The existence part of the proposition is proved in subsection \ref{sub_ExistenceLimDist} as a consequence of the Kronecker--Weyl Theorem and Helly's selection Theorem.
We conclude the proof of Theorem~\ref{Th_DistLim} in subsection~\ref{Subsec_MeanVar} by computing the mean and variance of the limiting logarithmic distribution $\mu_{\mathcal{S}}$.
Let us first make very precise the analogue of Fiorilli's bound on $L'/L$, $\operatorname{Re}(s)\leq 2$ that we need.

\subsection{Preliminary result on the logarithmic derivative of $L(f,s)$}\label{Sec_boundlogder}

\begin{lem}\label{Lm_boundlogder}
Let $L(f,\cdot)$ be an analytic $L$-function of degree $d$ and $\delta >0$. 
For all $s\in\mathbf{C}$ such that $\operatorname{Re}(s)\leq 2$, 
and $\lvert s - \rho\rvert > \delta$, for every zero or pole $\rho$ of $\Lambda(f,\cdot)$ (the completed $L$-function, see Definition \ref{Def_Lfunc}(\ref{Hyp_FunctEquation})), one has
$$\left\lvert \frac{L'(f,s)}{L(f,s)} \right\rvert \ll 
\log\left( \mathfrak{q}(f)(\lvert s\rvert +3)^{d}\right)(\lvert\operatorname{Re}(s)\rvert +  d\delta^{-1}) + \lvert m(L(f,\cdot),1) \rvert \delta^{-1}$$
where $\mathfrak{q}(f)$ is the analytic conductor of $L(f,s)$ as defined in Definition \ref{Def_Lfunc}(\ref{Hyp_FunctEquation}) and the implicit constant is absolute.
\end{lem}

\begin{proof}
By Definition \ref{Def_Lfunc}(\ref{Hyp_FunctEquation}), 
the completed $L$-function $\Lambda(f,\cdot)$ is of order $1$.
By Hadamard's factorization result \cite[Th. 5.6]{IK}, one has
\begin{align*}
-\frac{L'(f,s)}{L(f,s)} = \frac{\gamma'(f,s)}{\gamma(f,s)} -
\sum_{\rho}\frac{1}{s-\rho} + \frac{r}{s} + \frac{r}{s-1} + O(\log \mathfrak{q}(f))
\end{align*}
where the sum is over the zeros $\rho$ of $\Lambda(f,\cdot)$ 
and $r= - m(L(f,\cdot),1)$ is the order of the pole at $s=1$ of $L(f,s)$.

Using again Definition \ref{Def_Lfunc}(\ref{Hyp_FunctEquation}),
one has $\gamma(f,s)=\prod_{j=1}^{d}\Gamma(\frac{s+\kappa_{j}}{2})$.
Hence
$$\frac{\gamma'(f,s)}{\gamma(f,s)}= \sum_{j=1}^{d}\frac{\Gamma'(\frac{s+\kappa_{j}}{2})}{2\Gamma(\frac{s+\kappa_{j}}{2})}.$$
Using $\Gamma(s+1)=s\Gamma(s)$ 
we get $\frac{\Gamma'(s+1)}{\Gamma(s+1)} = \frac{1}{s} + \frac{\Gamma'(s)}{\Gamma(s)}$.
It suffices to study $\frac{\Gamma'(s)}{\Gamma(s)}$ for $\operatorname{Re}(s)>1$.
We write
\begin{align*}
\frac{\gamma'(f,s)}{\gamma(f,s)} = 
\sum_{i=1}^{d}\left( \sum_{k=1}^{\left[\operatorname{Re}\left(\frac{s+\kappa_{j}}{2}\right)\right] -1}\frac{1}{2}\frac{1}{\frac{s+\kappa_{i}}{2}-k}
+ \frac{\Gamma'}{2\Gamma}\left(\frac{\operatorname{Im}(s)+\kappa_{i}}{2} - \left[\operatorname{Re}\left(\frac{s+\kappa_{j}}{2}\right)\right] + 1\right)\right) 
\end{align*}
where $\left[\cdot\right]$ is the floor function. If for some $j$ one has $\left[\operatorname{Re}\left(\frac{s+\kappa_{j}}{2}\right)\right]<1$, 
then the corresponding sum is
$-\sum_{k=\left[\operatorname{Re}\left(\frac{s+\kappa_{j}}{2}\right)\right] -2}^{0}$.

Now for $\operatorname{Re}(s)>1$ one has $\frac{\Gamma'(s)}{\Gamma(s)}\ll \log\lvert s\rvert$.
We get 
\begin{align*}
\left\lvert \frac{\gamma'(f,s)}{\gamma(f,s)}\right\rvert \ll d\delta^{-1} + \log\left( \mathfrak{q}(f)(\lvert s\rvert +3)^{d}\right).
\end{align*}

Following the idea of the proof of \cite[(5.28)]{IK}, 
we now estimate $\sum_{\rho}\frac{1}{s-\rho}$.
Let $s=\sigma + it$ be a complex number satisfying $\sigma\leq 2$ and $\lvert s - \rho\rvert > \delta$, for every zero or pole $\rho$ of $\Lambda(f,\cdot)$.
One has
\begin{align*}
\sum_{\rho}\frac{1}{s-\rho} &= \sum_{\rho}\left(\frac{1}{s-\rho} - \frac{1}{3+it-\rho} \right) 
+ \sum_{\rho} \frac{1}{3+it-\rho} \\
& = \sum_{\rho}\left(\frac{1}{s-\rho} - \frac{1}{3+it-\rho} \right) 
+ O(\log(\mathfrak{q}(f)(\lvert t \rvert + 3)^{d}))
\end{align*}
thanks to \cite[(5.32)]{IK}.
Moreover for each zero $\rho=\beta + i\gamma$ of $\Lambda(f,\cdot)$, one has
\begin{align*}
\left\lvert \frac{1}{s-\rho} - \frac{1}{3+it-\rho} \right\rvert
&\leq \left\lvert \frac{3 - \sigma}{(3-\beta)(\sigma-\beta) - (t-\gamma)^{2} + i (3+\sigma -2\beta)(t-\gamma)} \right\rvert \\
&\ll \frac{\lvert\sigma\rvert+1}{\lvert 2(\sigma - \beta)  - (t-\gamma)^{2}\rvert}
\end{align*}
where the implicit constant is absolute.
If $\sigma <-1/2$, one has
\begin{align*}
\lvert 2(\sigma - \beta)  - (t-\gamma)^{2}\rvert \geq 1 + (t-\gamma)^{2}.
\end{align*}
Using again \cite[(5.32)]{IK}, 
we get
$$\sum_{\rho}\frac{1}{s-\rho}\ll (\lvert\operatorname{Re}(s)\rvert +1)\log(\mathfrak{q}(f)(\lvert t \rvert + 3)^{d})$$
with an absolute implicit constant.
If $-1/2\leq \sigma <2$, one has 
\begin{align*}
\left\lvert \frac{1}{s-\rho} - \frac{1}{3+it-\rho} \right\rvert \ll  \frac{1}{ \delta +  (t-\gamma)^{2}}.
\end{align*}
We get 
$$\sum_{\rho}\frac{1}{s-\rho}\ll \delta^{-1}\log(\mathfrak{q}(f)(\lvert t \rvert + 3)^{d})$$
where the implicit constant is absolute.
Putting everything together yields the result.

\end{proof}

\subsection{Approximation of $\psi(f,x)$}\label{Subsec_Approx1}

It is a standard step in proofs of theorems reminiscent of the Prime Number Theorem 
to begin with the study of the associated $\psi$-function~:
$$\psi(f,x) = \sum_{k=1}^{\infty}\sum_{p^{k}\leq x}\left(\sum_{j=1}^{d} \alpha_{j}(p)^k\right) \log p.$$
Note that for $\operatorname{Re}(s)>1$, one has
$$-\frac{L'(f,s)}{L(f,s)} = \sum_{k=1}^{\infty}\sum_{p}\left(\sum_{j=1}^{d} \alpha_{j}(p)^k\right)p^{-ks} \log p=: \sum_{n=1}^{\infty}\Lambda_{f}(n)n^{-s}.$$
Then Perron's Formula and integration around the zeros yields an explicit formula for $\psi(f,x)$.
\begin{lem}\label{Lm_ExplicitFomula}
Let $L(f,\cdot)$ be an analytic $L$-function, then
$$\psi(f,x) + m(L(f,\cdot),1)x = - \sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert\leq T}}\frac{x^{\rho}}{\rho} + R(f,x,T)$$
where $$R(f,x,T)= O\left(d\log x + d\frac{x}{T}(\log x)^2 
+ dx^{-1/4} \log x  \left(\log(\mathfrak{q}(f)T^{d})\right)^{2} + 
d\frac{\log(\mathfrak{q}(f)T^{d})x}{T} \right),$$
with an absolute implicit constant.
\end{lem}

\begin{proof}
Using Perron's Formula as in \cite[Cor. 5.3]{MV},
we obtain a principal term 
$$\frac{1}{2i\pi}\int_{c-iT}^{c+iT} -\frac{L'(f,s)}{L(f,s)} x^{s}\frac{{\mathop{}\!\mathrm{d}} s}{s}$$
where we choose $c=1+\frac{1}{\log x}$.
Using Cauchy's residue Theorem, 
we write this integral as a sum over the zeros and poles of $L(f,s)$ and an integral along the path defined by the lines $\operatorname{Re}(s) = c$, $\operatorname{Im}(s) = \pm T$, $\operatorname{Re}(s) = -\frac{1}{4}$  (avoiding the zeros).
Finally we use Lemma \ref{Lm_boundlogder} to bound the integral 
(see also \cite[Chap. 5, Ex. 7]{IK}).
\end{proof}

The error term in Lemma \ref{Lm_ExplicitFomula} is smaller than $x^{\frac{1}{2}}$ 
in the range $x^{\frac{1}{2}} (\log x)^{2} \ll T \ll \exp\left(x^{\frac{3}{4}}(\log x)^{-\frac{1}{2}}\right)$.
In the following we want to vary $x$ and $T$ independently so we begin with precising this error term.
Using Lemma \ref{Lm_ExplicitFomula} for $T=x$ yields
\begin{equation}\label{Eq_ExplicitFormula_x}
\psi(f,x) + m(L(f,\cdot),1)x  = -\sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert\leq x}}\frac{x^{\rho}}{\rho} + 
O\left(d\left(\log(\mathfrak{q}(f)x^{d})\right)^2 \right).
\end{equation}
Once $x$ and $T$ are fixed, we split the sum as follows:
\begin{equation}\label{Eq_splitSum}
\sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert\leq x}} = 
	\sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert\leq T}} + \sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert>T}}
	- \sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert> x}}=: \Sigma_{1} + \Sigma_{2} - \Sigma_{3},
\end{equation}
so that
\begin{align*}
\psi(f,x) + m(L(f,\cdot),1)x  = -\Sigma_{1} - \Sigma_{2} + \Sigma_{3} + 
O\left(d\left(\log(\mathfrak{q}(f)x^{d})\right)^2 \right).
\end{align*}
We deal with each of these terms separately below in order to obtain the following result.

\begin{prop}\label{Prop_decomp_psi}
One has 
\begin{align*}
\psi(f,x) + m(L(f,\cdot),1)x  = 
- \sum_{\substack {L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert\leq T}}\frac{x^{\rho}}{\rho} - \epsilon_{f}(x,T) + 
O\left(d\left(\log(\mathfrak{q}(f)x^{d})\right)^2 \right)
\end{align*}
where the function $\epsilon_{f}(x,T)$ satisfies
$$\int_{2}^{Y}\lvert \epsilon_{f}(e^{y},T)\rvert^{2} {\mathop{}\!\mathrm{d}} y \ll Y\frac{d^{2}\left(\log(\mathfrak{q}(f)T)\right)^2}{T} + \frac{d^{2}\log(\mathfrak{q}(f)T)^3}{T}$$
with an absolute implicit constant.
\end{prop}

\subsection{Bound for $\Sigma_{3}$}

We use the following generalization of \cite[Lem. 2.1]{FioEC}
\begin{lem}\label{Lm_Fiorilli2.1}
Let $L$ be an analytic $L$ function and $x \geq 2$.
One has
$$\left\lvert\sum_{\substack{L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert>x}}\frac{x^{\rho}}{\rho} \right\rvert \ll 
 d\left(\log(\mathfrak{q}(f)x)\right)^{2}$$
 with an absolute implicit constant.
\end{lem}

\begin{proof}
Using Cauchy's residue Theorem one has 
$$\sum_{\substack{L(f,\rho)=0 \\ \operatorname{Im}(\rho)>T}}\frac{x^{\rho}}{\rho}
 =\lim_{R_{2}\rightarrow\infty}\lim_{R_{1}\rightarrow\infty}\frac{1}{2i\pi} 
\int_{C(R_{1},R_{2},T)}\frac{L'(f,s)}{L(f,s)} x^{s}\frac{{\mathop{}\!\mathrm{d}} s}{s} $$
where $C(R_{1},R_{2},T)$ is the rectangular path with vertices
$c+iT$, $c+iR_{2}$, $-R_{1}+ iR_{2}$, $-R_{1}+iT$ to which we give the positive orientation
($c=1 +\frac{1}{\log x}$ as before).
If necessary we can move the path to avoid the zeros.
We can bound the integral over the vertical segment $\operatorname{Re}(s)=c$ using Perron's formula as in Lemma \ref{Lm_ExplicitFomula}.
Then we use Lemma \ref{Lm_boundlogder} to bound the rest of the integral.
Let $R_{1}\rightarrow\infty$ then $R_{2}\rightarrow\infty$, we obtain:
\begin{align}\label{Form_bound_queueSum}
\left\lvert\sum_{\substack{L(f,\rho)=0 \\ \lvert\operatorname{Im}(\rho)\rvert>T}}\frac{x^{\rho}}{\rho} \right\rvert \ll 
d\log x + d\frac{x}{T}(\log x)^2 
+ \frac{\log(\mathfrak{q}(f)T^{d})x}{T}.
\end{align}
Taking $T=x$ we get the result.
\end{proof}

\subsection{$L^{2}$-bound for $\Sigma_{2}$}

As in \cite[Lem. 3.3]{FioEC} and \cite[Lem. 2.2]{RS},we give a bound for the $L^{2}$-norm of the second term of the right hand side of (\ref{Eq_splitSum}).
\begin{lem}\label{Lm_epsilon}
Let $L(f,s)$ be an analytic $L$-function, and let $T,Y>2$ be fixed.
Define
\begin{equation}\label{Def_epsilon}
\epsilon_{f}(x,T):= x^{-\beta_{f,0}}\sum_{\substack{\rho=\beta + i\gamma \\ L(f,\rho)=0 \\ \lvert\gamma\rvert\geq T}}\frac{x^{\beta +i\gamma}}{\beta+ i\gamma}.
\end{equation}
One has
$$\int_{2}^{Y}\lvert \epsilon_{f}(e^{y},T)\rvert^{2} {\mathop{}\!\mathrm{d}} y \ll Y\frac{d^{2}\left(\log(\mathfrak{q}(f)T)\right)^2}{T} + \frac{d^{2}\log(\mathfrak{q}(f)T)^3}{T}$$
with an absolute implicit constant.
\end{lem}

\begin{proof}
The proof follows that of \cite[Lem. 2.2]{RS}.
We compute 
\begin{align*}
	\int_{2}^{Y}\lvert \epsilon(e^{y},T)\rvert^{2} {\mathop{}\!\mathrm{d}} y 
	&\ll \sum_{\substack{\rho_{1},\rho_{2} \\ \lvert\operatorname{Im}\rvert>T}}\int_{2}^{Y}\frac{e^{y(\rho_{1}+\overline{\rho_{2}} - 2\beta_{0})}}{\rho_{1}\overline{\rho_{2}}}{\mathop{}\!\mathrm{d}} y \\
		&\ll  \sum_{\substack{\rho_{1},\rho_{2} \\ \lvert\operatorname{Im}\rvert>T}}\frac{1}{\lvert\operatorname{Im}(\rho_{1})\rvert\lvert\operatorname{Im}(\rho_{2})\rvert}\min(Y,\lvert\rho_{1}+\overline{\rho_{2}} - 2\beta_{0}\rvert^{-1})\\
		&\ll \sum_{\substack{\rho_{1},\rho_{2} \\ \lvert\operatorname{Im}\rvert>T}}\frac{1}{\lvert\operatorname{Im}(\rho_{1})\rvert\lvert\operatorname{Im}(\rho_{2})\rvert}\min(Y,\lvert\operatorname{Im}(\rho_{1}) - \operatorname{Im}(\rho_{2})\rvert^{-1}).
	\end{align*}
	The diagonal term $\lvert\operatorname{Im}(\rho_{1}) - \operatorname{Im}(\rho_{2})\rvert\leq Y^{-1}$ is bounded by
	\begin{align*}
	Y\sum_{\lvert\gamma\rvert >T}\frac{d\log(\mathfrak{q}(f)\lvert\gamma\rvert)}{\lvert\gamma\rvert^{2}} \ll Y\frac{d^{2}\left(\log(\mathfrak{q}(f)T)\right)^{2}}{T}.
	\end{align*}
	For the off-diagonal contribution, we compare the sum with the integral (using properties of the zeros)
\begin{align*}
\int_{T}^{\infty}\int_{T}^{x-\frac{1}{Y}} d^{2}\frac{\log(\mathfrak{q}(f)x)\log(\mathfrak{q}(f)y)}{xy(x-y)}{\mathop{}\!\mathrm{d}} y {\mathop{}\!\mathrm{d}} x
\ll d^{2}\frac{\left(\log(\mathfrak{q}(f)T)\right)^3}{T} + d^{2}\frac{\left(\log(\mathfrak{q}(f)T)\right)^{2}}{T}\log(\mathfrak{q}(f)Y)
\end{align*}
with an absolute implicit constant.
\end{proof}
The proof of Proposition~\ref{Prop_decomp_psi} is therefore complete.

\subsection{Approximation of $\Sigma_{1}$}

We now decompose the first term of the right hand side of (\ref{Eq_splitSum}) to highlight the main term.

\begin{lem}\label{Lm_beta0}
Let $L(f,s)$ be an analytic $L$-function, and let $T>2$ be fixed.
Define 
$$\beta_{f,T}= \sup\lbrace \operatorname{Re}(\rho) : L(f,\rho)=0, \lvert\operatorname{Im}(\rho)\rvert\leq T, \operatorname{Re}(\rho)<\beta_{f,0} \rbrace.$$
One has
	$$x^{-\beta_{f,0}}\sum_{\substack{ \rho \\ L(f,\rho)=0 \\ \operatorname{Im}(\rho)\leq T}}\frac{x^{\rho}}{\rho} = 
	\sum_{\substack{ \gamma\leq T \\ L(f,\beta_{f,0} + i\gamma)=0}}\frac{x^{i\gamma}}{\beta_{f,0} + i\gamma} + O\left(x^{\beta_{f,T}-\beta_{f,0}}\left(\log(\mathfrak{q}(f)T)\right)^2\right).$$
\end{lem}

\begin{Rk}
We use the conventions: $\sup \emptyset = - \infty$ and for $x>0$ one has $x^{-\infty} = 0$.
\end{Rk}

\begin{proof}
Using \cite[Prop. 5.7.(1)]{IK}, we write
\begin{align*}
        x^{-\beta_{f,0}}\sum_{\substack { \operatorname{Re}(\rho) < \beta_{f,0} \\ \lvert\operatorname{Im}(\rho)\rvert\leq T}}\frac{x^{\rho}}{\rho}
        &\ll  x^{\beta_{f,T}-\beta_{f,0}} \sum_{\substack{ \operatorname{Re}(\rho) < \beta_{f,0} \\ \lvert\operatorname{Im}(\rho)\rvert\leq T}}\frac{1}{\lvert \rho \rvert} \\
        &\ll  x^{\beta_{f,T}-\beta_{f,0}}\left(\log(\mathfrak{q}(f)T)\right)^{2}.
	\end{align*}
	 The implicit constant is absolute.
\end{proof}

\subsection{Back to $E_{f}(x)$}\label{Subsec_ApproxFin}

The study for $\psi(f,x)$ is now almost settled.
However $E_{f}(x)$ contains another term of potential equal interest.

\begin{lem}\label{Lm_f2}
Let $L(f,s)$ be an analytic $L$-function, 
one has
\begin{align}
\theta(f,x) := \sum_{p\leq x}\lambda_{f}(p)\log p  = \psi(f,x) +m(L(f^{(2)},\cdot),1)x^{\frac{1}{2}} + o_{f}(x^{\frac{1}{2}}).
\end{align}
\end{lem}

\begin{proof}
The Ramanujan--Petersson Conjecture and the Prime Number Theorem yield
$$\sum_{p\leq x}\lambda_{f}(p)\log p = \psi(f,x) 
- \sum_{p^{2}\leq x}\left(\sum_{j=1}^{d} \alpha_{j}(p)^2\right) \log p
+ O(dx^{1/3}).$$
To evaluate the second term, we use Wiener--Ikehara's Tauberian Theorem for the function
$\frac{L'(f^{(2)},s)}{L(f^{(2)},s)}$ (see e.g. \cite[II.7.5]{Tenenbaum}). 
According to Definition \ref{Def_Lfunc}(\ref{Hyp_L2isLfunct}), this function extends meromorphically to the region $\operatorname{Re}(s) \geq 1$, with no poles except a simple pole at $s = 1$ with residue $-m(L(f^{(2)},\cdot),1)$.
We obtain
$$\sum_{p^{2}\leq x}\left(\sum_{j=1}^{d} \alpha_{j}(p)^2\right) \log p 
= -m(L(f^{(2)},\cdot),1)\sqrt{x} + o_{f}(\sqrt{x}).$$
\end{proof}

Finally, using Stieltjes integral, we write $E_{f}(x) = \frac{\log x}{x^{\beta_{f,0}}} \int_{2}^{x} \frac{{\mathop{}\!\mathrm{d}} (\theta(f,t) + m(L(f,\cdot),1)t)}{\log t}$.
By integration by parts we get
\begin{multline*}
 x^{\beta_{f,0}} E_{f}(x) =  \psi(f,x) + m(L(f,\cdot),1)x + m(L(f^{(2)},\cdot),1)x^{\frac{1}{2}} \\ +
 O\left(\log x  \int_{2}^{x} \frac{\psi(f,t) + m(L(f,\cdot),1)t}{t(\log t)^{2}}{\mathop{}\!\mathrm{d}} t \right)
 + o_{f}(x^{\frac{1}{2}}). 
\end{multline*}
We use again an integration by parts to evaluate the $O$ term.
From (\ref{Eq_ExplicitFormula_x}) and (\ref{Form_bound_queueSum}), after integrating and letting $T\rightarrow\infty$, we have
$$ \int_{2}^{x} \left(\psi(f,t) + m(L(f,\cdot),1)t\right){\mathop{}\!\mathrm{d}} t =
 - \sum_{L(f,\rho)=0}\frac{x^{\rho+1}}{\rho(\rho +1)}  + O_{f}(x(\log x)^{2}).$$
This series converges absolutely, so we are allowed to permute the limits.
We deduce that the $O$ term is $O_{f}(x^{\beta_{f,0}}/\log x)$.
Hence we have 
\begin{align}\label{Form_E_psi}
E_{f}(x) =  \frac{1}{ x^{\beta_{f,0}} }\left(\psi(f,x) + m(L(f,\cdot),1)x\right) + m(L(f^{(2)},\cdot),1)x^{\frac{1}{2}-\beta_{f,0}} 
 + o_{f}(1).
\end{align}

\subsection{Existence of the limiting distribution}\label{sub_ExistenceLimDist}

We can now prove Proposition~\ref{Prop_LimOfDist}. 
In particular we prove the first point of Theorem~\ref{Th_DistLim}: the existence of the limiting distribution for the function $E_{f}$.

Define (see Proposition \ref{Prop_LimOfDist}),
\begin{align*}
G_{f,T}(x) = \frac{m(L(f,\cdot),\beta_{f,0})}{\beta_{f,0}} + m(L(f^{(2)},\cdot),1)\delta_{\beta_{f,0}=\frac{1}{2}}
 -\sum_{ \gamma\in\mathcal{Z}_{f}^{*}(T)}2\operatorname{Re}\left(m(L(f,\cdot),\beta_{f,0} + i\gamma)\frac{x^{i\gamma}}{\beta_{f,0} + i\gamma}\right).
\end{align*}
We use (\ref{Form_E_psi}) where we evaluate $\psi(f,x)x^{-\beta_{f,0}}$ using Proposition~\ref{Prop_decomp_psi}
and the estimate for $\Sigma_{1}$ given in Lemma~\ref{Lm_beta0}.
We can now write
\begin{align}\label{Form_finale}
E_{f}(x)= G_{f,T}(x)
+ O_{f}\left(x^{\beta_{f,T}-\beta_{f,0}}(\log T)^2\right) 
- \epsilon_{f}(x,T)
 + o(1)
\end{align}
where the second term vanishes if the Riemann Hypothesis is satisfied.
We first prove that the real function $G_{f,T}$ admits a logarithmic distribution.

\begin{lem}\label{Lm_LimDistG}
Let $T>2$ fixed.
Then $G_{f,T}$ admits a limiting logarithmic distribution $\mu_{f,T}$.
\end{lem}

\begin{proof} 
This follows from the Kronecker--Weyl Theorem (see \cite[Lem. 2.3]{RS} or \cite[Prop. 2.4]{ANS}).
Precisely the version of the Kronecker--Weyl Theorem we need is the following (see \cite{MOF_KW}).
\begin{lem}[Kronecker--Weyl]\label{Lm_KroneckerWeyl}
Let $t_{1},\ldots,t_{N}$ be arbitrary real numbers.
Suppose $A$ is the topological closure of the $1$-parameter group $\lbrace y(t_{1},\ldots,t_{N}) : y\in\mathbf{R}\rbrace/\mathbf{Z}^{N}$ in the $N$-dimensional torus $\mathbf{T}^{N}:= (\mathbf{R}/\mathbf{Z})^{N}$.
Let $h: \mathbf{T}^{N}\rightarrow \mathbf{R}$ be a continuous function.
Then $A$ is a sub-torus of $\mathbf{T}^{N}$ and we have
\begin{equation}\label{Form_KW}
\lim_{Y\rightarrow\infty}\frac{1}{Y}\int_{0}^{Y}h(yt_{1},\ldots,yt_{N}){\mathop{}\!\mathrm{d}} y
= \int_{A}h(a){\mathop{}\!\mathrm{d}}\omega_{A}
\end{equation}
where $\omega_{A}$ is the normalized Haar measure on $A$.
\end{lem}
We can now show that $G_{f,T}$ admits a limiting logarithmic distribution $\mu_{f,T}$.
We write $\mathcal{Z}_{f}^{*}(T) =\lbrace \gamma_{1},\ldots,\gamma_{N(T)}\rbrace$.
Let $g:\mathbf{R}\rightarrow\mathbf{R}$ be a bounded Lipschitz continuous function,
one can associate to $g$ the continuous function on $\mathbf{T}^{N(T)}$ defined by
\begin{align}\label{form_tilde}
\tilde{g}(t)=  g\left(m_{f} - 2\operatorname{Re}\left(\sum_{k=1}^{N(T)}\frac{e^{2i\pi t_{k}}}{\beta_{f,0}+i\gamma_{k}}\right)\right).
\end{align}
One has 
$$\int_{2}^{Y} g(G_{f,T}(e^{y})){\mathop{}\!\mathrm{d}} y = \int_{2}^{Y} \tilde{g}\left(\frac{\gamma_{1}}{2\pi}y,\ldots,\frac{\gamma_{N(T)}}{2\pi}y\right){\mathop{}\!\mathrm{d}} y.$$
The Kronecker--Weyl Theorem gives the conclusion: 
the measure $\mu_{f,T}$ is the pull-back of the normalised Haar measure on the closure of 
$\lbrace (\frac{\gamma_{1}}{2\pi}y,\ldots,\frac{\gamma_{N(T)}}{2\pi}y) : y\in \mathbf{R}\rbrace/\mathbf{Z}^{N(T)}$ in $\mathbf{T}^{N(T)}$. 
\end{proof}

Next using (\ref{Form_finale}) we prove that $E_{f}$ admits a limiting logarithmic distribution.
Let $g$ be a continuous $C_{g}$-Lipschitz bounded function, one has 
\begin{multline*}
\int_{2}^{Y} g(E_{f}(e^{y})){\mathop{}\!\mathrm{d}} y = \int_{2}^{Y}g(G_{f,T}(e^{y})){\mathop{}\!\mathrm{d}} y 
+ O\left( C_{g}\int_{2}^{Y}e^{y(\beta_{f,T}-\beta_{f,0})}{\mathop{}\!\mathrm{d}} y\right) \\
+ O\left( C_{g}\int_{2}^{Y}\lvert \epsilon_{f}(e^{y},T)\rvert {\mathop{}\!\mathrm{d}} y \right)
+ o(C_{g} Y)
\end{multline*}
Dividing by $Y$, 
we can bound the inferior and superior limits as $Y \rightarrow \infty$
using the fact that $G_{f,T}$ has a limiting logarithmic distribution and Lemma \ref{Lm_epsilon} (with the Cauchy--Schwarz inequality). One has
\begin{multline}\label{Formule_BoundsLimDist}
\int_{\mathbf{R}}g {\mathop{}\!\mathrm{d}}\mu_{f,T} + O_{f}\left(C_{g} \frac{\log T }{\sqrt{T}} \right) \leq
\liminf_{Y\rightarrow\infty} \frac{1}{Y}  \int_{2}^{Y}g(E_{f}(e^{y})) {\mathop{}\!\mathrm{d}} y \\
\leq \limsup_{Y\rightarrow\infty} \frac{1}{Y}  \int_{2}^{Y}g(E_{f}(e^{y})) {\mathop{}\!\mathrm{d}} y  \leq
\int_{\mathbf{R}}g {\mathop{}\!\mathrm{d}} \mu_{f,T} + O_{f}\left(C_{g} \frac{\log T}{\sqrt{T}} \right).
\end{multline}
Then take $T$ arbitrary large, this proves that the inferior and superior limits coincide.

As in the proof of \cite[Th. 2.9]{ANS}, we apply Helly's Selection Theorem to the sequence of probability measures $(\mu_{f,T})_{T\geq 1}$;
this ensures the existence of a probability measure $\mu_{f}$ such that
for every continuous Lipschitz bounded function $g$ one has
\begin{align*}
\lim_{Y\rightarrow \infty} \frac{1}{Y}  \int_{2}^{Y}g(E_{f}(e^{y})) {\mathop{}\!\mathrm{d}} y   = \int_{\mathbf{R}} g {\mathop{}\!\mathrm{d}}\mu_{f}.
\end{align*} 
It concludes the proof of Proposition~\ref{Prop_LimOfDist}.

\begin{Rk}
To come back to the formulation of \cite{ANS}, 
we have used a more general result for functions of the type
$$\phi(y) = c + \operatorname{Re}\left( \sum_{\gamma_{n}\leq T} r_{n} e^{i\gamma_{n}y} \right) + \epsilon(y,T) + R(y,T),$$
for any $T$ large enough,
where 

$\left\{\begin{tabular}{@{}l}
$(\gamma_{n})_{n\in\mathbf{N}}$ is a non-decreasing sequence of positive numbers which tends to infinity,\\
$(r_{n})_{n\in\mathbf{N}}$ is a sequence of complex numbers, \\
  one has
$\lim_{T\rightarrow \infty} \lim_{Y\rightarrow \infty} \frac{1}{Y}\int_{2}^{Y} \lvert \epsilon(y,T)\rvert^{2} {\mathop{}\!\mathrm{d}} y =0$, \\
and for every $T\geq 2$, one has
$\lim_{Y\rightarrow \infty} \frac{1}{Y}\int_{2}^{Y} \lvert R(y,T)\rvert {\mathop{}\!\mathrm{d}} y =0.$
\end{tabular}\right.$

Those are almost periodic functions, and this point is reminiscent of \cite[Th. 2.9]{ANS}.
\end{Rk}

\subsection{Mean and Variance}\label{Subsec_MeanVar}
We complete the proof of Theorem~\ref{Th_DistLim} by showing the results on the decay of $\mu_{f}$ and computing its mean and variance.

Using (\ref{Formule_BoundsLimDist}) and information on the support of $\mu_{f,T}$ we can show that $\mu_{f}$ has exponential decay.
\begin{lem}\label{Lm_expDecay}
There exists a positive constant $c(f)$ depending only on $f$ such that
$$\mu_{f}(\mathbf{R} - [m_{f} - R, m_{f} + R])\ll_{f} e^{-c(f)\sqrt{R}}.$$
\end{lem}
\begin{proof}
One has 
\begin{align*}
\left\lvert\sum_{ \gamma\in\mathcal{Z}_{f}^{*}(T)}2\operatorname{Re}\left(m(L(f,\cdot),\beta_{f,0} + i\gamma)\frac{x^{i\gamma}}{\beta_{f,0} + i\gamma}\right)\right\rvert 
&\leq  \sum_{ \gamma\in\mathcal{Z}_{f}(T)}2\frac{1}{\lvert \beta_{f,0} + i\gamma\rvert} \\
&\ll_{f} (\log T)^{2}
\end{align*}
thanks to the fact that the completed $L$-function is of order $1$ (see Definition~\ref{Def_Lfunc}(\ref{Hyp_FunctEquation})).
Therefore the function $G_{f,T}(e^{y})$ is bounded.
We deduce that the measure $\mu_{f,T}$ has compact support included in 
$[m_{f} - c_{1}(\log T)^2 , m_{f} + c_{1}(\log T)^2]$ for some constant $c_{1}$ depending on $f$.
Using (\ref{Formule_BoundsLimDist}), we have
$$\mu_{f}(\mathbf{R} - [m_{f} - c_{1}(\log T)^2 , m_{f} + c_{1}(\log T)^2]) = O_{f}\left( \frac{\log T}{\sqrt{T}} \right).$$
Set $R=c_{1}(\log T)^2$, the result follows.
\end{proof}

The measure $\mu_{f}$ has exponential decay at infinity, hence it has finite moments.
The values for the mean and variance given in Theorem \ref{Th_DistLim} follow from computations for $\mu_{f,T}$, letting $T$ be arbitrarily large as we now explain (see \cite{FioEC}).
Let $T\geq2$ be fixed, one has
\begin{align*}
\int_{\mathbf{R}}t{\mathop{}\!\mathrm{d}}\mu_{f,T} &=\lim_{Y\rightarrow\infty} \frac{1}{Y} \int_{2}^{Y} G_{f,T}(e^{y}){\mathop{}\!\mathrm{d}} y \\
&=  \int_{2}^{Y} \left(m_{f}  -\sum_{ \gamma\in\mathcal{Z}_{f}^{*}(T)}2\operatorname{Re}\left(m(L(f,\cdot),\beta_{f,0} + i\gamma)\frac{e^{iy\gamma}}{\beta_{f,0} + i\gamma}\right)\right) {\mathop{}\!\mathrm{d}} y \\
&= m_{f} - \lim_{Y\rightarrow\infty} \frac{1}{Y}  O\left(\sum_{ \gamma\in\mathcal{Z}_{f}^{*}(T)}2m(L(f,\cdot),\beta_{f,0} + i\gamma)\frac{1}{\lvert\beta_{f,0} + i\gamma\rvert \lvert\gamma\rvert} \right) \\
&= m_{f}
\end{align*}
because the sum over $\mathcal{Z}_{f}^{*}(T)$ is finite.
We deduce that 
\begin{equation*}
\int_{\mathbf{R}}t{\mathop{}\!\mathrm{d}}\mu_{f} = m_{f}.
\end{equation*}
Hence the assertion on the mean of $\mu_{f}$ is proved.

For the computation of the variance, we cannot use linearity, we go back to the general case.
Set 
$$G_{\mathcal{S},T}(x) = m_{\mathcal{S}} - \sum_{\gamma\in \mathcal{Z}^{*}_{\mathcal{S}}(T)} 2\operatorname{Re}\left(M(\gamma)\frac{x^{i\gamma}}{\beta_{\mathcal{S},0} + i\gamma}\right)$$
where for $\gamma$ in $\mathcal{Z}_{\mathcal{S}}^{*}$, we denote $M(\gamma) = \sum_{f\in\mathcal{S}}a_{f} m(L(f,\cdot),\beta_{\mathcal{S},0} + i\gamma)$.
Then
\begin{align*}
\int_{\mathbf{R}}\lvert t - m_{\mathcal{S}}\rvert^{2}{\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T}
&= \lim_{Y\rightarrow\infty} \frac{1}{Y}\int_{2}^{Y} \left\lvert\sum_{\gamma\in \mathcal{Z}^{*}_{\mathcal{S}}(T)} \left(\frac{M(\gamma)e^{iy\gamma}}{\beta_{0} + i\gamma} + \frac{M(-\gamma)e^{-iy\gamma}}{\beta_{0} - i\gamma}\right)\right\rvert^{2} {\mathop{}\!\mathrm{d}} y \\
&= \lim_{Y\rightarrow\infty} \frac{1}{Y}{\mathop{\sum \Bigl.^{*}}\limits}_{\gamma,\lambda}\frac{M(\gamma)\overline{M(\lambda)}}{(\beta_{0} + i\gamma)(\beta_{0} - i\lambda)}\int_{2}^{Y}e^{i(\gamma-\lambda)y}{\mathop{}\!\mathrm{d}} y
\end{align*}
where the $\gamma$, $\lambda$ are in the index set $\mathcal{Z}_{\mathcal{S}}^{*}(T)\cup (- \mathcal{Z}_{\mathcal{S}}^{*}(T))$ (counted without multiplicities).
The diagonal term $\lambda=\gamma$ is the main term: 
$$\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}
\frac{2\lvert M(\gamma)\rvert^{2}}{\lvert \beta_{\mathcal{S},0} + i\gamma \rvert^{2}}.$$
The off-diagonal term vanishes as $Y\rightarrow\infty$ when $T$ is fixed.
One has 
\begin{align*}
{\mathop{\sum \Bigl.^{*}}\limits}_{\gamma\neq\lambda}\frac{M(\gamma)\overline{M(\lambda)}}{(\beta_{\mathcal{S},0} + i\gamma)(\beta_{\mathcal{S},0} - i\lambda)}\int_{2}^{Y}e^{i(\gamma-\lambda)y}{\mathop{}\!\mathrm{d}} y
&= O\left(  {\mathop{\sum \Bigl.^{*}}\limits}_{\gamma\neq\lambda}\frac{\lvert M(\gamma)\rvert\lvert M(\lambda)\rvert}{\lvert\gamma\rvert\lvert\lambda\rvert}\min(Y,\lvert \gamma-\lambda\rvert^{-1}) \right).
\end{align*}
Using \cite[Lem. 2.6]{FioEC},
we deduce that
\begin{equation*}
\int_{\mathbf{R}}\lvert t - m_{\mathcal{S}}\rvert^{2}{\mathop{}\!\mathrm{d}}\mu_{\mathcal{S}} = 
\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}}
\frac{2\lvert M(\gamma)\rvert^{2}}{\lvert \beta_{\mathcal{S},0} + i\gamma \rvert^{2}}.
\end{equation*}

This concludes the proof of Theorem~\ref{Th_DistLim}.
 

\section{Results under additional hypotheses}\label{Sec_AddHyp}

It is clear from the proof of Theorem \ref{Th_DistLim} 
that the properties of the set of non trivial $L$-function zeros of largest real part are related to the properties of $\mu_{\mathcal{S}}$.
In this section we investigate in more details what can be infered from additional hypotheses on the zeros.

\subsection{Existence of the bias and regularity of the distribution}\label{sub_Indep}

We show that the existence of self-sufficient zeros in $\mathcal{Z}_{\mathcal{S}}^{*}$ 
gives properties of smoothness for $\mu_{\mathcal{S}}$. 
Such results were previously obtained (e.g. in \cite{RS}) conditionally on LI.
Our main contribution in the following result is that we get the existence of the logarithmic density $\delta(\mathcal{S})$ (as defined in Definition~\ref{Def_logdens}) under a weaker hypothesis than LI.

\begin{prop}\label{Prop_Tautosuff}
	Suppose that there exists $\epsilon >0$ and $T_{\epsilon} >0$
	such that for every $T\geq T_{\epsilon}$ the set $\mathcal{Z}_{\mathcal{S}}^{*}(T^{\frac{1}{2} - \epsilon})$ 
	contains a $(T^{\frac{1}{2} - \epsilon},T)$-self-sufficient zero $\gamma_{T}$.
	Then $\delta(\mathcal{S})$ exists.
\end{prop}

\begin{proof}
Fix $T\geq T_{\epsilon}$.
	Following \cite[Part 3.1]{RS}, we compute the Fourier transform of $\mu_{\mathcal{S},T}$.
	We obtain
\begin{align*}
\hat{\mu}_{\mathcal{S},T}(\xi) &= \int_{A_{T}}\exp\left( -i\xi \left( m_{\mathcal{S}} - 2\operatorname{Re}\left(\sum_{\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)}\frac{M(\gamma)e^{2i\pi t_{\gamma}}}{\beta_{\mathcal{S},0}+i\gamma}\right) \right) \right){\mathop{}\!\mathrm{d}} t \\
&= e^{-im_{\mathcal{S}}\xi}\int_{A_{T}}\prod_{\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)}\exp\left(i\xi 2\operatorname{Re}\left(M(\gamma)\frac{e^{2i\pi t_{\gamma}}}{\beta_{\mathcal{S},0}+i\gamma}\right) \right){\mathop{}\!\mathrm{d}} t
\end{align*}
where $A_{T}$ is the closure of $\lbrace (\frac{\gamma_{1}}{2\pi}y,\ldots,\frac{\gamma_{N(T)}}{2\pi}y) : y\in \mathbf{R}\rbrace/\mathbf{Z}^{N(T)}$ in $\mathbf{T}^{N(T)}$.
The ordinate $\gamma_{T}$ is self-sufficient in $\mathcal{Z}_{\mathcal{S}}^{*}(T)$, hence one can write $A_{T}= \mathbf{T}\times A_{T}'$ and separate the integral:
\begin{multline*}
\hat{\mu}_{\mathcal{S},T}(\xi) = e^{-im_{\mathcal{S}}\xi}\int_{A'_{T}}\prod_{\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)-\lbrace\gamma_{0}\rbrace}\exp\left(i\xi 2\operatorname{Re}\left(M(\gamma)\frac{e^{2i\pi t_{\gamma}}}{\beta_{\mathcal{S},0}+i\gamma}\right) \right){\mathop{}\!\mathrm{d}} t \\
\int_{\mathbf{T}}\exp\left(i\xi 2\operatorname{Re}\left(M(\gamma_{T})\frac{e^{2i\pi \theta}}{\beta_{\mathcal{S},0}+i\gamma_{T}}\right) \right){\mathop{}\!\mathrm{d}}\theta.
\end{multline*}
The integral over $\mathbf{T}$ is a $0$-th Bessel function of the first kind:
\begin{align*}
\int_{\mathbf{T}}\exp\left(i\xi 2\operatorname{Re}\left(M(\gamma_{T})\frac{e^{2i\pi \theta}}{\beta_{\mathcal{S},0}+i\gamma_{T}}\right) \right){\mathop{}\!\mathrm{d}}\theta = J_{0}\left(\left\lvert\frac{2\xi M(\gamma_{T})}{\beta_{\mathcal{S},0} + i\gamma_{T}} \right\rvert\right).
\end{align*}
Using properties of the Bessel function (see e.g. \cite{Watson}) and 
the fact that the first integral on the right hand side is bounded from above by $1$, 
one can bound the Fourier transform of $\mu_{\mathcal{S},T}$:
\begin{align}\label{Bound_1T-Aut}
\lvert \hat{\mu}_{\mathcal{S},T}(\xi)\rvert &\leq \left\lvert J_{0}\left(\left\lvert\frac{2\xi M(\gamma_{T})}{\beta_{\mathcal{S},0} + i\gamma_{T}} \right\rvert\right) \right\rvert \\
&\leq \min\left( 1, \sqrt{\left\lvert\frac{\beta_{\mathcal{S},0} + i\gamma_{T}}{ \pi\xi M(\gamma_{T})}\right\rvert}  \right).\nonumber
\end{align}

Let us come back to the existence of $\delta(\mathcal{S})$.
We want to prove that the limits $$\limsup\frac{1}{Y}\int_{2}^{Y}\mathbf{1}_{\geq 0}(E_{\mathcal{S}}(e^{y})){\mathop{}\!\mathrm{d}} y$$
and
$$\liminf\frac{1}{Y}\int_{2}^{Y}\mathbf{1}_{\geq0}(E_{\mathcal{S}}(e^{y})){\mathop{}\!\mathrm{d}} y$$ coincide.
We write $\mathbf{1}_{\geq 0} = g_{n} + (\mathbf{1}_{\geq 0} - g_{n})$ where $g_n$ is the $n$-Lipschitz function satisfying
$$g_{n}(x)=\left\lbrace
\begin{array}{ll}
0 & \mbox{if $x\leq -1/2n$,}\\
1 & \mbox{if $x\geq 1/2n$,}\\
nx  + 1/2 & \mbox{otherwise.}
\end{array}
\right.$$
The functions $g_n$ and $\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert$ are bounded, continuous, $n$-Lipschitz.
Hence
\begin{align*}
\lim_{Y\rightarrow\infty}\frac{1}{Y}\int_{2}^{Y} g_{n}(G_{\mathcal{S},T}(e^{y})){\mathop{}\!\mathrm{d}} y = \int_{\mathbf{R}}g_{n}(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T}(t),
\end{align*}
and we have the same result if we replace $g_{n}$ by $\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert$.
Taking $T$ arbitrarily large, we approximate the limiting distribution $\mu_{\mathcal{S}}$.
Precisely one has
\begin{multline}\label{Expr_Indic}
\int_{\mathbf{R}}g_{n}(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} - \int_{\mathbf{R}}\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T}  + O_{\mathcal{S}}\left(n \frac{\log T}{\sqrt{T}} \right) \\ 
\leq
\liminf_{Y\rightarrow\infty} \frac{1}{Y}  \int_{2}^{Y}\mathbf{1}_{\geq 0}(E_{\mathcal{S}}(e^{y})) {\mathop{}\!\mathrm{d}} y 
\leq \limsup_{Y\rightarrow\infty} \frac{1}{Y}  \int_{2}^{Y}\mathbf{1}_{\geq 0}(E_{\mathcal{S}}(e^{y})) {\mathop{}\!\mathrm{d}} y \\ 
\leq
\int_{\mathbf{R}}g_{n}(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} + \int_{\mathbf{R}}\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} + O_{\mathcal{S}}\left(n \frac{\log T}{\sqrt{T}} \right).
\end{multline}

Moreover we can bound $\mu_{\mathcal{S},T}(\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert)$.
Using Parseval's formula (see e.g. \cite[Th. VI.2.2]{Katznelson}) we have:
\begin{align*}
\int_{\mathbf{R}}\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert {\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} &=
\int_{\mathbf{R}} 2n\frac{1-\cos(\xi/2n)}{\xi^{2}}\hat{\mu}_{\mathcal{S},T}(\xi){\mathop{}\!\mathrm{d}}\xi \\
&\ll \int_{\lvert\xi\rvert\leq \alpha(n)} \frac{1}{2n}\lvert\hat{\mu}_{\mathcal{S},T}(\xi)\rvert {\mathop{}\!\mathrm{d}}\xi    +     \int_{\lvert\xi\rvert\geq \alpha(n)} \frac{4n}{\xi^{2}}\lvert\hat{\mu}_{\mathcal{S},T}(\xi)\rvert {\mathop{}\!\mathrm{d}}\xi
\end{align*}
for $\alpha(n) <2n$.
Using (\ref{Bound_1T-Aut}), we get:
\begin{align*}
\int_{\mathbf{R}}\lvert \mathbf{1}_{\geq 0} - g_{n}\rvert {\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} 
&\ll  \frac{2\alpha(n)}{2n}    +     \int_{\lvert\xi\rvert\geq \alpha(n)} \frac{4n\sqrt{\gamma_{T}}}{\lvert\xi\rvert^{5/2}} {\mathop{}\!\mathrm{d}}\xi \\
&\ll \frac{\alpha(n)}{n} + \frac{n\sqrt{\gamma_{T}}}{\alpha(n)^{3/2}}.
\end{align*}
Choose $n=\sqrt{T^{1-\epsilon}}$ and $\alpha(n) = n^{1-\frac{\epsilon}{3}}$.
Since $\gamma_{T} \leq T^{\frac{1}{2}-\epsilon}$, 
the terms of rest in (\ref{Expr_Indic}) vanish as $T\rightarrow+\infty$.
It ensures that the inferior and superior limits coincide.
\end{proof}

Proposition~\ref{Prop_Tautosuff} is Theorem~\ref{Th_withLI}(\ref{Th1_Tselfsuff}), 
to prove the other points of Theorem~\ref{Th_withLI} we follow the same idea without dependence on $T$.

\begin{proof}[Proof of Theorem~\ref{Th_withLI}(\ref{Th1_LI1})]
The fact that $\mu_{\mathcal{S}}$ is continuous is a consequence of a theorem of Wiener (see \cite[Th. VI.2.11]{Katznelson}).
A necessary and sufficient condition for $\mu_{\mathcal{S}}$ to be continuous is:
\begin{align}\label{Cond_Wiener}
\lim_{Y\rightarrow\infty}\frac{1}{2Y}\int_{-Y}^{Y}\lvert \hat{\mu}_{\mathcal{S}}(\xi)\rvert^{2}{\mathop{}\!\mathrm{d}}\xi =  0.
\end{align}
In the case $\gamma_{T}= \gamma_{0}$ does not depend on $T$, the bound (\ref{Bound_1T-Aut}) becomes, for all $T>\gamma_{0}$,
\begin{align*}
\lvert \hat{\mu}_{\mathcal{S},T}(\xi)\rvert 
\leq \min\left( 1, \sqrt{\left\lvert\frac{\beta_{\mathcal{S},0} + i\gamma_{0}}{ \pi\xi M(\gamma_{0})}\right\rvert}  \right).
\end{align*}
Letting $T\rightarrow\infty$, the same bound holds for $\hat{\mu}_{\mathcal{S}}$.
In particular Condition~(\ref{Cond_Wiener}) holds. 
\end{proof}

\begin{proof}[Proof of Theorem~\ref{Th_withLI}(\ref{Th2_LI3})]
Let $\gamma_{1} <\gamma_{2} <\gamma_{3}$ be three self-sufficient elements of $\mathcal{Z}^{*}_{\mathcal{S}}$.
Following the lines of the previous proofs, we get that for all $T> \gamma_{3}$ one has 
\begin{align*}
\lvert \hat{\mu}_{\mathcal{S},T}(\xi)\rvert 
\leq \prod_{j=1}^{3}\min\left( 1, \sqrt{\left\lvert\frac{\beta_{\mathcal{S},0} + i\gamma_{j}}{ \pi\xi M(\gamma_{j})}\right\rvert}  \right)
\end{align*}
Letting $T\rightarrow\infty$, the same bound holds for $\hat{\mu}_{\mathcal{S}}$.
In particular one has $\hat{\mu}_{\mathcal{S}}\in L^{1}\cap L^{2}$,
Theorem~\ref{Th_withLI}(\ref{Th2_LI3}) follows by Fourier inversion.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{Th_withLI}(\ref{Th3_enoughIndep})]
As in the previous proofs we can write for all $T>0$, for all $\xi$,
\begin{align*}
\lvert \hat{\mu}_{\mathcal{S},T}(\xi) \rvert \leq \prod_{\substack{\gamma \in \mathcal{Z}^{*}_{\mathcal{S}}(T) \\ \text{self-sufficient}}} \min\left( 1, \sqrt{\left\lvert\frac{\beta_{\mathcal{S},0} + i\gamma}{ \pi\xi M(\gamma)}\right\rvert}  \right)
\end{align*}
We assume that there are infinitely many self-sufficient elements in $\mathcal{Z}^{*}_{\mathcal{S}}$.
For each $n \in \mathbf{N}$ there exists $T_{n} >0$ such that
$$\lvert \lbrace \gamma \in \mathcal{Z}^{*}_{\mathcal{S}}(T_{n}) : \gamma \text{ self-sufficient }\rbrace\rvert \geq 2n +1.$$
Hence there exists a constant $C_{n}$ depending only on $\mathcal{Z}^{*}_{\mathcal{S}}(T_{n})$ such that for every $T\geq T_{n}$, for every $\xi$ large enough (in terms of $T_{n}$) one has
\begin{align*}
\lvert \hat{\mu}_{\mathcal{S},T}(\xi) \rvert \leq \frac{C_{n}}{\lvert\xi\rvert^{n}\sqrt{\lvert \xi\rvert}}.
\end{align*}
Letting $T\rightarrow\infty$, the same bound holds for $\hat{\mu}_{\mathcal{S}}$.
In particular one has $\lvert \xi^{n}\hat{\mu}_{\mathcal{S}}(\xi)\rvert \rightarrow 0$ as $\lvert \xi\rvert \rightarrow \infty$.
By Fourier inversion, we get that the density of $\mu_{\mathcal{S}}$ is indefinitely differentiable.

The statement about fast decay is a consequence of the exponential decrease obtained in Theorem~\ref{Th_DistLim}.
\end{proof}

In the previous proofs we have used the decay at infinity of the Bessel $0$-th function $J_{0}$ 
to obtain the bounds for $\hat{\mu}_{\mathcal{S}}$.
Using the theory of oscillatory integrals we can deduce the decay of other functions.
We can in fact have condition~(\ref{Cond_Wiener}) under a weaker hypothesis.
\begin{prop}\label{Prop_SomeSelfSufficience}
Suppose that there exists $N\geq 1$ and $\lambda_{1},\ldots,\lambda_{N} \in \mathcal{Z}_{\mathcal{S}}^{*}$ such that 
\begin{align*}
\langle\lambda_{1},\ldots,\lambda_{N}\rangle_{\mathbf{Q}} \cap \langle \mathcal{Z}_{\mathcal{S}}^{*} - \lbrace \lambda_{1},\ldots,\lambda_{N}  \rbrace \rangle_{\mathbf{Q}} = \lbrace 0 \rbrace.
\end{align*}
Then the distribution $\mu_{\mathcal{S}}$ is conutinuous.
\end{prop}

\begin{proof}
The hypothesis implies that for every $T\geq \max\lbrace\lambda_{j} : 1\leq j\leq N \rbrace$, 
the sub-torus $A_{T}$ given by
the topological closure of the $1$-parameter group 
$\left\lbrace \frac{y}{2\pi} \left(\gamma_{1},\ldots,\gamma_{N(T)}\right) : y\in\mathbf{R} \right\rbrace/\mathbf{Z}^{N(T)}$
can be written as a cartesian product $\mathbf{T}(\lambda_{1},\ldots,\lambda_{N}) \times A'_{T}$
where the two components are respectively the sub-torus associated with the set $\lbrace \lambda_{1},\ldots,\lambda_{N}\rbrace$
and the set $\mathcal{Z}_{\mathcal{S}}^{*}(T) - \lbrace \lambda_{1},\ldots,\lambda_{N}  \rbrace$.
In particular for $T\geq \max\lbrace\lambda_{j} : 1\leq j\leq N \rbrace$,
the Fourier Transform of $\mu_{\mathcal{S},T}$ is
\begin{multline*}
\hat{\mu}_{\mathcal{S},T}(\xi) = e^{-im_{\mathcal{S}}\xi}
\int_{A'_{T}}\exp\left(2i\xi \sum_{\gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)-\lbrace\lambda_{1},\ldots,\lambda_{N}\rbrace}\operatorname{Re}\left(M(\gamma)\frac{e^{2i\pi t_{\gamma}}}{\beta_{\mathcal{S},0}+i\gamma}\right) \right){\mathop{}\!\mathrm{d}} \omega_{A_{T}}(t) \\
\int_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}\exp\left(2i\xi \sum_{j=1}^{N}\operatorname{Re}\left(M(\lambda_{j})\frac{e^{2i\pi \theta_{j}}}{\beta_{\mathcal{S},0}+i\lambda_{j}}\right) \right){\mathop{}\!\mathrm{d}}\omega_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}(\theta).
\end{multline*}
We deduce that
\begin{align}\label{Ineq_Fourier_some_selfsuff}
\lvert\hat{\mu}_{\mathcal{S},T}(\xi) \rvert \leq \left\lvert 
\int_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}\exp\left(2i\xi \sum_{j=1}^{N}\left\lvert\frac{M(\lambda_{j})}{\beta_{\mathcal{S},0}+i\lambda_{j}}\right\rvert \cos(2\pi \theta_{j} + w_{j}) \right){\mathop{}\!\mathrm{d}}\omega_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}(\theta)
\right\rvert
\end{align}
where $w_{j}$ is defined by $\frac{M(\lambda_{j})}{\beta_{\mathcal{S},0}+i\lambda_{j}} = \left\lvert \frac{M(\lambda_{j})}{\beta_{\mathcal{S},0}+i\lambda_{j}}\right\rvert e^{iw_{j}}$.
The right hand side of inequality~(\ref{Ineq_Fourier_some_selfsuff}) does not depend on $T$, hence for $\xi$ fixed, we can let $T\rightarrow\infty$ and obtain the same inequality for $\hat{\mu}_{\mathcal{S}}(\xi)$.

Thus we only need to prove that the right hand side of (\ref{Ineq_Fourier_some_selfsuff}) 
approaches $0$ when $\lvert\xi\rvert\rightarrow\infty$ to ensure condition~(\ref{Cond_Wiener}).
The function
$$\phi : \left\lbrace
\begin{array}{l l}
\mathbf{T}(\lambda_{1},\ldots,\lambda_{N}) & \rightarrow \mathbf{R} \\
(\theta_{1},\ldots,\theta_{N}) &\mapsto 2\sum_{j=1}^{N}\left\lvert\frac{M(\lambda_{j})}{\beta_{\mathcal{S},0}+i\lambda_{j}}\right\rvert \cos(2\pi \theta_{j} + w_{j})
\end{array}
\right.$$
is a non-constant analytic function on a compact set (see the proof of this assertion in Appendix~\ref{App_proofNonConst}).
Hence there exists $K\geq 1$ such that for each $\theta\in \mathbf{T}(\lambda_{1},\ldots,\lambda_{N})$
there exists a multi-index $\underline{k}$ of length $1\leq \lvert \underline{k} \rvert \leq K$
satisfying $\partial^{\underline{k}}\phi(\theta) \neq 0$.
By \cite[VIII 2.2 Prop. 5]{Stein}, one has
\begin{align*}
\lvert\hat{\mu}_{\mathcal{S}}(\xi) \rvert \leq \left\lvert 
\int_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}\exp\left(i\xi 2\sum_{j=1}^{N}\left\lvert\frac{M(\lambda_{j})}{\beta_{\mathcal{S},0}+i\lambda_{j}}\right\rvert \cos(2\pi \theta_{j} + w_{j}) \right){\mathop{}\!\mathrm{d}}\omega_{\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})}(\theta)
\right\rvert \ll \lvert\xi\rvert^{-1/K}
\end{align*}
with an implicit constant depending on $K$, $\phi$, and on a choice of partition of unity adapted to $\phi$. 
In particular condition~(\ref{Cond_Wiener}) holds, hence $\mu_{\mathcal{S}}$ is continuous.
\end{proof}

\subsection{Symmetry}\label{sub_Sym}

We prove Theorem~\ref{Th_Indep_Sym} by showing that under its hypotheses, for every $T$, 
the distribution $\mu_{\mathcal{S},T}$ is symmetric with respect to its mean.
For this we use the Kronecker--Weyl Theorem (Lemma \ref{Lm_KroneckerWeyl}),
and the following result.

\begin{lem}\label{Lem_Eq_sym}
	The following assertions are equivalent:
	\begin{itemize}
		\item for all integral linear combination  $\sum_{\gamma\in\mathcal{Z}_{\mathcal{S}}}k_{\gamma}\gamma = 0$, $k_{\gamma}\in\mathbf{Z}$, one has $\sum_{\gamma\in\mathcal{Z}_{\mathcal{S}}}k_{\gamma}\equiv 0\ [\bmod\ 2]$,
		\item For every finite subset $\lbrace \gamma_{1},\ldots\gamma_{N} \rbrace \subset \mathcal{Z}_{\mathcal{S}}$, the element $\left(\frac{1}{2},\ldots,\frac{1}{2}\right)$ is in the closure of the one parameter group 
		$$\lbrace (\frac{\gamma_{1}}{2\pi}y,\ldots,\frac{\gamma_{N}}{2\pi}y) : y\in \mathbf{R}\rbrace/\mathbf{Z}^{N}$$ in $\mathbf{T}^{N}$.
	\end{itemize}
\end{lem}
\begin{proof}
	We use the $\mathbf{Q}$-orthogonal space of the set 
	$\Gamma:=\lbrace (\frac{\gamma_{1}}{2\pi}y,\ldots,\frac{\gamma_{N}}{2\pi}y) : y\in \mathbf{R}\rbrace/\mathbf{Z}^{N}$.
	\begin{align*}
	\Gamma^{\bot}=\lbrace (r_{1},\ldots,r_{N})\in\mathbf{Q}^{N} : \sum_{i=1}^{N}r_{i}\frac{\gamma_{i}}{2\pi} =0\rbrace.
	\end{align*}
	It is sufficient to consider only the elements in $\mathbf{Z}^{N}$.
	Then the closure of $\Gamma$ is
	\begin{align*}
	A = (\Gamma^{\bot})_{\bot} = \lbrace (\theta_{1},\ldots,\theta_{N}) : \forall k\in \Gamma^{\bot}, 
	\sum_{i=1}^{N}k_{i}\theta_{i} =0\ [\bmod\ 1]  \rbrace.
	\end{align*}
	Hence $\left(\frac{1}{2},\ldots,\frac{1}{2}\right)\in A$ if and only if for every $k\in\Gamma^{\bot}$ one has $\sum_{i=1}^{N}k_{i} =0 \ [\bmod\ 2]$
\end{proof}

\begin{Rk}
In the formulation of Lemma~\ref{Lem_Eq_sym}, LI is equivalent to the fact that the closure of the one parameter group generated by a finite number of ordinates is always the largest possible (i.e. the $N$-dimensional torus when there are $N$ ordinates).
The improvement in Theorem~\ref{Th_Indep_Sym}, is that we only need to know that the element $\left(\frac{1}{2},\ldots,\frac{1}{2}\right)$ is in this group to obtain the symmetry.
\end{Rk}

\begin{proof}[Proof of Theorem~\ref{Th_Indep_Sym}]
By Lemma \ref{Lm_KroneckerWeyl} and Lemma~\ref{Lem_Eq_sym}, we deduce that for all $T$ large enough, one has $A_{T} = A_{T} + (\frac{1}{2},\ldots,\frac{1}{2})$.
This way we can change variables $a \rightarrow a + (\frac{1}{2},\ldots,\frac{1}{2})$ in the integral defining $\mu_{\mathcal{S},T}$.
For every $T>1$, and for every bounded Lipschitz continous function $g$ one has
\begin{align*}
\int_{\mathbf{R}}g(t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} &=
\int_{A_{T}}\tilde{g}(a){\mathop{}\!\mathrm{d}}\omega_{A_{T}} \\
&= \int_{A_{T}}\tilde{g}\left(a+ \left(\frac{1}{2},\ldots,\frac{1}{2}\right)\right){\mathop{}\!\mathrm{d}}\omega_{A_{T}}
= \int_{\mathbf{R}}g(2m_{f}- t){\mathop{}\!\mathrm{d}}\mu_{\mathcal{S},T} 
\end{align*}
where we use the definition of $\tilde{g}$ given in (\ref{form_tilde}). 
One has $\tilde{g}((a+ (\frac{1}{2},\ldots,\frac{1}{2}))) = \tilde{h}(a)$ where $h$ is the function given by $h(x)=g(2m_{f} -x)$. 
Take $T$ arbitrarily large: by (\ref{Formule_BoundsLimDist}) the property of symmetry is true for $\mu_{\mathcal{S}}$.
\end{proof}

\begin{Rk}
In particular under the condition of Theorem~\ref{Th_Indep_Sym} for the set of the zeros of maximal real part associated to a set $\mathcal{S}$ of $L$-functions, we deduce that if the prime number race associated to $\mathcal{S}$ is biased it implies that the mean is not $0$.
So if the prime number race is biased, either RH is satisfied, or at least one of the $L$-functions vanishes at a point of $[\frac{1}{2},1)$.
\end{Rk}

\subsection{Riemann Hypothesis and support}\label{sub_Support}

We generalize \cite[Th. 1.2]{RS}.

\begin{prop}
Suppose that for every $f\in \mathcal{S}$, one has $\operatorname{Re}(a_{f})\geq 0$, and that there exists $f\in\mathcal{S}$ such that $\operatorname{Re}(a_{f})> 0$.
Under the Generalized Riemann Hypothesis for the $L$-functions in $\mathcal{S}$, we have $\operatorname{supp}(\mu_{\mathcal{S}}) = \mathbf{R}$.
In particular $$0<\underline{\delta}(\mathcal{S})\leq \overline{\delta}(\mathcal{S})<1.$$
\end{prop}

\begin{proof}
Following \cite[2.2]{RS}, we get a lower bound for $\mu_{\mathcal{S}}(\mathbf{R}-[-A,A])$.
The idea is to find a lower bound for the measure of the set $\lbrace y\leq M : E_{\mathcal{S}}(e^{y})\geq A\rbrace$ as $M$ varies.
We study $E_{\mathcal{S}}(e^{y})$ on small intervals so that it does not vary too much.
The idea of Rubinstein--Sarnak is to give a lower bound for an integral over a small interval.

Precisely, let $\epsilon>0$, and $t\geq 1$. Following \cite{RS}, define:
$$ F_{\epsilon}(t) = \frac{1}{2\epsilon}\int_{t-\epsilon}^{t+\epsilon} E_{\mathcal{S}}(e^{y}){\mathop{}\!\mathrm{d}} y.$$
Using (\ref{Form_finale}) in the case the Riemann Hypothesis is satisfied, one can write
\begin{align*}
E_{\mathcal{S}}(e^{y}) = m_{\mathcal{S}}
 -\sum_{ \gamma\in\mathcal{Z}_{\mathcal{S}}^{*}(T)}2\operatorname{Re}\left(M(\gamma)\frac{e^{i\gamma y}}{\frac{1}{2} + i\gamma}\right) 
- \epsilon_{\mathcal{S}}(e^{y},T)
 + o(1).
\end{align*}
Using Formula (\ref{Form_bound_queueSum}), one has 
\begin{align*}
\lvert \epsilon_{\mathcal{S}}(e^{y},T)\rvert \ll_{\mathcal{S}} ye^{-\frac{y}{2}} + \frac{y^{2}e^{\frac{y}{2}}}{T} + \frac{e^{\frac{y}{2}}\log T}{T}
\end{align*}
Using the fact that the sum $\sum\frac{1}{\gamma^{2}}$ is finite, we deduce that
\begin{align*}
F_{\epsilon}(t) 
= \frac{2}{\epsilon}\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)} \operatorname{Re}(M(\gamma))
\frac{\sin(\gamma t)\sin(\gamma\epsilon)}{\gamma^{2}} 
+  O_{\mathcal{S}}\left(  e^{t/2}\left(\frac{t^{2}}{T}
+ \frac{\log T}{T}\right) + 1  \right). 
\end{align*}
Letting $T\rightarrow\infty$, one gets
\begin{align}\label{Formule_petite_diff}
F_{\epsilon}(t) &=  
\frac{2}{\epsilon}\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}} \frac{\sin(\gamma t)\sin(\gamma\epsilon)}{\gamma^{2}} + O_{\mathcal{S}}(1)
\end{align}
for all $t\geq 1$.

We want to show that $F_{\epsilon}(t)$ is large for enough $t\geq1$.
Let us study the auxiliary functions
$$\tilde{F}_{\epsilon,T}(t) :=  
\frac{2}{\epsilon}\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\operatorname{Re}(M(\gamma))\frac{\sin(\gamma t)\sin(\gamma\epsilon)}{\gamma^{2}}.$$
One has
\begin{align*}
\tilde{F}_{\epsilon,T}(\epsilon)
&= 2\epsilon\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)} \operatorname{Re}(M(\gamma))
\left(\frac{\sin(\gamma\epsilon)}{\gamma\epsilon}\right)^{2} \\
&\geq c_{0} \epsilon
\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(1/\epsilon)}\operatorname{Re}(M(\gamma))
= c_{0} \epsilon N(1/\epsilon),
\end{align*}
if $T\geq 1/\epsilon$.
Where we use the fact that $\operatorname{Re}(M(\gamma))$ is always non-negative and is positive for some $\gamma$'s, here $N$ is defined by
$N(T) := \sum_{\operatorname{Re}(a_{f})>0}\lvert \mathcal{Z}_{f}(T)\rvert$.  
Under the Riemann Hypothesis, one has $N(T) \asymp T\log T$.

We can find other values of $t$ for which $\tilde{F}_{\epsilon,T}(t)$ is large enough.
For every $m\in \mathbf{N}$, one has
\begin{align*}
\lvert \tilde{F}_{\epsilon,T}(\epsilon) - \tilde{F}_{\epsilon,T}((m+1)\epsilon) \rvert
 &\leq 2\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\operatorname{Re}(M(\gamma))
 \lvert \sin(\gamma\epsilon) - \sin(\gamma(m+1)\epsilon)\rvert
\frac{\lvert\sin(\gamma\epsilon)\rvert}{\gamma^{2}\epsilon} \\
  &\leq 2\max_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\left(\lVert \gamma m\epsilon\rVert\right)
 \sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\frac{\operatorname{Re}(M(\gamma))}{\gamma}
\end{align*} 
where $\lVert \cdot \rVert$ is the distance to $2\pi\mathbf{Z}$.
To ensure that $\tilde{F}_{\epsilon,T}((m+1)\epsilon)$ is of the same order of magintude as $\tilde{F}_{\epsilon,T}(\epsilon)$, we need that
\begin{align}\label{Cond_m}
\max_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\left(\lVert \gamma m\epsilon\rVert\right) \leq 
\frac{c_{0} \epsilon N(1/\epsilon)}{4\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(T)}\frac{\operatorname{Re}(M(\gamma))}{\gamma}}.
\end{align}
Then one has  $\tilde{F}_{\epsilon,T}((m+1)\epsilon)\geq \frac{c_{0}}{2} \epsilon N(1/\epsilon)$.

Let us show that if (\ref{Cond_m}) is satisfied then $F_{\epsilon}((m+1)\epsilon)$ is also large.
Let us fix an integer $M$, and define 
$G_{M} = \lbrace m\in \mathbf{Z} : 1/\epsilon\leq m\leq M/\epsilon \text{, and (\ref{Cond_m}) is true for m}\rbrace$.
Then for $m\in G_{M}$, one has:
\begin{align*}
\lvert F_{\epsilon}((m+1)\epsilon) - \tilde{F}_{\epsilon,T}((m+1)\epsilon) \rvert
&\leq \frac{2}{\epsilon}\sum_{ T<\gamma} \frac{\lvert\sin(\gamma (m+1)\epsilon)\sin(\gamma\epsilon)\rvert}{\gamma^{2}} + O(1)\\
&\ll \frac{2}{\epsilon}\frac{\log T}{T} + 1 \ll \frac{c_{0}}{4}\epsilon N(1/\epsilon)
\end{align*}
if we take $T=\epsilon^{-2}$.
Hence, for every $m\in G_{M}$, one has
$$F_{\epsilon}((m+1)\epsilon) \geq \frac{c_{0}}{4}\epsilon N(1/\epsilon).$$

The next step is to give a lower bound for the size of the set $G_{M}$.
We use the pigeonhole principle, splitting the $N(1/\epsilon)$-dimensional cube into cubes of size $\frac{c_{0}\epsilon N(1/\epsilon)}{8\sum_{\gamma\in\mathcal{Z}^{*}_{\mathcal{S}}(\epsilon^{-2})}\frac{\operatorname{Re}(M(\gamma))}{\gamma}}$.
We deduce that
\begin{align}\label{Form_Min_taille_GM}
\lvert G_{M}\rvert \gg M \left(\frac{c_{0}\log(1/\epsilon)}{8 \log(\epsilon^{-2})}\right)^{\epsilon\log(1/\epsilon)}.
\end{align}

For $m\in G_{M}$, there exists a set with positive measure around $(m+1)\epsilon$ in which $E_{\mathcal{S}}(e^{y})$ is large enough.
One has 
\begin{align*}
\frac{1}{2\epsilon}\int_{m\epsilon}^{(m+2)\epsilon} E_{\mathcal{S}}(e^{y})\mathbf{1}_{E_{\mathcal{S}}(e^{y})<\frac{c_{0}}{8}\epsilon N(1/\epsilon)}{\mathop{}\!\mathrm{d}} y
< \frac{c_{0}}{8}\epsilon N(1/\epsilon).
\end{align*}
Hence 
\begin{align*}
\frac{1}{2\epsilon}\int_{m\epsilon}^{(m+2)\epsilon} E_{\mathcal{S}}(e^{y})\mathbf{1}_{E_{\mathcal{S}}(e^{y})\geq\frac{c_{0}}{8}\epsilon N(1/\epsilon)}{\mathop{}\!\mathrm{d}} y
&\geq F_{\epsilon}((m+1)\epsilon) - \frac{c_{0}}{8}\epsilon N(1/\epsilon) \\
&\geq \frac{c_{0}}{8}\epsilon N(1/\epsilon).
\end{align*} 
Moreover by the Cauchy--Schwarz inequality one has
\begin{align*}
\frac{1}{2\epsilon}\int_{m\epsilon}^{(m+2)\epsilon} E_{\mathcal{S}}(e^{y})\mathbf{1}_{E_{\mathcal{S}}(e^{y})\geq\frac{c_{0}}{8}\epsilon N(1/\epsilon)}{\mathop{}\!\mathrm{d}} y
&\leq \frac{1}{2\epsilon} \left(\int_{m\epsilon}^{(m+2)\epsilon} E_{\mathcal{S}}(e^{y})^{2}{\mathop{}\!\mathrm{d}} y\right)^{1/2}
\lambda(m,\epsilon)^{1/2}
\end{align*}
where $\lambda(m,\epsilon)$ is the Lebesgue measure of the set $\lbrace y\in [m\epsilon,(m+2)\epsilon] : E_{\mathcal{S}}(e^{y})> \frac{c_{0}}{8}\epsilon N(1/\epsilon) \rbrace$.
Hence
\begin{align*}
\lambda(m,\epsilon) \geq 4\epsilon^{2} \left(\int_{m\epsilon}^{(m+2)\epsilon} E(e^{y})^{2}{\mathop{}\!\mathrm{d}} y\right)^{-1}
\left(\frac{c_{0}}{8}\epsilon N(1/\epsilon)\right)^{2}.
\end{align*}

We can now give a lower bound for the Lebesgue measure of the union of such sets.
\begin{align*}
\lambda\left(\lbrace y\in [1,M+2\epsilon] : E(e^{y})> \frac{c_{0}}{8}\epsilon N(1/\epsilon) \rbrace\right)
&\geq \frac{1}{2} \sum_{m\in G_{M}} \left(\int_{m\epsilon}^{(m+2)\epsilon} E(e^{y})^{2}{\mathop{}\!\mathrm{d}} y\right)^{-1}
\left(\frac{c_{0}}{4}\epsilon^{2} N(1/\epsilon)\right)^{2}.
\end{align*}
Using the Cauchy--Schwarz inequality, we get
\begin{align*}
\sum_{m\in G_{M}} \left(\int_{m\epsilon}^{(m+2)\epsilon} E(e^{y})^{2}{\mathop{}\!\mathrm{d}} y\right)^{-1}
&\geq \lvert G_{M}\rvert^{2}\left(2 \int_{\log 2}^{M+2} E(e^{y})^{2}{\mathop{}\!\mathrm{d}} y \right)^{-1}.
\end{align*}
We use (\ref{Form_finale}) and Lemma \ref{Lm_epsilon} for $T$ fixed.
There exists a constant $c$ such that 
$$\int_{\log 2}^{M+2} E(e^{y})^{2}{\mathop{}\!\mathrm{d}} y \leq cM.$$
Hence by (\ref{Form_Min_taille_GM}), we get
\begin{align*}
\lambda\left(\lbrace y\in [1,M+2\epsilon] : E(e^{y})> \frac{c_{0}}{8}\epsilon N(1/\epsilon) \rbrace\right)
\gg M
\left(\frac{\log(1/\epsilon)}{\log(\epsilon^{-2})}\right)^{2\epsilon\log(1/\epsilon)}
\left(\epsilon \log(1/\epsilon)\right)^{2}.
\end{align*}
Divide by $M$ and let $M\rightarrow\infty$.
The left hand side becomes 
$\mu((\frac{c_{0}}{8}\epsilon N(1/\epsilon),\infty))$.
Taking $A=\frac{c_{0}}{8}\epsilon N(1/\epsilon)$, 
the Theorem follows.

We have a lower bound for $\mu_{\mathcal{S}}(A,+\infty)$.
The same argument with $-\epsilon$ gives a lower bound for
$\mu(-\infty,-A)$.
\end{proof}

\begin{Rk}
	The proof is similar to the one in \cite{RS}.
	The hypothesis about $\operatorname{Re}(a_{f}) \geq 0$ does not seem very natural, but is necessary in our analysis. 	
It is satisfied in the case the set $\mathcal{S}$ is a singleton
i.e. in most of the examples presented in Section~\ref{Sec_Applications}.
In the case of the prime number race between congruence classes (Theorem~\ref{Th_RS_raceModq}), the condition holds when one studies the race between $1\ [\bmod\ q]$ and another invertible class modulo $q$.

\end{Rk}

In the case RH is not satisfied ($\beta_{\mathcal{S},0}> 1/2$), 
one can conjecture that $\mathcal{Z}_{\mathcal{S}}$ is not too large.
In particular it may have density equal to $0$ (in the set of all zeros).
This is the point of Conjecture \ref{Conj_ZeroDensThm}.

\begin{prop}\label{Prop_noRH_CompactSupp}
	Suppose Conjecture \ref{Conj_ZeroDensThm} holds,
	then the limiting logarithmic distribution $\mu_{\mathcal{S}}$ has compact support.
\end{prop}

\begin{proof}
	Under Conjecture \ref{Conj_ZeroDensThm}, the sum $\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}\frac{1}{\lvert \beta_{\mathcal{S},0} + i\gamma\rvert}$ converges,
	hence for every $T$, the limiting logarithmic distribution $\mu_{\mathcal{S},T}$ has compact support included in the interval 
	$$\left[m_{\mathcal{S}}- \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}\frac{1}{\lvert \beta_{\mathcal{S},0} + i\gamma\rvert} , m_{\mathcal{S}}+ \sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}}\frac{1}{\lvert \beta_{\mathcal{S},0} + i\gamma\rvert}\right].$$
	By Proposition \ref{Prop_LimOfDist}, $\mu_{\mathcal{S}}$ has compact support included in the same interval.	
\end{proof}

\begin{Rk}
Proposition~\ref{Prop_noRH_CompactSupp} could indicate a way to find completely biased prime number races: 
in the case one has an $L$-function with a zero $\beta_{0}$ in the interval $\left(\frac{1}{2},1\right)$ (e.g. a Siegel zero), 
and such that it has no zeros of larger real part,
we can imagine that there will not be many other zeros of maximal real part (if ever they exist). 
For example if
\begin{align*}
\sum_{\gamma \in\mathcal{Z}_{L}}\frac{1}{\lvert \beta_{0}+i\gamma\rvert} \leq m(L,\beta_{0})
\end{align*}
then we would have $\delta(L) = 0$.
But the existence of such an $L$-function seems very unlikely.
\end{Rk}

\subsection{Rank and extreme bias}\label{sub_ChebInequality}

In the process of looking for large biases, a strategy is to ensure that the distribution has 
strong concentration around its mean (the mean being itself far from zero).
We use the following corollary of Chebyshev's inequality (e.g. \cite[Prop. (1.2)]{Bass}).

\begin{lem}[Corollary to Chebyshev's inequality]\label{Lem_ChebyshevIneq}
	Let $X$ be a random variable with mean $\mathbb{E}(X)$ and variance $\operatorname{Var}(X)$.
 If $\mathbb{E}(X)\neq 0$, one has:
\begin{align}\label{ChebyshevInequality}
\mathbf{P}(X\mathbb{E}(X) <0)\leq \frac{\operatorname{Var}(X)}{\mathbb{E}(X)^{2}}.
\end{align}
\end{lem}

We are looking for random variables for which the variance is small face to the mean.

Using Theorem \ref{Th_DistLim}, we can compute the mean and variance of a random variable that has distribution equal to $\mu_{\mathcal{S}}$.
With upper bounds on the variance or lower bounds for the mean, 
we may deduce concentration results (hence bounds for the bias).

\begin{cor}\label{Cor_ChebyshevIneq}
	\begin{itemize}
		\item In the case $m_{\mathcal{S}}<0$ one has:
		\begin{align*}
		\overline{\delta}(\mathcal{S}) \leq \frac{2}{m_{\mathcal{S}}^{2}}\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{(\beta_{\mathcal{S},0}^{2}+\gamma^{2})}.
		\end{align*}
		\item In the case $m_{\mathcal{S}}>0$ one has:
		\begin{align*}
		\underline{\delta}(\mathcal{S}) \geq 1- \frac{2}{m_{\mathcal{S}}^{2}}\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{(\beta_{\mathcal{S},0}^{2}+\gamma^{2})}.
		\end{align*}
	\end{itemize}
\end{cor}

In particular if the ratio $\frac{2}{m_{\mathcal{S}}^{2}}\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{(\beta_{\mathcal{S},0}^{2}+\gamma^{2})}$ is small ($< 1/2$),
the bias is imposed by the sign of the mean (see Remark~\ref{Rk_mean_bias}).

\begin{proof}
Assume $m_{\mathcal{S}}<0$,
one has
\begin{align*}
\overline{\delta}(\mathcal{S}) &= \limsup\frac{1}{Y}\int_{2}^{Y}\mathbf{1}_{\geq 0}(E_{\mathcal{S}}(e^{y})){\mathop{}\!\mathrm{d}} y \\
&\leq \limsup\frac{1}{Y}\int_{2}^{Y}g_{n}(E_{\mathcal{S}}(e^{y})){\mathop{}\!\mathrm{d}} y
\end{align*}
where for $n\in\mathbf{N}$, the function $g_{n}$ is defined by
$$g_{n}(x)=\left\lbrace
\begin{array}{ll}
0 & \mbox{if $x\leq \frac{-1}{n}$,}\\
1 & \mbox{if $x\geq 0$,}\\
nx  + 1 & \mbox{otherwise.}
\end{array}
\right.$$
For every $n$, the function $g_{n}$ is continous $n$-Lipschitz and bounded, 
therefore one has
\begin{align*}
\limsup\frac{1}{Y}\int_{2}^{Y}g_{n}(E_{\mathcal{S}}(e^{y})){\mathop{}\!\mathrm{d}} y &= \mu_{\mathcal{S}}(g_{n}) \\
 &\leq \mu_{\mathcal{S}}\left[\frac{-1}{n},+\infty\right)  = \mathbf{P}\left(X \geq \frac{-1}{n}\right)
\end{align*}
if $X$ is a random variable of law $\mu_{\mathcal{S}}$.
Finally we write
\begin{align*}
\mathbf{P}\left(X \geq \frac{-1}{n}\right) = \mathbf{P}\left(-X \leq \frac{1}{n}\right) 
&=  \mathbf{P}\left(\mathbb{E}(X)-X \leq \mathbb{E}(X) +\frac{1}{n}\right)  \\
&\leq \mathbf{P}\left(\lvert\mathbb{E}(X)-X \rvert \geq \left\lvert m_{\mathcal{S}} +\frac{1}{n}\right\rvert\right)
\end{align*}
for $n$ large enough, so that $m_{\mathcal{S}} +\frac{1}{n} <0$.
By Chebyshev's inequality, we obtain that for $n$ large enough:
\begin{align*}
\overline{\delta}(\mathcal{S}) \leq \frac{\operatorname{Var}(X)}{\left\lvert m_{\mathcal{S}} +\frac{1}{n}\right\rvert^{2}}.
\end{align*}
Letting $n\rightarrow \infty$ and using the values obtained in Theorem~\ref{Th_DistLim} for the mean and the variance yields the result.

The case $m_{\mathcal{S}}>0$ follows from similar computations. One can also see \cite[Lem. 2.7]{FioEC}.
\end{proof}

We are intersted in evaluating more precisely the values of $m_{\mathcal{S}}$ 
and of the sum $\operatorname{Var}_{\mathcal{S}}=2\sum_{\gamma \in \mathcal{Z}_{\mathcal{S}}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{\beta_{\mathcal{S},0}^{2}+\gamma^{2}}$.
In the case $\mathcal{S} = \lbrace f \rbrace$ is a singleton, and if we assume GRH for $L(f,s)$, 
a first approximtion for $\mu_{f}$ is twice the analytic rank $m(L(f,\cdot),1/2)$.
By usual properties on the zeros of the $L$-functions (e.g. \cite[(5.27)]{IK}), we get
\begin{align*}
\log \mathfrak{q}(f)
\ll  \sum_{\gamma \in \mathcal{Z}_{f}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{\frac{1}{4}+\gamma^{2}} \ll
(\log \mathfrak{q}(f))^{2}.
\end{align*}
If we assume that the zeros have bounded multiplicities (as in \cite[Th. 1.2]{Fiorilli_HighlyBiased}, \cite{FioEC}),
it becomes 
\begin{align*}
\sum_{\gamma \in \mathcal{Z}_{f}^{*}}\frac{\lvert M(\gamma)\rvert^{2}}{\frac{1}{4}+\gamma^{2}} \asymp
\log \mathfrak{q}(f).
\end{align*}

To conclude, under GRH, if one wants to find a very biased prime number race, one needs to find an $L$-function with an analytic rank large with respect to its conductor.

This is the idea of \cite[Th. 1.2]{FioEC}: if one uses Chebyshev's inequality, 
under the hypothesis of bounded multiplicity of the zeros and assuming the existence of a family of elliptic curves such that
$$\limsup_{\mathfrak{q}(E)\rightarrow \infty} \frac{r_{an}(E)}{\sqrt{\log \mathfrak{q}(E)}} = \infty,$$
Then one gets $\inf \overline{\delta}(E) = 0$ in this family.
Theorem~\ref{Th_arbitraryBiased} generalizes this idea.

\begin{proof}[Proof of Theorem~\ref{Th_arbitraryBiased}]
Let $(X_{n})$ be a sequence of random variables of law $\mu_{f_{n}}$.
By Theorem~\ref{Th_DistLim}, our hypothesis implies that
\begin{align*}
\operatorname{Var}(X_{n}) 
= 2\sum_{\gamma \in \mathcal{Z}_{f_{n}}^{*}}\frac{m(L(f_{n},\cdot),\beta_{f_{n},0} + i\gamma)^{2}}{\beta_{f_{n},0}^{2}+\gamma^{2}}
= o\left( \frac{m_{f_{n}}^{2}}{\log \mathfrak{q}(f_{n})}\sum_{\gamma \in \mathcal{Z}_{f_{n}}}\frac{ \log \gamma}{\beta_{f_{n},0}^{2}+\gamma^{2}}  \right).
\end{align*}
By \cite[(5.27)]{IK}, we can bound the sum:
\begin{align*}
\sum_{\gamma \in \mathcal{Z}_{f_{n}}}\frac{ \log \gamma}{\beta_{f_{n},0}^{2}+\gamma^{2}} \ll \log \mathfrak{q}(f_{n}).
\end{align*}
In the end we have obtained
\begin{align*}
\operatorname{Var}(X_{n})  = o(m_{f_{n}}^{2}).
\end{align*}
We conclude using Corollary~\ref{Cor_ChebyshevIneq}.
\end{proof}

\noindent\emph{Acknowledgements.}
This paper contains some of the results of my doctoral dissertation.
I am very grateful to my advisor Florent Jouve for suggesting the problem, for all his advice, help and time spent correcting the first drafts of this paper.
I thank \'{E}tienne Fouvry and Daniel Fiorilli for their interest in this work and for the many conversations that have led to improvements in the results.
This work has also benefited from conversations with Farrell Brumley, Ga{\"e}tan Chenevier, Emmanuel Kowalski, Philippe Michel, Ze{\'e}v Rudnick and Miko\l{}aj Fr\k{a}czyk.
The computations present in this document have been performed with the \texttt{SageMath} \cite{sagemath} software.

\bibliographystyle{alpha} 
\bibliography{biblio}

\appendix
\section{}\label{App_proofNonConst}

In this appendix we prove the following result used in the proof of Proposition~\ref{Prop_SomeSelfSufficience}.
\begin{lem}
Let $N\geq 1$ be an integer, $\lambda_{1},\ldots,\lambda_{N} >0$ be distincts real numbers,
and $\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})$ be the the sub-torus of $\mathbf{T}^{N}$ given by
the topological closure of the $1$-parameter group 
$\left\lbrace \frac{y}{2\pi} \left(\lambda_{1},\ldots,\lambda_{N}\right) : y\in\mathbf{R} \right\rbrace/\mathbf{Z}^{N}$.
For every $(M_{j})_{1\leq j\leq N}, (w_{j})_{1\leq j\leq N} \in \mathbf{R}^{N}$, satifying $M_{j} > 0$ for all $1\leq j\leq N$,
one has that
$$\phi_{M,w} : \left\lbrace
\begin{array}{l l}
\mathbf{T}(\lambda_{1},\ldots,\lambda_{N}) & \rightarrow \mathbf{R} \\
(\theta_{1},\ldots,\theta_{N}) &\mapsto \sum_{j=1}^{N}M_{j}\cos(2\pi \theta_{j} + w_{j})
\end{array}
\right.$$
is a non-constant analytic function.
\end{lem}

\begin{proof}
The function $\phi$ is analytic as a finite sum of analytic functions.
It is defined for $\theta \in \mathbf{T}(\lambda_{1},\ldots,\lambda_{N})$ but with its coordinates as an element of $\mathbf{T}^{N}$, 
we begin with changing the variable.
One has
\begin{align*}
\mathbf{T}(\lambda_{1},\ldots,\lambda_{N}) = \left\lbrace (\theta_{1},\ldots,\theta_{N}) \in \mathbf{T}^{N} : \forall k\in\mathbf{Z}^{N},
\sum_{j=1}^{N} k_{j}\theta_{j} = 0 \Leftrightarrow \sum_{j=1}^{N} k_{j}\lambda_{j} = 0 \right\rbrace.
\end{align*} 
The condition given on $k\in\mathbf{Z}^{N}$ defines a vector subspace of $\mathbf{Q}^{N}$.
Let $d$ be the dimension of the subtorus $\mathbf{T}(\lambda_{1},\ldots,\lambda_{N})$.
For $1\leq j\leq N$ and $1\leq i \leq d$ there exists coefficients $r_{j,i} \in \mathbf{Q}$
satifying for all $k\in\mathbf{Z}^{N}$,
\begin{align*}
\left( \forall (t_{1},\ldots,t_{d}) \in \mathbf{T}^{d},
\sum_{j=1}^{N} k_{j}\sum_{i=1}^{d}r_{j,i}t_{i} = 0 \right) \Leftrightarrow \sum_{j=1}^{N} k_{j}\lambda_{j} = 0.
\end{align*}
As $\lambda_{1} \neq 0$ we can choose $(r_{1,1},r_{1,2},\ldots,r_{1,d}) = (1,0,\ldots,0)$.
Moreover, as $\lambda_{1},\ldots,\lambda_{N},-\lambda_{1},\ldots,-\lambda_{N}$ are distinct, the situation 
$(r_{j,1},r_{j,2},\ldots,r_{j,d})= \pm (r_{j',1},r_{j',2},\ldots,r_{j',d})$ for $j\neq j'$ does not happen.
Then we can deduce
\begin{align*}
\mathbf{T}(\lambda_{1},\ldots,\lambda_{N}) = \left\lbrace \left(t_{1},\sum_{k=1}^{d}r_{2,k}t_{k},\ldots,\sum_{k=1}^{d}r_{N,k}t_{k}\right) \in \mathbf{T}^{N} : (t_{1},\ldots,t_{d}) \in \mathbf{T}^{d}\right\rbrace.
\end{align*} 

We are reduced to proving that the function
$$\phi : \left\lbrace
\begin{array}{l l}
\mathbf{T}^{d} & \rightarrow \mathbf{R} \\
(t_{1},\ldots,t_{d}) &\mapsto \cos(2\pi t_{1} + w_{1}) + \sum_{j=2}^{N}\frac{M_{j}}{M_{1}}\cos(2\pi\sum_{k=1}^{d}r_{j,k}t_{k} + w_{j})
\end{array}
\right.$$
is non-constant.
In the case $N=1$, the sum is empty, and the result is clear.
We can assume that $N\geq 2$.

Assume this function is constant.
Then one has, for all $w \in\mathbf{R}$,
\begin{align*}
\lim_{T\rightarrow\infty}\frac{1}{T}\int_{-T}^{T} \phi(t_{1},\ldots,t_{d})\cos(2\pi t_{1} + w){\mathop{}\!\mathrm{d}} t_{1} 
= \lim_{T\rightarrow\infty}\frac{\phi(0,\ldots,0)}{T}\int_{-T}^{T} \cos(2\pi t_{1} + w){\mathop{}\!\mathrm{d}} t_{1}
= 0.
\end{align*}
However one also has
\begin{multline*}
\lim_{T\rightarrow\infty}\frac{1}{T}\int_{-T}^{T} \phi(t_{1},\ldots,t_{d})\cos(2\pi t_{1} + w){\mathop{}\!\mathrm{d}} t_{1} 
= \lim_{T\rightarrow\infty}\frac{1}{T}\int_{-T}^{T} \cos(2\pi t_{1} + w_{1})\cos(2\pi t_{1} + w) {\mathop{}\!\mathrm{d}} t_{1} \\
+ \sum_{j=2}^{N}M'_{j}\lim_{T\rightarrow\infty}\frac{1}{T}\int_{-T}^{T} \cos(2\pi\sum_{k=1}^{d}r_{j,k}t_{k} + w_{j}) \cos(2\pi t_{1} + w){\mathop{}\!\mathrm{d}} t_{1}.
\end{multline*}
Hence
\begin{align*}
\frac{1}{2}\cos(w - w_{1}) + \sum_{\substack{j=2 \\ r_{j,1} = \pm 1}}^{N}\frac{M'_{j}}{2}\cos\left(w \mp \left(2\pi\sum_{k=2}^{d}r_{j,k}t_{k} + w_{j}\right)\right) = 0.
\end{align*}
Taking the mean value of the left hand term when $t_{2},\ldots,t_{d}$ vary we get
\begin{align*}
\cos(w - w_{1}) + \sum_{\substack{j=2 \\ r_{j,1} = \pm 1 \\ r_{j,k} = 0 \forall k \geq 2}}^{N}M'_{j}\cos\left(w \mp  w_{j}\right) = 0.
\end{align*}
From the conditions on the coefficients $r_{j,k}$ we deduce that the sum in the left hand term is empty.
Hence we obtain for every $w\in\mathbf{R}$:
\begin{align*}
\cos(w - w_{1}) = 0,
\end{align*}
a contradiction. In particular the function $\phi$ is non constant, and the proof is complete.
\end{proof}

\end{document}

