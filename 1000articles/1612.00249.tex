
\documentclass[12pt, reqno]{amsart}
\usepackage{amssymb, amsthm, amsmath, amsfonts}
\usepackage{array, epsfig}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{color}

\setlength{\oddsidemargin}{-0.0in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.0in} \setlength{\textheight}{8.4in} \evensidemargin
\oddsidemargin
\parindent=8mm

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{convention}[theorem]{Convention}
\newtheorem{claim}[theorem]{Claim}

\newenvironment{case}[1][\textsc{Case}]
{\begin{trivlist}\item[\hskip \labelsep {\textsc{#1}}]}\end{trivlist}

\newenvironment{step}[1][\textsc{Step}]
{\begin{trivlist}\item[\hskip \labelsep {\textsc{#1}}]}\end{trivlist}

\begin{document}

\author{Zakhar Kabluchko}
\address{Zakhar Kabluchko: Institut f\"ur Mathematische Statistik,
Universit\"at M\"unster,
Orl\'eans--Ring 10,
48149 M\"unster, Germany}
\email{zakhar.kabluchko@uni-muenster.de}

\author{Vladislav Vysotsky}
\address{Vladislav Vysotsky:  Imperial College London, St.\ Petersburg Department of Steklov Mathematical Institute}
\email{vysotsky@asu.edu, v.vysotskiy@imperial.ac.uk}

\author{Dmitry Zaporozhets}
\address{Dmitry Zaporozhets: St.\ Petersburg Department of Steklov Mathematical Institute,
Fontanka~27,
191011 St.\ Petersburg,
Russia}
\email{zap1979@gmail.com}

\title[Convex hulls of random walks]{Convex hulls of random walks: Expected number of faces and face probabilities}
\keywords{Convex hull, random walk, random walk bridge, absorption probability, distribution-free probability, exchangeability, hyperplane arrangement, Whitney's formula, Zaslavsky's theorem, characteristic polynomial, Weyl chamber, finite reflection group, convex cone,  Wendel's formula, random polytope, average number of faces, average number of vertices,  discrete arcsine law}
\thanks{The work of V.V. is supported by People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA grant agreement n$^\circ$[628803]. His work is also supported in part by Grant 16-01-00367 by RFBR. The work of the second author is supported in part by Grant 16-01-00367 by RFBR and supported in part by Project SFB 701 of Bielefeld University}

\subjclass[2010]{Primary: 52A22, 60D05, 60G50; secondary:  60G70, 60G09, 52C35, 20F55, 52B11}

\begin{abstract}
Consider a random walk $S_i= \xi_1+\ldots+\xi_i$, $1\leq i\leq n$, starting at $S_0=0$, whose increments $\xi_1,\ldots,\xi_n$ are random vectors in $\mathbb R^d$, $d\leq n$.  We are interested in the properties of the convex hull $C_n:=\mathrm{Conv}(S_0,S_1,\ldots,S_n)$.
Assuming that the tuple $(\xi_1,\ldots,\xi_n)$ is exchangeable
and some general position condition holds, we prove that the expected number of $k$-dimensional faces of $C_n$ is given by the formula
$$
{\mathbb E} [\#\mathcal{F}_k(C_n)] = \frac{2\cdot k!}{n!} \sum_{l=0}^{\infty}\genfrac{[}{]}{0pt}{}{n+1}{d-2l}  \genfrac{\{}{\}}{0pt}{}{d-2l}{k+1},
$$
for all $0\leq k \leq d-1$, where $\genfrac{[}{]}{0pt}{}{n}{m}$ and $\genfrac{\{}{\}}{0pt}{}{n}{m}$ are Stirling numbers of the first and second kind, respectively.
Generalizing  the classical discrete arcsine law for the position of the maximum due to E.\ Sparre Andersen,
[On fluctuations of the sums of random variables, I: Math.\ Scand.\ 1 (1953) and II: Math.\ Scand.\ 2 (1954)],
we compute explicitly the probability that for given indices $0\leq i_1<\ldots <i_{k+1}\leq n$, the points $S_{i_1},\ldots,S_{i_{k+1}}$ form a $k$-dimensional face of $\mathrm{Conv}(S_0,S_1,\ldots,S_n)$. This is done in two different settings: for random walks with symmetrically exchangeable increments and for random bridges with exchangeable increments.
The main ingredient in the proof  is the computation of the probability that the origin is absorbed by a joint convex hull of several random walks and bridges whose increments are invariant with respect to the action of a direct product of finitely many reflection groups of types $A_{n-1}$ and $B_n$. This probability, in turn, is related to the number of Weyl chambers of a product-type reflection group that are intersected by a linear subspace in general position.
All formulae are distribution-free, that is do not depend on the distribution of the $\xi_k$'s.
\end{abstract}

\maketitle

\section{Statement of main results}\label{1306}

\subsection{Introduction}
Let $\xi_1,\ldots,\xi_n$ be (possibly dependent) random $d$-dimensional vectors with partial sums
$$
S_i = \xi_1 + \ldots + \xi_i,\quad  1\leq i\leq n,\quad  S_0=0.
$$
The sequence $S_0,S_1,\ldots,S_n$ will be referred to as \emph{random walk} or, if the additional boundary condition $S_n=0$ is imposed, a \emph{random bridge}.

In the one-dimensional case $d=1$, Sparre Andersen~\cite{Sparre2, sparre_andersen1,sparre_andersen2} derived remarkable formulae for several functionals of the random walk $S_0,S_1,\ldots,S_n$ including the number of positive terms and the position of the maximum.
More specifically, assuming that the distribution of the increments $(\xi_1,\ldots,\xi_n)$ is invariant under arbitrary signed permutations and that ${\mathbb{P}}[S_i=0]=0$ for all $1\leq i\leq n$, Sparre Andersen proved in~\cite[Theorem~C]{sparre_andersen2} the following \emph{discrete arcsine law} for the position of the maximum:
\begin{equation}\label{eq:arcsine_maximum_0}
{\mathbb{P}}\left[\max\{S_0,\ldots,S_n\} = S_i\right] =
\frac 1 {2^{2n}} \binom{2i}{i} \binom{2n-2i}{n-i},
\quad
i=0,\ldots,n.
\end{equation}
By symmetry, the same holds for the position of the minimum. Surprisingly, the above formula is distribution-free, that is its right-hand side does not depend on the distribution of $(\xi_1,\ldots,\xi_n)$ provided the symmetric exchangeability and the general position assumptions mentioned above are satisfied. Another unexpected consequence of this formula is that the maximum is most likely to be attained at $i=0$ or $i=n$ rather than at $i\approx n/2$, as one could na\"{i}vely guess. A discussion of the arcsine laws can be found in Feller's book~\cite[Vol II, Section XII.8]{Feller}.

Let us now turn to the general $d$-dimensional case and ask ourselves what could be an appropriate multidimensional generalization of~\eqref{eq:arcsine_maximum_0}. Of course, the maximum and the minimum are not well defined for multidimensional random walks, but instead we can consider vertices (and, more generally, faces) of the \emph{convex hull} 
\begin{equation*}
C_n := {\mathop{\mathrm{Conv}}\nolimits}(S_0,S_1,\ldots, S_n)
=\{\alpha_0 S_0+\ldots+\alpha_n S_n \colon \alpha_0,\ldots,\alpha_n \geq 0, \alpha_0+\ldots+\alpha_n =1\}.
\end{equation*}
Clearly, $C_n$ is a random polytope in ${\mathbb{R}}^d$ whose vertices belong to the collection $\{S_0,S_1,\ldots,S_n\}$. In the one-dimensional case, $C_n$ has two vertices, the maximum and the minimum, but in higher dimensions the question on the number of vertices (or, more generally, faces) and their position becomes non-trivial.
The main results of the present paper can be summarized as follows.
Under appropriate exchangeability and general position assumptions on the random walk or bridge, we compute
\begin{enumerate}
\item [(a)] 
the expected number of $k$-dimensional faces of $C_n$, for all $0\leq k\leq d-1$, and
\item [(b)] 
the probability that the simplex ${\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}})$ is a $k$-dimensional face of the convex hull $C_n$, for a given collection of indices $0\leq i_1 < \ldots < i_{k+1}\leq n$.
\end{enumerate}

All formulae turn out to be distribution-free. The probabilities in (b), referred to as \emph{face probabilities}, will be interpreted below in terms of the so-called \emph{absorption probabilities}, that is the probabilities that a joint convex hull of several random walks and random bridges contains the origin. In our recent work~\cite{KVZ15}, we showed that in the case of just \emph{one} random walk or bridge, the absorption probability can be computed in a purely geometric way by counting the Weyl chambers of a reflection group that are intersected  by a linear subspace in general position. Moreover, we showed in~\cite{KVZ15} that random walks correspond to Weyl chambers of type $B_n$, whereas random bridges correspond to type $A_{n-1}$ chambers.
In the present paper, we extend the results of~\cite{KVZ15} to \emph{joint} convex hulls of \emph{several} random walks and bridges. We shall show that the corresponding absorption probabilities can be interpreted in terms of Weyl chambers of \emph{product type}. Our main  formula for the absorption probabilities will be stated in Theorem~\ref{1435}, below.

We shall argue below that (b) can be viewed as a generalization of the discrete arcsine law to higher dimensions. Let us mention that there is another discrete arcsine law, also due to Sparre Andersen~\cite[Theorem~C]{sparre_andersen2},  for the number of positive terms in a random walk. A multidimensional generalization of this result will be studied in a separate paper~\cite{KVZ16_arcsine}.

Convex hulls of random walks, Brownian motions and L\'evy processes were much studied; see, e.g.,\ \cite{Baxter,SW,SS,Nielsen,Eldan0,kampf_etal,MCR10,kabluchko_zaporozhets_sobolev,vysotsky_zaporozhets,molchanov_wespi,wade_xu}.  These papers concentrate mostly on functionals like the volume and the perimeter, which are not distribution-free. Face probabilities for faces of maximal dimension were computed by Barndorff-Nielsen and Baxter~\cite{Nielsen} and Vysotsky and Zaporozhets~\cite{vysotsky_zaporozhets}. We shall recover the corresponding formula as a special case of our results, but our methods are different from that of~\cite{Nielsen, vysotsky_zaporozhets}.
A review of the literature on random convex hulls and random polytopes can be found in~\cite{MCR10} and~\cite{schneider_polytopes}, respectively.

\subsection{Expected number of \texorpdfstring{$k$}{k}-faces}
Recall that $C_n= {\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_n)$ denotes the convex hull of a $d$-dimensional random walk $(S_i)_{i=0}^n$ with increments $\xi_1,\ldots,\xi_n$. The increments are random vectors which may be dependent. Our first result is a formula for the expected number of $k$-dimensional faces  of $C_n$.
To state it, we need to impose the following assumptions on the joint distribution of  the increments: 
\begin{enumerate}
\item[$(\text{Ex})$] \textit{Exchangeability:} For every permutation $\sigma$ of the set $\{1,\ldots,n\}$ we have the distributional equality
$$
(\xi_{\sigma(1)},\ldots,  \xi_{\sigma(n)}) {\stackrel{d}{=}} (\xi_1,\ldots,\xi_n).
$$
\item[$(\text{GP})$] \textit{General position:}
For every $1\leq i_1 < \ldots < i_d\leq n$ the probability that the vectors $S_{i_1}, \ldots,S_{i_d}$ are linearly dependent is $0$.
\end{enumerate}

\begin{example}
Conditions $(\text{Ex})$ and $(\text{GP})$ are satisfied if $\xi_1,\ldots,\xi_n$ are independent identically distributed, and for every hyperplane $H_0\subset {\mathbb{R}}^d$ passing through the origin we have ${\mathbb{P}}[S_i\in H_0] = 0$, for all $1\leq i\leq n$. The proof of $(\text{GP})$ can be found in~\cite{KVZ16_arcsine}, while the proof of $(\text{Ex})$ is trivial.
\end{example}

Denote by $\mathcal F_k(C_n)$, where $0\leq k\leq d-1$, the set of all $k$-dimensional faces (or just $k$-faces, for short) of the polytope $C_n$. Under the above assumptions, all faces of $C_n$ are simplices, with probability $1$; see Remark~\ref{rem:simplicial} below.  Let $\# \mathcal F_k(C_n)$ be the number of $k$-faces of $C_n$.
\begin{theorem}\label{theo:expected_walk}
Let $(S_i)_{i=0}^n$ be a random walk in ${\mathbb{R}}^d$, $n\geq d$,  whose increments $\xi_1,\ldots,\xi_n$ satisfy conditions $(\text{Ex})$ and $(\text{GP})$.
Then, for any $0\leq k\leq d-1$, 
\begin{equation}\label{eq:E_F_k_C_n_main_theorem}
{\mathbb E} [\#\mathcal{F}_k(C_n)] = \frac{2\cdot k!}{n!} \sum_{l=0}^{\infty}{\genfrac{[}{]}{0pt}{}{{n+1}}{{d-2l}}}  {\genfrac{\{}{\}}{0pt}{}{{d-2l}}{{k+1}}}.
\end{equation}
\end{theorem}
 The right-hand side  contains the (signless) \emph{Stirling numbers of the first kind} ${\genfrac{[}{]}{0pt}{}{{n}}{{m}}}$ and the \emph{Stirling numbers of the second kind} ${\genfrac{\{}{\}}{0pt}{}{{n}}{{m}}}$ (where $n\in{\mathbb{N}}$, $1\leq m \leq n$) which are defined as the number of permutations of an $n$-element set with $m$ cycles and the number of partitions of an $n$-element set into $m$ non-empty subsets, respectively. The column exponential generating functions of the Stirling numbers are given by
\begin{equation}\label{eq:stirling_def}
\sum_{n=m}^{\infty} {\genfrac{[}{]}{0pt}{}{{n}}{{m}}}\frac{x^n}{n!} = \frac 1 {m!} \left(\log \frac 1 {1-t}\right)^m,
\quad
\sum_{n=m}^{\infty} {\genfrac{\{}{\}}{0pt}{}{{n}}{{m}}}\frac{x^n}{n!} = \frac 1 {m!} ({{\rm e}}^{t}-1)^m.
\end{equation}
For these and other properties of Stirling numbers, we refer to~\cite[Chapters~6,7]{graham_knuth_patashnik_book}. For $n\in{\mathbb{N}}$,  $m\in {\mathbb{Z}}\backslash \{1,\ldots,n\}$ we use the convention ${\genfrac{[}{]}{0pt}{}{{n}}{{m}}} = {\genfrac{\{}{\}}{0pt}{}{{n}}{{m}}}=0$, so that the sum in~\eqref{eq:E_F_k_C_n_main_theorem} contains only finitely many non-vanishing terms.
The Stirling numbers of the first kind can also be defined as the coefficients of the rising factorial
\begin{equation}\label{eq:rising_factorial}
t^{(n)} := t(t+1)(t+2)\ldots (t+n-1) = \sum_{j=0}^n {\genfrac{[}{]}{0pt}{}{{n}}{{j}}} t^j.
\end{equation}

\begin{remark}
Let us mention some special cases of Theorem~\ref{1249}. For faces of maximal dimension (where $k=d-1$ and only the term with $l=0$ is present) and vertices (where $k=0$), formula~\eqref{eq:E_F_k_C_n_main_theorem} simplifies to
$$
{\mathbb E}\, [\#\mathcal{F}_{d-1}(C_n)]
=
\frac {2(d-1)!} {n!}
{\genfrac{[}{]}{0pt}{}{{n+1}}{{d}}},
\quad
{\mathbb E}\, [\#\mathcal{F}_{0}(C_n)]
=
\frac 2 {n!} \sum_{l=0}^\infty
 {\genfrac{[}{]}{0pt}{}{{n+1}}{{d-2l}}},
$$
where we used the identities ${\genfrac{\{}{\}}{0pt}{}{{d}}{{d}}} = 1$ and ${\genfrac{\{}{\}}{0pt}{}{{d-2l}}{{1}}} = 1$ (for $d-2l\geq 1$).  For example, in dimension $d=2$, both the expected number of edges and the expected number of vertices of the random polygon $C_n$ are equal to $2H_{n} = 2 \left(1+\frac 12 +\ldots + \frac 1{n}\right)$ because ${\genfrac{[}{]}{0pt}{}{{n+1}}{2}} = n! H_{n}$. This result was known~\cite{Nielsen, vysotsky_zaporozhets}. In dimension $d=1$ we recover the trivial formula ${\mathbb E}\, [\#\mathcal{F}_{0}(C_n)] = 2$  (the two vertices being the maximum and the minimum of the random walk) because ${\genfrac{[}{]}{0pt}{}{{n+1}}{1}} = n!$.
\end{remark}

\begin{remark}
Using the fixed $k$ asymptotic formula for Stirling numbers of the first kind, see~\cite[page~160]{jordan_book} (or~\cite{wilf} for much more precise asymptotics),
\begin{equation}\label{eq:stirling_asympt}
\frac 1{(n-1)!} {\genfrac{[}{]}{0pt}{}{{n}}{{k}}} \sim \frac{(\log n)^{k-1}}{(k-1)!}, \quad n\to\infty, \quad k \text{ fixed},
\end{equation}
we obtain 
\begin{equation}\label{eq:F_k_asympt}
{\mathbb E}\, [\#\mathcal{F}_k(C_n)]  \sim \frac{2\cdot k!}{(d-1)!} {\genfrac{\{}{\}}{0pt}{}{{d}}{{k+1}}} (\log n)^{d-1}, \quad n\to\infty, \quad k, d \text{ fixed}.
\end{equation}
Here, $a_n\sim b_n$ means that $\lim_{n\to\infty} a_n/b_n = 1$.
\end{remark}

\begin{remark}\label{rem:simplicial}
Under assumptions $(\text{Ex})$ and $(\text{GP})$, each face $f\in\mathcal F_k(C_n)$ of the polytope $C_n$ is, with probability $1$, a $k$-dimensional simplex of the form
$$
f = {\mathop{\mathrm{Conv}}\nolimits}(S_{j_1(f)},\ldots,S_{j_{k+1}(f)})
$$
for some indices $0\leq j_1(f)<\ldots<j_{k+1}(f)\leq n$. It suffices to prove this for $k=d-1$ because all faces of a simplex are simplices. For all $0\leq i_1<\ldots<i_{d+1}\leq n$ we have
\begin{multline*}
{\mathbb{P}}[S_{i_1},\ldots,S_{i_{d+1}} \text{ are contained in a common hyperplane}]
\\
\begin{aligned}
&=
{\mathbb{P}}[S_{i_2}-S_{i_1},S_{i_3}-S_{i_1},\ldots,S_{i_{d+1}}-S_{i_1} \text{ are linearly dependent}] \\
&=
{\mathbb{P}}[S_{i_2-i_1},S_{i_3-i_1}, \ldots,S_{i_{d+1}-i_1} \text{ are linearly dependent}] \\
&=
0
\end{aligned}
\end{multline*}
by assumptions $(\text{Ex})$ and $(\text{GP})$. It follows that, with probability $1$, every $(d-1)$-dimensional face of $C_n$ contains at most $d$ vertices and, consequently, is a simplex.
\end{remark}

\subsection{Face probabilities for symmetric random walks}
In the next theorem we compute the probability that a given collection of points of the random walk forms a face of the convex polytope $C_n$. To state it, we need an assumption which, in addition to exchangeability, requires invariance with respect to sign changes:

\begin{itemize}
\item [$(\pm\text{Ex})$] \textit{Symmetric exchangeability:} For every permutation $\sigma$ of the set $\{1,\ldots,n\}$ and every ${\varepsilon}_1,\ldots,{\varepsilon}_n\in \{-1,+1\}$ there is a distributional equality
    $$
    (\xi_1,\ldots,\xi_n) {\stackrel{d}{=}} ({\varepsilon}_1 \xi_{\sigma(1)}, \ldots, {\varepsilon}_n \xi_{\sigma(n)}).
    $$
\end{itemize}
Random walks satisfying $(\pm\text{Ex})$ will be frequently referred to as \emph{symmetric}. For example, $(\pm\text{Ex})$ is satisfied if $\xi_1,\ldots,\xi_n$ are i.i.d.\ random vectors in ${\mathbb{R}}^d$ with centrally symmetric distribution (meaning that $\xi_1$ has the same distribution as $-\xi_1$).
\begin{theorem}\label{1249}
Let $(S_i)_{i=0}^n$ be a random walk in ${\mathbb{R}}^d$ whose increments $\xi_1,\ldots,\xi_n$ satisfy assumptions $(\pm\text{Ex})$ and $(\text{GP})$.
Fix some $0\leq k\leq d-1$ and let $0 \le i_1 < \ldots < i_{k+1} \le n$ be any indices. Then,
$$
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}}) \in \mathcal F_k(C_n)]
=
\frac{2(P_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + P_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots)}
{2^{i_1+n-i_{k+1}} i_1!(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1})!} ,
$$
where the $P_{i_1,\ldots,i_{k+1}}^{(n)}(j)$'s  are the coefficients of the polynomial
\begin{multline*}
(t+1)(t+3)\ldots (t+2i_1-1)\times(t+1)(t+3)\ldots (t+2(n-i_{k+1})-1)\\
\times\prod_{j=1}^k ((t+1)(t+2)\ldots (t+i_{j+1}-i_j-1)) = \sum_{j=0}^n P_{i_1,\ldots,i_{k+1}}^{(n)}(j) t^j.
\end{multline*}
For $j<0$ or $j>n$ we use the convention $P_{i_1,\ldots,i_{k+1}}^{(n)}(j) := 0$.
\end{theorem}

\begin{remark}
Take some $0\leq i\leq n$. For the probability that $S_i$ is a vertex of the convex hull $C_n$, we obtain by taking $k=0$ in Theorem~\ref{1249},
\begin{equation}\label{eq:S_i_is_vertex}
{\mathbb{P}}[S_{i} \in \mathcal F_{0}(C_n)] =
\frac{P_{i}^{(n)}(d-1) + P_{i}^{(n)}(d-3)+\ldots}{2^{n-1} i!(n-i)!},
\end{equation}
where the $P_{i}^{(n)}(j)$'s  are the coefficients of the polynomial
\begin{equation}\label{eq:S_i_is_vertex_cont}
(t+1)(t+3)\ldots (t+2i-1)\times(t+1)(t+3)\ldots (t+2(n-i)-1) = \sum_{j=0}^n P_{i}^{(n)}(j) t^j.
\end{equation}
In the one-dimensional case $d=1$, the convex hull is the interval
$$
C_n = \left[\min_{i=0,\ldots,n}S_i, \max_{i=0,\ldots,n} S_i\right],
$$
so that (by symmetry) the probability that $S_i$ is a vertex of $C_n$ is just twice the probability that $S_i$ is the maximum. Therefore, \eqref{eq:S_i_is_vertex} and~\eqref{eq:S_i_is_vertex_cont} yield
\begin{equation}\label{eq:arcsine_maximum}
{\mathbb{P}}[\max\{S_0,\ldots,S_n\} = S_i] =
\frac 1 {2^{2n}} \binom{2i}{i} \binom{2n-2i}{n-i}
= \frac {(2i-1)!!(2n-2i-1)!!}{(2i)!!(2n-2i)!!},
\end{equation}
for all $i=0,\ldots,n$, which recovers the discrete arcsine law for the position of the maximum due to Sparre Andersen~\cite[Theorem~C]{sparre_andersen2}, see also~\cite[Vol II, Section XII.8]{Feller}.
Thus, we can view Theorem~\ref{1249} as a multidimensional generalization of the discrete arcsine law.
\end{remark}

\begin{remark}
For faces of maximal possible dimension $k = d-1$, Theorem~\ref{1249} (with only $P^{(n)}_{i_1,\ldots,i_d}(0)$ being non-zero) recovers a formula of Vysotsky and Zaporozhets~\cite{vysotsky_zaporozhets}:
$$
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{d}}) \in \mathcal F_{d-1}(C_n)] = 2 \frac{(2i_1-1)!!}{(2i_1)!!}\frac{(2n - 2i_d-1)!!}{(2n - 2i_d)!!} \prod_{j=1}^{d-1} \frac{1}{i_{j+1} - i_j}.
$$
\end{remark}
\begin{remark}
The coefficients $P_{i_1,\ldots,i_{k+1}}^{(n)}$  allow for a natural probabilistic interpretation. Consider random variables
$$
K_n := {\mathbbm{1}}_{A_1} + {\mathbbm{1}}_{A_2}+ \ldots + {\mathbbm{1}}_{A_n}, \quad L_n := {\mathbbm{1}}_{A_2} + {\mathbbm{1}}_{A_4} + \ldots + {\mathbbm{1}}_{A_{2n}}, \quad n\in{\mathbb{N}},
$$
where $A_1,A_2,\ldots$ are independent random events with ${\mathbb{P}}[A_m] = 1/m$, $m\in{\mathbb{N}}$.  The generating functions of $K_n$ and $L_n$ are given by
$$
{\mathbb E} t^{K_n} = \frac{t(t+1)\ldots (t+n-1)}{n!}, \quad
{\mathbb E} t^{L_n} = \frac{(t+1)(t+3)\ldots (t+2n-1)}{2^n n!}.
$$

It is well known that the number of cycles  of a uniform random permutation on $n$ elements has the same distribution as $K_n$. This can be deduced from the connection between random uniform permutations and the Chinese restaurant process; see~\cite[Section~3.1]{pitman_book}. To give a similar interpretation of $L_n$, consider the group of \emph{signed} permutations of the set $\{1,\ldots,n\}$. Any such permutation can be written in the form
$$
\Sigma=
\begin{pmatrix}
1 & 2 &  \ldots  & n \\
{\varepsilon}_1 \sigma(1) & {\varepsilon}_2\sigma(2) &  \ldots &  {\varepsilon}_n \sigma(n)
\end{pmatrix},
$$
where $\sigma$ is a permutation on $\{1,\ldots,n\}$ and ${\varepsilon}_1,\ldots,{\varepsilon}_n\in \{-1,+1\}$. The permutation $\sigma$ can be decomposed into cycles. We call a cycle $w_1 \to w_2\to \ldots \to w_r\to w_1$ of $\sigma$ an \emph{even cycle} of the signed permutation $\Sigma$ if ${\varepsilon}_{w_1}\ldots {\varepsilon}_{w_r} = +1$, i.e.\ if making a full turn along the cycle does not change the sign.
Clearly, for a uniformly chosen random signed permutation, any cycle is even with probability $1/2$, independently of all other cycles. It follows that the number of even cycles has the same distribution as $L_n$. The symmetric group and the group of signed permutations, acting on ${\mathbb{R}}^n$ as the reflection groups $A_{n-1}$ and $B_n$, will play a major role in our proofs.

Let us now return to Theorem~\ref{1249}. Let $L_{i_1}^{(0)},K_{i_2-i_1}^{(1)}, \ldots, K_{i_{k+1}-i_k}^{(k)}, L_{n-i_{k+1}}^{(k+1)}$ be independent random variables with the same distributions as $L_{i_1},K_{i_2-i_1}, \ldots, K_{i_{k+1}-i_k}, L_{n-i_{k+1}}$. Then, Theorem~\ref{1249} states that
$$
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}}) \in \mathcal F_k(C_n)] =
2 \sum_{l=0}^\infty {\mathbb{P}}[L_{i_1}^{(0)} + K_{i_2-i_1}^{(1)} +  \ldots + K_{i_{k+1}-i_k}^{(k)} + L_{n-i_{k+1}}^{(k+1)} = d-2l-1].
$$
\end{remark}
\begin{example}\label{ex:non_symm_RW}
The assumption of \textit{symmetric} exchangeability is essential in Theorem~\ref{1249} (which is in sharp contrast with Theorem~\ref{theo:expected_walk}).  To see this,  consider i.i.d.\ standard normal random vectors  $\eta_1,\ldots,\eta_n$ in ${\mathbb{R}}^d$ and define
$$
\xi_1(t) := 1 + t\eta_1,\quad \ldots, \quad \xi_n(t) := 1 + t\eta_n, \quad t>0.
$$
Clearly, the random vectors $\xi_1(t),\ldots,\xi_n(t)$ satisfy assumptions $(\text{Ex})$ and $(\text{GP})$ for all $t>0$. On the other hand, the corresponding random walk $S_i(t):= \xi_1(t)+\ldots+\xi_i(t)$, $1\leq i\leq n$, starting at $S_0(t):=0$ satisfies
$$
p(t) := {\mathbb{P}}[S_0(t) \text{ is a vertex of } {\mathop{\mathrm{Conv}}\nolimits}(S_0(t),\ldots,S_n(t))] = {\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits}(S_1(t),\ldots,S_n(t))],
$$
which converges to $1$ as $t\to 0$ because ${\mathbbm{1}}_{0 \in {\mathop{\mathrm{Conv}}\nolimits}(S_1(t),\ldots,S_n(t))} \to 0$ as $t\to 0$, a.s. It follows that $p(t)$ cannot be given by~\eqref{eq:S_i_is_vertex} for sufficiently small $t$.
\end{example}

\subsection{Face probabilities for random bridges}
Random bridges are essentially random walks required to return to the origin after $n$ steps.
Let $\xi_1,\ldots,\xi_n$ be random vectors in ${\mathbb{R}}^d$ with partial sums $S_i= \xi_1+\ldots+\xi_i$, $1\leq i\leq n$, and $S_0=0$.
We impose the following assumptions on the increments $\xi_1,\ldots,\xi_n$:
\begin{itemize}
\item[$(\text{Br})$] \textit{Bridge property:} $S_n=\xi_1+\ldots+\xi_n = 0$ a.s.
\item[$(\text{Ex})$] \textit{Exchangeability:} For every permutation $\sigma$ of the set $\{1,\ldots,n\}$ we have the distributional equality
$$
(\xi_{\sigma(1)},\ldots,  \xi_{\sigma(n)}) {\stackrel{d}{=}} (\xi_1,\ldots,\xi_n).
$$
\item[$(\text{GP}')$] \textit{General position:}
For every $1\leq i_1 < \ldots < i_d \leq n-1$, the probability that the vectors $S_{i_1}, \ldots, S_{i_d}$ are linearly dependent, is $0$.
\end{itemize}
The bridge starts and terminates at the origin: $S_0=S_n =0$. Let us stress that, unlike in the case of random walks, we don't need any central symmetry  assumption on the increments. As above, we denote by $C_n= {\mathop{\mathrm{Conv}}\nolimits} (S_0,\ldots,S_n)$ the convex hull of $S_0,\ldots,S_n$ and by $\mathcal F_k(C_n)$ the set of its $k$-faces, where $0\leq k\leq d-1$.

\begin{theorem}\label{1249bridge}
Let $(S_i)_{i=0}^n$ be a random bridge in ${\mathbb{R}}^d$ whose increments $\xi_1,\ldots,\xi_n$ satisfy the above assumptions $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$.
Fix some $0\leq k\leq d-1$ and let $0 \le i_1 < \ldots < i_{k+1}  < n$ be any indices. Then,
$$
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}}) \in \mathcal F_k(C_n)]
=
\frac{
2(Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots)
}{(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1}+i_1)!},
$$
where the $Q_{i_1,\ldots,i_{k+1}}^{(n)}(j)$'s  are the coefficients of the polynomial
\begin{equation*}
\prod_{j=1}^{k+1} ((t+1)(t+2)\ldots (t+i_{j+1}-i_j-1)) = \sum_{j=0}^n Q_{i_1,\ldots,i_{k+1}}^{(n)}(j) t^j,
\end{equation*}
and we put $i_{k+2} = n+i_1$. For $j<0$ and $j>n$ we use the convention $Q_{i_1,\ldots,i_{k+1}}^{(n)}(j):=0$.
\end{theorem}
\begin{remark}
For faces of maximal dimension $k=d-1$ the above formula simplifies to
$$
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{d}}) \in \mathcal F_{d-1}(C_n)]= \frac 2 {(i_2-i_1) \ldots (i_d - i_{d-1}) (n-i_d + i_1)},
$$
which recovers a result obtained in~\cite{vysotsky_zaporozhets}.
\end{remark}
\begin{remark}
At the other extreme, taking $k=0$ in Theorem~\ref{1249bridge} yields the following formula for the probability that $S_i$, where $0\leq i <n$, is a vertex of the convex hull $C_n$:
\begin{equation}\label{eq:exp_vertices_bridge}
{\mathbb{P}}[S_{i} \in \mathcal F_{0}(C_n)] = \frac 2 {n!} \left({\genfrac{[}{]}{0pt}{}{{n}}{{d}}} + {\genfrac{[}{]}{0pt}{}{{n}}{{d-2}}}+\ldots\right).
\end{equation}
Note that the result does not depend on $i$ which becomes quite straightforward  if one notices the cyclic exchangeability: $(\xi_1,\ldots,\xi_n)$ has the same distribution as $(\xi_{i+1},\ldots,\xi_n,\xi_1,\ldots,\xi_{i-1})$ for all $i=0,\ldots,n-1$.  Since ${\genfrac{[}{]}{0pt}{}{{n}}{{1}}} = (n-1)!$,  in the one-dimensional case $d=1$ this reduces to a classical result of Sparre Andersen stating that
$$
{\mathbb{P}}[\max\{S_0,\ldots,S_{n}\} = S_i]  = \frac 1 n, \quad i=0, \ldots, n-1.
$$
\end{remark}

\subsection{Shift averages of face probabilities for non-symmetric random walks}
Let us finally again turn to random walks. As was argued in Example~\ref{ex:non_symm_RW}, face probabilities for \emph{non-symmetric} exchangeable random walks do not enjoy distribution freeness. On the other hand, the next theorem states that certain shift averages of face probabilities are distribution-free.
\begin{theorem}\label{theo:1139}
Let $C_n={\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_n)$ be the convex hull of a random walk $(S_i)_{i=0}^n$ in ${\mathbb{R}}^d$ whose increments $(\xi_1,\ldots,\xi_n)$ satisfy conditions $(\text{Ex})$ and $(\text{GP})$, but do not need to satisfy $(\pm\text{Ex})$.
Then, for all $0\leq k \leq d-1$ and for all indices $1\leq l_1 < \ldots < l_k \leq n$,
\begin{multline*}
\frac 1 {n+1-l_k} \sum_{i=0}^{n-l_k} {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)]
\\=
\frac 1 {n+1} \sum_{i=0}^{n} {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)]
\\=
\frac{
2(Q_{0, l_1,\ldots,l_{k}}^{(n+1)}(d-k-1) + Q_{0,l_1,\ldots,l_{k}}^{(n+1)}(d-k-3)+\ldots)
}{l_1! (l_2-l_1)!\ldots  (l_{k}-l_{k-1})! (n+1-l_{k})!},
\end{multline*}
where in the second line we put $S_{i+l_j} = S_{(i+l_j)-(n+1)}$ if $i+l_j\geq n+1$.
\end{theorem}
The proof of Theorem~\ref{theo:1139}  will be given in Section~\ref{sec:proof_shifted}. This proof, as well as the proof of Theorem~\ref{theo:expected_walk} uses a construction which allows to build a bridge out of a non-symmetric random walk $S_0,\ldots,S_n$ by adding an additional increment $\xi_{n+1} = -S_n$ and reshuffling the $n+1$ increments randomly to enforce exchangeability.
For faces of maximal dimension $k=d-1$, a different proof of Theorem~\ref{theo:1139} (without the middle term)  was given by Vysotsky and Zaporozhets~\cite{vysotsky_zaporozhets}.

 

\section{Absorption probability for the joint convex hull}

\subsection{Connection to the absorption probabilities}
Let us describe the idea of proof of Theorems~\ref{1249} an~\ref{1249bridge}. For concreteness, consider a symmetric random walk $(S_i)_{i=0}^n$ in the three-dimensional space ${\mathbb{R}}^3$. Given some $0\leq i_1 < i_2 \leq n$ we ask for the probability that the segment $[S_{i_1},S_{i_2}]$ is an edge of the polytope $C_n = {\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_n)$. Denote by $l$ the line passing through the points $S_{i_1}$ and $S_{i_2}$, and let $h$ be some two-dimensional plane orthogonal to the line $l$. The intersection point of $l$ and $h$ is denoted by $P_0$.
Consider an orthogonal projection of the random walk $S_0,\ldots,S_n$ onto the plane $h$. Since the projection of the points $S_{i_1}$ and $S_{i_2}$ is $P_0$ (which we from now on view as the ``origin'' of the plane $h$),  we can split the projected random walk path into three components: two ``walks'' from $P_0$ to the projections of $S_0$ and $S_n$, and one ``bridge'' from $P_0$ to $P_0$. The basic geometric observation underlying our proof of Theorems~\ref{1249}, \ref{1249bridge} is as follows: $[S_{i_1},S_{i_2}]$ is an edge of the convex hull $C_n$ if and only if the joint convex hull of these three projected paths does not contain the point $P_0$.
Thus, we need to compute the so-called \textit{non-absorption probability}, that is the probability that a joint convex hull of several random walks and bridges starting at the origin does not contain the origin. Problems of this type for just one random walk or random bridge were considered in our recent work~\cite{KVZ15}.

\subsection{Absorption probability for joint convex hulls}
Consider a collection of $s$ random walks and $r$ random bridges in ${\mathbb{R}}^d$ whose increments have a joint distribution invariant under the following transformations: we are allowed to perform any signed permutation of the increments inside any random walk, and any permutation of increments inside any random bridge. The next theorem provides a distribution-free formula for the probability that the joint convex hull of such random walks and bridges absorbs the origin. A particular case of this theorem was stated without proof in~\cite[Theorem~2.7]{KVZ15}.

\begin{theorem}\label{1435}
Fix $s,r\in {\mathbb{N}}_0$ (not vanishing simultaneously), $m_1,\ldots,m_s, n_1,\ldots,n_r\in{\mathbb{N}}$  and consider $d$-dimensional random vectors
\begin{equation}\label{1942}
\xi_1^{(1)}, \ldots, \xi_{n_1}^{(1)}, \;\; \ldots,\;\; \xi_1^{(s)}, \ldots, \xi_{n_s}^{(s)},\;\;
\eta_1^{(1)}, \ldots, \eta_{m_1}^{(1)},\;\; \ldots,\;\; \eta_1^{(r)}, \ldots, \eta_{m_r}^{(r)}
\end{equation}
such that $\eta_1^{(j)}+ \ldots+ \eta_{m_j}^{(j)}=0$ a.s.\ for every $1\leq j \leq r$.
Assume that for all permutations $\sigma^{(1)}\in {\text{\rm Sym}}(n_1)$, $\ldots$, $\sigma^{(s)} \in {\text{\rm Sym}}(n_s)$, $\theta^{(1)}\in {\text{\rm Sym}}(m_1)$, $\ldots$, $\theta^{(r)} \in {\text{\rm Sym}}(m_r)$ and all signs ${\varepsilon}_1^{(1)}, \ldots, {\varepsilon}_{n_1}^{(1)}$, $\ldots$, ${\varepsilon}_1^{(s)}, \ldots, {\varepsilon}_{n_s}^{(s)} \in \{-1, +1\}$ we have the distributional equality
\begin{multline} \label{eq:invar_product}
\left(
\xi_1^{(1)}, \ldots, \xi_{n_1}^{(1)}, \;\; \ldots,\;\; \xi_1^{(s)}, \ldots, \xi_{n_s}^{(s)},
\;\;
\eta_1^{(1)}, \ldots, \eta_{m_1}^{(1)},\;\; \ldots, \;\; \eta_1^{(r)}, \ldots, \eta_{m_r}^{(r)}
\right)\\
{\stackrel{d}{=}}
\left(
{\varepsilon}_1^{(1)} \xi_{\sigma_1(1)}^{(1)}, \ldots, {\varepsilon}_{n_1}^{(1)} \xi_{\sigma_1(n_1)}^{(1)},
\;\;\ldots, \;\;
{\varepsilon}_{1}^{(s)}\xi_{\sigma_s(1)}^{(s)}, \ldots, {\varepsilon}_{n_s}^{(s)}\xi_{\sigma_s(n_s)}^{(s)},\right.
\\
\left.\eta_{\theta_1(1)}^{(1)}, \ldots,  \eta_{\theta_1(m_1)}^{(1)},
\;\;\ldots,\;\;
\eta_{\theta_r(1)}^{(r)}, \ldots, \eta_{\theta_r(m_r)}^{(r)}
\right).
\end{multline}
Consider a collection of $s$ ``random walks'' $(S_l^{(1)})_{l=1}^{n_1},\ldots, (S_l^{(s)})_{l=1}^{n_s}$ and $r$ ``random bridges'' $(R_l^{(1)})_{l=1}^{m_1},\ldots, (R_l^{(r)})_{l=1}^{m_r}$ defined by
\begin{align*}
S_l^{(i)} &= \xi_1^{(i)} + \ldots + \xi_{l}^{(i)}, \,  1\leq i \leq s,\,  1\leq l \leq n_i,\\
R_l^{(j)} &= \eta_1^{(j)} + \ldots + \eta_{l}^{(j)}, \,  1\leq j \leq r,\,  1\leq l \leq m_j.
\end{align*}
Write $H$ for the joint convex hull of these walks and bridges, that is
\begin{equation}\label{eq:def_joint_convex_hull_H}
H={\mathop{\mathrm{Conv}}\nolimits} \left(S_1^{(1)}, \ldots, S_{n_1}^{(1)}, \;\; \ldots,\;\; S_1^{(s)}, \ldots, S_{n_s}^{(s)},
\;\;
R_1^{(1)}, \ldots, R_{m_1-1}^{(1)}, \;\; \ldots,\;\; R_1^{(r)}, \ldots, R_{m_r-1}^{(r)}\right).
\end{equation}
Assume, finally, that the following general position condition holds: With probability $1$, any size $d$ subset of the list on the right-hand side of~\eqref{eq:def_joint_convex_hull_H} is linearly independent.
Then,
\begin{equation}\label{eq:absorption}
{\mathbb{P}}[0\in H]=
\frac{2(P(d+1) + P(d+3)+\ldots)}{2^{n_1} n_1!\ldots 2^{n_s} n_s! m_1!\ldots  m_r!},
\end{equation}
where the $P(j)$'s (which also depend on $s,r,n_1,\ldots,n_s,m_1,\ldots,m_r$) are the coefficients of the polynomial
\begin{equation}\label{eq:def_p_k}
\prod_{i=1}^s ((t+1)(t+3)\ldots (t+2n_i-1))\times \prod_{j=1}^r ((t+1)(t+2)\ldots (t+m_j-1)) = \sum_{j=0}^\infty P(j) t^j. 
\end{equation}
\end{theorem}

\begin{remark}
Theorem~\ref{1435} computes the so-called \emph{absorption probability}. The non-absorption probability is given by
\begin{equation}\label{eq:non_absorption}
{\mathbb{P}}[0\notin H]=
\frac{2(P(d-1) + P(d-3)+\ldots)}{2^{n_1} n_1!\ldots 2^{n_s} n_s! m_1!\ldots  m_r!},
\end{equation}
with the usual convention $P(j):=0$ for $j < 0$. To see the equivalence of~\eqref{eq:absorption} and~\eqref{eq:non_absorption} note that
$$
\sum_{j=0}^{\infty} P(j) = 2^{n_1} n_1!\ldots 2^{n_s} n_s! m_1!\ldots  m_r!,
\quad
\sum_{j=0}^{\infty} (-1)^j P(j) = 0,
$$
which is obtained by taking $t=+1$ and $t= -1$ in~\eqref{eq:def_p_k}.
\end{remark}

\begin{remark}\label{rem:non_gen_position_absorption}
Without the general position condition, it holds that
\begin{equation*}
{\mathbb{P}}[0\in {\mathop{\mathrm{Int}}\nolimits} H]\leq
\frac{2(P(d+1) + P(d+3)+\ldots)}{2^{n_1} n_1!\ldots 2^{n_s} n_s! m_1!\ldots  m_r!}
\leq
{\mathbb{P}}[0\in H],
\end{equation*}
where ${\mathop{\mathrm{Int}}\nolimits} H$ is the interior of $H$. We omit the proof of these inequalities because it is analogous to the proof of Proposition~2.10 in~\cite{KVZ15}.
\end{remark}

\section{Proof of Theorem~\ref{1435}}

\subsection{Symmetry groups and Weyl chambers}
In a recent work~\cite{KVZ15},  we showed that absorption probabilities for symmetric random walks (respectively, random bridges) can be interpreted geometrically using Weyl chambers of type $B_n$ (respectively, $A_{n-1}$).  We are going to extend this by  showing that Theorem~\ref{1435}, dealing with convex hulls of \emph{several} walks and bridges, can be interpreted in terms of Weyl chambers corresponding to a \emph{direct product} of several reflection groups. The possibility of extension to direct products was mentioned without proof in Theorem~2.7 of~\cite{KVZ15}.
We start by recalling some relevant definitions.

The \emph{reflection group of type $B_n$} is the symmetry group of the regular cube $[-1,1]^n$ (or of its dual, the regular crosspolytope). The elements of this group act on ${\mathbb{R}}^n$ by permuting the coordinates in arbitrary way and multiplying any number of coordinates by $-1$. The number of elements of this group is $2^n n!$. We shall not distinguish between an abstract group and its action because it is convenient for our purposes.

The \emph{reflection group of type $A_{n-1}$} is the symmetric group ${\text{\rm Sym}}(n)$ which acts on ${\mathbb{R}}^{n}$ by permuting the coordinates. The number of elements of this group is $n!$. The action of this group leaves the following hyperplane invariant:
$$
L_n= \{(x_1,\ldots,x_{n})\in {\mathbb{R}}^{n}\colon x_1+\ldots+x_{n} = 0\},
$$
which explains why the subscript $n-1$ rather than $n$ appears in the standard notation $A_{n-1}$.
Note that the group $A_{n-1}$ is the symmetry group of the regular simplex with $n$ vertices (defined as the convex hull of the standard basis in ${\mathbb{R}}^{n}$).

The {\it fundamental Weyl chambers} of type $A_{n-1}$ and $B_n$ are the following convex cones in ${\mathbb{R}}^n$:
\begin{align*}
{\mathcal{C}}(A_{n-1}) &:=\{(x_1,\ldots,x_{n})\in {\mathbb{R}}^n \colon x_1<x_2<\ldots < x_{n}\},\\
{\mathcal{C}}(B_n) &:= \{(x_1,\ldots,x_n)\in{\mathbb{R}}^n\colon 0< x_1<x_2<\ldots < x_n\}.
\end{align*}
Observe that ${\mathcal{C}}(A_{n-1})$ is a {\it fundamental domain} for the reflection group $A_{n-1}$. This means that the cones of the form $g{\mathcal{C}}(A_{n-1})$, $g \in A_{n-1}$, are pairwise disjoint and their closures cover the whole of ${\mathbb{R}}^n$. The closures of the cones $g{\mathcal{C}}(A_{n-1})$ will be referred to as \emph{Weyl chambers of type $A_{n-1}$}. Similarly, the cone ${\mathcal{C}}(B_n)$ is a fundamental domain for the reflection group $B_n$, and the closures of the cones $g{\mathcal{C}}(B_n)$, $g\in B_n$, are called \emph{Weyl chambers of type $B_n$}. Note that there are $n!$ Weyl chambers of type $A_{n-1}$ and $2^n n!$ Weyl chambers of type $B_n$.

In the sequel, fundamental role will be played by the following \emph{reflection group of  direct product type}:
$$
G := B_{n_1} \times \ldots \times B_{n_s}\times A_{m_1-1} \times \ldots \times A_{m_r-1}.
$$
This group acts  in a natural way on ${\mathbb{R}}^{n_1}\times \ldots \times {\mathbb{R}}^{n_s}\times {\mathbb{R}}^{m_1}\times \ldots \times {\mathbb{R}}^{m_r} \equiv {\mathbb{R}}^n$,  where
$$
n=n_1+\ldots+n_s+m_1+\ldots+m_r.
$$
Let  $e_1^{(i)},\ldots, e_{n_i}^{(i)}$ be the standard basis of ${\mathbb{R}}^{n_i}$ (for all $1\leq i\leq s$) and let $f_1^{(j)},\ldots, f_{m_j}^{(j)}$ be the standard basis of ${\mathbb{R}}^{m_j}$ (for all $1\leq j\leq r$). Then, the elements of $G$ can be represented as tuples of the form
\begin{equation}\label{eq:g_def}
g=(g_{\sigma^{(1)}, {\varepsilon}^{(1)}}, \ldots, g_{\sigma^{(s)}, {\varepsilon}^{(s)}},h_{\theta^{(1)}}, \ldots, h_{\theta^{(r)}}),
\end{equation}
where $\sigma^{(i)}\in {\text{\rm Sym}}(n_i),\theta^{(j)}\in {\text{\rm Sym}}(m_j)$ are permutations, ${\varepsilon}^{(i)}:=({\varepsilon}_1^{(i)},\ldots,{\varepsilon}_{n_i}^{(i)})\in \{-1,+1\}^{n_i}$,  each $g_{\sigma^{(i)}, {\varepsilon}^{(i)}}$ is an orthogonal transformation of ${\mathbb{R}}^{n_i}$ defined by
\begin{equation}\label{eq:g_def1}
g_{\sigma^{(i)},{\varepsilon}^{(i)}} (e_k^{(i)}) = {\varepsilon}_k^{(i)} e_{\sigma^{(i)}(k)}^{(i)}, \quad k=1,\ldots,n_i,
\end{equation}
while each $h_{\theta^{(j)}}$ is an orthogonal transformation of ${\mathbb{R}}^{m_j}$ defined by
\begin{equation}\label{eq:g_def2}
h_{\theta^{(j)}} (f_l^{(j)}) = f_{\theta^{(j)}(l)}^{(j)}, \quad l=1,\ldots,m_j.
\end{equation}
The total number of elements in the group  $G$ is $2^{n_1} n_1!\ldots 2^{n_s} n_s!m_1!\ldots  m_r!$.

\subsection{Absorption probability and subspaces intersecting Weyl chambers}
Consider the Weyl chambers
\begin{align*}
C_B^{(i)} &:= \{(x_1^{(i)}, \ldots, x_{n_i}^{(i)})\in {\mathbb{R}}^{n_i} \colon 0< x_1^{(i)}< \ldots < x_{n_i}^{(i)}\} \subset {\mathbb{R}}^{n_i},\\
C_A^{(j)} &:= \{(y_1^{(j)}, \ldots, y_{m_j}^{(i)})\in {\mathbb{R}}^{m_j} \colon y_1^{(j)}< \ldots < y_{m_j}^{(i)}\} \subset {\mathbb{R}}^{m_j}
\end{align*}
and their direct product
$$
C:= C_B^{(1)}\times \ldots \times C_B^{(s)}\times C_A^{(1)}\times \ldots \times C_A^{(r)} \subset {\mathbb{R}}^{n_1}\times \ldots \times {\mathbb{R}}^{n_s}\times {\mathbb{R}}^{m_1}\times \ldots \times {\mathbb{R}}^{m_r}\equiv {\mathbb{R}}^n.
$$
Let $\bar C$ denote the closure of $C$.
Note that $C$ is a fundamental domain for the action of $G$ on ${\mathbb{R}}^n$. The closed convex cones $g \bar C$, where $g\in G$, are called \emph{Weyl chambers (of product type)}.
Let $L_{m_j}$ be the hyperplane leaved invariant by the action of the group $A_{m_j-1}$:
\begin{equation}\label{eq:L_m_j_def}
L_{m_j}= \{(x_1,\ldots,x_{m_j})\in {\mathbb{R}}^{m_j}\colon x_1+\ldots+x_{m_j} = 0\}, \quad 1\leq j\leq r,
\end{equation}
and consider the linear subspace
$$
L:= {\mathbb{R}}^{n_1}\times \ldots \times {\mathbb{R}}^{n_s}\times L_{m_1}\times \ldots \times L_{m_r} \subset {\mathbb{R}}^n.
$$
Note that the action of $G$ leaves $L$ invariant. Let $A$ be a $d\times n$-matrix with the columns
\begin{equation}\label{eq:list_of_vectors}
\xi_1^{(1)},\ldots,\xi_{n_1}^{(1)},\;\; \ldots,\;\; \xi_1^{(s)},\ldots,\xi_{n_s}^{(s)},\;\;
\eta_1^{(1)}, \ldots, \eta_{m_1}^{(1)},\;\; \ldots,\;\; \eta_1^{(r)}, \ldots, \eta_{m_r}^{(r)}.
\end{equation}
We can view $A: {\mathbb{R}}^n \to {\mathbb{R}}^d$ as a linear operator mapping the standard basis of ${\mathbb{R}}^n$
\begin{equation}\label{eq:standard_basis}
e_1^{(1)},\ldots,e_{n_1}^{(1)},\;\;\ldots,\;\; e_1^{(s)},\ldots,e_{n_s}^{(s)},\;\;
f_1^{(1)}, \ldots, f_{m_1}^{(1)},\;\; \ldots,\;\; f_1^{(r)}, \ldots, f_{m_r}^{(r)}
\end{equation}
to the vectors listed in~\eqref{eq:list_of_vectors}. The next lemma states that the absorption probability equals  the probability that the random linear subspace $(\operatorname*{Ker} A)\cap L$ intersects any given Weyl chamber $g \bar C$ in a non-trivial way.
\begin{lemma}\label{1436}
Under the assumptions of Theorem~\ref{1435}, for every $g \in G$,
\begin{multline*}
{\mathbb{P}}[0\in {\mathop{\mathrm{Conv}}\nolimits} (S_1^{(1)}, \ldots, S_{n_1}^{(1)},\;\; \ldots,\;\; S_1^{(s)}, \ldots, S_{n_s}^{(s)},\;\;
R_1^{(1)}, \ldots, R_{m_1}^{(1)},\;\;\ldots,\;\; R_1^{(r)}, \ldots, R_{m_r}^{(r)})]
\\={\mathbb{P}}[(\operatorname*{Ker} A) \cap  L\cap (g\bar C) \neq \{0\}].
\end{multline*}
\end{lemma}
\begin{proof}
We are interested in the probability of the event
$$
E := \{(\operatorname*{Ker} A) \cap L \cap  (g\bar C) \neq \{0\}\}= \{ \operatorname*{Ker} (Ag) \cap L \cap \bar C \neq \{0\}\}.
$$
Recall that $g:{\mathbb{R}}^n\to{\mathbb{R}}^n$ is a linear operator given by~\eqref{eq:g_def}, \eqref{eq:g_def1}, \eqref{eq:g_def2}. The columns of the matrix $Ag$ are
\begin{align*}
&{\varepsilon}_1^{(1)} \xi_{\sigma^{(1)}(1)}^{(1)},  \ldots, {\varepsilon}_{n_1}^{(1)} \xi_{\sigma^{(1)}(n_1)}^{(1)},
\;\;\ldots, \;\;
{\varepsilon}_{1}^{(s)}\xi_{\sigma^{(s)}(1)}^{(s)}, \ldots, {\varepsilon}_{n_s}^{(s)}\xi_{\sigma^{(s)}(n_s)}^{(s)},\\
& \eta_{\theta^{(1)}(1)}^{(1)}, \ldots, \eta_{\theta^{(1)}(n_1)}^{(1)},
\;\;\ldots,\;\; \eta_{\theta^{(r)}(1)}^{(r)}, \ldots, \eta_{\theta^{(r)}(m_r)}^{(r)},
\end{align*}
as one can easily check by computing the action of $A g$ on the standard basis of ${\mathbb{R}}^n$; see~\eqref{eq:standard_basis}. So, we can write the event $E$ in the form
\begin{multline}\label{eq:E_product}
E = \Big\{\exists (x^{(1)},\ldots, x^{(s)},y^{(1)},\ldots, y^{(r)}) \in (\bar C_B^{(1)}\times \ldots \times \bar C_B^{(s)}\times \bar C_A^{(1)}\times \ldots \times \bar C_A^{(r)})\cap (L {\backslash}\{0\})
\colon\\
\sum_{i=1}^{s} \left({\varepsilon}_1^{(i)} \xi^{(i)}_{\sigma^{(i)}(1)} x_1^{(i)} + \ldots + {\varepsilon}_n^{(i)} \xi^{(i)}_{\sigma^{(i)}(n_i)} x_{n_i}^{(i)}\right)
+\sum_{j=1}^{r} \left( \eta^{(j)}_{\theta^{(j)}(1)} y_1^{(j)} + \ldots + \eta^{(j)}_{\theta^{(j)}(m_j)} y_{m_j}^{(j)}\right)= 0\Big\}.
\end{multline}
There is a bijective correspondence between $x^{(i)}= (x_1^{(i)},\ldots,x_{n_i}^{(i)})\in \bar C_B^{(i)}$ and $\tilde{x}^{(i)} = (\tilde{x}_1^{(i)},\ldots,\tilde{x}_{n_i}^{(i)})\in {\mathbb{R}}_{\geq 0}^{n_i}$ given by
$$
x_1^{(i)}=\tilde{x}_1^{(i)}, \;\; x_2^{(i)}=\tilde{x}_1^{(i)}+\tilde{x}_2^{(i)},\;\; \ldots, \;\;  x_{n_i}^{(i)}= \tilde{x}_{1}^{(i)}+\ldots+\tilde{x}_{n_i}^{(i)}.
$$
Similarly, there is a bijective correspondence between $y^{(j)}= (y_1^{(i)},\ldots,y_{m_j}^{(j)})\in \bar C_A^{(j)}\cap L_{m_j}$ and $\tilde{y}^{(j)} = (\tilde{y}_1^{(j)},\ldots,\tilde{y}_{m_j-1}^{(j)})\in {\mathbb{R}}_{\geq 0}^{m_j-1}$ given by
$$
\tilde{y}_1^{(j)}=y_2^{(j)}-y_1^{(j)},\;\; \ldots, \;\; \tilde{y}_{m_j-1}^{(j)}=y_{m_j}^{(j)}-y_{m_j-1}^{(j)},
$$
or, equivalently,
$$
y_1^{(j)}=\tilde{y}_0^{(j)}, \;\; y_2^{(j)}=\tilde{y}_0^{(j)}+\tilde{y}_1^{(j)},\;\; \ldots, \;\;  y_{m_j}^{(j)}= \tilde{y}_{1}^{(j)}+\ldots+\tilde{y}_{m_j-1}^{(j)},
$$
where $\tilde{y}_0^{(j)}$ is chosen such that the condition $y_1^{(j)}+\ldots+y_{m_j}^{(j)}=0$ holds.
Thus, we have
\begin{multline*}
E = \Big\{\exists (\tilde{x}^{(1)},\ldots, \tilde{x}^{(s)},\tilde{y}^{(1)},\ldots, \tilde{y}^{(r)}) \in
({\mathbb{R}}_{\geq 0}^{n_1}\times \ldots \times {\mathbb{R}}_{\geq 0}^{n_s}\times {\mathbb{R}}_{\geq 0}^{m_1-1}\times \ldots \times {\mathbb{R}}_{\geq 0}^{m_r-1}){\backslash} \{0\}
\colon\\
\sum_{i=1}^{s} \sum_{k=1}^{n_i} \tilde{x}_k^{(i)} \left({\varepsilon}_k^{(i)} \xi^{(i)}_{\sigma^{(i)}(k)} + \ldots + {\varepsilon}_{n_i}^{(i)} \xi^{(i)}_{\sigma^{(i)}(n_i)}\right)
+\sum_{j=1}^{r} \sum_{l=1}^{m_j-1} \tilde{y}_l^{(j)} \left( \eta^{(j)}_{\theta^{(j)}(l+1)} + \ldots +  \eta^{(j)}_{\theta^{(j)}(m_j)}\right)
 = 0\Big\}
\end{multline*}
modulo null sets, where we omitted the term
$$
\tilde{y}_0^{(j)} \left(\eta^{(j)}_{\theta^{(j)}(1)} + \ldots +  \eta^{(j)}_{\theta^{(j)}(m_j)}\right)
=
\eta_1^{(j)}+ \ldots+ \eta_{m_j}^{(j)}=0
\quad
\text{a.s.}
$$
which vanishes by the bridge condition.
The invariance assumption~\eqref{eq:invar_product} implies the distributional equality
\begin{multline}
\left(\Big\{{\varepsilon}_k^{(i)} \xi^{(i)}_{\sigma^{(i)}(k)} + \ldots + {\varepsilon}_{n_i}^{(i)}
\xi^{(i)}_{\sigma^{(i)}(n_i)}\Big\}_{\substack{i=1,\ldots,s\\k=1,\ldots, n_i}},
\Big\{\eta^{(j)}_{\theta^{(j)}(l+1)} + \ldots +  \eta^{(j)}_{\theta^{(j)}(m_j)}\Big\}_{\substack{j=1,\ldots,r\\l=1,\ldots, m_j-1}}
\right)
\\
{\stackrel{d}{=}}
\left(
\Big\{S_{n_i-k+1}^{(i)}\Big\}_{\substack{i=1,\ldots,s\\k=1,\ldots, n_i}},
\Big\{R_{m_j-l}^{(j)}\Big\}_{\substack{j=1,\ldots,r\\l=1,\ldots, m_j-1}}
\right).
\end{multline}
Therefore,
\begin{multline*}
{\mathbb{P}}[E]
= {\mathbb{P}}\Big[\exists (\tilde{x}^{(1)},\ldots, \tilde{x}^{(s)},\tilde{y}^{(1)},\ldots, \tilde{y}^{(r)}) \in
({\mathbb{R}}_{\geq 0}^{n_1}\times \ldots \times {\mathbb{R}}_{\geq 0}^{n_s}\times {\mathbb{R}}_{\geq 0}^{m_1-1}\times \ldots \times {\mathbb{R}}_{\geq 0}^{m_r-1}){\backslash} \{0\}
\colon \\
\sum_{i=1}^s \left(\tilde{x}_1^{(i)} S_{n_i}^{(i)} + \tilde{x}_2^{(i)} S_{n_i-1}^{(i)} + \ldots + \tilde{x}_{n_i}^{(i)} S_1^{(i)}\right)
\\+\sum_{j=1}^r \left(\tilde{y}_1^{(i)} R_{m_j-1}^{(i)} + \tilde{y}_2^{(j)} R_{m_j-1}^{(j)} + \ldots + \tilde{y}_{m_j-1}^{(j)} R_1^{(j)}\right) = 0\Big].
\end{multline*}
The term on the right-hand side is the probability that the joint convex hull of the walks $S_k^{(i)}$, $1\leq k\leq n_i$, $1\leq i\leq s$, and the bridges $R_l^{(j)}$, $1\leq l\leq m_j-1$, $1\leq j\leq r$, contains $0$. This proves the lemma.
\end{proof}

\subsection{Hyperplane arrangements}
Now we need some results from the theory of hyperplane arrangements~\cite{OT92,rS07}.
A \emph{linear hyperplane arrangement} (or simply ``\textit{arrangement}'') ${\mathcal{A}}$ is a finite set of distinct hyperplanes in ${\mathbb{R}}^n$ that pass through the origin.
The \emph{rank} of an arrangement ${\mathcal{A}}$ is the codimension of the intersection of all hyperplanes in the arrangement:
$$
{\mathop{\mathrm{rank}}\nolimits}({\mathcal{A}})=n-\dim\left(\bigcap_{H\in{\mathcal{A}}}H\right).
$$
Equivalently, the rank is  the dimension of the space spanned by the normals to the hyperplanes in ${\mathcal{A}}$.
The \emph{characteristic polynomial} $\chi_{\mathcal{A}}(t)$ of the arrangement ${\mathcal{A}}$ is defined by
\begin{equation}\label{1459}
\chi_{\mathcal{A}}(t)=\sum_{{\mathcal{B}}\subset{\mathcal{A}}}(-1)^{\#{\mathcal{B}}}t^{n-{\mathop{\mathrm{rank}}\nolimits}({\mathcal{B}})},
\end{equation}
where $\#{\mathcal{B}}$ denotes the number of elements in the set ${\mathcal{B}}$, and ${\mathop{\mathrm{rank}}\nolimits}(\varnothing) = 0$ under convention that the intersection over the empty set of hyperplanes is ${\mathbb{R}}^n$. The original definition of the characteristic polynomial uses the notions of the intersection poset of ${\mathcal{A}}$ and the M\"obius function on it; see~\cite[Section~1.3]{rS07}. The equivalence of both definitions was proved by Whitney; see, e.g., \cite[Lemma~2.3.8]{OT92} or~\cite[Theorem~2.4]{rS07}.

Denote by ${\mathcal{R}}({\mathcal{A}})$  the finite set of open connected components (``\emph{regions}'' or ``\emph{chambers}'') of the complement ${\mathbb{R}}^n\setminus\cup_{H\in{\mathcal{A}}} H$ of the hyperplanes. 
The following fundamental result due to Zaslavsky~\cite{tZ75} (see also~\cite[Theorem~2.5]{rS07}) expresses the number of regions of the arrangement ${\mathcal{A}}$ in terms of its characteristic polynomial:
\begin{equation}\label{1112}
\# {\mathcal{R}}({\mathcal{A}})=(-1)^n\chi_{\mathcal{A}}(-1).
\end{equation}

The \emph{lattice} ${\mathcal{L}}({\mathcal{A}})$ generated by an arrangement ${\mathcal{A}}$ in ${\mathbb{R}}^n$ consists of all linear subspaces that can be represented as intersections of some of the hyperplanes from ${\mathcal{A}}$, that is
$$
{\mathcal{L}}({\mathcal{A}}) = \left\{\bigcap_{H\in{\mathcal{B}}}H \colon {\mathcal{B}} \subset {\mathcal{A}}\right\}.
$$
By definition, ${\mathbb{R}}^n\in {\mathcal{L}}({\mathcal{A}})$, corresponding to the empty intersection over ${\mathcal{B}}=\varnothing$. Let $M_{n-d}$ be a linear subspace in ${\mathbb{R}}^n$ of codimension $d\leq n-1$. We say that $M_{n-d}$ is in \emph{general position} with respect to ${\mathcal{A}}$ if for all $K\in {\mathcal{L}}({\mathcal{A}})$,
\begin{equation}\label{1222}
\dim (M_{n-d}\cap K) =
\begin{cases}
\dim K -  d, &\text{if } \dim K \geq d,\\
0, &\text{if } \dim K \leq d.
\end{cases}
\end{equation}
The next theorem provides a formula for the number of regions in ${\mathcal{R}}({\mathcal{A}})$ intersected by a linear subspace in general position. We refer to~\cite{KVZ15} for its proof.
\begin{theorem}\label{1229}
Let $M_{n-d}$ be linear subspace in ${\mathbb{R}}^n$ of codimension $d$ that is in general position w.r.t.\ to a linear hyperplane arrangement ${\mathcal{A}}$. Let
\begin{equation}\label{eq:chi_def}
\chi_{\mathcal{A}}(t)=\sum_{k=0}^n (-1)^{n-k} a_kt^k
\end{equation}
be the characteristic polynomial of ${\mathcal{A}}$. Then, the number of regions in ${\mathcal{R}}({\mathcal{A}})$ intersected by $M_{n-d}$ is given by
$$
\#\{R\in {\mathcal{R}}({\mathcal{A}})\colon R\cap M_{n-d}\ne\varnothing\}
=
2(a_{d+1} + a_{d+3} +\ldots),
$$
where we put $a_k=0$ for $k\notin\{0,\ldots,n\}$.
\end{theorem}

Let us consider a special case: the \emph{reflection arrangements} in ${\mathbb{R}}^n$ of types $A_{n-1}$ and $B_n$.
These arrangements consist  of the hyperplanes
\begin{align}
{\mathcal{A}}(A_{n-1})&\colon \quad  \{x_i = x_j\}, \quad 1\leq j < j \leq n, \label{eq:arr_A}\\
{\mathcal{A}}(B_n)&\colon \quad \{x_i = x_j\}, \quad \{x_i = -x_j\}, \quad \{x_k = 0\}, \quad 1\leq i < j \leq n, \quad 1\leq k\leq n, \label{eq:arr_B}
\end{align}
where $(x_1,\ldots,x_n)$ are the coordinates on ${\mathbb{R}}^n$.
It is easily seen that the regions in ${\mathcal{R}}({\mathcal{A}}(A_{n-1}))$ and ${\mathcal{R}}({\mathcal{A}}(B_n))$ are precisely the interiors of the Weyl chambers of type $A_{n-1}$ and $B_n$.

The characteristic polynomials of the reflection arrangements (see pp.~63--64 and Corollary 2.2 on p.~28 in~\cite{rS07}) are given by
\begin{align}
&\chi_{{\mathcal{A}}(A_{n-1})}(t) = t (t-1) \ldots (t-(n-1))= \sum_{k=1}^{n} (-1)^{n-k} {\genfrac{[}{]}{0pt}{}{{n}}{{k}}} t^k,  \label{eq:chi_A} \\
\chi&_{{\mathcal{A}}(B_{n})}(t) = (t-1)(t-3)\ldots (t-(2n-1)) = \sum_{k=0}^{n} (-1)^{n-k} B(n,k)t^k, \label{eq:chi_B}
\end{align}
where ${\genfrac{[}{]}{0pt}{}{{n}}{{k}}}$ (the Stirling numbers of the first kind) and $B(n,k)$ (their $B$-analogues) have the following generating functions:
\begin{equation*}
t(t+1) \ldots (t+n-1) = \sum_{k=1}^\infty {\genfrac{[}{]}{0pt}{}{{n}}{{k}}} t^k,
\quad
(t+1)(t+3)\ldots (t+2n-1) = \sum_{k=0}^n B(n,k) t^k.
\end{equation*}

\subsection{Proof of Theorem~\ref{1435}}
We are now ready to complete the proof of Theorem~\ref{1435}. Applying Lemma~\ref{1436} to all $g\in G$ and taking the arithmetic mean, we obtain
\begin{align}\label{eq:reduction}
{\mathbb{P}}[0\in H]
=
\frac 1 {\# G} \sum_{g\in G} {\mathbb{P}}[(\operatorname*{Ker} A) \cap L \cap (g\bar C) \neq \{0\}]
=
\frac {{\mathbb E} N} {\# G} ,
\end{align}
where the random variable
\begin{equation} \label{eq:N=}
N := \sum_{g\in G} {\mathbbm{1}}_{\{(\operatorname*{Ker} A) \cap L\cap  (g\bar C) \neq \{0\}\}}
\end{equation}
counts the number of Weyl chambers of the form $g\bar C$, $g\in G$, intersected by  $(\operatorname*{Ker} A)\cap L$ nontrivially.

Given arbitrary arrangements ${\mathcal{A}}_1,\ldots,{\mathcal{A}}_M$ in ${\mathbb{R}}^{q_1},\ldots,{\mathbb{R}}^{q_M}$, define their \emph{direct product} as the following arrangement in ${\mathbb{R}}^q \equiv {\mathbb{R}}^{q_1+\ldots+q_M}$:
\begin{multline*}
  {\mathcal{A}}_1\times\ldots\times{\mathcal{A}}_M= \\
  \{ H \times {\mathbb{R}}^{q-q_1} \}_{H \in {\mathcal{A}}_1}\;\bigcup\; \{ {\mathbb{R}}^{q_1} \times H \times {\mathbb{R}}^{q-q_1-q_2}\}_{H \in {\mathcal{A}}_2} \;\bigcup\; \ldots
  \;\bigcup\; \{ {\mathbb{R}}^{q-q_M} \times H\}_{H \in {\mathcal{A}}_M}.
\end{multline*}
Consider a reflection arrangement ${\mathcal{A}}$ of type $B_{n_1} \times \ldots \times B_{n_s}\times A_{m_1-1} \times \ldots \times A_{m_r-1}$, that is
$$
{\mathcal{A}}={\mathcal{A}}(B_{n_1}) \times \ldots \times {\mathcal{A}}(B_{n_s})\times {\mathcal{A}}(A_{m_1-1}) \times \ldots \times {\mathcal{A}}(A_{m_r-1}).
$$
The characteristic polynomial of a direct product of arrangements is the product of the individual characteristic polynomials (Lemma~2.50 on p.~43 in~\cite{OT92}), hence
\begin{align*}
(-1)^{n} \chi_{\mathcal{A}} (-t)
&=\prod_{i=1}^s ((t+1)(t+3)\ldots (t+2n_i-1)) \times\prod_{j=1}^r (t(t+1)\ldots (t+m_j-1)) \\
&=
\sum_{k=r}^{n} P(k-r) t^k,
\end{align*}
where we recalled the notation $P(k)$ from~\eqref{eq:def_p_k}. It is easy to see that $N$ is the number of regions in ${\mathcal{R}}({\mathcal{A}})$ intersected by $(\operatorname*{Ker} A) \cap L$.
\begin{lemma}\label{lem:gen_pos}
If the general position assumption imposed in Theorem~\ref{1435} holds, then the random linear subspace $(\operatorname*{Ker} A)\cap L$ has codimension $d+r$ in ${\mathbb{R}}^n$ and is in general position w.r.t.\ ${\mathcal{A}}$, with probability $1$.
\end{lemma}
Postponing the proof of the lemma for a moment, we apply Theorem~\ref{1229} to obtain that
$$
N = 2 (P(d+1) + P(d+3) +\ldots)
\quad \text{a.s.}
$$
Combining this equation with \eqref{eq:reduction} completes the proof of Theorem~\ref{1435}. \hfill $\Box$

\begin{proof}[Proof of Lemma~\ref{lem:gen_pos}]
In the case of just one random walk or random bridge, we proved the lemma in~\cite[Section 4.2]{KVZ16_arcsine}. The proof in the direct product case is similar and we sketch only the main ideas. Consider a linear subspace $K$ from the lattice generated by the arrangement ${\mathcal{A}}$. That is, $K$ can be represented as an intersection of some hyperplanes from ${\mathcal{A}}$ and, consequently,
\begin{equation}\label{eq:K_product}
K= K_1\times \ldots \times K_{s} \times K_1'\times \ldots \times K_r'
\end{equation}
for some $K_i\in {\mathcal{L}}({\mathcal{A}}(B_{n_i}))$, $1\leq i\leq s$, and $K_j'\in {\mathcal{L}}({\mathcal{A}}(A_{m_j-1}))$, $1\leq j\leq r$.
Our aim is to prove that
\begin{equation}\label{eq:gen_pos_need0}
\dim (K\cap L \cap \operatorname*{Ker} A) =
\begin{cases}
\dim K - d - r, & \text{ if } \dim K \geq d+r,\\
0, & \text{ if } \dim K \leq d+r.
\end{cases}
\end{equation}
Note in passing that taking $K={\mathbb{R}}^n$ would yield ${\mathop{\mathrm{codim}}\nolimits} (L \cap \operatorname*{Ker} A)= d+r$.  In fact, it suffices to prove that
\begin{equation}\label{eq:gen_pos_need}
\dim (K\cap \operatorname*{Ker} A) =
\begin{cases}
\dim K - d, & \text{ if } \dim K \geq d+r,\\
r, & \text{ if } \dim K \leq d+r.
\end{cases}
\end{equation}
To see that~\eqref{eq:gen_pos_need} implies~\eqref{eq:gen_pos_need0}, note that $K\cap \operatorname*{Ker} A$ contains the $r$-dimensional linear subspace $L^{\bot}$. Indeed, for every $1\leq j\leq r$ we have
$$
A(f_1^{(j)} + \ldots + f_{m_j}^{(j)}) = \eta_1^{(j)} + \ldots + \eta_{m_j}^{(j)} = 0
$$
by definition of $A$ and the bridge property, whence $L^{\bot} \subset \operatorname*{Ker} A$. To see that $L^{\bot} \subset K$, recall that by definition of the arrangement of type $A_{m_j-1}$, see~\eqref{eq:arr_A}, the vector $(1,\ldots,1)$ (with $m_j$ $1$'s) belongs to all hyperplanes from ${\mathcal{A}}(A_{m_j-1})$ and hence, to all linear subspaces from ${\mathcal{L}}({\mathcal{A}}(A_{m_j-1}))$.

Next we are going to write down an explicit system of equations defining $K$. Recall that $(x_1^{(i)}, \ldots, x_{n_i}^{(i)})$ are coordinates on ${\mathbb{R}}^{n_i}$, while $(y_1^{(j)}, \ldots, y_{m_j}^{(j)})$ are coordinates on ${\mathbb{R}}^{m_j}$. Let us first look at the lattice generated by the hyperplane arrangement ${\mathcal{A}}(A_{m_j-1})$; see~\eqref{eq:arr_A} for its definition.  Any linear subspace belonging to this lattice is given by a system of equations of the following type. Decompose the variables $y_{1}^{(j)},\ldots, y_{m_j}^{(j)}$ into some number, say $q(j)$, of non-empty groups, and then require the variables inside the same group to be equal to each other. Linear subspaces belonging to  the lattice generated by the hyperplane arrangement ${\mathcal{A}}(B_{n_i-1})$, see~\eqref{eq:arr_A} for its definition, can be described as follows. Decompose the variables $x_1^{(i)}, \ldots, x_{n_i}^{(i)}$ into some number, say $p(i)+1$ of groups (all groups being non-empty except possibly the last one). Require the variables in the last group to be $0$. For each group except the last one,  multiply the variables in the group by $\pm 1$'s, and require the resulting signed variables to be equal to each other.

Taking all the equations described above together, we obtain a system of equations defining $K$.  However, since the distribution of the linear subspace $(\operatorname*{Ker} A)\cap L$ is invariant w.r.t.\ the action of $G$, after transforming everything by a suitable $g\in G$, we can assume without loss of generality that $K$ is given by the following simplified system of equations.
For every $1\leq i \leq s$ we have the equations
\begin{align*}
&\gamma_1(i):= x^{(i)}_1 = \ldots = x^{(i)}_{u_{1}(i)},\\
&\gamma_2(i) := x^{(i)}_{u_{1}(i) +1} = \ldots = x^{(i)}_{u_{2}(i)},\\
&\ldots,\\
&\gamma_{p(i)}(i):= x^{(i)}_{u_{p(i)-1}(i)+1} = \ldots = x^{(i)}_{u_{p(i)}(i)},\\
&x^{(i)}_{u_{p(i)}(i)+1} = \ldots = x^{(i)}_{n_i} = 0,
\end{align*}
with some $0=u_{0}(i) < u_1(i) < \ldots < u_{p(i)}(i) \leq n_i$, and for every $1\leq j\leq r$, we have the equations
\begin{align*}
&\delta_1(j):=  y^{(j)}_1 = \ldots = y^{(j)}_{v_{1}(j)},\\
&\delta_2(j) := y^{(j)}_{v_{1}(j) +1} = \ldots = y^{(j)}_{v_{2}(j)},\\
&\ldots,\\
&\delta_{q(j)}(j):= y^{(j)}_{v_{q(j)-1}(j)+1} = \ldots = x^{(j)}_{m_j},
\end{align*}
with some $0=v_{0}(j) < v_1(j) < \ldots < v_{q(j)}(j) = m_j$. As coordinates on $K$, we use the variables $\gamma_1(i), \ldots, \gamma_{p(i)}(i)$ ($1\leq i \leq s$) and $\delta_1(j), \ldots, \delta_{q(j)}(j)$ ($1\leq j \leq r$). Note that
\begin{equation}\label{eq:dim_K}
\dim K = \sum_{i=1}^s p(i) + \sum_{j=1}^r q(j).
\end{equation}
The linear subspace $\operatorname*{Ker} A$ is given by the following equation
\begin{equation}\label{eq:Ker_A}
\sum_{i=1}^s \sum_{l=1}^{n_i} x_l^{(i)} \xi_l^{(i)}   + \sum_{j=1}^r \sum_{l=1}^{m_j} y_l^{(i)} \eta_l^{(i)} = 0.
\end{equation}
Inside $K$, the linear subspace $K\cap \operatorname*{Ker} A$ is given by the equation
\begin{multline}\label{eq:gen_pos_main}
\sum_{i=1}^s
\left(\gamma_1(i) S^{(i)}_{u_1(i)} +   \gamma_2(i) (S^{(i)}_{u_2(i)} - S^{(i)}_{u_1(i)}) +\ldots + \gamma_{p(i)}(i) (S^{(i)}_{u_{p(i)}(i)} - S^{(i)}_{u_{p(i)-1}(i)}) \right)
\\+
\sum_{j=1}^r \left( \delta_1(j) R^{(j)}_{v_1(j)} +   \delta_2(j) (R^{(j)}_{v_2(j)} - R^{(j)}_{v_1(j)}) +\ldots + \delta_{q(j)}(j) (0 - R^{(j)}_{v_{q(j)-1}(j)})\right)
=0.
\end{multline}
Recall that the random walks and bridges take values in ${\mathbb{R}}^d$, so that, effectively, \eqref{eq:Ker_A} and~\eqref{eq:gen_pos_main} are systems of $d$ equations each.

Let $\dim K \geq d+r$. Then, by the general position assumption from Theorem~\ref{1435} the collection of vectors
$$
S^{(i)}_{u_1(i)}, S^{(i)}_{u_2(i)},\ldots, S^{(i)}_{u_{p(i)}(i)}, \;\;\; (1\leq i \leq s),\;\;
R^{(j)}_{v_1(j)}, R^{(j)}_{v_2(j)}, \ldots, R^{(j)}_{v_{q(j)-1}(j)}
\;\; (1\leq j\leq r)
$$
spans linearly the whole ${\mathbb{R}}^d$ because the total number of the vectors is at least $d$; see~\eqref{eq:dim_K}. It follows that the system of $d$ equations in~\eqref{eq:gen_pos_main} has full rank, hence the dimension of the set of its solutions is $\dim K - d$, thus proving the first case of~\eqref{eq:gen_pos_need}.  Let now $\dim K \leq d+r$. Then, we can find a linear subspace $K'\supset K$ such that $\dim K'= d+r$ and $K'\in {\mathcal{L}}({\mathcal{A}})$. Applying the above to $K'$, we obtain $\dim (K'\cap L \cap \operatorname*{Ker} A) = 0$, hence $\dim (K\cap L \cap \operatorname*{Ker} A) = 0$, thus proving the second case in~\eqref{eq:gen_pos_need}.
\end{proof}

\section{Proof of Theorems~\ref{1249} and~\ref{1249bridge}}
\begin{proof}[Proof of Theorem~\ref{1249}]
Given $k+1$ vectors $x_1,\ldots,x_{k+1}\in{\mathbb{R}}^d$  denote by ${\mathop{\mathrm{aff}}\nolimits}(x_1,\ldots,x_{k+1})$ their affine hull and by ${\mathop{\mathrm{aff}}\nolimits}^\perp(x_1,\ldots,x_{k+1})$ the orthogonal complement of ${\mathop{\mathrm{aff}}\nolimits}(x_1,\ldots,x_{k+1})$.

Let $\cdot|h$ denote the orthogonal projection onto $h:={\mathop{\mathrm{aff}}\nolimits}^\perp(S_{i_1},\ldots,S_{i_{k+1}})$. Note that $\dim h = d - k$ a.s.\ because
$$
(S_{i_2} -S_{i_1},\ldots, S_{i_{k+1}} - S_{i_1}) {\stackrel{d}{=}} (S_{i_2-i_1},\ldots, S_{i_{k+1}-i_1})
$$
by $(\pm\text{Ex})$ (in fact, condition $(\text{Ex})$ suffices) and the vectors on the right-hand side are a.s.\ linearly independent by $(\text{GP})$.  Projecting the path $S_0,\ldots,S_n$ onto $h$, we obtain $2$ random walks that start at $P_0:=S_{i_1}|h = \ldots= S_{i_{k+1}}|h$, as well as $k$ random bridges that  start and terminate at $P_0$. We are going to apply Theorem~\ref{1435} to these $s=2$ random walks and $r=k$ random bridges. The increments of the two random walks are given by
\begin{align*}
&\xi_1^{(1)}=-\xi_{i_1}|h, \;\;  \xi_2^{(1)}=-\xi_{i_1-1}|h, \;\;  \ldots, \;\; \xi_{i_1}^{(1)}=-\xi_1|h,\\
&\xi_2^{(1)}=\xi_{i_{k+1}+1}|h,\;\; \xi_2^{(2)}=\xi_{i_{k+1}+2}|h, \;\; \ldots, \;\; \xi_{n-i_{k+1}}^{(1)}=\xi_n|h,
\end{align*}
while the increments of the random bridges are given by
$$
\eta_1^{(j)}=\xi_{i_j+1}|h, \;\; \ldots, \;\; \eta_{i_{j+1}-i_j}^{(j)}=\xi_{i_{j+1}-1}|h,
\quad j=1,\ldots k.
$$
Clearly, these increments satisfy the invariance assumptions of Theorem~\ref{1435}, namely one can permute the increments within the walks/bridges and change the signs of the increments in both random walks without changing the joint distribution of the increments.
Denote by $H\subset h$ the joint convex hull of random walks and bridges with these increments. The key observation is as follows:  ${\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}})$ is a $k$-face of $C_n$  if and only if $H$ does not contain $P_0$ (viewed as the origin of $h$).  Postponing the verification of the general position assumption for a moment, we apply Theorem~\ref{1435}, see also~\eqref{eq:non_absorption},  to obtain that
$$
{\mathbb{P}}[P_0\notin H]=
\frac{2(P_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + P_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots)}{2^{i_1+n-i_{k+1}} i_1!(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1})!}
$$
with the same generating function for the $P_{i_1,\ldots,i_{k+1}}^{(n)}(j)$'s as in Theorem~\ref{1249}.

To complete the proof of  Theorem~\ref{1249} we need to verify the general position assumption of Theorem~\ref{1435}. Let $T_1,\ldots,T_{d-k}$ be any $d-k$ vectors from the list
\begin{align*}
&S_{i_1-1} - S_{i_1}, S_{i_1-2} - S_{i_1}, \ldots, S_{1} - S_{i_1}    \quad \text{(first walk, unprojected)}, \\
&S_{i_1+1} - S_{i_1}, S_{i_1+2} - S_{i_1}, \ldots,S_{i_2-1} - S_{i_1}    \quad \text{(first bridge, unprojected)}, \\
&\ldots,\\
&S_{i_k+1} - S_{i_k}, S_{i_k+2} - S_{i_k}, \ldots,S_{i_{k+1}-1} - S_{i_k}    \quad \text{($k$-th bridge, unprojected)}, \\
&S_{i_{k+1}+1} - S_{i_{k+1}}, S_{i_{k+1}+2} - S_{i_{k+1}}, \ldots,S_{n} - S_{i_{k+1}} \quad \text{(second walk, unprojected)}.
\end{align*}
We have to show that  $T_1|h,\ldots, T_{d-k}|h$ are linearly independent with probability $1$. Since the orthogonal complement of $h$ is spanned by $S_{i_2}-S_{i_1},\ldots, S_{i_{k+1}} - S_{i_k}$, we need to show that the vectors
\begin{equation}\label{eq:list_GP_face_probab}
T_1,\ldots,T_{d-k}, S_{i_2}-S_{i_1},\ldots, S_{i_{k+1}} - S_{i_k}
\end{equation}
are linearly independent, with probability $1$. But it is easy to see that their linear hull coincides with the linear hull of
$$
S_{j_1}-S_{j_0}, S_{j_2}-S_{j_1}, \ldots, S_{j_d}-S_{j_{d-1}}
$$
for some $0\leq j_0< j_1 < \ldots < j_d \leq n$. By assumptions $(\pm\text{Ex})$ and $(\text{GP})$, this linear hull has maximal possible dimension $d$, thus proving the linear independence of the vectors~\eqref{eq:list_GP_face_probab}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{1249bridge}]
The main idea is the same as in the previous proof. Consider the linear space $h:={\mathop{\mathrm{aff}}\nolimits}^\perp(S_{i_1},\ldots,S_{i_{k+1}})$ and note that $\dim h = d-k$ a.s.\ by the same argument as in the previous proof. Projecting the closed path $S_0,\ldots,S_n$ onto $h$, we obtain $k+1$ random bridges in $h$ starting and terminating at $P_0:=S_{i_1}|h = \ldots= S_{i_{k+1}}|h$. The random bridge with number $j+1\in \{2,\ldots,k+1\}$ is the projection of the path $S_{i_j}, S_{i_j+1},\ldots, S_{i_{j+1}}$ and has increments
$$
\eta_1^{(j+1)}=\xi_{i_j+1}|h, \;\;  \eta_2^{(j+1)}=\xi_{i_j+2}|h, \;\;  \ldots, \;\; \eta_{i_{j+1} - i_j}^{(j+1)}=\xi_{i_{j+1}}|h, \quad j=1,\ldots,k,
$$
while the random bridge with number $1$ is the projection of the path $S_{i_{k+1}},\ldots, S_{n-1}, S_n=S_0=0, S_1,\ldots, S_{i_1}$ and its increments are
$$
\eta_1^{(1)}=\xi_{i_{k+1}+1} | h, \;\; \ldots, \;\; \eta_{n-i_{k+1}}^{(1)}=\xi_{n}|h,\;\;
\eta_{n-i_{k+1} + 1}^{(1)}=\xi_{1} | h, \;\; \ldots, \;\; \eta_{n-i_{k+1} + i_1}^{(1)}=\xi_{i_1}|h.
$$
Again we observe that the invariance condition of Theorem~\ref{1435} is satisfied for these $r=k+1$ random bridges (and $s=0$ ransom walks) because the joint distribution of the increments is invariant with respect to arbitrary permutations of the increments within the bridges. The general position assumption of Theorem~\ref{1435} will be verified below. Now observe that ${\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots,S_{i_{k+1}})$ is a $k$-face of $C_n$  if and only if $H$ (the joint convex hull of these bridges) does not contain $P_0$.  Theorem~\ref{1435}, see also~\eqref{eq:non_absorption},  yields
$$
{\mathbb{P}}[P_0\notin H]=
\frac{
2(Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots)
}{(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1}+i_1)!}
$$
with the same generating function for the $Q_{i_1,\ldots,i_{k+1}}^{(n)}(j)$'s as in Theorem~\ref{1249bridge}.
To verify the general position assumption of Theorem~\ref{1435},  let $T_1,\ldots,T_{d-k}$ be any $d-k$ vectors from the list
\begin{align*}
&S_{i_{k+1}+1} - S_{i_{k+1}}, S_{i_{k+1}+2} - S_{i_{k+1}}, \ldots, S_{n-1}- S_{i_{k+1}},
 0 - S_{i_{k+1}}, S_1-S_{i_{k+1}},\ldots,S_{i_1-1}-S_{i_{k+1}},  \\%\quad \text{(first bridge, unprojected)},\\
&S_{i_1+1} - S_{i_1}, S_{i_1+2} - S_{i_1}, \ldots,S_{i_2-1} - S_{i_1}, \\
&\ldots,\\
&S_{i_k+1} - S_{i_k}, S_{i_k+2} - S_{i_k}, \ldots,S_{i_{k+1}-1} - S_{i_k}.
\end{align*}
Our aim is to prove that $T_1|h,\ldots, T_{d-k}|h$ are linearly independent with probability $1$. The orthogonal complement of $h$ is spanned by $S_{i_2}-S_{i_1},\ldots, S_{i_{k+1}} - S_{i_k}$, hence our task reduces to showing that the vectors
\begin{equation}\label{eq:list_GP_face_probab_bridge}
T_1,\ldots,T_{d-k}, S_{i_2}-S_{i_1},\ldots, S_{i_{k+1}} - S_{i_k}
\end{equation}
are linearly independent, with probability $1$. But their linear hull coincides with the linear hull of
$$
S_{j_1}-S_{j_0}, S_{j_2}-S_{j_1}, \ldots, S_{j_d}-S_{j_{d-1}}
$$
for suitable $0\leq j_0< j_1 <\ldots < j_d < n$. By assumptions $(\text{Ex})$ and $(\text{GP}')$, this linear hull has maximal possible dimension $d$, thus proving the linear independence of the vectors~\eqref{eq:list_GP_face_probab_bridge}.
\end{proof}

\section{Expected number of faces of a random walk}
\subsection{Method of proof}
As a direct corollary of Theorem~\ref{1249}, we obtain a formula for the expected number of $k$-dimensional faces of $C_n$ under assumptions $(\pm\text{Ex})$ and $(\text{GP})$:
\begin{equation}\label{eq:expect_faces_walk_big_sum}
{\mathbb E} [\#\mathcal{F}_k(C_n)]
=
2\sum_{0\leq i_1<\ldots<i_{k+1}\leq n}\frac{P_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + P_{i_1,\ldots,i_{k+1}}(d-k-3)+\ldots}{2^{i_1+n-i_{k+1}} i_1!(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1})!},
\end{equation}
for all $0\leq k\leq d-1$.  Similarly, it follows from Theorem~\ref{1249bridge} that for bridges satisfying $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$, we have
\begin{equation}\label{eq:expect_faces_bridge_big_sum}
{\mathbb E}\, [\#\mathcal{F}_k(C_n)  ]
=
2 \sum_{0\leq i_1<\ldots<i_{k+1} < n}\frac{Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots}{(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1}+i_1)!}.
\end{equation}
In particular, the expected number of faces is distribution-free both for walks under $(\pm\text{Ex})$, $(\text{GP})$, and for bridges under  $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$.

In Section~\ref{subsec:simplification} we shall evaluate the sum on the right-hand side of~\eqref{eq:expect_faces_walk_big_sum}, thus proving Theorem~\ref{theo:expected_walk} for \emph{symmetric} random walks. In order to remove the unnecessary symmetry assumption, we shall prove in
Section~\ref{subsec:walks_equals_bridge} that the expected number of $k$-faces of any random walk of length $n$ satisfying assumptions $(\text{Ex})$, $(\text{GP})$ is the same as for any random bridge of length $n+1$ satisfying assumptions $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$ with $n$ replaced by $n+1$. In particular, the expected number of $k$-faces of a random walk is distribution-free provided $(\text{Ex})$ and $(\text{GP})$ hold. To complete the proof of Theorem~\ref{theo:expected_walk} it remains to compute the expected number of faces for \emph{just one} particular random walk of our choice satisfying $(\text{Ex})$ and $(\text{GP})$. Taking an arbitrary random walk which satisfies $(\pm\text{Ex})$ and $(\text{GP})$ and applying~\eqref{eq:expect_faces_walk_big_sum} together with the result of Section~\ref{subsec:simplification} completes the proof of Theorem~\ref{theo:expected_walk}.

\subsection{Proof of Theorem~\ref{theo:expected_walk} in the symmetric case}\label{subsec:simplification}
We are going to prove that under assumptions $(\pm\text{Ex})$ and $(\text{GP})$,
\begin{equation}\label{eq:exp_walk_symmetric}
{\mathbb E} [\# \mathcal F_k(C_n)]= \frac{2\cdot k!}{n!} \sum_{l=0}^{\infty}{\genfrac{[}{]}{0pt}{}{{n+1}}{{d-2l}}}  {\genfrac{\{}{\}}{0pt}{}{{d-2l}}{{k+1}}}.
\end{equation}
Recall from~\eqref{eq:rising_factorial} that $t^{(j)} = t(t+1)\ldots (t+j-1)$ denotes the rising factorial. Let $[t^N] f(t) = \frac 1 {N!} f^{(N)}(0)$ be the coefficient of $t^N$ in the Taylor expansion of a function $f$ around $0$. For $m\in{\mathbb{N}}_0$ define
\begin{multline*}
R_n(m)
= [t^m] \sum_{j_0,\ldots,j_{k+1}}
\left(\frac{(t+1)(t+3)\ldots (t+2j_0-1)}{2^{j_0} j_0!}
\frac{(t+1)(t+3)\ldots (t+2j_{k+1}-1)}{2^{j_{k+1}} j_{k+1}!}
\frac{t^{(j_1)}}{t j_1!}\ldots \frac{t^{(j_k)}}{t j_k!}\right),
\end{multline*}
where the sum  is taken over all $j_0, j_{k+1}\in{\mathbb{N}}_0$ and $j_1,\ldots,j_k\in{\mathbb{N}}$ such that $j_0+\ldots+j_{k+1} = n$.
With this notation,  Theorem~\ref{1249} (see also~\eqref{eq:expect_faces_walk_big_sum}) implies that
$$
{\mathbb E} [\# \mathcal F_k(C_n)]
=
2\sum_{l=0}^{\infty} R_n(d-k-2l-1).
$$
To prove~\eqref{eq:exp_walk_symmetric} it suffices to  show that
\begin{equation}\label{eq:R_n_m}
R_n(m) =\frac{k!}{n!} {\genfrac{\{}{\}}{0pt}{}{{m+k+1}}{{k+1}}} {\genfrac{[}{]}{0pt}{}{{n+1}}{{k+m+1}}}.
\end{equation}
Expanding the product yields
$$
R_n(m) = [t^mx^n] \left(\left(\sum_{j=0}^{\infty}\frac{(t+1)(t+3)\ldots (t+2j-1)}{2^{j} j!} x^j\right)^2 \left(\sum_{j=1}^\infty\frac{t^{(j)}}{t j!}x^j \right)^k \right)
$$
Using Newton's formulae
$$
\sum_{j=1}^\infty\frac{t^{(j)}}{t j!}x^j = \frac{(1-x)^{-t} - 1}{t},
\quad
\sum_{j=0}^{\infty}\frac{(t+1)(t+3)\ldots (t+2j-1)}{2^{j} j!} x^j = (1-x)^{-\frac 12 (t+1)},
$$
we obtain
$$
R_n(m)= [t^mx^n] \left((1-x)^{-t-1} \left(\frac{(1-x)^{-t} - 1}{t}\right)^k \right)
=[t^mx^n] \left({{\rm e}}^{-a(t+1)} \left(\frac{{{\rm e}}^{-at} - 1}{t}\right)^k \right),
$$
where we introduced the notation $a=a(x) = \log (1-x)$.
Let us look at the term
$$
{{\rm e}}^{-a(t+1)} \left(\frac{{{\rm e}}^{-at} - 1}{t}\right)^k
=
a^{k+1} t {{\rm e}}^{-a}  \left(\frac{at}{{{\rm e}}^{-at} - 1}\right)^{-(k+1)}
+ a^k {{\rm e}}^{-a}  \left(\frac{at}{{{\rm e}}^{-at} - 1}\right)^{-k}.
$$
As a consequence of~\eqref{eq:stirling_def}, we have
$$
\left( \frac {1-{{\rm e}}^{-at}}{at}\right)^{k} = \sum_{m=0}^{\infty} (-1)^{m+k} \binom{k+m}{m}^{-1} {\genfrac{\{}{\}}{0pt}{}{{k+m}}{{k}}} \frac{(at)^m}{m!}.
$$
Using this formula twice, we obtain
\begin{align*}
[t^m] {{\rm e}}^{-a(t+1)} \left(\frac{{{\rm e}}^{-at} - 1}{t}\right)^k
&=
(-1)^{k+m} \frac{1}{(m+k)!} a^{m+k}{{\rm e}}^{-a}  \left({\genfrac{\{}{\}}{0pt}{}{{m+k}}{{k+1}}}(k+1)! +{\genfrac{\{}{\}}{0pt}{}{{m+k}}{{k}}}k!\right)\\
&=
(-1)^{k+m} \frac{k!}{(m+k)!} a^{m+k}{{\rm e}}^{-a} {\genfrac{\{}{\}}{0pt}{}{{m+k+1}}{{k+1}}}.
\end{align*}
Now recall that $a= \log (1-x)$ and use the first generating function in~\eqref{eq:stirling_def} to get
\begin{align*}
[x^n](a^{m+k}{{\rm e}}^{-a})
&=
[x^n] \frac{(\log (1-x))^{k+m}}{(1-x)}\\
&=
-\frac 1{k+m+1} [x^n] \frac{\partial}{\partial x} (\log (1-x))^{k+m+1}\\
&=
(-1)^{k+m}  \frac{(k+m)!}{n!} {\genfrac{[}{]}{0pt}{}{{n+1}}{{k+m+1}}}.
\end{align*}
Taking everything together, we obtain~\eqref{eq:R_n_m}, thus completing the proof.

\subsection{Relation between bridges and walks}\label{subsec:walks_equals_bridge}
The next result completes the proof of Theorem~\ref{1249}.
\begin{theorem}\label{1455}
Let $(S_i)_{i=0}^n$ be a random walk in ${\mathbb{R}}^d$ whose increments $(\xi_1,\ldots,\xi_n)$ satisfy conditions $(\text{Ex})$ and $(\text{GP})$. Further, let $(S_i')_{i=0}^{n+1}$ be a random bridge of length $n+1$  satisfying conditions $(\text{Br})$, $(\text{Ex})$ and $(\text{GP}')$ with $n$ replaced by $n+1$ (in particular, $S_0'=S_{n+1}'=0$). Write $C_n={\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_n)$ and $C_{n+1}'= {\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_{n+1})$ for the corresponding convex hulls. Then, for all $0\leq k \leq d-1$,
\begin{equation}\label{1613}
{\mathbb E} [\#\mathcal{F}_k(C_n)] =
{\mathbb E} [\#\mathcal{F}_k(C_{n+1}')].
\end{equation}
\end{theorem}

\begin{proof}
From~\eqref{eq:expect_faces_bridge_big_sum} we know that ${\mathbb E} [\#\mathcal{F}_k(C_{n+1}')]$ does not depend on the choice of the particular bridge $(S_i')_{i=0}^{n+1}$. Hence, it suffices to prove~\eqref{1613} for just one random bridge of our choice. This bridge will be constructed as follows.
Start with a random walk $(S_i)_{i=0}^n$ satisfying $(\text{Ex})$ and $(\text{GP})$, and consider the closed path $S_0,S_1,\ldots, S_n, 0$.  Although this path returns to $0$ at step $n+1$,  its increments are not exchangeable because the last increment $-S_n$ need not have the same distribution as, say, $\xi_1$. In order to enforce exchangeability, we shall consider a random permutation of the increments of this closed path.
More precisely, write $\xi_{n+1}:=-S_n$, so that $S_{n+1}:=\xi_1+\dots+\xi_n+\xi_{n+1}=0$.
Consider a random permutation ${{\boldsymbol{\varsigma}}}$ which is uniformly distributed on the symmetric group ${\text{\rm Sym}}(n+1)$ and independent of the random walk $(S_i)_{i=0}^n$. Let $(S^{{\boldsymbol{\varsigma}}}_i)_{i=0}^{n+1}$ be a random bridge starting from $S^{{\boldsymbol{\varsigma}}}_0:=0$ and defined as
$$
S^{{\boldsymbol{\varsigma}}}_i:=\xi_{{{\boldsymbol{\varsigma}}}(1)}+\dots+\xi_{{{\boldsymbol{\varsigma}}}(i)}, \quad 1\leq i \leq n+1.
$$
Denote by $C^{{\boldsymbol{\varsigma}}}_{n+1}:={\mathop{\mathrm{Conv}}\nolimits}(S^{{\boldsymbol{\varsigma}}}_0,S^{{\boldsymbol{\varsigma}}}_1,\ldots,S^{{\boldsymbol{\varsigma}}}_{n+1})$ its convex hull. It is clear that $(S^{{\boldsymbol{\varsigma}}}_i)_{i=0}^{n+1}$ satisfies conditions $(\text{Br})$, $(\text{Ex})$, and $(\text{GP}')$ with $n$ replaced by $n+1$.
By the distribution freeness of the expected number of $k$-faces under $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$, see~\eqref{eq:expect_faces_bridge_big_sum}, it remains to prove that
\begin{equation}\label{1614}
{\mathbb E} [\#\mathcal{F}_k(C_n)]={\mathbb E} [\#\mathcal{F}_k(C^{{\boldsymbol{\varsigma}}}_{n+1})].
\end{equation}
We have
\begin{equation}\label{1838}
{\mathbb E} [\#\mathcal{F}_k(C^{{\boldsymbol{\varsigma}}}_{n+1})]=\frac{1}{(n+1)!}\sum_{\sigma\in{\text{\rm Sym}}(n+1)}{\mathbb E} [\#\mathcal{F}_k({\mathop{\mathrm{Conv}}\nolimits}(S^\sigma_1,\dots,S^\sigma_{n+1}))],
\end{equation}
where for a deterministic permutation $\sigma\in {\text{\rm Sym}}(n+1)$, $(S^{\sigma}_i)_{i=0}^{n+1}$ is a stochastic process starting from $S^{\sigma}_0:=0$  and defined as
$$
S^{\sigma}_i:=\xi_{\sigma(1)}+\dots+\xi_{\sigma(i)}, \quad 1\leq i \leq n+1.
$$
Fix some permutation $\sigma\in {\text{\rm Sym}}(n+1)$ and let $r\in \{1,\ldots,n+1\}$ be such that $\sigma(r)=n+1$. We have
\begin{multline*}
  {\mathop{\mathrm{Conv}}\nolimits}(S^\sigma_1,S^\sigma_2,\dots,S^\sigma_{n+1})={\mathop{\mathrm{Conv}}\nolimits}(S^\sigma_{r},\dots,S^\sigma_{n+1},S^\sigma_1,\dots,S^\sigma_{r-1})\\
  =S^\sigma_{r}+{\mathop{\mathrm{Conv}}\nolimits}(0,S^\sigma_{r+1}-S^\sigma_{r},\dots,S^\sigma_{n+1}-S^\sigma_{r},S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_1,\dots,S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_{r-1}),
\end{multline*}
where in the second equality we used that $S^\sigma_{n+1}=0$. Since a shift does not change the number of faces, we arrive at
\begin{multline}\label{1839}
  \#\mathcal{F}_k({\mathop{\mathrm{Conv}}\nolimits}(S^\sigma_1,S^\sigma_2,\dots,S^\sigma_{n+1}))\\
  =\#\mathcal{F}_k({\mathop{\mathrm{Conv}}\nolimits}(0,S^\sigma_{r+1}-S^\sigma_{r},\dots,S^\sigma_{n+1}-S^\sigma_{r},S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_1,\dots,S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_{r-1})).
\end{multline}
It follows from $\sigma(r)=n+1$ and condition $(\text{Ex})$ that
$$
(\xi_{\sigma(r+1)},\dots,\xi_{\sigma(n+1)},\xi_{\sigma(1)},\dots,\xi_{\sigma(r-1)}){\stackrel{d}{=}}(\xi_1,\dots,\xi_n),
$$
which implies, by taking partial sums of both sides,
\begin{equation}\label{1840}
 (0,S^\sigma_{r+1}-S^\sigma_{r},\dots,S^\sigma_{n+1}-S^\sigma_{r},S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_1,\dots,S^\sigma_{n+1}-S^\sigma_{r}+S^\sigma_{r-1}){\stackrel{d}{=}}(0,S_1,\dots,S_n).
\end{equation}
Combining~\eqref{1839} and~\eqref{1840}, we obtain that for every deterministic $\sigma\in{\text{\rm Sym}}(n+1)$,
$$
{\mathbb E} [\#\mathcal{F}_k({\mathop{\mathrm{Conv}}\nolimits}(S^\sigma_1,S^\sigma_2,\dots,S^\sigma_{n+1}))] = {\mathbb E} [\#\mathcal{F}_k(C_n)].
$$
Inserting this into~\eqref{1838} yields~\eqref{1614}, and the theorem follows.
\end{proof}

\section{Proof of Theorem~\ref{theo:1139}}\label{sec:proof_shifted}
The following two propositions, taken together, yield Theorem~\ref{theo:1139}. The proof of the first of them utilizes the same idea of reshuffling the increments as in the proof of Theorem~\ref{1455}.
\begin{proposition}\label{prop:shift}
With the same notation and assumptions as in Theorem~\ref{1455}, for all indices $1\leq l_1 < \ldots < l_k \leq n$,
\begin{multline}\label{eq:shift_faces_statement}
\frac 1 {n+1} \sum_{i=0}^n {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)]
= {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(0, S_{l_1}',\ldots, S_{l_{k}}')\in {\mathcal{F}}_k(C_{n+1}')]\\
=
\frac{
2(Q_{0, l_1,\ldots,l_{k}}^{(n+1)}(d-k-1) + Q_{0,l_1,\ldots,l_{k}}^{(n+1)}(d-k-3)+\ldots)
}{l_1! (l_2-l_1)!\ldots  (l_{k}-l_{k-1})! (n+1-l_{k})!}
,
\end{multline}
where we put $S_{i+l_j} = S_{(i+l_j)-(n+1)}$ if $i+l_j\geq n+1$. 
\end{proposition}
\begin{proof}
We have to prove the first equality because the second one follows from Theorem~\ref{1249bridge}.  Start with a random walk $(S_i)_{i=0}^n$ whose increments $\xi_1,\ldots,\xi_n$ satisfy $(\text{Ex})$ and $(\text{GP})$, define $\xi_{n+1} = -S_n$, reshuffle the increments $\xi_1,\ldots,\xi_{n+1}$ according to a random, uniform permutation ${{\boldsymbol{\varsigma}}}\in {\text{\rm Sym}}(n+1)$, and denote the corresponding partial sums by $(S_{i}^{{\boldsymbol{\varsigma}}})_{i=0}^{n+1}$, as in the previous proof. Since $(S_{i}^{{\boldsymbol{\varsigma}}})_{i=0}^{n+1}$ is a random bridge satisfying $(\text{Br})$, $(\text{Ex})$, $(\text{GP}')$ (with $n$ replaced by $n+1$), we have
\begin{multline}\label{eq:shift_faces}
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(0, S_{l_1}',\ldots, S_{l_{k}}')\in {\mathcal{F}}_k(C_{n+1}')]
\\=
\frac 1 {(n+1)!} \sum_{\sigma\in {\text{\rm Sym}}(n+1)}
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma}, S_{l_1}^\sigma,\ldots, S_{l_{k}}^\sigma) \in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma},S_1^{\sigma}, \ldots, S_{n+1}^{\sigma}))],
\end{multline}
where $S^{\sigma}_i:=\xi_{\sigma(1)}+\dots+\xi_{\sigma(i)}$, $1\leq i \leq n+1$, for a deterministic permutation $\sigma\in {\text{\rm Sym}}(n+1)$. Let $r\in \{1,\ldots,n+1\}$ be such that $\sigma(r) = n+1$. Recall that $S_{n+1}^\sigma = 0$. Shifting everything by $S_r^{\sigma}$ yields
\begin{multline}\label{eq:wspom1}
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma}, S_{l_1}^\sigma,\ldots, S_{l_{k}}^\sigma) \in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma},S_1^{\sigma}, \ldots, S_{n+1}^{\sigma}))]
\\=
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma}-S_r^{\sigma}, S_{l_1}^\sigma-S_r^{\sigma},\ldots, S_{l_{k}}^\sigma-S_r^{\sigma})
\in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma}-S_r^{\sigma},S_1^{\sigma}-S_r^{\sigma}, \ldots, S_{n}^{\sigma}-S_r^{\sigma}))].
\end{multline}
Recall from~\eqref{1840} that
\begin{equation}\label{1840a}
(S_r^{\sigma}- S_r^{\sigma},S^\sigma_{r+1}-S^\sigma_{r},\dots,S^\sigma_{n+1}-S^\sigma_{r},S^\sigma_1-S^\sigma_{r},\dots,S^\sigma_{r-1}-S^\sigma_{r})
{\stackrel{d}{=}}(0,S_1,\dots,S_n).
\end{equation}
Note that $S_l^\sigma-S_r^{\sigma}$ on the left-hand side corresponds to $S_{l-r}$ on the right-hand side, provided we agree to understand all indices modulo $(n+1)$.   Applying~\eqref{1840a} to the right-hand side of~\eqref{eq:wspom1},  we arrive at
\begin{multline*}
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma}, S_{l_1}^\sigma,\ldots, S_{l_{k}}^\sigma) \in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0^{\sigma},S_1^{\sigma}, \ldots, S_{n+1}^{\sigma}))]
\\=
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_{-r}, S_{l_1-r},\ldots, S_{l_{k}-r})
\in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0,S_1, \ldots, S_{n}))].
\end{multline*}
Taking the sum over all $\sigma \in {\text{\rm Sym}}(n+1)$ and observing that for any fixed $r\in \{1,\ldots,n+1\}$ there are $n!$ permutations $\sigma$ for which $\sigma(r) = n+1$, we arrive at
\begin{multline*}
\text{RHS of~\eqref{eq:shift_faces}}
= \frac{n!}{(n+1)!} \sum_{r=1}^{n+1} {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_{-r}, S_{l_1-r},\ldots, S_{l_{k}-r})
\in {\mathcal{F}}_k({\mathop{\mathrm{Conv}}\nolimits} (S_0,S_1, \ldots, S_{n}))].
\end{multline*}
Replacing $r$ by $n+1-i$ and recalling that the indices are considered modulo $(n+1)$ completes the proof of~\eqref{eq:shift_faces_statement}.
\end{proof}

\begin{proposition}\label{1139}
With the same notation and assumptions as in Theorem~\ref{1455}, for all indices $1\leq l_1 < \ldots < l_k \leq n$,
\begin{multline}\label{1921}
\frac 1 {n+1-l_k} \sum_{i=0}^{n-l_k} {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)]\\
=
\frac{
2(Q_{0, l_1,\ldots,l_{k}}^{(n+1)}(d-k-1) + Q_{0,l_1,\ldots,l_{k}}^{(n+1)}(d-k-3)+\ldots)
}{l_1! (l_2-l_1)!\ldots  (l_{k}-l_{k-1})! (n+1-l_{k})!}\\
={\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(0, S_{l_1}',\ldots, S_{l_{k}}')\in {\mathcal{F}}_k(C_{n+1}')].
\end{multline}
\end{proposition}
\begin{proof}
The main idea is the same as in the  proofs of Theorems~\ref{1249} and~\ref{1249bridge} combined with the method of reshuffling from the proof of Theorem~\ref{1455}. As above, consider the linear space $h:={\mathop{\mathrm{aff}}\nolimits}^\perp(S_0,S_{l_1},\ldots,S_{l_{k}})$ and note that $\dim h = d-k$.  Projecting the path $S_0,\ldots,S_n$ onto $h$, we obtain  $k$ random bridges that  start and terminate at the origin. The increments of the $j$-th random bridge are given by
$$
\eta_1^{(j)}=\xi_{l_{j-1}+1}|h, \;\; \ldots, \;\; \eta_{l_{j}-l_{-1}}^{(j)}=\xi_{l_{j+1}}|h,
\quad j=1,\ldots k,
$$
where $l_0:=0$.
The increments $\xi_{l_{k}+1},\dots,\xi_n$ which are not in the above bridges are treated as follows. Define $\xi_{n+1} := -S_n$, reshuffle the increments $\xi_{l_{k}+1},\dots,\xi_n, \xi_{n+1}$ according to a random, uniform permutation ${{\boldsymbol{\varsigma}}}\in{\text{\rm Sym}}(n+1-l_k)$, and consider the $(k+1)$st random bridge that also starts and terminates at $0$ with the increments given by
$$
\eta_1^{(k+1)}=\xi_{l_k+{{\boldsymbol{\varsigma}}}(1)}|h, \;\; \ldots, \;\; \eta_{n+1-l_{k}}^{(k+1)}=\xi_{l_k+{{\boldsymbol{\varsigma}}}(n+1-l_{k})}|h.
$$
Clearly, the invariance condition of Theorem~\ref{1435} is satisfied for these $r=k+1$ random bridges (and $s=0$ random walks). The general position assumption of Theorem~\ref{1435} is verified in the same manner as in the  proofs of Theorems~\ref{1249} and~\ref{1249bridge}. Let $H_{{\boldsymbol{\varsigma}}}$ denote the random hull of these $k+1$ random bridges.

Now let $\rho$ be a uniform over $\{1,\dots,n+1-l_{k}\}$ random variable defined by $l_k+{{\boldsymbol{\varsigma}}}(\rho)=n+1$, which means $\eta_\rho^{(k+1)}=\xi_{n+1}$.  Assumption $(\text{Ex})$ implies that for $i=0,\dots,n-l_{k}$,
$$
{\mathbb{P}}[0\notin H_{{\boldsymbol{\varsigma}}}|\rho=n+1-l_k-i]={\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)].
$$
Multiplying by $\frac{1}{n+1-l_k}$ and summing up over $i=0,\dots,n-l_k$  leads to
$$
\frac{1}{n+1-l_k} \sum_{i=0}^{n-l_k}  {\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits}(S_{i}, S_{i+l_1},\ldots, S_{i+l_{k}})\in {\mathcal{F}}_k(C_n)]={\mathbb{P}}[0\notin H_{{\boldsymbol{\varsigma}}}].
$$
Now the first equality of~\eqref{1921} follows by applying~\eqref{eq:absorption} from Theorem~\ref{1435} to $H_{{\boldsymbol{\varsigma}}}$. The second one follows from Theorem~\ref{1249bridge}.
\end{proof}

\section{Further remarks and conjectures}

\subsection{Results not requiring the general position assumption}
Let $(S_i)_{i=0}^n$ and $(S_i')_{i=0}^n$ be two random walks with increments $(\xi_1,\ldots, \xi_n)$ and $(\xi_1',\ldots, \xi_n')$, respectively, such that $(\xi_1,\ldots, \xi_n)$ satisfies $(\pm\text{Ex})$ and $(\text{GP})$, while $(\xi_1',\ldots, \xi_n')$ satisfies $(\pm\text{Ex})$ only.  Denote the corresponding convex hulls by $C_n := {\mathop{\mathrm{Conv}}\nolimits}(S_0,\ldots,S_n)$ and $C_n' := {\mathop{\mathrm{Conv}}\nolimits}(S_0',\ldots,S_n')$. All faces of $C_n$ are simplices, but this is in general not true for $C_n'$. The face probabilities of $C_n'$ are not distribution-free, but it can be shown that
\begin{multline}\label{eq:no_general_pos_inequality}
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_{i_1}', \ldots, S_{i_{k+1}}') \in \mathcal F_k(C_n')]
\leq
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_{i_1}, \ldots, S_{i_{k+1}}) \in \mathcal F_k(C_n)]\\
\leq
{\mathbb{P}}[{\mathop{\mathrm{Conv}}\nolimits} (S_{i_1}', \ldots, S_{i_{k+1}}')\subset F' \text{ for some } F' \in \mathcal F_k(C_n')],
\end{multline}
where the distribution-free  probability in the middle was calculated in Theorem~\ref{1249}. To prove this, one argues in the same way as in the proof of Theorem~\ref{1249}, but uses Remark~\ref{rem:non_gen_position_absorption} instead of Theorem~\ref{1435}. Similar inequalities hold for bridges violating the general position assumption.
From~\eqref{eq:no_general_pos_inequality} it is possible to deduce that under assumption $(\text{Ex})$,
$$
{\mathbb E} [\mathcal F^{-}_k (C_n')] \leq  \frac{2\cdot k!}{n!} \sum_{l=0}^{\infty}{\genfrac{[}{]}{0pt}{}{{n+1}}{{d-2l}}}  {\genfrac{\{}{\}}{0pt}{}{{d-2l}}{{k+1}}}
\leq
{\mathbb E} [\mathcal F^{+}_k (C_n')],
$$
where $\mathcal F^{-}_k(C_n')$ is the set of simplicial $k$-faces of $C_n'$, while $\mathcal F^{+}_k(C_n')$ is the number of collections $1\leq i_1 < \ldots < i_{k+1} \leq n$ such that ${\mathop{\mathrm{Conv}}\nolimits}(S_{i_1}, \ldots, S_{i_{k+1}})$ is contained in some $k$-face $F'$ of $C_n'$.

\subsection{Expected total number of faces}
Assuming  $(\text{Ex})$ and $(\text{GP})$, let us compute the expected number of faces of $C_n$ in all dimensions together.
We need the numbers $\widehat  c_N$ (which appeared in~\cite{sprugnoli}) defined by
\begin{equation}\label{eq:def_hat_c_N}
\widehat  c_N = \sum_{k=1}^N (k-1)!  {\genfrac{\{}{\}}{0pt}{}{{N}}{{k}}} = N! [t^N] \log \left(\frac 1 {2-{{\rm e}}^t}\right) = (N-1)! [t^{N-1}] \left(\frac{{{\rm e}}^t}{2-{{\rm e}}^t}\right).
\end{equation}
From Theorem~\ref{theo:expected_walk} and~\eqref{eq:stirling_asympt} we obtain the formula
$$
{\mathbb E} \left[\sum_{k=0}^{d-1} \mathcal F_{k}(C_n)\right]
=
\frac 2 {n!}\sum_{l=0}^\infty  {\genfrac{[}{]}{0pt}{}{{n+1}}{{d-2l}}} \widehat  c_{d-2l}
\sim  \frac{2 \widehat  c_{d}}{(d-1)!} (\log n)^{d-1},
$$
where the asymptotics is for $n\to\infty$ and fixed $d\in{\mathbb{N}}$.
The numbers $\widehat  c_N$ are similar to the \textit{ordered Bell numbers} $\mathcal O_N$ which count the number of weak orderings on a set of $N$ elements and are given by
$$
\mathcal O_N = \sum_{k=0}^N k! {\genfrac{\{}{\}}{0pt}{}{{N}}{{k}}} = \frac 12 \sum_{i=0}^\infty \frac{i^N}{2^i}.
$$

\subsection{Open questions}
It remains open to compute the expected number of faces and the face probabilities for the \emph{simple} random walk on the lattice ${\mathbb{Z}}^d$, even for $d=2$.
It is natural to ask for the limit distribution of the (appropriately normalized) number of $k$-faces of $C_n$. For convex hulls of i.i.d.\ Gaussian samples, variance asymptotics for the face numbers were established recently by Calka and Yukich~\cite{calka_yukich}. Example~\ref{ex:non_symm_RW} shows that the absorption probability is not distribution-free for (non-symmetric) random walks satisfying $(\text{Ex})$ and $(\text{GP})$. It is likely that among such random walks the absorption probability attains its maximum value for random walks satisfying $(\pm\text{Ex})$.  A similar result for convex hulls of i.i.d.\ samples was proved by Wagner and Welzl~\cite{wagner_welzl}.

\bibliography{thesis4}
\bibliographystyle{plain}

\end{document}

\begin{proof}[Proof of Theorem~\ref{theo:expected_bridge}]
As a direct corollary of Theorem~\ref{1249bridge}, we obtain that
\begin{equation*}
{\mathbb E}\, [\#\mathcal{F}_k(C_n)  ]
=
2 \sum_{0\leq i_1<\ldots<i_{k+1} < n}\frac{Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-1) + Q_{i_1,\ldots,i_{k+1}}^{(n)}(d-k-3)+\ldots}{(i_2-i_1)!\ldots  (i_{k+1}-i_k)! (n-i_{k+1}+i_1)!}.
\end{equation*}
In view of this, to prove Theorem~\ref{theo:expected_bridge} it suffices to show that for all $m= d-k-2l-1\in{\mathbb{N}}_0$,
$$
\sum_{0\leq i_1<\ldots<i_{k+1} < n}\frac{Q_{i_1,\ldots,i_{k+1}}^{(n)}(m)}{(i_2-i_1)!\ldots  (i_{k+1}-i_{k})! (n-i_{k+1}+i_1)!}
=
\frac {k!} {(n-1)!} {\genfrac{[}{]}{0pt}{}{{n}}{{k+m+1}}}  {\genfrac{\{}{\}}{0pt}{}{{m+k+1}}{{k+1}}}.
$$
In the following it will be convenient to replace $k+1$ by $k$, so that our formula takes the form
$$
\sum_{0\leq i_1<\ldots<i_{k} < n}\frac{Q_{i_1,\ldots,i_{k}}^{(n)}(m)}{(i_2-i_1)!\ldots  (i_{k}-i_{k-1})! (n-i_{k}+i_1)!}
=
\frac {(k-1)!} {(n-1)!} {\genfrac{[}{]}{0pt}{}{{n}}{{k+m}}} {\genfrac{\{}{\}}{0pt}{}{{m+k}}{{k}}}.
$$
\vspace*{2mm}
\noindent
\emph{Step 1.}
Let us argue that
\begin{equation}\label{eq:sum_Q_sum_Q_tilde}
\sum_{0\leq i_1<\ldots<i_{k} < n}\frac{Q_{i_1,\ldots,i_{k}}^{(n)}(m)}{(i_2-i_1)!\ldots  (i_{k}-i_{k-1})! (n-i_{k}+i_1)!}
=
\frac nk \sum_{\substack{j_1,\ldots,j_{k} \in {\mathbb{N}}\\ j_1+\ldots+j_{k} = n}} \widetilde Q_{j_1,\ldots,j_{k}} (m),
\end{equation}
where for $j_1,\ldots, j_{k}\in {\mathbb{N}}$ we define the coefficients $\widetilde Q_{j_1,\ldots,j_{k}}(m)$, $m\in{\mathbb{N}}_0$, by the formula
\begin{equation}\label{eq:Q_tilde_def}
\prod_{i=1}^{k} \frac{(t+1)(t+2)\ldots (t+j_i-1)}{j_i!} = \sum_{m=0}^{\infty} \widetilde Q_{j_1,\ldots,j_{k}}(m) t^m.
\end{equation}
Consider the finite sets
\begin{align*}
I_{n,k} & = \{ \mathbf i = (i_1,\ldots,i_{k})\in {\mathbb{N}}_0^{k} \colon 0\leq i_1<\ldots<i_{k} < n\},\\
J_{n,k} & = \{\mathbf j= (j_1,\ldots,j_{k})\in {\mathbb{N}}^{k} \colon j_1+\ldots+j_{k} =n \}.
\end{align*}
Define a map $\varphi: I_{n,k} \to J_{n,k}$ which maps $\mathbf i$ to $\mathbf j = \varphi(\mathbf i)$, where
$$
j_1 = i_2-i_1, \quad j_2 = i_3-i_2, \quad\ldots,\quad j_{k-1} = i_{k} - i_{k-1}, \quad j_{k} = n+i_1 - i_{k}.
$$
We can imagine the elements of $I_{n,k}$ as $k$-element subsets of the set of vertices of a regular $n$-gon (the vertices can be identified with the numbers $0,\ldots,n-1$). Let us call two elements $\mathbf i, \mathbf i' \in I_{n,k}$ equivalent (notation: $\mathbf i\sim \mathbf i'$) if the corresponding subsets can be mapped to each other by rotation of the regular $n$-gon. On the other hand, let us call $\mathbf j, \mathbf j'\in J_{n,k}$ equivalent if the corresponding sequences
???TO BE EXTENDED.

\vspace*{2mm}
\noindent
\emph{Step 2.}
To complete the proof of the theorem, we need to evaluate the sum on the right-hand side of~\eqref{eq:sum_Q_sum_Q_tilde}.  We are going to show that for all $n\in{\mathbb{N}}$, $n\geq k$,
\begin{equation}\label{eq:sum_tilde_Q}
\sum_{\substack{j_1,\ldots,j_k \in {\mathbb{N}}\\ j_1+\ldots+j_k = n}} \widetilde Q_{j_1,\ldots,j_{k}} (m) =
\frac {k!} {n!} {\genfrac{[}{]}{0pt}{}{{n}}{{k+m}}} {\genfrac{\{}{\}}{0pt}{}{{m+k}}{{k}}}.
\end{equation}
Expanding the product, we obtain
$$
[x^n] \left(\sum_{j=1}^\infty \frac{t^{(j)}}{j! t} x^j\right)^k =
\sum_{\substack{j_1,\ldots,j_k \in {\mathbb{N}}\\ j_1+\ldots+j_k = n}} \prod_{i=1}^{k} \frac{(t+1)(t+2)\ldots (t+j_i-1)}{j_i!}.
$$
Using the definition of $\widetilde Q_{j_1,\ldots,j_{k}} (m)$ in~\eqref{eq:Q_tilde_def} and Newton's formula $(1-x)^{-t} = \sum_{j=0}^\infty t^{(j)} x^j$, we can write
\begin{align*}
\sum_{\substack{j_1,\ldots,j_k \in {\mathbb{N}}\\ j_1+\ldots+j_k = n}}  \widetilde Q_{j_1,\ldots,j_{k}} (m)
&=
[x^n t^m] \left(\sum_{j=1}^\infty \frac{t^{(j)}}{j! t} x^j\right)^k
=
[x^n t^m] \left(\frac{(1-x)^{-t} - 1}{t}\right)^k\\
&=
[x^n t^m] \left( \left(\frac{{{\rm e}}^{-t \log (1-x)} - 1}{t \log (1-x)} \right)^k (\log (1-x))^k \right)
,
\end{align*}
As a consequence of~\eqref{eq:stirling_def} we have
$$
\left( \frac {1-{{\rm e}}^{-s}}{s}\right)^{k} = \sum_{m=0}^{\infty} (-1)^{m+k} \binom{k+m}{m}^{-1} {\genfrac{\{}{\}}{0pt}{}{{k+m}}{{k}}} \frac{s^m}{m!}.
$$
Using this formula with $s= t \log (1-x)$ we obtain
$$
\sum_{\substack{j_1,\ldots,j_k \in {\mathbb{N}}\\ j_1+\ldots+j_k = n}}  \widetilde Q_{j_1,\ldots,j_{k}} (m)
=
  \frac{(-1)^{m+k}}{m!} \binom{k+m}{m}^{-1} {\genfrac{\{}{\}}{0pt}{}{{k+m}}{{k}}} \,  [x^n] (\log (1-x))^{k+m}.
$$
It remains to recall the generating function in~\eqref{eq:stirling_def}
$$
\frac{(-1)^{k+m}}{(k+m)!} (\log (1-x))^{k+m}  = \sum_{n=k+m}^\infty {\genfrac{[}{]}{0pt}{}{{n}}{{k+m}}} \frac {x^n}{n!},
$$
which completes the proof of~\eqref{eq:sum_tilde_Q}.
\end{proof}

\subsection{Further results and conjectures}
Denote the angle at which the convex hull $C_n$ is visible from the point $S_i$ by $\Omega_{n,i}$. Note that $\Omega_{n,i} = 1$ if $S_i\in C_n$ (because we normalize the spherical angle such that the full angle is $1$). Then,
$$
{\mathbb E} [\Omega_{n,i} | S_i \notin C_n] = .
$$
More generally, denote by
$$
{\mathop{\mathrm{Cone}}\nolimits} (C_n) = {\mathop{\mathrm{Cone}}\nolimits} (0,S_1,\ldots,S_n) = \{\lambda x \colon \lambda >0, x\in C_n\}
$$
the convex cone generated by $S_1,\ldots,S_n$. Note that ${\mathop{\mathrm{Cone}}\nolimits} (C_n) ={\mathbb{R}}^d$ if and only if the absorption event $0\in C_n$ occurs. Recall that $v_k$ denotes the $k$-th intrinsic conic volume of  a cone. Then,
$$
{\mathbb E} [v_k ({\mathop{\mathrm{Cone}}\nolimits} (C_n)) | 0\notin C_n] = ....
$$

Let $\mathcal F_{k}(C_n; i)$ be the number of $k$-dimensional faces containing the point $S_i$. If $S_i$ is not a vertex of the convex hull $C_n$, this number is $0$. In the bridge case ${\mathbb E} \mathcal F_{k}(C_n;i)$ does not depend on $i$ by cyclic symmetry. Also, for symmetry reasons we have
$$
n {\mathbb E}\mathcal F_{k}(C_n;i) =  (k+1) {\mathbb E} \mathcal F_{k}(C_n).
$$
Asymptotically, it follows that
$$
{\mathbb E}\mathcal F_{k}(C_n;i)  = \frac{k+1}{n} {\mathbb E} \mathcal F_{k}(C_n) \sim \frac{2(k+1)!}{(d-1)!n}{\genfrac{\{}{\}}{0pt}{}{{d}}{{k+1}}} (\log n)^{d-1}
\sim (k+1)!{\genfrac{\{}{\}}{0pt}{}{{d}}{{k+1}}} {\mathbb{P}}[S_i \in \mathcal F_{0}(C_n)].
$$
Another way of writing this is
$$
\lim_{n\to\infty} {\mathbb E} [\mathcal F_{k}(C_n;i) | S_i \in \mathcal F_{0}(C_n;i)] = (k+1)!{\genfrac{\{}{\}}{0pt}{}{{d}}{{k+1}}}.
$$
On the other hand side, $(k+1)!{\genfrac{\{}{\}}{0pt}{}{{d}}{{k+1}}}$ is the number of $k$-dimensional faces in the decomposition of a $(d-1)$-dimensional space into Weyl chambers of type $A_{d-1}$.

\subsection{Multidimensional version of the arcsine law for the number of positive terms}
Consider a one-dimensional random walk $S_1,\ldots,S_n$. Consider the random variable
\begin{equation}\label{eq:def_N_n}
N_n = \sum_{k=1}^n {\mathbbm{1}}_{\{S_k >0\}}
\end{equation}
counting the number of positive points of the random walk. A classical result of Sparre Andersen states that
\begin{equation}\label{eq:arcsine_occupation1}
{\mathbb{P}}[N_n = m] = \frac 1 {2^{2m}} \binom{2m}{m} \binom{2n-2m}{n-m},
\quad m=0,\ldots, n.
\end{equation}
What is a multidimensional analogue of this result? Although we are not able to define a multidimensional analogue of the random variable $N_n$ itself, we shall find an appropriate generalization of its factorial moments. We can rewrite~\eqref{eq:arcsine_occupation1} as follows:
\begin{equation}\label{eq:arcsine_occupation2}
{\mathbb E} \binom{N_n}{k} = \frac 1 {2^{2k}} \binom {2k}{k} \binom{n}{k},
\quad k=0,\ldots,n.
\end{equation}
Let us now turn to the multidimensional setting. Given a random walk $S_0,\ldots,S_n$ in ${\mathbb{R}}^d$, define a random variable
\begin{equation}\label{eq:def_M_n_k}
M_{n,k} = \frac{1}{2} \sum_{1\leq i_1 < \ldots < i_k\leq n} {\mathbbm{1}}_{\{0\notin {\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots, S_{i_k})\}}
\end{equation}
counting the number of simplices of the form ${\mathop{\mathrm{Conv}}\nolimits} (S_{i_1},\ldots, S_{i_k})$ not containing the origin.
In the one-dimensional case $d=1$ we have
$$
M_{n,k} = \frac 12 \binom {N_n}{k} + \frac 12 \binom {n-N_n}{k}
\quad \text{ and }\quad
{\mathbb E} M_{n,k} = {\mathbb E} \binom{N_n}{k}.
$$
Therefore, we can view ${\mathbb E} M_{n,k}$ as a multidimensional generalization of ${\mathbb E} \binom{N_n}{k}$. The next theorem is a multidimensional generalization of~\eqref{eq:arcsine_occupation2}.
\begin{theorem}\label{theo:arcsine_multidim}
For all $k=0,\ldots,n$ we have
$$
{\mathbb E} M_{n,k} = \frac 1 {2^k k!} \binom n k (B(k, d-1) + B(k, d-3) +\ldots),
$$
where $B(k,j)$ are coefficients in the formula
$
(t+1) (t+3) \ldots (t+2k-1) = \sum_{j=1}^{k} B(k,j) t^j.
$
\end{theorem}
For $d=1$ Theorem~\ref{theo:arcsine_multidim} reduces to~\eqref{eq:arcsine_occupation2} in view of $B(k,0) = (2k-1)!!$.
\begin{proof}
 We subdivide the collection $(\xi_1,\ldots,\xi_n)$  into $k$ groups of lengths $j_1:=i_1,j_2:=i_2-i_1,\ldots,j_k:=i_k-i_{k-1}$ and one group of length  $n-(j_1+\ldots+j_k)$ as follows:
$$
\underbrace{(\xi_1,\ldots,\xi_{i_1})}_{\text{length $j_1$}},
\;\;
\underbrace{(\xi_{i_1+1},\ldots,\xi_{i_2})}_{\text{length $j_2$}},
\;\; \ldots,\;\;
\underbrace{(\xi_{i_{k+1} +1},\ldots, \xi_{i_k})}_{\text{length $j_k$}},
\;\;
 (\xi_{i_{k}+1},\ldots,\xi_n).
$$
The last, $(k+1)$-st group will be mostly ignored in the sequel. Let $\sigma$ be any permutation on the set $\{1,\ldots,k\}$ and let ${\varepsilon}=({\varepsilon}_1,\ldots,{\varepsilon}_k) \in \{-1,+1\}^k$ be a sequence of $\pm1$'s of length $k$.
Permute the first $k$ groups of the above list according to $\sigma$ and then multiply these groups by ${\varepsilon}_1,\ldots,{\varepsilon}_k$.
The $(k+1)$-st group stays unchanged.
Clearly, our symmetric exchangeability assumption on $(\xi_1,\ldots,\xi_n)$ implies that this operation does not change the distribution of $(\xi_1,\ldots,\xi_n)$. That is, ignoring the last group we have the following distributional equality:
$$
(\xi_1,\ldots,\xi_{i_k})
{\stackrel{d}{=}}
(\underbrace{{\varepsilon}_1 \xi_{i_{\sigma(1)-1} + 1},\ldots, {\varepsilon}_1 \xi_{i_{\sigma(1)}}}_{\text{length $j_{\sigma(1)}$}},
\underbrace{{\varepsilon}_2 \xi_{i_{\sigma(2)-1} + 1},\ldots, {\varepsilon}_2 \xi_{i_{\sigma(2)}}}_{\text{length $j_{\sigma(2)}$}},
\ldots,
\underbrace{{\varepsilon}_k \xi_{i_{\sigma(k)-1} + 1},\ldots, {\varepsilon}_k \xi_{i_{\sigma(k)}}}_{\text{length $j_{\sigma(k)}$}}).
$$
Denote by $T_l$ the sum of the $\xi_i$'s in the $l$-th group, that is (with the convention $i_0:=0$)
$$
T_l := \xi_{i_{l-1} +1} +\ldots + \xi_{i_l}, \quad l=1,\ldots, k.
$$
From the above distributional equality it follows that for all $\sigma\in{\text{\rm Sym}}(k)$ and ${\varepsilon}\in\{-1,+1\}^k$,
\begin{multline*}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{\sigma(1)}},S_{j_{\sigma(1)}+j_{\sigma(2)}},\ldots,S_{j_{\sigma(1)}+\ldots+j_{\sigma(k)}})]
\\=
{\mathbb{P}} [0\notin {\mathop{\mathrm{Conv}}\nolimits}({\varepsilon}_1 T_{\sigma(1)}, {\varepsilon}_1 T_{\sigma(1)} + {\varepsilon}_2 T_{\sigma(2)}, {\varepsilon}_1 T_{\sigma(1)}+\ldots+{\varepsilon}_k T_{\sigma(k)})].
\end{multline*}
Taking the mean over all $\sigma\in {\text{\rm Sym}}(k)$ and ${\varepsilon}\in \{-1,+1\}^k$, we obtain
\begin{multline*}
\frac 1 {2^k k!} \sum_{\sigma \in {\text{\rm Sym}}(k)} \sum_{{\varepsilon}\in \{-1,+1\}^k}
{\mathbb{P}} [0\notin {\mathop{\mathrm{Conv}}\nolimits}({\varepsilon}_1 T_{\sigma(1)}, {\varepsilon}_1 T_{\sigma(1)} + {\varepsilon}_2 T_{\sigma(2)}, {\varepsilon}_1 T_{\sigma(1)}+\ldots+{\varepsilon}_k T_{\sigma(k)})]
\\
\begin{split}
&=
\frac 1 {2^k k!} \sum_{\sigma \in {\text{\rm Sym}}(k)} \sum_{{\varepsilon}\in \{-1,+1\}^k}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{\sigma(1)}},S_{j_{\sigma(1)}+j_{\sigma(2)}},\ldots,S_{j_{\sigma(1)}+\ldots+j_{\sigma(k)}})]\\
&=
\frac 1 {k!} \sum_{\sigma \in {\text{\rm Sym}}(k)}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{\sigma(1)}},S_{j_{\sigma(1)}+j_{\sigma(2)}},\ldots,S_{j_{\sigma(1)}+\ldots+j_{\sigma(k)}})],
\end{split}
\end{multline*}
where the last equality holds because the probability in the second line does not depend on ${\varepsilon}$.

We are going to compute the right-hand side. To this end, introduce a new probability measure ${\mathbb{P}}_*$ under which ${\varepsilon}$ and $\sigma$ are random, sampled uniformly from the sets $\{-1,+1\}^k$ and ${\text{\rm Sym}}(k)$, respectively, and independent of the random vector $(\xi_1,\ldots,\xi_n)$ which has the same joint law as before. Under ${\mathbb{P}}_*$, the collection of $k$ vectors
$$
\eta_l := {\varepsilon}_l T_{\sigma(l)}, \quad l=1,\ldots,k
$$
is symmetrically exchangeable. It is also clear that $(\eta_1,\ldots,\eta_l)$ has a joint Lebesgue density on ${\mathbb{R}}^{k d}$ under ${\mathbb{P}}_*$. Therefore, by Theorem~\ref{theo:absorption}, we have
$$
{\mathbb{P}}_* [0\notin {\mathop{\mathrm{Conv}}\nolimits}({\varepsilon}_1 T_{\sigma(1)}, {\varepsilon}_1 T_{\sigma(1)} + {\varepsilon}_2 T_{\sigma(2)}, {\varepsilon}_1 T_{\sigma(1)}+\ldots+{\varepsilon}_k T_{\sigma(k)})]
=
\frac {B(k, d-1) + B(k, d-3) +\ldots} {2^k k!}.
$$
It follows that the expression on the left-hand side is
\begin{multline*}
\frac 1 {2^k k!} \sum_{\sigma \in {\text{\rm Sym}}(k)} \sum_{{\varepsilon}\in \{-1,+1\}^k}
{\mathbb{P}} [0\notin {\mathop{\mathrm{Conv}}\nolimits}({\varepsilon}_1 T_{\sigma(1)}, {\varepsilon}_1 T_{\sigma(1)} + {\varepsilon}_2 T_{\sigma(2)}, {\varepsilon}_1 T_{\sigma(1)}+\ldots+{\varepsilon}_k T_{\sigma(k)})]\\
\begin{split}
&=
{\mathbb{P}}_* [0\notin {\mathop{\mathrm{Conv}}\nolimits}({\varepsilon}_1 T_{\sigma(1)}, {\varepsilon}_1 T_{\sigma(1)} + {\varepsilon}_2 T_{\sigma(2)}, {\varepsilon}_1 T_{\sigma(1)}+\ldots+{\varepsilon}_k
T_{\sigma(k)})]
\\
&=
\frac {B(k, d-1) + B(k, d-3) +\ldots} {2^k k!},
\end{split}
\end{multline*}
and we infer
$$
\frac 1 {k!} \sum_{\sigma \in {\text{\rm Sym}}(k)}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{\sigma(1)}},S_{j_{\sigma(1)}+j_{\sigma(2)}},\ldots,S_{j_{\sigma(1)}+\ldots+j_{\sigma(k)}})]
=\frac {B(k, d-1) + B(k, d-3) +\ldots} {2^k k!}
.
$$
Now we take the sum over all $(j_1,\ldots,j_k)$ from the following set:
$$
J_{n,k} := \{(j_1,\ldots,j_k)\in {\mathbb{N}}^k\colon j_1+\ldots+j_k\leq n\}.
$$
Since the cardinality of $J_{n,k}$ is $\binom nk$ and the right-hand side does not depend on $(j_1,\ldots,j_k)$, we obtain
\begin{multline*}
\frac 1 {2^k k!}\binom nk (B(k, d-1) + B(k, d-3) +\ldots)
\\
\begin{split}
&=
\frac 1 {k!} \sum_{\sigma \in {\text{\rm Sym}}(k)}  \sum_{(j_1,\ldots,j_k) \in J_{n,k}}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{\sigma(1)}},S_{j_{\sigma(1)}+j_{\sigma(2)}},\ldots,S_{j_{\sigma(1)}+\ldots+j_{\sigma(k)}})]\\
&=
\sum_{(j_1,\ldots,j_k) \in J_{n,k}}
{\mathbb{P}}[0\notin {\mathop{\mathrm{Conv}}\nolimits} (S_{j_{1}},S_{j_{1}+j_{2}},\ldots,S_{j_{1}+\ldots+j_{k}})].
\end{split}
\end{multline*}
To complete the proof, observe that the right-hand side is nothing but ${\mathbb E} M_{n,k}$.
\end{proof}

Let us now pass to the large $n$ limit. The arcsine law can be stated as follows:
$$
\lim_{n\to\infty} {\mathbb{P}}\left[\frac{N_n}{n}\leq x \right] = \frac 2 \pi \arcsin \sqrt x, \quad 0\leq x\leq 1.
$$
In terms of moments, this can be stated as follows:
$$
\lim_{n\to\infty} {\mathbb E} \left[\left(\frac{N_n}{n}\right)^k\right] = \frac 1 {2^{2k}} \binom {2k}{k}.
$$
We have a generalization of this to the $d$-dimensional case:
$$
\lim_{n\to\infty} \frac{{\mathbb E} M_{n,k}}{n^k} = \frac {1}{2^k k!} (B(k,d-1) + B(k, d-3)+\ldots).
$$
Let us now consider random bridges. The number of positive terms in the bridge is denoted by $N_n$ and defined in the same way as in~\eqref{eq:def_N_n}.
The distribution of $N_n$ is discrete uniform on $\{0,\ldots,n-1\}$ by a result of Sparre Andersen~\cite[Theorem~D]{sparre_andersen2}, that is
$$
{\mathbb{P}}[N_n = m] = \frac 1n, \quad m=0,\ldots,n-1.
$$
In terms of factorial moments, this can be stated as
$$
{\mathbb E} \binom{N_n}{k} = \frac 1{k} \binom{n-1}{k-1}, \quad k=1,\ldots, n.
$$
To state a $d$-dimensional generalization, consider a random variable $M_{n,k}$ counting the number of
simplices of the form ${\mathop{\mathrm{Conv}}\nolimits} (S_{i_1},\ldots, S_{i_{k-1}})$ not containing the origin:
by
\begin{equation}\label{eq:def_M_n_k_Br}
M_{n,k} = \frac{1}{2} \sum_{1\leq i_1 < \ldots < i_{k-1}\leq n} {\mathbbm{1}}_{\{0\notin {\mathop{\mathrm{Conv}}\nolimits}(S_{i_1},\ldots, S_{i_{k-1}})\}}.
\end{equation}
Note a slight difference to~\eqref{eq:def_M_n_k}.
In the one-dimensional case $d=1$ we have
$$
M_{n,k} = \frac 12 \binom {N_n}{k-1} + \frac 12 \binom {n-N_n-1}{k-1}
\quad \text{ and }\quad
{\mathbb E} M_{n,k} = {\mathbb E} \binom{N_n}{k-1},
$$
so that ${\mathbb E} M_{n,k}$ is a multidimensional generalization of ${\mathbb E} \binom{N_n}{k-1}$.
\begin{theorem}\label{theo:arcsine_multidim_Br}
For all $k=0,\ldots,n$ we have
$$
{\mathbb E} M_{n,k} = \frac 1 {k!} \binom {n-1} {k-1} \left({\genfrac{[}{]}{0pt}{}{{k}}{{d-1}}} + {\genfrac{[}{]}{0pt}{}{{k}}{{d-3}}} +\ldots\right).
$$
\end{theorem}

