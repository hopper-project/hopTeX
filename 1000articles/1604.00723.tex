\documentclass[10pt,twocolumn,twoside]{IEEEtran} 
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epsfig,amssymb}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{array}
\usepackage{amsfonts,booktabs}
\usepackage{lipsum,amsmath,multicol}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{times}
\usepackage{pdfsync}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[subnum]{cases}
\usepackage{filecontents}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{setspace}
\geometry{left=0.65in, right=0.65in, top=0.5in, bottom=0.7in}
\addtolength{\abovedisplayskip}{-2mm}
\addtolength{\belowdisplayskip}{-2mm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{property}{Property}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

\begin{document}
\title{Convergence Analysis of Quantized Primal-dual Algorithms in Network Utility Maximization Problems}
\author{Ehsan Nekouei, Tansu Alpcan, Girish N.~Nair, Robin J.~Evans 
\thanks{Department of Electrical and Electronic Engineering, The University of Melbourne, VIC 3010, Australia. E-mails: \{ehsan.nekouei,tansu.alpcan,gnair,robinje\}@unimelb.edu.au }
}
\maketitle
\thispagestyle{empty}
\begin{abstract}
This paper investigates the asymptotic and non-asymptotic behavior of the quantized primal-dual (PD) algorithm in network utility maximization (NUM) problems,
in which a group of agents  maximize the sum of their individual concave objective functions under linear constraints. 
 In the asymptotic scenario, we 
use the information-theoretic notion of {\em differential entropy power} to establish universal bounds on the maximum exponential convergence rates of joint PD, primal and dual variables under optimum-achieving quantization schemes. These results provide trade-offs between the speed of exponential convergence, the agents' objective functions, the communication bit rates, and the number of agents and constraints. In the non-asymptotic scenario, we obtain lower bounds on the mean square distance of joint PD, primal and dual variables from the optimal solution at any time instant. These bounds hold regardless of the quantization scheme used.
\end{abstract}
\begin{IEEEkeywords}
\end{IEEEkeywords}

\section{Introduction}
\subsection{Motivation}
With continuing advances in  networking technology, our societies have become increasingly dependent on network-based technologies for performing everyday tasks. 
For example, consider data transfer using the internet, environmental monitoring  using wireless sensor networks, and online storage or computation in the ``cloud''. 
In all these applications, a limited number of resources, \emph{e.g.}, bandwidth, memory and CPU time, are shared among a group of networked devices, hereafter called \emph{agents}, 
to deliver the required service. As the quality of the delivered task is highly dependent on how the network resources are shared among the agents, 
resource allocation algorithms have become  vital components of these technologies. 

 

In the seminal work \cite{KMT98}, Kelly \emph{et al.} introduced the network utility maximization (NUM) approach, which provides decentralized frameworks, \emph{e.g.,} primal, dual and primal-dual (PD) decomposition methods, for solving large-scale resource allocation problems.    
In each decomposition method, the computational burden of solving the resource allocation problem is distributed among agents, and the task of information transfer between different agents is handled by an underlying communication network. The problem of devising efficient decomposition methods for NUM problems has been extensively studied in the past decade, \emph{e.g.,} see \cite{SS07} and references therein. 
Our aim in this paper is is to analyze the impact of quantized communications in NUM problems, using information-theoretic ideas.

\subsection{Related Work}
Although the performance of distributed optimization algorithms, and in particular NUM algorithms, under perfect communication networks is well understood, 
the investigation of the impact of imperfect communications on these optimization algorithms is relatively a new research area that has attracted much interests in recent years, \emph{e.g.,} see \cite{Nedic08}-\cite{YH14}.

Nedi\'{c} {\em et al.} \cite{Nedic08} considered a convex optimization problem, in which a set of agents collaboratively minimize a sum of individual objective functions. They proposed an averaging-based algorithm and studied its convergence rate under an infinite-level, uniform quantization scheme.  In \cite{Rabbat05}, the authors proposed an incremental algorithm for solving a convex optimization problem. They analyzed the convergence of the proposed algorithm under a uniform quantization scheme.

Yuan \emph{et al.} \cite{YXZR12} considered a constrained optimization problem in which a group of agents cooperate to minimize the sum of their local convex objective functions subject to a set of global constraints. They proposed a dual averaging algorithm and analyzed its convergence under uniform deterministic/stochastic quantization schemes. The authors in \cite{YH14} proposed a distributed sub-gradient algorithm for solving  an unconstrained multi-agent convex optimization problem, and studied its convergence under uniform zoom-in quantization. Finally, the authors of \cite{CL10} studied the problem of minimizing an upper bound on the distortion due to quantization in distributed iterative algorithms. They established  the optimality of different quantization structures under various distortion measures. Different from the literature discussed above, in this paper we study the speed of \emph{exponential} convergence of quantized PD algorithm in solving NUM problems. Moreover, our main results are independent of the structure of the underlying quantization scheme and hence can be applied to a more general class  than uniform quantizers.

In \cite{NNA15-CDC}, we studied the convergence behavior of the PD algorithm in a quadratic NUM problem under quantized communications. 
 In the current paper, the objective functions of agents belong to the class of concave and twice continuously differentiable functions. This complicates our analysis as the PD update rule becomes non-linear in primal variables. Here, we study the impact of quantized communications on the convergence behavior of \emph{joint PD, primal and dual} variables in both \emph{asymptotic and non-asymptotic} regimes.

\subsection{Contributions}
We consider a NUM problem in which  $M$ agents  maximize the sum of their local concave objective functions subject to $N$ linear constraints using a quantized PD algorithm with a random initial condition. \textcolor{black}{As standard in the NUM literature, \emph{e.g.,} see  \cite{KMT98}, \cite{SS07} and references therein, we assume that the primal variables are updated by agents, and each dual variable is updated by a network node (NN) which has access to the knowledge of the the constraint associated with its dual variable}.
Thus, agents and NNs need to exchange the quantized values of primal and dual variables to execute the PD algorithm.
We investigate the impact of quantized communications between agents and NNs on the rate of exponential mean-square convergence of the PD algorithm  under optimum achieving (OA) quantization schemes. The OA quantization schemes allow the primal and dual variables to converge to their optimal values as the time instance $k$ tends to infinity. 

First, using the information-theoretic notion of {\em differential entropy power}, we establish universal, explicit bounds on the fastest speed of asymptotic exponential mean square convergence for the PD, primal and dual variables to their corresponding optimal values (Theorem \ref{Theo: DDE}, \ref{Theo: DDE-P} and \ref{Theo: DDE-D}).
\textcolor{black}{Unlike previous studies of quantized optimization, a significant feature of these bounds is that they are completely independent of the OA quantization scheme employed,  making them 
applicable to {\em all} quantized PD algorithms. Given the utility functions, constraints and aggregate data rates (bits/sample)  of the agents and NN's, these results
give  system designers a way to determine in advance what exponential convergence speeds are  impossible to achieve.}
We note that the entropy power method has been used to study the stability of feedback control systems under quantization, \emph{e.g.,} see \cite{NE04} and \cite{FMS10}, \textcolor{black}{as well as convergence in quantized games \cite{NNA15}}.

\textcolor{black}{Next, we obtain a bound on the fastest speed of exponential mean square convergence of PD variables in quadratic NUM problems under zoom-in quantization schemes (see Theorem \ref{Theo: EDE-New} for more details). This bound is significantly  tighter in the high data rate regime than Theorem \ref{Theo: DDE}.} We also derive  lower bounds on the mean square distance of PD, primal and dual variables from their corresponding optimal solutions for any given $k$ under quantized communication between agents and NNs (see Corollaries \ref{Coro: FTE} and \ref{Coro: FTE-P}). \textcolor{black}{Finally, we propose a uniform, zoom-in quantization scheme which allows the PD algorithm to converge to the optimal solution (Theorem 5).}

 
The organization of the paper is as follows. The next section describes our system model and assumptions. Section \ref{Sec: R&D} states our asymptotic and non-asymptotic results on the convergence of PD algorithm under quantization. In Section \ref{Sec: AOAQ}, we propose an OA quantization scheme. Section \ref{Sec: NR} presents our numerical results, and section \ref{Sec: Conc} concludes the paper. All  proofs are relegated to the appendices, to aid the fluency of the paper.

\section{System Model and Problem Formulation}\label{Sec: SM}
Consider a convex optimization problem in which $M$ agents maximize the sum of their individual objective functions subject to a set of linear equality constraints. 
 Let $x^i$ and $U_{i}{\left({x^i}\right)}$ represent the decision variable of agent $i$ and its objective function, respectively. It is assumed that the objective function of each agent is concave in its decision variable. The agents are interested in the solution of the following NUM problem:
  \begin{eqnarray}\label{Prob}
 \begin{array}{cc}
\underset{{\ensuremath{\boldsymbol{{x}}}}}{\rm maximize} & \sum\limits_{i}^M U_i{\left({x^i}\right)}\\
{\rm Subject\quad to} &  A{\ensuremath{\boldsymbol{{x}}}}= {\ensuremath{\boldsymbol{{b}}}}\\
  \end{array},
 \end{eqnarray}
where, $M$ is the number of agents, ${\ensuremath{\boldsymbol{{b}}}}\in {\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^N$, $A\in {\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N\times M}$, $N$ is the number of constraints,  and ${\ensuremath{\boldsymbol{{x}}}}=\left[x^1,\cdots,x^M\right]^\top$. We impose the condition $N<M$ to ensure that the feasible set of the optimization problem \eqref{Prob} is non-empty. The objective function in \eqref{Prob} is concave and the constraints are linear, thus, the optimization problem \eqref{Prob} can be solved using standard convex optimization techniques.

 Under the PD algorithm, the primal and dual variables are update according to
\begin{align}\label{EQ: NQU}
x^{i}_{k}&=x^{i}_{k-1}+\mu_{k-1} {\left({\frac{d }{d x^i}U_i{\left({x^i_{k-1}}\right)}-A^\top_i{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}}\right)},\nonumber  1\leq i\leq M\\
\lambda^{j}_{k}&=\lambda^j_{k-1}+\mu_{k-1}{\left({\bar{A}_j{\ensuremath{\boldsymbol{{x}}}}_{k-1}-b_j}\right)}\quad 1\leq j\leq N
\end{align}
, respectively, where $\mu_{k-1}$ is the step size of the algorithm at time $k-1$, $x^i_k$ and $\lambda^j_k$ denote the values of  $i$th primal variable and $j$th dual variable at time $k$, respectively, ${\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}=\left[\lambda_{k-1}^1,\cdots,\lambda_{k-1}^N\right]^\top$, $A_i$ denotes the $i$th column of the matrix $A$ and $\bar{A}_j$ denotes the $j$th row of matrix $A$. To obtain the solution of the optimization problem \eqref{Prob}, we consider a primal-dual (PD) decomposition approach in which the primal variables, \emph{i.e.,} agents' decision variables, are updated by agents at each time. \textcolor{black}{Also, at each time step of the PD algorithm,  the $j$th dual variable, \emph{i.e.,} $\lambda^j$, is updated by the $j$th network node (NN) which has the knowledge of parameters characterizing the constraint associated with $\lambda^j$, \emph{i.e.,} $A_j$ and $b_j$.}  The vector of PD variables at time $k$, \emph{i.e.,} ${\ensuremath{\boldsymbol{{y}}}}_k$, is defined as the vector concatenation of ${\ensuremath{\boldsymbol{{x}}}}_k$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_k$, \emph{i.e.,}
\begin{eqnarray}
{\ensuremath{\boldsymbol{{y}}}}_k=
\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{x}}}}_k\\
{\ensuremath{\boldsymbol{{\lambda}}}}_k
\end{array}
\right].\nonumber
\end{eqnarray}
 In this paper, it is assumed that the initial primal and dual variables, \emph{i.e.,} ${\ensuremath{\boldsymbol{{x}}}}_0$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_0$, are drawn randomly according to the probability density functions $p_{{\ensuremath{\boldsymbol{{x}}}}_0}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$ and $p_{{\ensuremath{\boldsymbol{{\lambda}}}}_0}{\left({{\ensuremath{\boldsymbol{{\lambda}}}}}\right)}$, respectively. \textcolor{black}{By allowing the initial condition to be random,  the primal and dual variables become random variables. This allows us to use information theoretic tools to study the speed of exponential convergence of the primal-dual algorithm under quantized communications.} We further impose the following assumptions on the objective functions of agents, step size $\mu_k$, $p_{{\ensuremath{\boldsymbol{{x}}}}_0}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$ and $p_{{\ensuremath{\boldsymbol{{\lambda}}}}_0}{\left({{\ensuremath{\boldsymbol{{\lambda}}}}}\right)}$. 
\begin{enumerate}
\item The agents' objective functions are concave and twice continuously differentiable.
\item $U^{\rm min}_i\leq \frac{d^2}{d {x^i}^2}U_i{\left({x^i}\right)}\leq U^{\rm max}_i<0$ for $x_i\in {\ensuremath{{\ensuremath{\mathbb{{R}}}}}}$ and all $i$.
\item $\mu_k\leq \min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}$ for all $k$.
\item The sequence $\left\{\mu_k\right\}_k$ converges to $\mu^\star>0$.
\item The random vectors ${\ensuremath{\boldsymbol{{x}}}}_0$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_0$ are mutually independent and the distributions of ${\ensuremath{\boldsymbol{{x}}}}_0$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_0$ have finite differential entropies. That is,
\begin{eqnarray}
\left | - \int p_{{\ensuremath{\boldsymbol{{x}}}}_{0}}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}{\ensuremath{\log{\left({{p_{{\ensuremath{\boldsymbol{{x}}}}_{0}}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}}}\right)}}}d{\ensuremath{\boldsymbol{{x}}}}\right | <\infty\nonumber\\
\left | - \int p_{{\ensuremath{\boldsymbol{{\lambda}}}}_{0}}{\left({{\ensuremath{\boldsymbol{{\lambda}}}}}\right)}{\ensuremath{\log{\left({{p_{{\ensuremath{\boldsymbol{{\lambda}}}}_{0}}{\left({{\ensuremath{\boldsymbol{{\lambda}}}}}\right)}}}\right)}}}d{\ensuremath{\boldsymbol{{\lambda}}}}\right | <\infty\nonumber
\end{eqnarray}
\end{enumerate}
\textcolor{black}{Assumptions 1 and 2 are standard in the optimization literature. Assumption 2 implies that the objective functions of agents are strongly concave and the first derivative of each objective function is Lipschitz. Assumption 4 implies that the unquantized update rule does not employ a diminishing step-size rule as the PD update rule may not converge  exponentially with diminishing step-size rule. Assumptions 3 and 4, which are not commonly used in the literature, allow us to use the entropy power method. Assumption 5 implies that the initial condition injects a minimum  amount of uncertainty to the PD algorithm, and the amount of uncertainty due to the initial condition is bounded. Variants of assumption 5 are used in the quantized feedback control literature, \emph{e.g.,} see \cite{NE04}.
}

\subsection{Quantizer Structure}

To execute the PD update rule \eqref{EQ: NQU}, the agents and NNs require the knowledge of dual and primal variables, respectively. 
Since the agents and NNs are not necessarily co-located, the information exchange between NNs and agents is performed via broadcast communication channels, as described in the next subsection. 
Due to the capacity limitations of these channels,  only quantized versions of the primal and dual variables can be exchanged between NNs and agents. 

\textcolor{black}{To transmit $x^i_k$ to NNs, agent $i$ encodes $x^i_k$ to $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}$ using an adaptive encoder mapping  of the form
\begin{eqnarray}
\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}=E_{i,k}^{{\ensuremath{\boldsymbol{{x}}}}}{\left({\left\{x^i_n\right\}_{n=0}^k,\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,n}\right\}_{n=0}^{k-1}}\right)}. \nonumber
\end{eqnarray} 
 Then, it broadcasts $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}$ to NNs. The output of the encoder of agent $i$ at time $k$, \emph{i.e.,} $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}$, belongs to the finite alphabet set $\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}$. Thus, agent $i$ requires $\log_2{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}}\right|}$ bits to transmit its encoded symbol to NNs.    A large value of ${\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}}\right|}$ indicates that agent $i$ transmits its decision variable with a high precision to NNs whereas a low ${\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}$  indicates low quality communication between agent $i$ and NNs.}

 \textcolor{black}{Upon receiving $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}$, each NN reconstructs a  quantized estimate of $x^i_k$, \emph{i.e.,} ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{i,k}}}}$, 
using the decoder mapping ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{i,k}}}}=D^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}{\left({\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,n}\right\}_{n=0}^k}\right)}$.
}
\textcolor{black}{Similarly, at time $k$, NN $j$ chooses the symbol $\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}$ from the finite alphabet set $\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}$ according to the adaptive encoding map 
\begin{eqnarray}\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}=E_k^{\lambda}{\left({\left\{\lambda^j_n\right\}_{n=0}^k,\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,n}\right\}_{n=0}^{k-1}}\right)},\nonumber
\end{eqnarray}
 and broadcasts $\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}$ to all agents. Then, agents construct the quantized version of $\lambda^j_{k}$, \emph{i.e.,} ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{j,k}}}}$, using the decoding map ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{j,k}}}}=D^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}{\left({\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,n}\right\}_{n=0}^k}\right)}$. Note that our formulation allows the encoded symbol at time $k$ to depend on the current and past values of primal/dual variables as well as the past outputs of the encoder.}
 \textcolor{black}{We refer to $\mathcal{Q}=\!\!\left\{\!\!\left\{\!E^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}{\left({\cdot}\right)}\!,\!D^{{\ensuremath{\boldsymbol{{x}}}}}_{i,k}{\left({\cdot}\right)}\!\right\}_i\!,\!\left\{\!E^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}{\left({\cdot}\right)}\!, \!D^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,k}{\left({\cdot}\right)}\!\right\}_j\!\!\right\}_{k=0}^\infty$ as a quantization scheme.}
Also, the quantized version of the PD variables at time $k$ under the quantization scheme $\mathcal{Q}$ is denoted by ${\ensuremath{Q^{{}}_{{k}}}}$, \emph{i.e.,}
\begin{eqnarray}
{\ensuremath{Q^{{}}_{{k}}}}=
\left[
\begin{array}{c}
{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k}}}}\\
{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k}}}}\nonumber
\end{array}
\right],
\end{eqnarray}
where ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k}}}}\!\!=\!\!\left[{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{1,k}}}},\cdots,{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{M,k}}}}\right]^\top$ and ${\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k}}}}\!\!=\!\!\left[{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{1,k}}}},\cdots,{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{N,k}}}}\right]^\top$. 

Next, we define three notions of data rate for a given quantization scheme $\mathcal{Q}$. Later, these data rates are used to study the convergence behavior of primal, dual and PD variables. The average aggregate data rate per unit time for transmitting the primal variables to NNs under the quantization scheme $\mathcal{Q}$, $R_{{\ensuremath{\boldsymbol{{x}}}}}$, is defined as
\begin{eqnarray}
R_{{\ensuremath{\boldsymbol{{x}}}}}=\lim_{k\rightarrow\infty}\frac{1}{k}\sum_{t=0}^{k-1}{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)}
\end{eqnarray}
Similarly, we define the average aggregate data rate per unit time for broadcasting the dual variables to agents under the quantization scheme $\mathcal{Q}$, $R_{{\ensuremath{\boldsymbol{{\lambda}}}}}$, as
\begin{eqnarray}\label{Eq: R-dual}
R_{{\ensuremath{\boldsymbol{{\lambda}}}}}=\lim_{k\rightarrow\infty}\frac{1}{k}\sum_{t=0}^{k-1}{\left({\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}}\right)}
\end{eqnarray}
 Finally, the average total date rate per unit time under the quantization scheme $\mathcal{Q}$, \emph{i.e.,} $R_{\mathcal{Q}}$, is defined as
\begin{eqnarray}\label{Eq: R_total}
R_{\mathcal{Q}}=\lim_{k\rightarrow\infty}\frac{1}{k}\sum_{t=0}^{k-1}{\left({{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)}+\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}}\right)}
\end{eqnarray}

The quantized PD update rule under the quantization scheme $\mathcal{Q}$ can be written as
\begin{align}\label{EQ: QU}
x^{i}_{k}&=x^{i}_{k-1}+\mu_{k-1} {\left({\frac{d }{d x^i}U_i{\left({x^i_{k-1}}\right)}-A^\top_i{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k-1}}}}}\right)},\nonumber\\
\lambda^{j}_{k}&=\lambda^j_{k-1}+\mu_{k-1}{\left({\bar{A}_j{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k-1}}}}-b_j}\right)}
\end{align}
Let ${\ensuremath{\boldsymbol{{x}}}}^\star$, ${\ensuremath{\boldsymbol{{\lambda}}}}^\star$ be the optimal primal and dual solutions, respectively. Also, let  ${\ensuremath{\boldsymbol{{y}}}}^\star$ be the vector concatenation of  ${\ensuremath{\boldsymbol{{x}}}}^\star$, ${\ensuremath{\boldsymbol{{\lambda}}}}^\star$. We define $\epsilon_k={\ensuremath{\boldsymbol{{y}}}}_k-{\ensuremath{\boldsymbol{{y}}}}^\star$ as the difference between the PD variables at time $k$ and the optimal solution. Let ${\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}$ denote the distance of the PD variables at time $k$ from optimal solution, \emph{i.e.,}
\begin{eqnarray}
{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}=\sqrt{\sum_{i=1}^M{\left({x^i_k-{x^i}^\star}\right)}^2+\sum_{j=1}^N{\left({\lambda^j_k-{\lambda^j}^\star}\right)}^2}
\end{eqnarray}
where ${x^i}^\star$ and ${\lambda^j}^\star$ are the optimal values of the primal variable $x^i$ and the dual variable $\lambda^j$, respectively. Then, the mean square distance (MSD) of the PD variables from the optimal solution at time $k$ under the quantization scheme $\mathcal{Q}$ is defined as ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon_k}}}}}\right\Vert_{{2}}}^2} \right]}}$. We define the MSD of the primal variables from the optimal primal solution at time $k$ as ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}}\right\Vert_{{2}}}^2} \right]}}$ where ${\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}={\ensuremath{\boldsymbol{{x}}}}_k-{\ensuremath{\boldsymbol{{x}}}}^\star$. Similarly, the MSD of dual variables at time $k$ from the optimal dual solution is defined as ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right\Vert_{{2}}}^2} \right]}}$ where ${\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{\lambda}}}}}={\ensuremath{\boldsymbol{{\lambda}}}}_k-{\ensuremath{\boldsymbol{{\lambda}}}}^\star$. 
 
Next, we define the class of optimum achieving (OA) quantization schemes.
\begin{definition}\label{Def: OA}
The quantization scheme $\mathcal{Q}$ is called an OA quantization scheme if, under $\mathcal{Q}$, the primal and dual variables converge to their optimal values ${\ensuremath{\boldsymbol{{x}}}}^\star$ and ${\ensuremath{\boldsymbol{{\lambda}}}}^\star$, receptively. That is: 
\begin{eqnarray}
\lim_{k\rightarrow\infty}{\ensuremath{\boldsymbol{{x}}}}_k={\ensuremath{\boldsymbol{{x}}}}^\star\nonumber\\
\lim_{k\rightarrow\infty}{\ensuremath{\boldsymbol{{\lambda}}}}_k={\ensuremath{\boldsymbol{{\lambda}}}}^\star\nonumber
\end{eqnarray}
\end{definition}
The Definition \ref{Def: OA} implies that, under an OA quantization scheme, the quantization error does not impede the convergence of the  PD algorithm to the optimal solution. Thus, under an OA quantization scheme, the PD algorithm converges to the optimal solution of the optimization problem regardless of the quantized  communication between agents and NNs.

\subsection{Communication Graph and Communication Cost}

\textcolor{black}{The communication topology is represented  by a bipartite graph induced by the $N\times M$ constraint matrix $A$.  In this graph, edges exist only between agents and  network nodes (NNs), which  form two disjoint sets of vertices.
There exists an edge  between agent $i$ and NN $j$ 
  in the communication graph if and only if $A_{ji}\neq 0$. The communication mechanism is broadcast in nature,
with each vertex  `listening' and broadcasting only to those other vertices with which it shares an edge.
This is implemented by uniquely assigning every vertex in the graph   one of $N+M$ disjoint transmission radio-frequency bands ({\em frequency division multiplexing}) or one of $N+M$ disjoint time slots per cycle ({\em time division multiplexing}),
before the system is deployed. Any other vertex that needs to listen to a transmission just tunes in to the appropriate frequency band or time slot dedicated to the corresponding transmitter. 
Note that the edges do not represent individual one-to-one channels, but indicate the  broadcast transmitter-receiver structure of the system.}

\textcolor{black}{Under typical digital modulation formats, the width of the frequency band/time-slot allocated to agent $i$ and/or the average transmission power it consumes to broadcast its encoded  symbols 
to all NNs $j$ with $A_{ji}\neq 0$ will be proportional to its average data rate $R^i_{{\ensuremath{\boldsymbol{{x}}}}}:=\lim_{k\rightarrow\infty}\frac{1}{k}\sum_{t=0}^{k-1}\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}$. 
Similarly, the band/slot-width and/or transmission power used by NN $j$ to broadcast its encoded dual symbols 
to all agents $i$ with $A_{ji}\neq0$  is typically proportional to $R^j_{{\ensuremath{\boldsymbol{{\lambda}}}}}:=\lim_{k\rightarrow\infty}\frac{1}{k}\sum_{t=0}^{k-1}\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}$.
Equation \eqref{Eq: R_total}, which can be intuitively interpreted as $\sum_{i=1}^M R^i_{{\ensuremath{\boldsymbol{{x}}}}}+\sum_{j=1}^NR^j_{{\ensuremath{\boldsymbol{{\lambda}}}}}$, then captures the total amount of physical resources, \emph{i.e.,} time, bandwidth or power,  required for the system to communicate. It can be seen that this cost scales like $O(N+M)$ as the network grows in size. Note that due to the broadcast nature of the system,
 every transmission can be heard by multiple receivers, without the transmitter having to use up extra resources. 
}

\section{Results and Discussions}\label{Sec: R&D}
In this section, we analyze the impact of quantized communications on the mean square distance (MSD) of primal-dual (PD), primal and dual variables from the optimal solution in two different regimes: $(i)$ Asymptotic regime, $(ii)$ Non-asymptotic regime. In the asymptotic regime, we are concerned with the behavior of MSD under OA quantization schemes as the time index $k$ tends to infinity. To this end, the notion of distance decay exponent (DDE) is introduced which captures the rate of exponential convergence of MSD to zero. 
We establish universal lower bounds on the DDE of PD variables, primal variables and dual variables (see Theorems \ref{Theo: DDE}, \ref{Theo: DDE-P}, \ref{Theo: DDE-D} and \ref{Theo: EDE-New} for more details).
In the non-asymptotic regime, we are concerned with the behavior of the MSD for any finite $k$. Here, our results provide universal lower bounds on the MSD of PD, primal and dual variables, from the optimal solution, for any finite $k$ (see Corollaries \ref{Coro: FTE} and \ref{Coro: FTE-P} for more details). We start by presenting our asymptotic results in the next subsection.
\subsection{Asymptotic behavior of MSD in PD algorithm}
In this subsection, first, we introduce the notion of distance decay exponent (DDE) for the PD, primal and dual variables.  Then, we derive universal lower bounds on the DDE of PD, primal and dual variables.
\begin{definition}
Let $\mathcal{Q}$ be an OA quantization scheme. Then, the DDE of the PD, primal and dual variables under $\mathcal{Q}$ are  defined as
\begin{eqnarray}
\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}},\nonumber\\
\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}}\right\Vert_{{2}}}^2} \right]}},\nonumber\\
\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{\lambda}}\right\Vert_{{2}}}^2} \right]}},\nonumber
\end{eqnarray}
respectively.
\end{definition}
The DDEs capture the  speed of exponential mean square convergence of the PD, primal and dual variables to their corresponding optimal solutions. They are non-positive quantities, and a more negative DDE indicates faster convergence to the optimal solution. \textcolor{black}{Also, a zero DDE implies slower-than-exponential convergence.} In this subsection, the information-theoretic notion of entropy power is used to establish universal lower bounds on the DDE of the PD/primal/dual variables. 

The next theorem provides a universal lower bound on the  DDE of the PD variables under OA quantization schemes.
\begin{theorem}\label{Theo: DDE}
Let  $\mathcal{Q}$  be an OA quantization scheme. Then, the DDE of PD variables under $\mathcal{Q}$ can be lower bounded as
\begin{align}
&\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}\nonumber\\
&\hspace{1cm}\geq\frac{2}{N+M}{\left({\sum_{i=1}^M\log{\left({1+\mu^\star \frac{d^2}{d {x^i}^2}U_i{\left({{x^i}^\star}\right)}}\right)}-R_{\mathcal{Q}}}\right)}.
\end{align}
where ${x^i}^\star$ is the optimal value of the primal variable $x^i$.
\end{theorem}
\begin{IEEEproof}
Please see Appendix \ref{App: DDE}.
\end{IEEEproof}
Theorem \ref{Theo: DDE} establishes an explicit universal lower bound on the DDE of PD variables under OA quantization schemes. 
This bound is universal in the sense that it is independent of the structure of quantizer, and is thus applicable to all quantization schemes.

According to Theorem \ref{Theo: DDE}, for a given average total data rate $R_{\mathcal{Q}}$, the PD variables  converge to the optimal solution at most exponentially fast.
The speed of this exponential convergence is bounded by the average total data rate under the quantization scheme, \emph{i.e.,} $R_{\mathcal{Q}}$, 
and also by the behavior of the objective functions of agents around the optimal solution. As stated in Theorem \ref{Theo: DDE}, the lower bound on the DDE for PD variables decreases linearly with $R_{\mathcal{Q}}$.
Note that as $R_{\mathcal{Q}}$ becomes large, the NNs and agents have more precise information about the primal and dual variables. \textcolor{black}{Thus one might intuitively expect a  quantized PD algorithm to converge faster 
to the optimal solution as $R_{\mathcal{Q}}$ increases. The result above is consistent with this intuition.}

The lower bound on the DDE also increases with the second derivatives of the agents' objective functions  at the optimal solution. 
As these second derivatives becomes less negative, 
the objective function becomes flatter near the optimal solution \textcolor{black}{and the quantized PD algorithm can be expected to converge more slowly. This result is also in concordance with this intuition.}

The next theorem establishes a universal lower bound on the DDE of primal variables in the quantized PD update rule under an OA quantization scheme.
\begin{theorem}\label{Theo: DDE-P}
Consider the OA quantization scheme $\mathcal{Q}$. Then, the DDE of the primal variables under $\mathcal{Q}$ is lower bounded as
\begin{align}
&\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\nonumber\\
&\hspace{1cm}\geq\frac{2}{M}{\left({\sum_{i=1}^M\log{\left({1+\mu^\star \frac{d^2}{d {x^i}^2}U_i{\left({{x^i}^\star}\right)}}\right)}-R_{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right)}.
\end{align}
\end{theorem}
\begin{IEEEproof}
Please see Appendix \ref{App: DDE-P}.
\end{IEEEproof}
According to Theorem \ref{Theo: DDE-P},  the  exponential convergence speed of the primal variables is limited by the behavior of objective functions of agents around the optimal solution, the average aggregate data rate for transmission of dual variables and the number of agents. Different from the PD bound in Theorem \ref{Theo: DDE}), this  lower bound on the DDE of the primal variables depends only on the average aggregate data rate for transmission of dual variables, \emph{i.e.,} $R_{{\ensuremath{\boldsymbol{{\lambda}}}}}$, rather than on the average total data rate under the quantization scheme $\mathcal{Q}$. This observation signifies the role of the quantized dual variables on the convergence of the primal variables. 

In the next theorem, we study the DDE for dual variables.
\begin{theorem}\label{Theo: DDE-D}
The DDE of dual variables under the OA quantization scheme $\mathcal{Q}$ satisfies 
\begin{align}
&\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq-\frac{2}{N}R_{{\ensuremath{\boldsymbol{{x}}}}}.
\end{align}
\end{theorem}
\begin{IEEEproof}
Please see Appendix \ref{App: DDE-D}.
\end{IEEEproof}
Theorem \ref{Theo: DDE-D} establishes a universal  bound on the fastest possible exponential convergence rate of the dual variables under any OA quantization  scheme $\mathcal{Q}$. The lower bound in Theorem \ref{Theo: DDE-D} is controlled by the number of constraints and the average aggregate data rate for transmission of primal variables to NNs. Compared to the PD lower bound, it does not depend on the behavior of the objective functions of agents and is only limited by the average aggregate data rate for transmission of the primal variables, \emph{i.e.,} $R_{{\ensuremath{\boldsymbol{{x}}}}}$, rather than the average total data rate  $R_{\mathcal{Q}}$. 

\textcolor{black}{Next, we derive a lower bound on the DDE of the PD algorithm in quadratic NUM problems under zoom-in quantization schemes (see Definition \ref{Def: Zoom-in}). This bound is tighter compared with the lower bound in Theorem \ref{Theo: DDE} at the high data-rate regime. In a quadratic NUM problem, the objective function of agent $i$ is given by $U_i{\left({x^i}\right)}=-\frac{a_i}{2}{\left({x^i}\right)}^2+c_ix^i+f_i$ where $a_i$ is a positive constant. The unquantized PD algorithm for quadratic NUM problems can be written as  
\begin{align}\label{Eq: QUR}
x^{i}_{k}&={\left({1-\mu a_i}\right)}x^{i}_{k-1}+\mu {\left({c_i-A^\top_i{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}}\right)},\nonumber  1\leq i\leq M\\
\lambda^{j}_{k}&=\lambda^j_{k-1}+\mu{\left({\bar{A}_j{\ensuremath{\boldsymbol{{x}}}}_{k-1}-b_j}\right)}\quad 1\leq j\leq N
\end{align}
 Let ${\ensuremath{\boldsymbol{{y}}}}_k$ be the vector concatenation of ${\ensuremath{\boldsymbol{{x}}}}_k$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_k$. Then, \eqref{Eq: QUR} can be written as 
\begin{align}
{\ensuremath{\boldsymbol{{y}}}}_{k}=T{\ensuremath{\boldsymbol{{y}}}}_{k-1}+\mu
\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{c}}}}\\
-{\ensuremath{\boldsymbol{{b}}}}
\end{array}
\right]\nonumber
\end{align}
 where ${\ensuremath{\boldsymbol{{c}}}}=\left[c_1\cdots,c_M\right]^\top$ and the matrix $T$ is defined as  
\begin{align}
T=\left[
 \begin{array}{cc}
	{\rm Diag}{\left({1-\mu a_1,\cdots,1-\mu a_M}\right)}&-\mu A^\top\\
	\mu A&I_{N}
\end{array}
\right]
\end{align}
in which $I_N$ denotes an $N$-by-$N$ identity matrix and ${\rm Diag}{\left({1-\mu a_1,\cdots,1-\mu a_M}\right)}$ is a diagonal matrix with the $i$th diagonal element equal to $1-\mu a_i$.} 

\textcolor{black}{Let $\tilde{Q}_k=\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{M,n},\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{N,n}\right\}_{n=0}^k$ be the collection of encoders' outputs up to time $k$, respectively. The quantized PD update rule is denoted by ${\ensuremath{\boldsymbol{{y}}}}_{k+1}={\ensuremath{\hat{T}\left({{\ensuremath{\boldsymbol{{y}}}}_k},{\tilde{q}_k}\right)}}$ where $\tilde{q}_k$ is a realization of $\tilde{Q}_k$. We use $C_k{\left({\tilde{q}_k}\right)}$ to represent the quantization cell corresponding to $\tilde{q}_k$. Next, a zoom-in quantization scheme is defined.}
\begin{definition}\label{Def: Zoom-in}
 \textcolor{black}{Consider the quantization scheme $\mathcal{Q}$, and let $C_k{\left({\tilde{q}_k}\right)}$ be the quantization cell at time $k$  which contains ${\ensuremath{\boldsymbol{{y}}}}_k$. Then, $\mathcal{Q}$ is a zoom-in quantization scheme if at time $k+1$ the image of $C_k{\left({\tilde{q}_k}\right)}$ under ${\ensuremath{\hat{T}\left({\cdot},{\tilde{q}_k}\right)}}$ is quantized for all $k\in{\ensuremath{{\ensuremath{\mathbb{{N}}}}}}_0=\left\{0,1,2,\cdots\right\}$.}
\end{definition}

\textcolor{black}{In addition to the assumptions in Section \ref{Sec: SM}, we assume that 
\begin{enumerate}
\item The matrix $T$ is invertible and all its eigenvalues are inside the unit circle in complex plane. 
\item A zoom-in quantization scheme is employed and each primal/dual variable is independently quantized. 
\item The distributions of initial primal and dual variables, \emph{i.e.,} $p_{{\ensuremath{\boldsymbol{{x}}}}_0}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$ and $p_{{\ensuremath{\boldsymbol{{\lambda}}}}_0}{\left({{\ensuremath{\boldsymbol{{\lambda}}}}}\right)}$, are bounded and have  finite support sets.
\end{enumerate} }

\begin{theorem}\label{Theo: EDE-New}
\textcolor{black}{Consider  any zoom-in quantization scheme $\mathcal{Q}$ with $\rho=\frac{\delta^{\rm max}_k}{\delta^{\rm min}_k}$ (for all $k$) where $\delta^{\rm max}_k$ and $\delta^{\rm min}_k$ are the maximum and minimum quantization steps under $\mathcal{Q}$ at time $k$, respectively. Let ${\rm B}$ be the hypercube centered at the origin with the $i$th side length equal to $4\rho{\left|{T_{ii}}\right|}+2{\left\Vert{T}\right\Vert_{{\infty}}}$ where ${\left\Vert{\cdot}\right\Vert_{{\infty}}}$ denotes the norm infinity and $T_{ii}$ is the $i$ diagonal entry of matrix $T$. Let $\beta_T$ be the number elements in the set ${\rm B}\cap T{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ where the lattice $T{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ is defined as $T{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}=\left\{T{\ensuremath{\boldsymbol{{I}}}},\quad {\ensuremath{\boldsymbol{{I}}}}\in{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}\right\}$ and ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ is the lattice of integers in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$. 
Then, the DDE of the PD variables  under $\mathcal{Q}$ for quadratic NUM problems is lower bounded as 
\begin{align}\label{Eq: EDE-New}
\liminf_{k\rightarrow\infty}\frac{1}{k+1}&\log{{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k+1}}\right\Vert_{{2}}}^2} \right]}}}\nonumber\\
&\geq -\frac{2}{M+N}{\ensuremath{\log{\left({{\frac{ \beta_T}{{\left({\prod_{i=1}^{M+N}{\left|{T_{ii}}\right|}}\right)}}}}\right)}}} 
\end{align}.}
\end{theorem}
\begin{IEEEproof}
\textcolor{black}{Please see Appendix \ref{App: EDE-New}.}
\end{IEEEproof}
\textcolor{black}{Theorem \ref{Theo: EDE-New} establishes a bound on the fastest possible exponential convergence speed of quantized PD algorithms in quadratic NUM problems, under any zoom-in quantization scheme. The lower bound in Theorem \ref{Theo: EDE-New} depends on the number of agents, number of constrains and $\beta_T$. The constant $\beta_T$ depends on the dynamics of unquantized PD algorithm, \emph{i.e.,} matrix $T$, and can be interpreted as the number of lattice points in ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ which lie in ${\rm B}$ after applying the linear transformation $T$ to ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$. Fig. \ref{F-lattice} shows the two dimensional lattice of integers ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^2$ and its image after applying a linear transformation. In Fig. \ref{F-lattice} $(b)$, the number of lattice points in the square is equal to $\beta_T$. Since the transformation $T$ is linear, ${\ensuremath{\boldsymbol{{0}}}}$ always lies in ${\rm B}$ which implies $\beta_T\geq 1$.}

\begin{figure}[!t]
\centering{\includegraphics[scale=0.5]{Lattice.eps}}
\caption{Two dimensional lattice of integers ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^2$ $(a)$ and the lattice $T{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^2$ (b).} \label{F-lattice}
\end{figure}

\textcolor{black}{Consider the PD algorithm in a quadratic NUM problem under the zoom-in quantization scheme $\mathcal{Q}$ with $\rho=\frac{\delta^{\max}_k}{\delta^{\min}_k}$. For the PD algorithms, Theorems \ref{Theo: DDE} and \ref{Theo: EDE-New} can be combined into
\begin{align}\label{Eq: LB-COMB}
&\liminf_{k\rightarrow\infty}\frac{1}{k+1}\log{{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k+1}}\right\Vert_{{2}}}^2} \right]}}}\geq \nonumber\\
&\frac{2}{M+N}{\left({{\left({\sum_{i=1}^M {\ensuremath{\log{\left({{1-\mu a_i}}\right)}}}}\right)}-\min{\left({{\ensuremath{\log{\left({{\beta_T}}\right)}}},R_{\mathcal{Q}}}\right)} }\right)}
\end{align}
 If the quantization intervals for each primal/dual variable is divided to $K\geq 2$ equal length intervals, data-rate under the quantization $\mathcal{Q}$ \emph{i.e.,} $R_{\mathcal{Q}}$,  will increase by ${\left({N+M}\right)}{\ensuremath{\log{\left({{K}}\right)}}}$ bits and $\rho$ does not change. Hence, according to \eqref{Eq: LB-COMB}, the lower bound in Theorem \ref{Theo: EDE-New} becomes tighter compared to that in Theorem \ref{Theo: DDE} as $R_{\mathcal{Q}}$ (or $K$) becomes large. This observation shows that the exponential convergence speed of the quantized PD algorithm in quadratic NUM problems cannot be made arbitrarily fast by increasing $R_{\mathcal{Q}}$. }

\textcolor{black}{An upper bound on $\beta_T$ can be obtained by finding the number of lattice points of ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ which lie in the smallest hypercube containing the image of ${\rm B}$ under $T^{-1}$. Let $T^{-1}{\left({\rm B}\right)}$ be the image of the hypercube ${\rm B}$ under linear transformation $T^{-1}$. Let ${\rm B}^\star_{T^{-1}}$ be the smallest hypercube containing $T^{-1}{\left({\rm B}\right)}$. Then, $\beta_T$ is upper bounded by $\prod_{i}{\left({\left\lfloor l^\star_i\right\rfloor+1}\right)}$ where $l^\star_i$ is the $i$th side length of ${\rm B}^\star_{T^{-1}}$. In our numerical results, this upper bound on $\beta_T$ is used to compute the lower bound in Theorem \ref{Theo: EDE-New}.}
\subsection{MSD of the PD algorithm in non-asymptotic regime}
In this subsection, we establish universal lower bounds on the mean square distance (MSD) of primal-dual (PD), primal and dual variables from their corresponding optimal solutions at any finite time instance $k$. Unlike Theorems \ref{Theo: DDE}, \ref{Theo: DDE-P} and \ref{Theo: DDE-D}, the following results are not limited to optimum achieving (OA) quantization schemes. Thus, they give rise to universal lower bounds on the MSD of PD, primal and dual variables from their corresponding optimal solutions, under arbitrary quantization schemes. Our results in this subsection indicate that the distance between the optimization variables and the  optimal solution cannot be made arbitrarily close to zero at a given time instance $k$.  
We start by presenting our non-asymptotic lower bound on the MSD of the PD variables.
\begin{corollary}\label{Coro: FTE}
Consider the PD algorithm under the quantization scheme $\mathcal{Q}$. Then, the MSD of the PD variables from the optimal solution at time $k$ can be lower bounded as
\begin{align}\label{Eq: LB-G}
&\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq {\ensuremath{\log{\left({{\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{M+N}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}}}\right)}}}+ \nonumber\\
&\hspace{1cm}\frac{2}{N+M}\left(\sum_{i=1}^M\sum_{n=0}^{k-1}{{\ensuremath{\log{\left({{1+\mu_nU^{\min}_i}}\right)}}}}\right.\nonumber\\
&\hspace{1cm}+\left.\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]-\sum_{t=0}^{k-1}{\left({{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)}+\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}}\right)}\right),
\end{align}
\end{corollary}
\begin{IEEEproof}
The proof directly follows from inequalities \eqref{Eq: MS-LB} and \eqref{Eq: Aux-1} in Appendix \ref{App: DDE}.
\end{IEEEproof}
Corollary \ref{Coro: FTE} provides a universal lower bound on the MSD of PD variables under quantized communications between agents and NNs.  This result indicates that at a given time the PD variables cannot be arbitrarily close to the optimal solution (in the mean square sense), and imposes a lower bound on the MSD of PD variables from the optimal solution at a given time. According to Corollary \ref{Coro: FTE}, the MSD of PD variables from the optimal solution at time $k$ is bounded from below by the behavior of second derivative of objective functions of agents along the trajectories of primal variables up to time $k-1$, the total number of bits exchanged between agents and NNs up to time $k-1$, the differential entropy of distribution of initial PD variables, \emph{i.e.,} $\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]$, and the number of constraints and agents. The impacts of objective functions of agents and the data rate between agents and NNs on the lower bound in \eqref{Eq: LB-G} are similar to those in Theorem \ref{Theo: DDE}, and they are not discussed to avoid repetition.

Note that the entropy power of ${\ensuremath{\boldsymbol{{y}}}}_{0}$, \emph{i.e.,} $\frac{1}{2\pi{\ensuremath{{\rm e}^{{}}}}}{\ensuremath{{\rm e}^{{\frac{2}{N+M}\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]}}}}$ is a measure of effective support volume of the random vector ${\ensuremath{\boldsymbol{{y}}}}_{0}$. Thus, as $\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]$ becomes large, the size of effective support set of ${\ensuremath{\boldsymbol{{y}}}}_{0}$ increases, \emph{i.e.,} ${\ensuremath{\boldsymbol{{y}}}}_0$ will be distributed on a larger region of ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$. As a result, the MSD of the PD variables from the optimal solution increases since ${\ensuremath{\boldsymbol{{y}}}}_{0}$ effectively takes value from a larger set,  a behavior predicted by Corollary \ref{Coro: FTE}.

The next corollary establishes a lower bound on the MSD of primal/dual variables:
\begin{corollary}\label{Coro: FTE-P}
Let ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_k}\right\Vert_{{2}}}^2} \right]}}$ and ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_k}\right\Vert_{{2}}}^2} \right]}}$ be the MSD of the primal variables and dual variables, respectively, at time $k$ from the optimal solution. Then, we have
\begin{align}
&\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq {\ensuremath{\log{\left({{\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{M}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}}}\right)}}}+ \nonumber\\
&\hspace{0cm}\frac{2}{M}\left(\sum_{i=1}^M\sum_{n=0}^{k-1}{{\ensuremath{\log{\left({{1+\mu_nU_i^{\min}}}\right)}}}}+\mathsf{h}\left[{\ensuremath{\boldsymbol{{x}}}}_{0}\right]-\sum_{t=0}^{k-1}\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}\right),\nonumber\\
&\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq {\ensuremath{\log{\left({{\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{N}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}}}\right)}}}+ \nonumber\\
&\hspace{2cm}+\frac{2}{N}{\left({\mathsf{h}\left[{\ensuremath{\boldsymbol{{\lambda}}}}_{0}\right]-\sum_{t=0}^{k-1}\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)},\nonumber
\end{align}
\end{corollary}
\begin{IEEEproof}
The proof is similar to that of Corollary \ref{Coro: FTE} and is omitted to avoid repetition. 
\end{IEEEproof}

\section{An Optimum Achieving Quantization Scheme}\label{Sec: AOAQ}

\textcolor{black}{In this section, we propose a zoom-in uniform optimum achieving (OA) quantization scheme for the PD algorithm. We refer to this quantization scheme as $\mathcal{Q}_{\rm a}$. We also prove that PD algorithm under the quantization scheme $\mathcal{Q}_{\rm a}$  converges to the optimal solution of the optimization problem \eqref{Prob}. To this end, we assume that the unquantized PD algorithm forms a contraction map with contraction constant $\alpha\in\left[0,\left.1\right)\right.$. We assume that $\alpha$ is known by all agents and NNs.  Let ${\rm Q}_{{\rm a},k}{\left({\cdot}\right)}$ and $\delta_k$ denote the quantizer and the quantization step employed by agents and NNs at time $k$. $\delta_k$ is set to $\alpha^{k+1}$. }

\textcolor{black}{At time $k=0$, agent $i$ generates $x^i_0$ according to the uniform distribution on the interval $\left(-L\alpha,L\alpha\right)$ where $L$ is a positive integer. Similarly, NN $j$ generates $\lambda^j_0$ ($1\leq j\leq N$) using the uniform distribution on $\left(-L\alpha,L\alpha\right)$. Next, agents and NNs quantize the initial primal and dual variables, respectively, using the midpoint uniform quantizer on $\left(-L\alpha,L\alpha\right)$ with the quantization step $\delta_0=\alpha$. Thus, the quantizer employed by agents and NNs at time $k=0$, is given by ${\rm Q}_{{\rm a},0}{\left({z}\right)}=\left\lfloor \frac{z}{\alpha}\right\rfloor\alpha+\frac{\alpha}{2}$ for $z\in\left(-L\alpha,L\alpha\right)$ where $\left\lfloor \cdot\right\rfloor$ is the floor function. Each agent/NN only needs $\left\lceil \log_2{\left({2L}\right)}\right\rceil$ bits to communicate its initial primal/dual variable, respectively, where $\left\lceil \cdot\right\rceil$ is the ceiling function.}

\textcolor{black}{Let $I^{x^i}_{k+1}$ be the interval centered at $C^{x^i}_{k+1}={\rm Q}_{{\rm a},k}{\left({x^i_{k}}\right)}+\left\lfloor \frac{x^i_{k+1}-x^i_{k}}{\delta_k}\right\rfloor\delta_k$ with length $2\left\lceil \frac{2}{\alpha}\right\rceil\delta_{k+1}$. It can be shown that $x^i_{k+1}$ belongs to this interval (see the proof of Theorem \ref{Theo: Conv} for more details).  At time $k+1$, a uniform midpoint quantizer is employed to quantize $x^{i}_{k+1}$. To this end, first, agent $i$ transmits $\left\lfloor \frac{x^i_{k+1}-x^i_{k}}{\delta_k}\right\rfloor$ to NNs. Since $\delta_k$ and ${\rm Q}_{{\rm a},k}{\left({x^i_{k}}\right)}$ are known by NNs, each NN can compute $C^{x^i}_{k+1}$ using $\left\lfloor \frac{x^i_{k+1}-x^i_{k}}{\delta_k}\right\rfloor$. The knowledge of $C^{x^i}_{k+1}$ allows each NN to update its decoder at time $k+1$. Note that $\left\lfloor \frac{x^i_{k+1}-x^i_{k}}{\delta_k}\right\rfloor$ is an integer which can be transmitted using finite number of bits. Next, agent $i$ computes ${\rm \hat{Q}}_{k+1}{\left({\frac{x^i_{k+1}-C^{x^i}_{k+1}}{\delta_{k+1}}}\right)}$ where $x^i_{k+1}\in I^{x^i}_{k+1}$ and ${\rm \hat{Q}}_{k+1}{\left({\cdot}\right)}$ is given by 
\begin{eqnarray}\label{Eq: UQant}
\hspace{-2mm}{\rm \hat{Q}}_{k+1}{\left({z}\right)}=\!\!\left\{\!\!\!\!
\begin{array}{cc}
\vspace{2mm}\left\lceil \frac{2}{\alpha}\right\rceil-1 &{\left({\left\lceil \frac{2}{\alpha}\right\rceil-1}\right)}\leq z\leq \left\lceil \frac{2}{\alpha}\right\rceil\\
\vspace{2mm}\left\lfloor \frac{z}{\delta_{k+1}}\right\rfloor& -\left\lceil \frac{2}{\alpha}\right\rceil\leq z\leq {\left({\left\lceil \frac{2}{\alpha}\right\rceil-1}\right)}\\
\end{array}
\right.
\end{eqnarray}
Finally, agent $i$ transmits ${\rm \hat{Q}}_{k+1}{\left({\frac{x^i_{k+1}-C^{x^i}_{k+1}}{\delta_{k+1}}}\right)}$ to NNs using $\left\lceil \log_2{\left({2\left\lceil \frac{2}{\alpha}\right\rceil}\right)}\right\rceil$ bits. Then, each NN constructs the quantized version of $x^i_{k+1}$ using ${\rm Q}_{{\rm a},k+1}{\left({x^i_{k+1}}\right)}=C^{x^i}_{k+1}+{\rm \hat{Q}}_{k+1}{\left({x^i_{k+1}-C^{x^i}_{k+1}}\right)}\delta_{k+1}+\frac{\delta_{k+1}}{2}$.}

\textcolor{black}{Consider the interval $I^{\lambda^j}_{k+1}$ which is centered at $C^{\lambda^j}_{k+1}={\rm Q}_{{\rm a},k}{\left({\lambda^j_{k}}\right)}+\left\lfloor \frac{\lambda^j_{k+1}-\lambda^j_{k}}{\delta_k}\right\rfloor\delta_k$ with the length  $2\left\lceil \frac{2}{\alpha}\right\rceil\delta_{k+1}$. It can be shown that the $\lambda^j_{k+1}$ belongs to $I^{\lambda^j}_{k+1}$ (see the proof of Theorem \ref{Theo: Conv} for more details). At time $k+1$, NN $j$, first, broadcasts $\left\lfloor \frac{\lambda^j_{k+1}-\lambda^j_{k}}{\delta_k}\right\rfloor$ to agents which allows the agents to construct $C^{\lambda^j}_{k+1}$. Then, it broadcasts  ${\rm \hat{Q}}_{k+1}{\left({\frac{\lambda^j_{k+1}-C^{\lambda^j}_{k+1}}{\delta_{k+1}}}\right)}$ to all agents where ${\rm \hat{Q}}_{k+1}{\left({\cdot}\right)}$ is given by \eqref{Eq: UQant}. Finally, agents construct the quantized version of $\lambda^j_{k+1}$ as ${\rm Q}_{{\rm a},k+1}{\left({\lambda^j_{k+1}}\right)}=C^{\lambda^j}_{k+1}+{\rm \hat{Q}}_{k+1}{\left({\frac{\lambda^j_{k+1}-C^{\lambda^j}_{k+1}}{\delta_{k+1}}}\right)}\delta_{k+1}+\frac{\delta_{k+1}}{2}$.}

\textcolor{black}{The next theorem shows that the quantized PD algorithm under $\mathcal{Q}_{\rm a}$ converges to the optimal solution.
\begin{theorem}\label{Theo: Conv}
The PD algorithm under the quantization scheme $\mathcal{Q}_{\rm a}$ converges  exponentially to the optimal solution of the optimization problem \eqref{Prob}. 
\end{theorem}
\begin{IEEEproof}
Please see Appendix \ref{App:Conv}.
\end{IEEEproof}}
\section{Numerical results}\label{Sec: NR}
\textcolor{black}{This section presents our numerical results illustrating the behavior of mean square distance (MSD) of primal-dual (PD) variables from the optimal solution, with time index $k$ for a quadratic network utility maximization (NUM) problem with 10 agents and 5 constraints under the quantization scheme $\mathcal{Q}_{a}$. 
In our numerical results, the step-size of the PD algorithm, $\mu_k$, is set to $0.019$ for all $k$, $\alpha=0.9495$, $L=5$. The initial PD variables are independently drawn according to the uniform distribution on $\left(-L\alpha,L\alpha\right)$. For $k\geq 1$,  3 bits is used to quantize each primal/dual variable.}

\begin{figure}[t]
\centering
\subfigure[]
{\includegraphics[scale=0.6]{Primal_time.eps}
\label{F1}}
\caption{\small Time evolution of the primal variable $x^1$ under the quantization scheme $\mathcal{Q}_{\rm a}$ and unquantized PD update rule. The initial PD variables are the same in both graphs.}
\label{F1-2}
\end{figure}
\textcolor{black}{Fig. \ref{F1-2} shows the time evolution of the primal variable $x^1$ under the quantization scheme $\mathcal{Q}_{\rm a}$ and unquantized PD update rule. The initial PD variables are the same for both graphs. According to Fig. \ref{F1-2}, the trajectories of $x^1$, under both quantization scheme $\mathcal{Q}_{\rm a}$ and the unquantized PD update rule, converge to the optimal value of ${x^1}$ as the time index $k$ becomes large. The same behavior continues to hold for other primal/dual variables. }

\begin{figure}[!t]
\centering{\includegraphics[scale=0.6]{EXP_total.eps}}
\caption{$\log$-MSD divided by $k$ for primal-dual variables versus time index ($k$) under the quantization scheme $\mathcal{Q}_{\rm a}$.} \label{F3}
\end{figure}
\textcolor{black}{Fig. \ref{F3} depicts $\log$-MSD divided by $k$ for the PD variables , \emph{i.e.,} $\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}$, as a function of time index $k$. The lower bounds on the distance decay exponent (DDE) of PD variables, predicted by Theorem \ref{Theo: DDE} and Theorem \ref{Theo: EDE-New},  are equal to $ -6.24$ and $-4.64$, respectively. As Fig. \ref{F3} shows, $\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}$ stays above the predicted values by Theorems \ref{Theo: DDE} and \ref{Theo: EDE-New} as $k$ becomes large.}

\section{Conclusions}\label{Sec: Conc}
In this paper, we have studied the convergence behavior of the  quantized primal-dual (PD) algorithm in solving network utility maximization problems. First, using the information-theoretic notion of entropy power, we established universal bounds on the fastest speed of exponential mean square convergence of PD, primal and dual variables to the optimal solution under optimum achieving quantization schemes. Here, our results provide universal trade-offs between the speed of convergence of the quantized PD algorithm, data rate under the quantization, objective functions of agents,  the number of agents and the number of constraints.   Next, we established universal lower bounds on the mean square distance of PD, primal and dual variables from the optimal solution of the NUM problem for any finite time index. 
\appendices

\section{Proof of Theorem \ref{Theo: DDE}}\label{App: DDE}

This appendix presents the main steps of the proof of Theorem \ref{Theo: DDE}. To this end, first, the notion of conditional differential entropy power of a random vector is defined. Then, we use the notion of entropy power to establish a universal lower bound on the DDE of the PD variables. The differential entropy power of the random vector ${\ensuremath{\boldsymbol{{z}}}}\in {\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M} $ conditioned on the event $A=a$, denoted by ${\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}$, is defined as
\begin{eqnarray}
{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}=\frac{1}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{M+N}{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}}}}},\nonumber
\end{eqnarray}
where ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}$ is the conditional differential entropy of ${\ensuremath{\boldsymbol{{z}}}}$ given  $A=a$ defined as
\begin{eqnarray}
{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}=-\int{\ensuremath{\log{\left({{p{\left({\left.{\ensuremath{\boldsymbol{{z}}}}\right|A=a}\right)}}}\right)}}}p{\left({\left.{\ensuremath{\boldsymbol{{z}}}}\right|A=a}\right)}d{\ensuremath{\boldsymbol{{z}}}},\nonumber
\end{eqnarray}
where $p{\left({\left.{\ensuremath{\boldsymbol{{z}}}}\right|A=a}\right)}$ is the conditional distribution of ${\ensuremath{\boldsymbol{{z}}}}$ given $A=a$.
Using the entropy maximizing property of Gaussian distributions, the conditional entropy power of ${\ensuremath{\boldsymbol{{z}}}}$ given $A=a$ can be upper bounded \cite{NE04} as
\begin{eqnarray}\label{Eq: CEP-UB}
{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}\leq {\ensuremath{{\rm e}^{{1/{\left({M+N}\right)}-1}}}}{\ensuremath{\mathsf{E}\left[\left.{{\left\Vert{{\ensuremath{\boldsymbol{{z}}}}}\right\Vert_{{2}}}^2}\right|{A=a}\right]}},
\end{eqnarray}
where ${\ensuremath{\mathsf{E}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}$ is conditional expectation of ${\ensuremath{\boldsymbol{{z}}}}$ given $A=a$. Let $\mathsf{E}_A\left[{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}\right]$ denote the average conditional entropy power of ${\ensuremath{\boldsymbol{{z}}}}$ given $A=a$. Using \eqref{Eq: CEP-UB}, $\mathsf{E}_A\left[{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A=a}\right]}}\right]$ can be upper bounded as
\begin{eqnarray}\label{Eq: ACEP-UB}
{\mathsf E}_{A}\left[{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{z}}}}}\right|{A}\right]}}\right]\leq {\ensuremath{{\rm e}^{{1/{\left({M+N}\right)}-1}}}}{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{z}}}}}\right\Vert_{{2}}}^2} \right]}}.
\end{eqnarray}

Next, the inequality \eqref{Eq: ACEP-UB} is used to establish the universal lower bound on the DDE of the PD variables under OA quantization schemes. \textcolor{black}{To this end, let $\mathcal{D}_{k-1}=\left\{\hat{Q}_n={\ensuremath{\boldsymbol{{\hat{q}}}}}_n\right\}_{n=0}^{k-1}$ where $\hat{Q}_n=\left[\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{M,n},\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{N,n}\right]$ and ${\ensuremath{\boldsymbol{{\hat{q}}}}}_n$ is a possible realization of $\hat{Q}_n$.} Using \eqref{Eq: ACEP-UB}, ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}$ can be lower bounded as
\begin{eqnarray}\label{Eq: MS-LB}
{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}&\geq &{\ensuremath{{\rm e}^{{1-\frac{1}{M+N}}}}}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{N}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}\nonumber\\
&\stackrel{{\left({*}\right)}}{\geq}&\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{M+N}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{M+N}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}}}}},
\end{eqnarray}
where ${\left({*}\right)}$ is obtained using the Jensen inequality. The term ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}}$ on the right hand side of \eqref{Eq: MS-LB} can be expanded as
\begin{align}\label{Eq: TI}
{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}}&={\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_k-{\ensuremath{\boldsymbol{{y}}}}^\star}\right|{\mathcal{D}_{k-1}}\right]}}\nonumber\\
&\stackrel{{\left({*}\right)}}{=}{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}},
\end{align}
where ${\left({*}\right)}$ follows from the translation invariance property of differential entropy \textcolor{black}{as ${\ensuremath{\boldsymbol{{y}}}}^\star$ is a constant vector (see \cite{Cover} Theorem 8.6.3 page 253)}. 

The next lemma establishes a useful expression between ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_n}\right|{\mathcal{D}_{k-1}}\right]}}$ and ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}}$ for $n\leq k$, which is used to further expand ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_k}\right|{\mathcal{D}_{k-1}}\right]}}$.

\begin{lemma}\label{Lem: Entropy-Equality}
For $n\leq k$, ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_n}\right|{\mathcal{D}_{k-1}}\right]}}$ can be expanded as
\begin{align}
&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{n}}\right|{\mathcal{D}_{k-1}}\right]}}={\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}}+\nonumber\\
&\mathsf{E}\left[\sum_{j=1}^M\log\left(1+\mu_{n-1}\left.\frac{d ^2}{d {x^j}^2}U_{j}\left({\ensuremath{\boldsymbol{{x}}}}^{j}_{n-1}\!\right)\right)\right|\mathcal{D}_{k-1}\right]
\end{align}
\end{lemma}
\begin{IEEEproof}
Let $\tilde{x}^i_n={x}^{i}_{n}+\mu_{n} {\left({\frac{d}{d {x}^{i}}U_i{\left({{x}^{i}_{n}}\right)}}\right)}$ and ${\ensuremath{\boldsymbol{{\tilde{x}}}}}_n=\left[\tilde{x}^i_1,\cdots,\tilde{x}^i_M\right]^\top$. Let ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_n$ be the vector concatenation of ${\ensuremath{\boldsymbol{{\tilde{x}}}}}_n$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_n$. \textcolor{black}{This lemma is proved in two steps. First, it is shown that the conditional differential entropy of ${\ensuremath{\boldsymbol{{y}}}}_n$ given $\mathcal{D}_k$ is equal to that of ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}$ given $\mathcal{D}_k$ (see \eqref{Eq: ENT-Eq-1}). Next, a relation between the conditional differential entropy of ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}$ given $\mathcal{D}_k$ and that of ${\ensuremath{\boldsymbol{{y}}}}_{n-1}$ given $\mathcal{D}_k$ is established. Note that, ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_n}\right|{\mathcal{D}_{k-1}}\right]}}$ can be written as
\begin{eqnarray}\label{Eq: ENT-Eq-1}
{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_n}\right|{\mathcal{D}_{k-1}}\right]}}&=&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_n,{\ensuremath{\boldsymbol{{\lambda}}}}_n}\right|{\mathcal{D}_{k-1}}\right]}}\nonumber\\
&\stackrel{*}{=}&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\tilde{x}}}}}_{n-1},{\ensuremath{\boldsymbol{{\lambda}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}}\nonumber\\
&=&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}}
\end{eqnarray}
where ${\left({*}\right)}$ follows from the translation invariance property of the differential entropy and the fact that  $Q_{k-1}$ is fixed given $\mathcal{D}_{k-1}=\left\{\hat{Q}_n={\ensuremath{\boldsymbol{{\hat{q}}}}}_n\right\}_{n=0}^{k-1}$.} Next, we derive an expression for the probability density function (PDF) of ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_n$ in terms of the PDF of ${\ensuremath{\boldsymbol{{y}}}}_{n}$. Let $p_{{\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n}}{\left({{\ensuremath{\boldsymbol{{y}}}}\left|\mathcal{D}_{k-1}\right.}\right)}$ and $p_{{\ensuremath{\boldsymbol{{y}}}}_{n}}{\left({{\ensuremath{\boldsymbol{{y}}}}\left|\mathcal{D}_{k-1}\right.}\right)}$ to denote the PDFs of ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_n$ and ${\ensuremath{\boldsymbol{{y}}}}_{n}$, respectively, conditioned on $\mathcal{D}_{k-1}$.  Let ${\ensuremath{\boldsymbol{{F}}}}{\left({\cdot}\right)}$ represent the mapping between ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_n$ and ${\ensuremath{\boldsymbol{{y}}}}_n$, \emph{i.e.,} ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_n={\ensuremath{\boldsymbol{{F}}}}{\left({{\ensuremath{\boldsymbol{{y}}}}_n}\right)}$.  Note that $0<1+\mu_{n}\frac{d^2}{d {x^i}^2}U_i{\left({x^i}\right)}<1$ since  $0<\mu_{n}< \min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}$ which implies that the mapping ${\ensuremath{\boldsymbol{{F}}}}{\left({\cdot}\right)}$ is invertible. 
\textcolor{black}{Thus, the change-of-variables formula for invertible diffeomorphisms of random vectors (see {\em e.g.}, (4.63) in \cite{leon-garcia}) can be applied to write}

	\begin{eqnarray}\label{Eq: PDF-R}
	p_{{\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}}\!\!{\left({{\ensuremath{\boldsymbol{{y}}}}\left|\mathcal{D}_{k-1}\right.}\right)}=\frac{1}{\det J_{{\ensuremath{\boldsymbol{{F}}}}}\!\!\left[{\ensuremath{\boldsymbol{{F}}}}^{-1}\!\!{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\right]}p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}\!\!{\left({{\ensuremath{\boldsymbol{{F}}}}^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\left|\mathcal{D}_{k-1}\right.}\right)},\nonumber\\
	
	\end{eqnarray}
	where $J_{{\ensuremath{\boldsymbol{{F}}}}}\left[{\ensuremath{\boldsymbol{{x}}}}\right]$ is Jacobian of  ${\ensuremath{\boldsymbol{{F}}}}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$ evaluated at ${\ensuremath{\boldsymbol{{x}}}}$. Using \eqref{Eq: PDF-R}, the conditional entropy of ${\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}$ given $\mathcal{D}_{k-1}$ can be written as
\begin{align}
&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\tilde{y}}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}}\nonumber\\
&=\int{\ensuremath{\log{\left({{\det J_{{\ensuremath{\boldsymbol{{F}}}}}\left[{\ensuremath{\boldsymbol{{F}}}}^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\right]}}\right)}}}\frac{1}{\det J_{{\ensuremath{\boldsymbol{{F}}}}}\left[{\ensuremath{\boldsymbol{{F}}}}^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\right]}\nonumber\\
&\hspace{5cm}p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({F^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\left|\mathcal{D}_{k-1}\right.}\right)}d{\ensuremath{\boldsymbol{{y}}}}\nonumber\\
&-\int{\ensuremath{\log{\left({{p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({F^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\left|\mathcal{D}_{k-1}\right.}\right)}}}\right)}}}\frac{1}{\det J_{{\ensuremath{\boldsymbol{{F}}}}}\left[{\ensuremath{\boldsymbol{{F}}}}^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\right]}\nonumber\\
&\hspace{5cm}p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({{\ensuremath{\boldsymbol{{F}}}}^{-1}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\left|\mathcal{D}_{k-1}\right.}\right)}d{\ensuremath{\boldsymbol{{y}}}},\nonumber\\
&\stackrel{{\left({*}\right)}}{=}\int{\ensuremath{\log{\left({{\det J_{{\ensuremath{\boldsymbol{{F}}}}}\left[{\ensuremath{\boldsymbol{{z}}}}\right]}}\right)}}}p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({{\ensuremath{\boldsymbol{{z}}}}\left|\mathcal{D}_{k-1}\right.}\right)}d{\ensuremath{\boldsymbol{{z}}}}\nonumber\\
&\hspace{1cm}-\int{\ensuremath{\log{\left({{p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({{\ensuremath{\boldsymbol{{z}}}}\left|\mathcal{D}_{k-1}\right.}\right)}}}\right)}}}p_{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}{\left({{\ensuremath{\boldsymbol{{z}}}}\left|\mathcal{D}_{k-1}\right.}\right)}d{\ensuremath{\boldsymbol{{z}}}},\nonumber\\
&=\!\!\sum_{j=1}^M\mathsf{E}\!\left[\left.\!\log\!\!\left(\!1\!+\!\mu_{n-1}\frac{d ^2}{d {x^j}^2}U_{j}\!\!{\left({x^j_{n-1}}\right)}\right)\right|{\mathcal{D}_{k-1}}\right]\nonumber\\
&\hspace{1cm}+{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{n-1}}\right|{\mathcal{D}_{k-1}}\right]}},
\end{align}	
where ${\left({*}\right)}$ follows from the change of variable ${\ensuremath{\boldsymbol{{z}}}}=F^{-1}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$.
\end{IEEEproof}
Using Lemma \ref{Lem: Entropy-Equality}, ${\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}$ can be further expanded as
\begin{align}\label{Eq: Aux-AE-LB}
&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}={\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{0}}\right|{\mathcal{D}_{k-1}}\right]}}+\nonumber\\
&\sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[\left.{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}}\right|{\mathcal{D}_{k-1}}\right]}}
\end{align}
Using \eqref{Eq: Aux-AE-LB}, ${\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}$ can be written as
\begin{align}\label{Eq: Aux-0}
&{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}\nonumber\\
&\hspace{0cm}=\sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}+{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{0}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}},
\end{align}
The following lemma, adapted from  \cite{NE04},
establishes a lower bound on ${\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}$:
\begin{lemma}\label{Lem: NE}
The average conditional entropy of ${\ensuremath{\boldsymbol{{y}}}}_0$ given $\mathcal{D}_{k-1}$, \emph{i.e.,} ${\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{0}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}$, can be lower bounded as
\begin{eqnarray}
{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{0}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}\!\geq \!\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]\!-\!\sum_{t=0}^{k-1}\!{\left({\!\!\!{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)}\!\!+\!\!\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}\!\!}\right)}.\nonumber
\end{eqnarray}
\end{lemma}
\begin{IEEEproof}
 Follows directly from the first inequality in appendix C in  \cite{NE04}; alternatively, it can be derived from (8.48) and (8.89) in \cite{Cover}.
\end{IEEEproof}

Applying Lemma \ref{Lem: NE} to \eqref{Eq: Aux-0}, we have
\begin{align}\label{Eq: Aux-1}
&{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{y}}}}_{k}}\right|{\mathcal{D}_{k-1}}\right]}}} \right]}}\nonumber\\
&\hspace{1cm}\geq \sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}\nonumber\\
&\hspace{1cm}+\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]-\sum_{t=0}^{k-1}{\left({{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)}+\sum_{j=1}^N\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}}\right)},
\end{align}
Since ${\ensuremath{\boldsymbol{{x}}}}_0$ and ${\ensuremath{\boldsymbol{{\lambda}}}}_0$ are independent, the differential entropy of ${\ensuremath{\boldsymbol{{y}}}}_0$ can be written as $\mathsf{h}\left[{\ensuremath{\boldsymbol{{y}}}}_{0}\right]=\mathsf{h}\left[{\ensuremath{\boldsymbol{{x}}}}_{0}\right]+\mathsf{h}\left[{\ensuremath{\boldsymbol{{\lambda}}}}_{0}\right]$ which implies that ${\ensuremath{\boldsymbol{{y}}}}_0$ has finite differential entropy. Using \eqref{Eq: MS-LB}, \eqref{Eq: TI}, \eqref{Eq: Aux-1} and the fact that ${\ensuremath{\boldsymbol{{y}}}}_{0}$ has a finite entropy,  the DDE can be lower bounded as
\begin{align}\label{Eq: Aux-2}
&\liminf_{k\longrightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq \frac{2}{M+N} {\left({\liminf_{k\longrightarrow \infty}\sum_{j=1}^M\frac{1}{k}\sum_{n=0}^{k-1}\right.\nonumber\\
&\left.{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}-R_{\mathcal{Q}}}\right)}.
\end{align}
In the next lemma, we study the asymptotic behavior of the first term in the right hand side of equation \eqref{Eq: Aux-2}.
\begin{lemma}\label{Lem: Limit}
Consider the primal-dual update rule \eqref{EQ: QU} under an OA quantization scheme. Then, we have
\begin{align}
\lim_{k\longrightarrow \infty}\sum_{j=1}^M&\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}=\nonumber\\
&\sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu^\star\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x^{j}}^\star}\right)}}}\right)}}}.\nonumber
\end{align}
\end{lemma}
\begin{IEEEproof}
To prove this lemma, first we define the events $E$ and $E_n$ as
\begin{eqnarray}
E&=&\left\{{\left\Vert{{\ensuremath{\boldsymbol{{x}}}}-{\ensuremath{\boldsymbol{{x}}}}^\star}\right\Vert_{{2}}}\leq \delta, {\left|{\mu-\mu^\star}\right|}\leq \delta\right\},\nonumber\\
E_n&=&\left\{{\left\Vert{{\ensuremath{\boldsymbol{{x}}}}_{n}-{\ensuremath{\boldsymbol{{x}}}}^\star}\right\Vert_{{2}}}\leq \delta, {\left|{\mu_n-\mu^\star}\right|}\leq \delta\right\}.\nonumber
\end{eqnarray}
respectively, where $\delta>0$. Then, we have
\begin{align}\label{Eq: Expansion}
&\frac{1}{k}\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}\nonumber\\
&=\frac{1}{k}\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}{\left({{\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}+{\ensuremath{\mathsf{1}_{\left\{{E^{c}_n}\right\}}}}}\right)}} \right]}},\nonumber\\
&\geq \inf_{\left\{{\ensuremath{\boldsymbol{{x}}}},\mu\right\}\in E}\sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu \frac{d ^2}{d {x^j}^2}U_j{\left({{x}^{j}}\right)}}}\right)}}}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}} \right]}}\nonumber\\
&\hspace{2cm}+\sum_{j=1}^M{\ensuremath{\log{\left({{1+\inf_n\mu_nU_j^{\rm min}}}\right)}}}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E^{c}_n}\right\}}}}} \right]}},
\end{align}
where $E^{c}_n$ is the complement of the event $E_n$. Recall that the quantization scheme $\mathcal{Q}$ is an OA quantization scheme. Therefore, we have $\lim_{k\longrightarrow \infty} {{\ensuremath{\boldsymbol{{x}}}}}_{k}={\ensuremath{\boldsymbol{{x}}}}^\star$ for any initial vector ${\ensuremath{\boldsymbol{{x}}}}_0$ in the support set of $p_{{\ensuremath{\boldsymbol{{x}}}}_0}{\left({{\ensuremath{\boldsymbol{{x}}}}}\right)}$ which implies ${\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}\longrightarrow 1$ almost surely and $\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}\longrightarrow 1$ almost surely. Applying Fatou's Lemma to \eqref{Eq: Expansion}, we have $ \liminf_{k\longrightarrow\infty}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}} \right]}}\geq 1$. Using Lebesgue dominated convergence Theorem and the fact that $\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E^{c}_n}\right\}}}}\longrightarrow 0$ almost surely, we have $\lim_{k\longrightarrow \infty}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E^{c}_n}\right\}}}}} \right]}}=0$. Hence,
\begin{align}
&\hspace{-1cm}\liminf_{k\longrightarrow \infty}\frac{1}{k}\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}
\geq\nonumber\\
& \hspace{1cm}\inf_{\left\{{\ensuremath{\boldsymbol{{x}}}},\mu\right\}\in E}\sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu \frac{d^2}{d {x^j}^2} U_j{\left({{\ensuremath{\boldsymbol{{x}}}}^{j}}\right)}}}\right)}}}.\nonumber
\end{align}
Since, $\delta>0$ is arbitrary, we have
\begin{align}
\liminf_{k\longrightarrow \infty}\frac{1}{k}&\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}
\geq\nonumber\\
& \sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu^\star \frac{d^2}{d {x^j}^2} U_j{\left({{{x}^{j}}^\star}\right)}}}\right)}}}\nonumber
\end{align}
To prove the other direction, note that
\begin{align}
&\frac{1}{k}\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}\leq\nonumber\\
& \sup_{\left\{{\ensuremath{\boldsymbol{{x}}}},\mu\right\}\in E}\sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu \frac{d^2}{d {x^j}^2} U_j{\left({{x}^{j}}\right)}}}\right)}}}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E_n}\right\}}}}} \right]}}\nonumber\\
&+\sum_{j=1}^M{\ensuremath{\log{\left({{1+\sup_n\mu_nU^{\rm max}_j}}\right)}}}{\ensuremath{\mathsf{E}\left[{\frac{1}{k}\sum_{n=0}^{k-1}{\ensuremath{\mathsf{1}_{\left\{{E^{c}_n}\right\}}}}} \right]}}.\nonumber
\end{align}
Following similar steps as before, it can be easily shown that
\begin{align}
\limsup_{k\longrightarrow \infty}&\frac{1}{k}\sum_{n=0}^{k-1}\sum_{j=1}^M{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({{x}^{j}_{n}}\right)}}}\right)}}}} \right]}}
\leq\nonumber\\
& \sum_{j=1}^M{\ensuremath{\log{\left({{1+\mu^\star\frac{d^2}{d {x^j}^2} U_j{\left({{{x}^{j}}^\star}\right)}}}\right)}}},\nonumber
\end{align}
which completes the proof.
\end{IEEEproof}
Applying Lemma \ref{Lem: Limit} to \eqref{Eq: Aux-2}, we have
\begin{align}
&\liminf_{k\rightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{2}}}^2} \right]}}\nonumber\\
&\hspace{1cm}\geq\frac{2}{N+M}{\left({\sum_{i=1}^m\log{\left({1+\mu^\star \frac{d^2}{d {x^i}^2}U_i{\left({{x^i}^\star}\right)}}\right)}-R_{\mathcal{Q}}}\right)}.
\end{align}
which completes the proof.
\section{Proof of Theorem \ref{Theo: DDE-P}}\label{App: DDE-P}
The proof essentially follows from similar steps as the proof of Theorem \ref{Theo: DDE}. Here, we state the main steps of the proof for the sake of completeness. Let $\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}=\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{n}={\ensuremath{\boldsymbol{{\hat{q}}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_n\right\}_{n=0}^{k-1}$ where  $\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_n=\left[\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{N,n}\right]$ and  ${\ensuremath{\boldsymbol{{\hat{q}}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_n$ is a realization of $\hat{Q}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_n$. Then using \eqref{Eq: ACEP-UB}, ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}}\right\Vert_{{2}}}^2} \right]}}$ can be lower bounded as
\begin{eqnarray}\label{Eq: MDPLB}
{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}}\right\Vert_{{2}}}^2} \right]}}&\geq &\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{M}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{M}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{x}}}}}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}}}}}}\nonumber\\
&=&\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{M}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{M}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_k}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}}}}}},
\end{eqnarray}
where the equality follows from translation invariance of the differential entropy. Using similar technique as the proof of Lemma \ref{Lem: Entropy-Equality} in Appendix \ref{App: DDE}, one can show
\begin{align}
&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{n}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}={\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{n-1}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}+\nonumber\\
&\mathsf{E}\left[\sum_{j=1}^M\log\left(1+\mu_{n-1}\left.\frac{d ^2}{d {x^j}^2}U_{j}\left({\ensuremath{\boldsymbol{{x}}}}^{j}_{n-1}\!\right)\right)\right|\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}\right]
\end{align}
for $n\leq k$. Thus, we have
\begin{align}
&{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{k}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}={\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{0}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}+\nonumber\\
&\sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[\left.{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}
\end{align}
and
\begin{align}
&{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{k}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}}\nonumber\\
&\hspace{1cm}=\sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}\nonumber\\
&\hspace{2cm}+{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{0}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}},
\end{align}
Using Lemma \ref{Lem: NE} from Appendix \ref{App: DDE}, ${\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{k}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}}$ can be lower bounded as
\begin{align}\label{Eq: Aux-P}
&{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{x}}}}_{k}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right]}}} \right]}}\nonumber\\
&\hspace{1cm}\stackrel{{\left({*}\right)}}{\geq} \sum_{j=1}^M\sum_{n=0}^{k-1}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}\nonumber\\
&\hspace{2cm}+\mathsf{h}\left[{\ensuremath{\boldsymbol{{x}}}}_{0}\right]-\sum_{t=0}^{k-1}\sum_{j=1}^N{\left({\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{j,t}}\right|}}\right)},
\end{align}
Thus, we have
\begin{align}
&\liminf_{k\longrightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq \frac{2}{M} {\left({\liminf_{k\longrightarrow \infty}\sum_{j=1}^M\frac{1}{k}\sum_{n=0}^{k-1}\right.\nonumber\\
&\left.{\ensuremath{\mathsf{E}\left[{{\ensuremath{\log{\left({{1+\mu_n\frac{d ^2}{d {x^j}^2}U_{j}{\left({x^{j}_{n}}\right)}}}\right)}}}} \right]}}-R_{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right)}.
\end{align}
The proof is complete by appealing to Lemma \ref{Lem: Limit} in Appendix \ref{App: DDE}.
\section{Proof of Theorem \ref{Theo: DDE-D}}\label{App: DDE-D}
Here, we state the main steps of the proof of Theorem \ref{Theo: DDE-D}. \textcolor{black}{Let $\mathcal{D}^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}=\left\{\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{n}={\ensuremath{\boldsymbol{{\hat{q}}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_n\right\}_{n=0}^{k-1}$ where $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_n=\left[\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{1,n},\cdots,\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_{M,n}\right]^\top$ and ${\ensuremath{\boldsymbol{{\hat{q}}}}}^{{\ensuremath{\boldsymbol{{x}}}}}_n$ is a realization of $\hat{Q}^{{\ensuremath{\boldsymbol{{x}}}}}_n$.} Then using \eqref{Eq: ACEP-UB}, ${\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right\Vert_{{2}}}^2} \right]}}$ can be lower bounded as
\begin{eqnarray}
{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right\Vert_{{2}}}^2} \right]}}&\geq &\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{N}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{N}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\epsilon}}}}_k^{{\ensuremath{\boldsymbol{{\lambda}}}}}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}}\right]}}} \right]}}}}}}\nonumber\\
&=&\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{N}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{N}{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\lambda}}}}_0}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}}\right]}}} \right]}}}}}},
\end{eqnarray}
where the equality follows from translation invariance of the differential entropy and the update equation for the dual variables is linear in ${\ensuremath{\boldsymbol{{\lambda}}}}_{n}$. Using Lemma \ref{Lem: NE} from Appendix \ref{App: DDE}, ${\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\lambda}}}}_{0}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}}\right]}}} \right]}}$ can be lower bounded as
\begin{align}
&{\ensuremath{\mathsf{E}\left[{{\ensuremath{\mathsf{h}\left[\left.{{\ensuremath{\boldsymbol{{\lambda}}}}_{0}}\right|{\mathcal{D}^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}}\right]}}} \right]}}\geq \mathsf{h}\left[{\ensuremath{\boldsymbol{{\lambda}}}}_{0}\right]-\sum_{t=0}^{k-1}{\left({\sum_{i=1}^M\log{\left|{\mathcal{A}^{{\ensuremath{\boldsymbol{{x}}}}}_{i,t}}\right|}}\right)},
\end{align}
Thus, we have
\begin{align}
&\liminf_{k\longrightarrow\infty}\frac{1}{k}\log{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}^{{\ensuremath{\boldsymbol{{\lambda}}}}}_k}\right\Vert_{{2}}}^2} \right]}}\geq -\frac{2}{N} R_{{\ensuremath{\boldsymbol{{x}}}}}.
\end{align}

\section{}\label{App: EDE-New}
\textcolor{black}{In this appendix, first, we establish a series of preliminary results in Subsection \ref{Sub-sec: Prelim}. Then, in Subsection \ref{Sub-sec: Proof}, we  use these preliminary results to prove Theorem \ref{Theo: EDE-New}. }
\subsection{Preliminary Lemmas}\label{Sub-sec: Prelim}
\textcolor{black}{ The next lemma shows that the quantization cells and their images under the quantized update rule are in the form of hypercubes. }
\begin{lemma}\label{Lem: Hyper-Cube}
\textcolor{black}{ The quantization cell $C_k{\left({\tilde{q}_k}\right)}$ is a hypercube in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$ for all $k$ and $\tilde{q}_k$. Also, {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}, \emph{i.e.,} the image of $C_k{\left({\tilde{q}_k}\right)}$ under the quantized update rule, is a hypercube.}
\end{lemma}
\begin{IEEEproof}
\textcolor{black}{ The proof of this lemma is based on mathematical induction. First, note that the quantized PD algorithm can be written as 
\begin{align}\label{EQ: QUR}
x^{i}_{k}&=T_{ii}x^{i}_{k-1}+\sum_{j=M+1}^{M+N}T_{ij}Q^{\lambda^j}_k+\mu c_i,\nonumber\\
\lambda^{j}_{k}&=\lambda^j_{k-1}+\sum_{i=1}^MT_{ji}Q^{x^i}_k-\mu b_j
\end{align}
 Since all the primal and dual variables are separately quantized, the quantization cells at time $k=0$, \emph{i.e.,} $C_0{\left({\tilde{q}_0}\right)}$s, are in the form of hypercubes. According to \eqref{EQ: QUR}, $x^{i}_{1}$ and $\lambda^{j}_{1}$ only depend on $x^{i}_{0}$ and $\lambda^{j}_{0}$, respectively, given $C_0{\left({\tilde{q}_0}\right)}$ (as $Q^{{\ensuremath{\boldsymbol{{x}}}}}_0$ and $Q^{{\ensuremath{\boldsymbol{{\lambda}}}}}_0$ are fixed given $C_0{\left({\tilde{q}_0}\right)}$). This observation implies that the image of $C_0{\left({\tilde{q}_0}\right)}$ under ${\ensuremath{\hat{T}\left({\cdot},{\tilde{q}_0}\right)}}$ is a hypercube. Now, assume that at time $k$ $C_k{\left({\tilde{q}_k}\right)}$ and ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ are hypercubes. Since the quantization scheme is zoom-in, ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ is quantized at time $k+1$. Therefore, $C_{k+1}{\left({\tilde{q}_{k+1}}\right)}$ is a hypercube as the primal/dual variables are independently quantized. Also, $Q^{{\ensuremath{\boldsymbol{{x}}}}}_{k+1}$ and $Q^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k+1}$ are fixed given $C_{k+1}{\left({\tilde{q}_{k+1}}\right)}$. Thus, the image of $C_{k+1}{\left({\tilde{q}_{k+1}}\right)}$ under ${\ensuremath{\hat{T}\left({\cdot},{\tilde{q}_{k+1}}\right)}}$ is a hypercube which completes the proof. }
\end{IEEEproof}
\textcolor{black}{The next lemma establishes an upper bound on the probability density function (PDF) of ${\ensuremath{\boldsymbol{{y}}}}_k$, \emph{i.e.,} $p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$.}
\begin{lemma}\label{Lem: DUB}
\textcolor{black}{Consider the quantization cell $C_k{\left({\tilde{q}_k}\right)}$ at time $k$ where $\tilde{q}_k$ is a realization of $\tilde{Q}_k$. Let $G{\left({{\tilde{q}_{k}}}\right)}$ be the number of  quantization cells at time $k$ which their images under the quantized update rule overlap with that of $C_k{\left({\tilde{q}_k}\right)}$. Let $G^\star_{k+1}=\max_{\tilde{q}_k}G{\left({{\tilde{q}_k}}\right)}$. Then, the PDF of ${\ensuremath{\boldsymbol{{y}}}}_{k+1}$ can be upper bounded as 
\begin{align}
&p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\leq \frac{P_{\rm max}G_{k+1}^\star}{{\left({\prod_{i=1}^{M+N}{\left|{T_{ii}}\right|}}\right)}^{k+1}}
\end{align}
where $T_{ii}$ is the $i$th diagonal entry of $T$ and $P_{\rm max}$ is the maximum of PDF of ${\ensuremath{\boldsymbol{{y}}}}_0$.}
\end{lemma}
\begin{IEEEproof} 
\textcolor{black}{Let ${\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}$ be the hypercube centered at ${\ensuremath{\boldsymbol{{y}}}}$ with side length $r$.  Using Lebesgue differentiation theorem, the PDF of ${\ensuremath{\boldsymbol{{y}}}}_{k+1}$ can be written as
\begin{eqnarray}
p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}=\lim_{r\downarrow0}\frac{{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right\}}}}{{\rm Vol}{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right)}}
\end{eqnarray}
where ${\rm Vol}{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right)}$ is the volume of ${\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}$. ${\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right\}}}$ can be written as 
\begin{align}\label{Eq: Prob-Expansion}
{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right\}}}=\sum_{\tilde{q}_k}{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)},\tilde{Q}_k=\tilde{q}_k}\right\}}}
\end{align}
where $\tilde{q}_k$ is a realization of $\tilde{Q}_k$. With a slight abuse of notation, let ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ denote the image of $C_k{\left({\tilde{q}_k}\right)}$ under ${\ensuremath{\hat{T}\left({\cdot},{\tilde{q}_k}\right)}}$. Then, ${\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)},\tilde{Q}_k=\tilde{q}_k}\right\}}}$ can be written as 
\begin{align}\label{Eq: Prob-Eq}
{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)},\tilde{Q}_k=\tilde{q}_k}\right\}}}&\nonumber\\
&\hspace{-5cm}={\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right)}}\right\}}}{\ensuremath{\mathsf{I}_{\left\{{{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}
\end{align}
where the constant ${\ensuremath{\mathsf{I}_{\left\{{{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}$ is equal to one if the intersection of $ {\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}$ and ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ is non-empty and is equal to zero otherwise. 
}

\textcolor{black}{Now, we find the set of initial conditions which allow the quantized PD algorithm to arrive in ${\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ at time $k+1$ with $\tilde{Q}_k=\tilde{q}_k$. Let $S_{k+1}={\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$. Since $S_{k+1}$ is a subset of ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$, we can find $S_k\subset C_k{\left({\tilde{q}_k}\right)}$ such that $S_{k+1}={\ensuremath{\hat{T}\left({S_k},{\tilde{q}_k}\right)}}$. Since the quantization scheme is zoom-in, $ C_k{\left({\tilde{q}_k}\right)}\subset {\ensuremath{\hat{T}\left({C_{k-1}},{\tilde{q}_{k-1}}\right)}}$ which implies $S_k\subset{\ensuremath{\hat{T}\left({C_{k-1}},{\tilde{q}_{k-1}}\right)}}$. Hence, we can find $S_{k-1}\subset C_{k-1}{\left({\tilde{q}_{k-1}}\right)}$ such that $S_k={\ensuremath{\hat{T}\left({S_{k-1}},{\tilde{q}_{k-1}}\right)}}$. Using  backward induction, we can find a sequence of sets $\left\{S_n\right\}_{n=0}^k$ with $S_n\subset C_n{\left({\tilde{q}_n}\right)}$ such that we have $S_{n+1}={\ensuremath{\hat{T}\left({S_n},{\tilde{q}_n}\right)}}$ with $\tilde{Q}_k=\tilde{q}_k$. 
Thus, we have 
\begin{align}\label{Eq: Prob-UpB}
{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right)}}\right\}}}&={\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{0}\in S_0}\right\}}}\nonumber\\
&\leq P_{\rm max} {\rm Vol}{\left({S_0}\right)} 
\end{align}
where $P_{\rm max}$ is the maximum of PDF of ${\ensuremath{\boldsymbol{{y}}}}_0$ and ${\rm Vol}{\left({S_0}\right)}$ is the volume of $S_0$.}

\textcolor{black}{According to Lemma \ref{Lem: Hyper-Cube}, ${\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ is a hypercube. Also, ${\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}$ is a hypercube, thus,  $S_{k+1}={\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ is a hypercube. Since $S_k\subset C_k{\left({\tilde{q}_k}\right)}$ and  $Q^{{\ensuremath{\boldsymbol{{x}}}}}_k$ and $Q^{{\ensuremath{\boldsymbol{{\lambda}}}}}_k$ are fixed given $C_k{\left({\tilde{q}_k}\right)}$,  $x^{i}_{k+1}$ ($\lambda^{j}_{k+1}$) only depends on $x^{i}_{k}$ ($\lambda^{j}_{k}$). Thus, $S_k$ is a hypercube. Also, the length of the $i$th side of $S_{k+1}$ is equal to the length of the $i$th side of $S_{k}$ multiplied by ${\left|{T_{ii}}\right|}$. This is due to the fact that the quantized PD algorithm can be written as $x^{i}_{k}=T_{ii}x^{i}_{k-1}+\sum_{j=M+1}^{M+N}T_{ij}Q^{\lambda^j}_k+\mu c_i$ and $\lambda^{j}_{k}=\lambda^j_{k-1}+\sum_{i=1}^MT_{ji}Q^{x^i}_k-\mu b_j$. Hence, we have ${\rm Vol}{\left({S_{k+1}}\right)}={\rm Vol}{\left({S_k}\right)}\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}$. Using backward induction, we have 
\begin{align}\label{Eq: Vol-Eq}
{\rm Vol}{\left({S_{k+1}}\right)}={\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}{\rm Vol}{\left({S_0}\right)}
\end{align}
The volume of $S_{k+1}$ can be upper bounded as
\begin{align}\label{Eq: Vol-Ineq}
{\rm Vol}{\left({S_{k+1}}\right)}&={\rm Vol}{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right)}\nonumber\\
&\leq {\rm Vol}{\left({{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right)}.
\end{align}
Combining \eqref{Eq: Prob-Expansion}, \eqref{Eq: Prob-Eq}, \eqref{Eq: Prob-UpB}, \eqref{Eq: Vol-Eq} and \eqref{Eq: Vol-Ineq}, we have
\begin{align}
&{\ensuremath{\mathsf{Pr}\left\{{{\ensuremath{\boldsymbol{{y}}}}_{k+1}\in{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}}\right\}}}\nonumber\\
&\leq \frac{P_{\rm max}}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}\sum_{\tilde{q}_k}{\ensuremath{\mathsf{I}_{\left\{{{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}
\end{align}
Note that $\lim_{r\downarrow0}{\ensuremath{\mathsf{I}_{\left\{{{\rm B}{\left({{\ensuremath{\boldsymbol{{y}}}},r}\right)}\cap {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}={\ensuremath{\mathsf{I}_{\left\{{{\ensuremath{\boldsymbol{{y}}}}\in {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}$ where ${\ensuremath{\mathsf{I}_{\left\{{{\ensuremath{\boldsymbol{{y}}}}\in {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}$ is equal to one  if ${\ensuremath{\boldsymbol{{y}}}}\in {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}$ and is equal to zero otherwise. Let $G_{k+1}^{{\ensuremath{\boldsymbol{{y}}}}}=\sum_{\tilde{q}_k}{\ensuremath{\mathsf{I}_{\left\{{{\ensuremath{\boldsymbol{{y}}}}\in {\ensuremath{\hat{T}\left({C_k},{\tilde{q}_k}\right)}}}\right\}}}}$, \emph{i.e.,} the number of quantization cells at time $k$ which their images under the quantized update rule \emph{jointly} overlap at ${\ensuremath{\boldsymbol{{y}}}}$. Thus, we have 
\begin{align}\label{}
&p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\leq \frac{P_{\rm max}G_{k+1}^{{\ensuremath{\boldsymbol{{y}}}}}}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}
\end{align} }

\textcolor{black}{ Note that $\max_{{\ensuremath{\boldsymbol{{y}}}}}G_{k+1}^{{\ensuremath{\boldsymbol{{y}}}}}$ is the maximum number of quantization cells at time $k$ which their images jointly overlap with each other under ${\ensuremath{\hat{T}\left({\cdot},{\tilde{q}_k}\right)}}$. For a quantization cell $C_k{\left({\tilde{q}_k}\right)}$, let $G^\prime{\left({{\tilde{q}_k}}\right)}$ be the maximum number of quantization cells at time $k$ which their images under  the quantized update rule \emph{jointly} overlap with the image of  $C_k{\left({\tilde{q}_k}\right)}$. Also, let $G{\left({{\tilde{q}_k}}\right)}$ be the number of  quantization cells at time $k$ which their images under  the quantized update rule overlap with the image of  $C_k{\left({\tilde{q}_k}\right)}$. Clearly, we have $G^\prime{\left({{\tilde{q}_k}}\right)}\leq G{\left({{\tilde{q}_k}}\right)}$. Thus, we have 
\begin{align}
\max_{{\ensuremath{\boldsymbol{{y}}}}}G^{k+1}_{{\ensuremath{\boldsymbol{{y}}}}}&= \max_{{\tilde{q}_k}}G^\prime{\left({{\tilde{q}_k}}\right)}\nonumber\\
&\leq\max_{{\tilde{q}_k}}G{\left({{\tilde{q}_k}}\right)}\nonumber\\
&=G^\star_{k+1}
\end{align}
Hence, the PDF of ${\ensuremath{\boldsymbol{{y}}}}_{k+1}$ can be upper bounded as 
\begin{align}
&p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}\leq \frac{P_{\rm max}G_{k+1}^\star}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}
\end{align} } 
\end{IEEEproof} 
\textcolor{black}{The next two results are used in Lemma \ref{Lem: NLP} to obtain an upper bound on $G^\star_{k+1}$. The next lemma derives a necessary condition for two hypercubes to overlap.}
\begin{lemma}\label{Lem: HQC}
\textcolor{black}{Consider two hypercubes $C_1$ and $C_2$ in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$ which the length of the $j$th side of $C_i$ is given by $l_i^j>0$ for $i=1,2$ and $1\leq j\leq N+M$. Let ${\ensuremath{\boldsymbol{{y}}}}_{C_1}$ and ${\ensuremath{\boldsymbol{{y}}}}_{C_2}$ be any two points in $C_1$ and $C_2$, respectively. Then, a necessary condition for $C_1$ and $C_2$  to overlap with each other is given by 
\begin{align}
-{\left({l^j_1+l^j_2}\right)}\leq {\ensuremath{\boldsymbol{{y}}}}^j_{C_2}-{\ensuremath{\boldsymbol{{y}}}}^j_{C_1}\leq l^j_1+l^j_2 \quad \textit{for all $j$}\nonumber
\end{align}
for any ${\ensuremath{\boldsymbol{{y}}}}_{C_1}\in C_1$ and ${\ensuremath{\boldsymbol{{y}}}}_{C_2}\in C_2$ where ${\ensuremath{\boldsymbol{{y}}}}^j_{C_i}$ is the $j$th entry of  ${\ensuremath{\boldsymbol{{y}}}}_{C_i}$.}
 \end{lemma}
\begin{IEEEproof}
\textcolor{black}{Without loss of generality, we assume  $C_1$ and $C_2$ which are specified by ${\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_1}\leq {\ensuremath{\boldsymbol{{y}}}}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_1}$ and ${\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_2}\leq {\ensuremath{\boldsymbol{{y}}}}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_2}$, receptively, where ${\ensuremath{\boldsymbol{{y}}}}\in{\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$. Then, for ${\ensuremath{\boldsymbol{{y}}}}\in C_1\cap C_2$, we have 
\begin{align}
{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}&\leq {\ensuremath{\boldsymbol{{y}}}}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}\nonumber\\
{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}_{C_2}&\leq {\ensuremath{\boldsymbol{{y}}}}-{\ensuremath{\boldsymbol{{y}}}}_{C_2}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}_{C_2}\nonumber
\end{align}
where $\leq$ indicates component-wise inequality. Combining the above inequalities, we obtain a necessary condition for $C_1$ and $C_2$ to overlap as follows 
\begin{align}
-{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}_{C_2}}\right)}-&{\left({{\ensuremath{\boldsymbol{{y}}}}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_1}}\right)}\nonumber\\
&\leq {\ensuremath{\boldsymbol{{y}}}}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}\leq \nonumber\\
&{\left({{\ensuremath{\boldsymbol{{y}}}}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_2}}\right)}+{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}}\right)}\nonumber
\end{align}
 Note that ${\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_i}-{\ensuremath{\boldsymbol{{y}}}}_{C_i}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_i}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_i}$ and ${\ensuremath{\boldsymbol{{y}}}}_{C_i}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_i}\leq {\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_i}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_i}$. Thus, the above condition can be further relaxed to the following necessary condition for $C_1$ and $C_2$ to overlap:
\begin{align}
-{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_2}}\right)}-&{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_1}}\right)}\nonumber\\
&\leq {\ensuremath{\boldsymbol{{y}}}}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}_{C_1}\leq \nonumber\\
&{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_2}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_2}}\right)}+{\left({{\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_1}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_1}}\right)}
\end{align}
The proof is complete by appealing to the fact $l^j_i$ is the $j$th element of the ${\ensuremath{\boldsymbol{{y}}}}^{\rm max}_{C_i}-{\ensuremath{\boldsymbol{{y}}}}^{\rm min}_{C_i}$.}
\end{IEEEproof}
\textcolor{black}{Consider the lattice $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}={\ensuremath{\boldsymbol{{y}}}}+\delta^{\rm min}_k{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ where, ${\ensuremath{\boldsymbol{{y}}}}\in {\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$, $\delta^{\rm min}_k$ is the minimum quantization step at time $k$, and ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ is the  $N+M$ dimensional integer lattice in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$. Next, we define the minimum distance assignment rule which assigns a unique lattice point of $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$ to each quantization cell. Then, in Lemma \ref{Lem: NLP-aux}, we derive an upper bound on the number of quantization cells which a lattice point of $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$ can be assigned to under minimum distance assignment rule. }
\begin{definition}
\textcolor{black}{Consider the lattice $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$. Under minimum distance assignment rule, a lattice point of $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$ is assigned to each quantization cell as follows. If a quantization cell contains only one point of $\delta^{\rm min}_k$, that point is assigned to the corresponding cell. If a quantization cell contains more than one lattice point, then, a lattice point with the smallest distance to its cell representative is assigned to the corresponding cell.}
\end{definition}

\textcolor{black}{Since each side length of quantization cells at time $k$ is greater or equal to $\delta^{\rm min}_k$, every quantization cell at time $k$ at least contains one point of $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$. 
\begin{lemma}\label{Lem: NLP-aux}
Every lattice point in $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}$ can at most  be assigned to $G^\star_k$ quantization cells at time $k$.
\end{lemma}}
\begin{IEEEproof}
\textcolor{black}{This lemma is proved by contradiction. Let $G{\left({{\tilde{q}_{k-1}}}\right)}$ be the number of  quantization cells at time $k-1$ which their images under the quantized update rule overlap with that of $C_{k-1}{\left({\tilde{q}_{k-1}}\right)}$. Then, $G^\star_{k}$ can be written as $G^\star_{k}=\max_{\tilde{q}_{k-1}}G{\left({{\tilde{q}_{k-1}}}\right)}$. Assume that there exists a point ${\ensuremath{\boldsymbol{{y}}}}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}$ with ${\ensuremath{\boldsymbol{{I}}}}\in{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ which can be assigned to $G>G^\star_k$ quantization cells at time $k$. Since, under a zoom-in quantization scheme, at time $k$, the image of a quantization cell at time $k-1$ is quantized, ${\ensuremath{\boldsymbol{{y}}}}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}$ belongs to the images of $G$ quantization cells at time $k-1$. This observation implies that image of $G$ quantization cells at time $k-1$ under the quantized update rule overlap with each other at ${\ensuremath{\boldsymbol{{y}}}}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}$. Thus, we have  $G\leq G^\star_k$ which contradicts with our assumption. }
\end{IEEEproof}
\begin{lemma}\label{Lem: NLP}
\textcolor{black}{Let $\beta_T$ be the number points in the lattice $T{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ which lie in a hypercube centered at the origin with the $i$th side length equal to $4\rho{\left|{T_{ii}}\right|}+2{\left\Vert{T}\right\Vert_{{\infty}}}$ where ${\left\Vert{\cdot}\right\Vert_{{\infty}}}$ denotes the norm infinity, $T_{ii}$ is the $i$th diagonal entry of matrix $T$, and ${\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ is the lattice of integers in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$. 
Then, $G^\star_{k+1}\leq \beta_T^{k+1}$.}
\end{lemma}
\begin{IEEEproof}
\textcolor{black}{Consider two distinct quantization cells at time $k$ $C_k{\left({\tilde{q}_k}\right)}$ and $C_k{\left({\tilde{q}^\prime_k}\right)}$ with the cell representatives ${\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}\in C_k{\left({\tilde{q}_k}\right)}$ and ${\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)}\in C_k{\left({\tilde{q}^\prime_k}\right)}$. Let ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ and ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k^\prime}\right)}},{\tilde{q}_k^\prime}\right)}}$ be the images of $C_k{\left({\tilde{q}_k}\right)}$ and $C_k{\left({\tilde{q}_k^\prime}\right)}$, respectively,  under the quantized update rule. ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ and ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k^\prime}\right)}},{\tilde{q}_k^\prime}\right)}}$ are hypercubes in ${\ensuremath{{\ensuremath{\mathbb{{R}}}}}}^{N+M}$, thus, Lemma \ref{Lem: HQC} can be used to obtain a necessary condition for ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ and ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k^\prime}\right)}},{\tilde{q}_k^\prime}\right)}}$ to overlap. Note that the $j$th side length of ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ is less than or equal to ${\left|{T_{jj}}\right|}\delta^{\rm max}_k$. This due to the facts that $j$th side length of ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ is equal to the $j$th side length of $C_k{\left({\tilde{q}_k}\right)}$ multiplied by ${\left|{T_{jj}}\right|}$ and each side length of quantization cells at time $k$ is less than or equal to $\delta^{\rm max}_k$. Similarly, the $j$th side length of $T{\left({C_k{\left({\tilde{q}^\prime_k}\right)},\tilde{q}^\prime_k}\right)}$ is less than or equal to ${\left|{T_{jj}}\right|}\delta^{\rm max}_k$. 
Thus, using Lemma \ref{Lem: HQC}, the necessary condition for ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ and ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}^\prime_k}\right)}},{\tilde{q}^\prime_k}\right)}}$ to overlap is given by 
\begin{align}\label{Eq: ini-nec-con}
-2\delta^{\rm max}_k{\left|{T_{jj}}\right|}\leq\hat{T}_j{\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)},\tilde{q}_k}\right)}&-\hat{T}_j{\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)},\tilde{q}^\prime_k}\right)}\nonumber\\
& \leq 2\delta^{\rm max}_k{\left|{T_{jj}}\right|}  \quad \forall j
\end{align}
Note that the quantization does not have any impact on the  representative of quantization cells. Thus, the quantized update rule for cell representatives is the same as the unquantized update rule. Thus, ${\ensuremath{\hat{T}\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}-{\ensuremath{\hat{T}\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)}},{\tilde{q}^\prime_k}\right)}}=T{\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}-{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)}}\right)}$.
}
\textcolor{black}{ The cell representative of $C_k{\left({\tilde{q}^\prime_k}\right)}$ can be written as ${\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)}={\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}+{\ensuremath{\boldsymbol{{\psi}}}}$ where ${\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}$ is the lattice point of $\Lambda{\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}}\right)}$  assigned to $C_k{\left({\tilde{q}^\prime_k}\right)}$ and ${\ensuremath{\boldsymbol{{\psi}}}}$ is an $N+M$ dimensional vector with the $j$th entry satisfying $-\delta^{\rm min}_k< \psi_j<\delta^{\rm min}_k$. Thus, we have ${\ensuremath{\hat{T}\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}-{\ensuremath{\hat{T}\left({{\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}^\prime_k}\right)}},{\tilde{q}^\prime_k}\right)}}=\delta^{\rm min}_kT{\ensuremath{\boldsymbol{{I}}}}+T{\ensuremath{\boldsymbol{{\psi}}}}$.
Using \eqref{Eq: ini-nec-con}, a necessary condition for ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}_k}\right)}},{\tilde{q}_k}\right)}}$ and ${\ensuremath{\hat{T}\left({C_k{\left({\tilde{q}^\prime_k}\right)}},{\tilde{q}^\prime_k}\right)}}$ to overlap can be obtained as 
\begin{align}
-2\rho T_d-\frac{1}{\delta^{\rm min}_k}T{\ensuremath{\boldsymbol{{\psi}}}}\leq T{\ensuremath{\boldsymbol{{I}}}}\leq2\rho T_d-\frac{1}{\delta^{\rm min}_k}T{\ensuremath{\boldsymbol{{\psi}}}}\nonumber
\end{align}
where $T_d=\left[{\left|{T_{ii}}\right|}\right]_i^\top$ and $\rho=\frac{\delta^{\rm max}_k}{\delta^{\rm min}_k}$ and $\leq$ indicates component-wise inequality. Using the fact that ${\left|{\frac{{\psi}_m}{\delta^{\rm min}_k}}\right|}\leq 1$, the above condition can be relaxed to 
\begin{align}\label{Eq: Overlap-Cond}
-2\rho T_d-{\left\Vert{T}\right\Vert_{{\infty}}}{\ensuremath{\mathbf{1}}}_{N+M}\leq T{\ensuremath{\boldsymbol{{I}}}}\leq2\rho T_d+{\left\Vert{T}\right\Vert_{{\infty}}}{\ensuremath{\mathbf{1}}}_{N+M}
\end{align}
 where ${\ensuremath{\mathbf{1}}}_{N+M}$ is an $N+M$ dimensional vector with all entries equal to one. Recall that $G{\left({\tilde{q}_k}\right)}$ is the number of quantization cells which overlap with $C_k{\left({\tilde{q}_k}\right)}$. The number of quantization cells satisfying \eqref{Eq: Overlap-Cond}  provides an upper bound on $G{\left({\tilde{q}_k}\right)}$ as  \eqref{Eq: Overlap-Cond} is a necessary condition for two cells to overlap with each other. Let $\beta_T$ be the number of vectors ${\ensuremath{\boldsymbol{{I}}}}\in{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$ which satisfy \eqref{Eq: Overlap-Cond}. According to Lemma \ref{Lem: NLP-aux},  for any ${\ensuremath{\boldsymbol{{I}}}}\in{\ensuremath{{\ensuremath{\mathbb{{Z}}}}}}^{N+M}$,  the lattice point ${\ensuremath{\boldsymbol{{y}}}}_r{\left({\tilde{q}_k}\right)}+\delta^{\rm min}_k{\ensuremath{\boldsymbol{{I}}}}$ can be assigned to at most $G^\star_k$  quantization cells at time $k$. Thus, the number of quantization cell for which \eqref{Eq: Overlap-Cond} holds is upper bounded by $\beta_T G^\star_k$ and, we have $G{\left({\tilde{q}_k}\right)}\leq \beta_T G^\star_k$. 
		Since this bound is independent of $C_k{\left({\tilde{q}_k}\right)}$, we have $G^\star_{k+1}\leq \beta_T G^\star_k$. Following a similar argument, it can be easily shown than $G^\star_1\leq \beta_T$. Hence, we have $G^\star_{k+1}\leq \beta_T^{k+1}$ which completes the proof. 
}
\end{IEEEproof}
\subsection{Proof of Theorem \ref{Theo: EDE-New}}\label{Sub-sec: Proof}
\textcolor{black}{Now, we are ready to prove Theorem \ref{Theo: EDE-New}. To this end, first, we use Lemma \ref{Lem: DUB}  to establish a lower bound on the differential entropy of ${\ensuremath{\boldsymbol{{y}}}}_{k+1}$ a follows:
\begin{align}\label{Eq: Ent-LB}
{\ensuremath{\mathsf{h}\left[{{\ensuremath{\boldsymbol{{y}}}}_{k+1}} \right]}}
&=\int -{\ensuremath{\log{\left({{ p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}}}\right)}}}p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}d{\ensuremath{\boldsymbol{{y}}}}\nonumber\\
&\stackrel{(a)}{\geq} -\log{ \frac{P_{\rm max}G_{k+1}^\star}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}} \int p_{{\ensuremath{\boldsymbol{{y}}}}_{k+1}}{\left({{\ensuremath{\boldsymbol{{y}}}}}\right)}d{\ensuremath{\boldsymbol{{y}}}}\nonumber\\
&= -\log{ \frac{P_{\rm max}G_{k+1}^\star}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}} 
\end{align}
where $(a)$ follows from the fact that $-{\ensuremath{\log{\left({{x}}\right)}}}$ is decreasing in $x$. Let $\epsilon_{k+1}={\ensuremath{\boldsymbol{{y}}}}_{k+1}-{\ensuremath{\boldsymbol{{y}}}}^\star$. Substituting $A=\Omega$, where $\Omega$ is the sample space of the underlying probability space, and ${\ensuremath{\boldsymbol{{z}}}}=\epsilon_{k+1}$ in equation \eqref{Eq: CEP-UB}, we have  
\begin{align}\label{Eq: MSE-LB}
{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k+1}}\right\Vert_{{2}}}^2} \right]}}
&\geq  \frac{{\ensuremath{{\rm e}^{{1-\frac{1}{N+M}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}{\ensuremath{{\rm e}^{{\frac{2}{N+M}{\ensuremath{\mathsf{h}\left[{{\ensuremath{\boldsymbol{{y}}}}_{k+1}} \right]}}}}}}
\end{align}
Combining \eqref{Eq: Ent-LB} and \eqref{Eq: MSE-LB}, we have 
\begin{align}\label{Eq: EXP-LB-A3}
\log{{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k+1}}\right\Vert_{{2}}}^2} \right]}}}&\geq \log{\frac{{\ensuremath{{\rm e}^{{1-\frac{1}{N+M}}}}}}{{2\pi {\ensuremath{{\rm e}^{{}}}}}}}\nonumber\\
&-\frac{2}{N+M}\log{ \frac{P_{\rm max}G_{k+1}^\star}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}^{k+1}}} 
\end{align}}

\textcolor{black}{Combining \eqref{Eq: EXP-LB-A3} and Lemma \ref{Lem: NLP}, we have 
\begin{align}
\liminf_{k\rightarrow\infty}\frac{1}{k+1}&\log{{\ensuremath{\mathsf{E}\left[{{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k+1}}\right\Vert_{{2}}}^2} \right]}}}\nonumber\\
&\geq -\frac{2}{N+M}\log{\frac{ \beta_T}{{\left({\prod_{i=1}^{N+M}{\left|{T_{ii}}\right|}}\right)}}} 
\end{align}
which completes the proof.
}
\section{Proof of Theorem \ref{Theo: Conv}}\label{App:Conv}
\textcolor{black}{In this appendix, first, we show that $x^i_{k+1}$ and $\lambda^j_{k+1}$ belong to $I^{x^i}_{k+1}$ and $I^{\lambda^j}_{k+1}$, respectively. Note that ${\left|{x^i_{k+1}-x^i_k-\left\lfloor \frac{x^i_{k+1}-x^i_k}{\delta_k}\right\rfloor\delta_k}\right|}\leq \delta_k$. Thus, 
\begin{align}
&{\left|{x^i_{k+1}-C^{x^i}_{k+1}}\right|}&\nonumber\\
&= {\left|{x^i_{k+1}-x^i_{k}+x^i_{k}-{\rm Q}_{{\rm a},k}{\left({x^i_k}\right)}-\left\lfloor \frac{x^i_{k+1}-x^i_k}{\delta_k}\right\rfloor\delta_k}\right|}\nonumber\\
&{\leq} {\left|{x^i_{k+1}-x^i_{k}-\left\lfloor \frac{x^i_{k+1}-x^i_k}{\delta_k}\right\rfloor\delta_k}\right|}+{\left|{x^i_{k}-{\rm Q}_{{\rm a},k}{\left({x^i_k}\right)}}\right|}\nonumber\\
&\stackrel{{\left({a}\right)}}{\leq2}\delta_k\nonumber\\
&\leq \left\lceil \frac{2}{\alpha}\right\rceil\alpha\delta_k\nonumber\\
&= \left\lceil \frac{2}{\alpha}\right\rceil\delta_{k+1}
\end{align}
where $(a)$ follows from the fact that the quantization error is less than the quantization step $\delta_k$. Thus, $x^i_{k+1}$ belongs to $I^{x^i}_{k+1}$. Similarly, we have ${\left|{\lambda^j_{k+1}-\lambda^j_k-\left\lfloor \frac{\lambda^j_{k+1}-\lambda^j_{k}}{\delta_k}\right\rfloor\delta_k}\right|}\leq \delta_k$. Thus, we have 
\begin{align}
&{\left|{\lambda^j_{k+1}-C^{\lambda^j}_{k+1}}\right|}\nonumber\\
&\leq {\left|{\lambda^j_{k+1}-\lambda^j_k-\left\lfloor \frac{\lambda^j_{k+1}-\lambda^j_{k}}{\delta_k}\right\rfloor\delta_k}\right|}+{\left|{\lambda^j_k-{\rm Q}_{{\rm a},k}{\left({\lambda^j_k}\right)}}\right|}\nonumber\\
&\leq 2\delta_k\nonumber\\
&\leq \left\lceil \frac{2}{\alpha}\right\rceil\delta_{k+1}
\end{align}
which implies that $\lambda^j_{k+1}$ belongs to $I^{\lambda^j}_{k+1}$.}

\textcolor{black}{Now, we prove the convergence of the PD algorithm under the quantization scheme $\mathcal{Q}_{\rm a}$. Note that the PD update rule can be written as 
\begin{align}
{x}^{i}_{k}&=x^{i}_{k-1}+\mu_{k-1} {\left({\frac{d }{d x^i}U_i{\left({x^i_{k-1}}\right)}-A^\top_i{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}}\right)},\nonumber  \\
&\hspace{2cm}+\mu_{k-1}A^\top_i{\left({{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}-Q^{{\ensuremath{\boldsymbol{{\lambda}}}}}_{k-1}}\right)}\nonumber\\
{\lambda}^{j}_{k}&=\lambda^j_{k-1}+\mu_{k-1}{\left({A_j{\ensuremath{\boldsymbol{{x}}}}_{k-1}-b_j}\right)}+\mu_{k-1}A_j{\left({Q^{{\ensuremath{\boldsymbol{{x}}}}}_{k-1}-{\ensuremath{\boldsymbol{{x}}}}_{k-1}}\right)}\nonumber
\end{align}
Since the unquantized update rule forms a contraction map, we have 
\begin{align}\label{Eq: Norm-UB}
&{\left\Vert{
{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{}}}\leq\alpha {\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{k-1}}\right\Vert_{{}}}+ \mu_{k-1}
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{x}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k-1}}}}\\
{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k-1}}}}
\end{array}
\right]}\right\Vert_{{}}}
\end{align}
where ${\left\Vert{\cdot}\right\Vert_{{}}}$ is the norm in which the unquantized PD algorithm forms a contraction mapping. The second term in the right hand side of \eqref{Eq: Norm-UB} can be upper bounded as 
\begin{align}\label{Eq: Norm-UB-1}
& \mu_{k-1}
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{x}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k-1}}}}\\
{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k-1}}}}
\end{array}
\right]}\right\Vert_{{}}},\nonumber\\
&\stackrel{(a)}{\leq} \mu_{k-1}C
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{x}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k-1}}}}\\
{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k-1}}}}
\end{array}
\right]}\right\Vert_{{\infty}}},\nonumber\\
&\stackrel{(b)}{\leq} \min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}C
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]}\right\Vert_{{\infty}}}{\left\Vert{\left[
\begin{array}{c}
{\ensuremath{\boldsymbol{{x}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{x}}}}}}_{{k-1}}}}\\
{\ensuremath{\boldsymbol{{\lambda}}}}_{k-1}-{\ensuremath{Q^{{{\ensuremath{\boldsymbol{{\lambda}}}}}}_{{k-1}}}}
\end{array}
\right]}\right\Vert_{{\infty}}},\nonumber\\
&\stackrel{(c)}{=} \min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}C
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]}\right\Vert_{{\infty}}}\delta_{k-1},
\end{align}
where $C$ is a positive constant, $(a)$ follows from the fact that all the norms on a finite dimensional vector space are equivalent, $(b)$ follows from $0<\mu_{n}< \min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}$, and the multiplicative property of norm infinity, and $(c)$ follows from the fact that the quantization error at time $k-1$ is less than or equal to $\delta_{k-1}$ for all primal and dual variables.}

\textcolor{black}{Using \eqref{Eq: Norm-UB} and \eqref{Eq: Norm-UB-1}, ${\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}\right\Vert_{{}}}$ can be upper bounded as 
\begin{align}\label{Eq: Exp-Con}
&{\left\Vert{{{\ensuremath{\boldsymbol{{\epsilon}}}}_k}}\right\Vert_{{}}}\!\leq\! 
\alpha^k\!{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{0}}\right\Vert_{{}}}\!+\!\min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}C
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]}\right\Vert_{{\infty}}}\!\sum_{i=1}^{k}\!\alpha^{i-1}\delta_{k-i}\nonumber\\
&\stackrel{(a)}{=} \alpha^k{\left\Vert{{\ensuremath{\boldsymbol{{\epsilon}}}}_{0}}\right\Vert_{{}}}+\min_i\frac{1}{{\left|{U^{\rm min}_i}\right|}}C
{\left\Vert{
\left[
\begin{array}{cc}
{\ensuremath{\boldsymbol{{0}}}}&A^\top\\
-A&{\ensuremath{\boldsymbol{{0}}}}
\end{array}
\right]}\right\Vert_{{\infty}}}k\alpha^k
\end{align}
where $(a)$ follows from the fact that $\delta_k=\alpha^{k+1}$. The last inequality in \eqref{Eq: Exp-Con} implies that the PD algorithm under the quantization scheme $\mathcal{Q}_{\rm a }$ converges exponentially to the optimal solution.}
\bibliographystyle{IEEEtran}
\begin{thebibliography}{1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}

\BIBdecl
\bibitem{KMT98}
F.~P.~Kelly, A.~Maulloo, and D.~Tan, ``Rate control for communication
networks: Shadow prices, proportional fairness and stability," \emph{J. Oper. Res}., vol. 49, no. 3, pp. 237â252,  1998.

\bibitem{SS07}
S.~Shakkottai and R.~Srikant. ``Network optimization and control.", \emph{Found. Trends Netw.}, vol.~2, no.~3 pp.~271-379, 2007.
\bibitem{AB05}
T.~Alpcan and T.~Basar, ``A Utility-Based Congestion Control Scheme for Internet-Style Networks with Delay," \emph{IEEE Trans. Netw.}, vol.~13, no.~6, pp.~1261-1274,  2005.

\bibitem{Nedic08}
A.~Nedi\'{c}, A.~Olshevsky, A.~Ozdaglar and J.N.~Tsitsiklis, ``Distributed subgradient methods and quantization effects," IEEE Conf. on Decision and Control Conference (CDC), pp.4177--4184,  2008.

\bibitem{Rabbat05}
M.G.~Rabbat and R.D.~Nowak, ``Quantized incremental algorithms for distributed optimization," \emph{IEEE J. Sel. Areas. Comm.}, vol. 23, no. 4, pp. 798--808,  2005.

\bibitem{CL10}
Y.~Cui, and V.K.N.~Lau, ``Convergence-optimal quantizer design of distributed contraction-based iterative algorithms with quantized message passing.", \emph{IEEE Trans. Signal Process.}, vol.~58, no.~10, pp.~5196--5205,  2010.

\bibitem{YXZR12}
D.~Yuan, S.~Xu, H.~Zhao and L.~Rong, ``Distributed dual averaging method for multi-agent optimization with quantized communication," \emph{Systems $\&$ Control Letters}, vol.~61, no.~11, pp.~1053-1061,  2012.

\bibitem{YH14}
P.~Yi and Y.~Hong, ``Quantized Subgradient Algorithm and Data-Rate Analysis for Distributed Optimization," \emph{IEEE Trans. Control Netw. Syst.}, vol.~1, no.~4, pp.~380-392,  2014.

\bibitem{NNA15}
E.~Nekouei, G.~N.~Nair and T.~Alpcan, ``Performance Analysis of Gradient-Based Nash Seeking Algorithms Under Quantization'', in press, \emph{IEEE Trans. Autom. Control}, 2016.

\bibitem{NNA15-CDC}
E.~Nekouei, G.~N.~Nair and T.~Alpcan, ``Convergence Analysis of Quantized Primal-dual Algorithm in Quadratic Network Utility Maximization Problems," 54th \emph{Int. Conf. on Decision and Control (CDC)}, pp. 2655--2660, 2015.

\bibitem{NE04}
G.~N.~Nair and R.J.~Evans, ``Stabilizability of stochastic linear systems with finite feedback data rates," \emph{SIAM J. Control Optim.}, vol.~43, no.~2, pp.~413--436, 2004.

\bibitem{FMS10}
J.S.~Freudenberg, R.H.~Middleton, V.~Solo, ``Stabilization and disturbance attenuation Over a Gaussian Communication Channel," \emph{IEEE Trans. Autom. Control}, vol.~55, no.~3, pp.795--799,  2010.

\bibitem{Cover}
T.~M.~Cover and J.~A.~Thomas, ``\emph{Elements of Information Theory}", 2nd ed., New York: Wiley, 2005.

\bibitem{leon-garcia}
A.~Leon-Garcia,``Probability and Random Processes for Electrical Engineering'', 2nd ed., Massachusetts: Addison-Wesley, 1994.

 \end{thebibliography}

\end{document} 
