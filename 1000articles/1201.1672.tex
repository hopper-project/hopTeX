\documentclass[10pt, a4paper]{amsart}  
\usepackage{amssymb} 
\usepackage{mathabx} 
\usepackage{epic}  
\usepackage{xypic}  
\usepackage[vcentermath]{youngtab} 
\usepackage[backref=page, colorlinks=false]{hyperref} 
\usepackage{amsthm}
\usepackage[capitalise]{cleveref} 
\usepackage[font=footnotesize,margin={8pt,8pt}]{caption}
\usepackage{url} 
\urlstyle{rm}

\makeatletter

\makeatother

\newtheoremstyle{note}
  {5pt}
  {5pt}
  {\small}
  {}
  {\bfseries}
  {.}
  {.5em}
  {}

\theoremstyle{plain}
    \newtheorem{lemma}{Lemma}[section]
    \newtheorem{thm}[lemma]{Theorem}
    \newtheorem{prop}[lemma]{Proposition}
    \newtheorem{corol}[lemma]{Corollary}
    \newtheorem{otherthm}[lemma]{Theorem}
    \newtheorem{scholium}[lemma]{Scholium}
    
    \newtheorem{scho}[lemma]{Scholium}
	\newtheorem{claim}[lemma]{Claim}
	\newtheorem*{repeatedthm}{Theorem \ref{t.schubert}}
\theoremstyle{definition}
    \newtheorem*{question}{Question}
\theoremstyle{remark}
    \newtheorem*{ack}{Acknowledgements}
\theoremstyle{note}  
    \newtheorem{rem}[lemma]{Remark}
	\newtheorem{example}[lemma]{Example}

\crefname{thm}{Theorem}{Theorems} 
\Crefname{thm}{Theorem}{Theorems} 
\crefname{otherthm}{Theorem}{Theorems}  
\Crefname{otherthm}{Theorem}{Theorems} 
\crefname{prop}{Proposition}{Propositions} 
\Crefname{prop}{Proposition}{Propositions}

\numberwithin{equation}{section}         

\setcounter{secnumdepth}{3}              

\setcounter{tocdepth}{1}                
\hypersetup{bookmarksdepth=subsection} 

  

\crefname{subsection}{\S}{\S\S} 
\Crefname{subsection}{\S}{\S\S} 

 

\title[Universal Regular Control]{Universal Regular Control for Generic Semilinear Systems}
\author[Bochi]{Jairo Bochi}
\address{Pontif\'icia Universidade Cat\'olica do Rio de Janeiro (PUC--Rio)}
\email{jairo@mat.puc-rio.br}
\author[Gourmelon]{Nicolas Gourmelon}
\address{Institut de Math\'{e}matiques de Bordeaux, Universit\'{e} Bordeaux I}
\email{ngourmel@math.u-bordeaux1.fr}
\date{\today}
\thanks{J.B.\ was partially supported by CNPq, Universit\'{e} Bordeaux I, Rede Franco--Brasileira em Matem\'atica, and FAPERJ. N.G.\ was partially supported by FAPERJ}
\keywords{Discrete-time systems; semilinear systems; bilinear systems; universal regular control}
\subjclass[2010]{93C10; 93B05, 93C55}

\begin{document}

\begin{abstract} 
We consider discrete-time projective semilinear control systems $\xi_{t+1} = A(u_t) \cdot \xi_t$, where the states $\xi_t$ are in projective space ${\mathbb{R}\mathrm{P}}^{d-1}$, inputs $u_t$ are in a manifold ${\mathcal{U}}$ of arbitrary finite dimension, and $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{R}})$ is a differentiable mapping.

An input sequence $(u_0,\ldots,u_{N-1})$ is called universally regular if for any initial state $\xi_0 \in {\mathbb{R}\mathrm{P}}^{d-1}$, the derivative of the time-$N$ state with respect to the inputs is onto.

In this paper we deal with the universal regularity of constant input sequences $(u_0, \dots, u_0)$. Our main result states that generically in the space of such systems, for sufficiently large $N$, all constant inputs of length $N$ are universally regular, with the exception of a discrete set. More precisely, the conclusion holds for a $C^2$-open and $C^\infty$-dense set of maps $A$, and $N$ only depends on $d$ and on the dimension of ${\mathcal{U}}$. We also show that the inputs on that discrete set are nearly universally regular; indeed there is a unique non-regular initial state, and its corank is $1$.

In order to establish the result, we study the spaces of bilinear control systems. We show that the codimension of the set of systems for which the zero input is not universally regular coincides with the dimension of the control space. The proof is based on careful matrix analysis and some elementary algebraic geometry. Then the main result follows by applying standard transversality theorems.
\end{abstract}

\maketitle

\section{Introduction}\label{s.intro}

\subsection{Basic definitions and some questions}

Consider discrete-time control systems of the form:
\begin{equation}\label{e.general CS}
x_{t+1} = F(x_t,u_t), \qquad (t = 0,1,2, \dots)
\end{equation}
where $F \colon {\mathcal{X}} \times {\mathcal{U}} \to {\mathcal{X}}$ is a map.
We will always assume that the space ${\mathcal{X}}$ of states and the space ${\mathcal{U}}$ of controls are manifolds, and that the map $F$ is continuously differentiable.

A sequence $(x_0, \dots, x_N ; u_0, \dots, u_{N-1})$ satisfying \eqref{e.general CS} is called a trajectory of length $N$; it is uniquely determined by the initial state $x_0$ and the 
input 
$(u_0,\dots,u_{N-1})$.
Let $\phi_N$ denote the time-$N$ transition map,
which gives the final state as a function of the initial state and the input:
\begin{equation}\label{e.final state}
x_N = \phi_N(x_0; u_0, \dots, u_{N-1}).
\end{equation}

We say that the system \eqref{e.general CS} is \emph{accessible} from $x_0$ in time $N$ if 
the set $\phi_N(\{x_0\} \times {\mathcal{U}}^N)$ 
of final states that can be reached from the initial state $x_0$ 
has nonempty interior.

The implicit function theorem gives a sufficient condition for accessibility.
If the derivative of the map $\phi_N(x_0; \cdot)$ at input $(u_0,\dots,u_{N-1})$
is an onto linear map 
then we say that the trajectory determined by $(x_0; u_0, \dots, u_{N-1})$ is \emph{regular}.
So the existence of such a regular trajectory implies that the system
is accessible from $x_0$ in time $N$.

\medskip

Let us call an input $(u_0, \dots, u_{N-1})$ \emph{universally regular}
if for every $x_0 \in {\mathcal{X}}$, the trajectory determined by $(x_0; u_0, \dots, u_{N-1})$ is regular;
otherwise the input is called \emph{singular}.

The concept of universal regularity is central in this paper; 
it was introduced by Sontag 
in \cite{Sontag_92} in the context of continuous-time control systems. The discrete-time analogue was considered by Sontag and Wirth in \cite{Sontag_Wirth_98}.
They showed that if the system \eqref{e.general CS} is accessible from every initial condition $x_0$ 
in uniform time $N$ then universally regular inputs do exist, provided one assumes the map $F$ to be analytic. In fact, under those hypotheses they showed that universally regular inputs are abundant: in the space of inputs of sufficiently large length, singular ones form a set of positive codimension.

\medskip

In this paper, we are interested in control systems \eqref{e.general CS} where
the next state $x_{t+1}$ depends linearly on the previous state $x_t$ 
(but non-linearly on $u_t$, in general). 
This means that the state space is ${\mathbb{K}}^d$,
where ${\mathbb{K}}$ is either ${\mathbb{R}}$ or ${\mathbb{C}}$,
and that \eqref{e.general CS} now takes the form: 
\begin{equation}\label{e.semilin CS}
x_{t+1} = A(u_t) \cdot x_t, \qquad \text{where } A \colon {\mathcal{U}} \to {\mathrm{Mat}}_{d \times d}({\mathbb{K}}).
\end{equation}
Following \cite{CK_93}, we call this a \emph{semilinear control system}.

In the case that the map $A$ above takes values in the set ${\mathrm{GL}}(d,{\mathbb{K}})$ of invertible matrices
of size $d \ge 2$, we consider the corresponding projectivized control system:
\begin{equation}\label{e.proj semilin CS}
\xi_{t+1} = A(u_t) \cdot \xi_t, 
\end{equation}
where the states $\xi_t$ take value in the projective space ${\mathbb{K}\mathrm{P}}^{d-1} = {\mathbb{K}}^d_* / {\mathbb{K}}_*$.
We call this a \emph{projective semilinear control system}.
The projectivized system is also a useful tool
for the study of the original system \eqref{e.semilin CS}:
see e.g.\ \cite{Wirth_98,CK_book}.

Universally regular inputs for projective semilinear control systems
were first considered by Wirth in \cite{Wirth_98}.
Under his working hypotheses, the existence and abundance of such inputs 
is guaranteed by the aforementioned result of \cite{Sontag_Wirth_98};
then he uses universally regular inputs to obtain global controllability properties.

\medskip

The purpose of this paper is to establish results on the existence and abundance of 
universally regular inputs for projective semilinear control systems. 
Differently from \cite{Sontag_Wirth_98, Wirth_98}, we will not necessarily assume our systems to be analytic.
Let us consider systems \eqref{e.proj semilin CS} with ${\mathbb{K}}={\mathbb{R}}$ 
and $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{R}})$ a map of class $C^r$, for some fixed $r\ge 1$.
To compensate for less rigidity, we do not try to obtain results that work for all $C^r$ maps $A$, 
but only for \emph{generic} ones, i.e., those maps in a residual (dense $G_\delta$) 
subset, or, even better, in an open dense subset.

To make things more precise, assume ${\mathcal{U}}$ is a $C^\infty$ (real) manifold without boundary.
All manifolds are assumed to be Hausdorff paracompact with a countable base of open sets, and of finite dimension.
We will always consider the space $C^r({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$ endowed with the strong $C^r$ topology
(which coincides with the usual uniform $C^r$ topology in the case that ${\mathcal{U}}$ is compact).

Hence the first question we pose is this: 
\begin{quote}
Taking $N$ sufficiently large, is it true that for $C^r$-generic maps $A$, the set of universally regular inputs in ${\mathcal{U}}^N$ is itself generic?
\end{quote}
It turns out that this question has a positive answer.
Actually, in a work in preparation we show that for generic maps $A$, 
all inputs in ${\mathcal{U}}^N$ are universally regular, except for those in 
a stratified closed set of positive codimension.
So another natural question is this:
\begin{quote}
Fixed parameters $d$, $\dim {\mathcal{U}}$, $N$, and $r$,
what is the minimum codimension of the set of singular inputs in ${\mathcal{U}}^N$
that can occur for $C^r$-generic maps $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{R}})$?
\end{quote}
In full generality, this question seems to be very difficult.
A simpler setting would be to restrict to \emph{non-resonant inputs}, 
namely those inputs $(u_0,\dots,u_{N-1})$ such that $u_i \neq u_j$ whenever $i \neq j$.  
In this paper we consider the most resonant case.
Define a \emph{constant} input of length $N$ 
as an element of ${\mathcal{U}}^N$ of the form $(u_0, u_0, \dots, u_0)$.
We propose ourselves to study universal regularity of inputs of this form.

\subsection{The main result}\label{ss.main_statements}

We prove that generically the singular constant inputs form a very small set:
 
\begin{thm}\label{t.main}
Given $d\ge 2$ and $m \ge 1$, there exists an integer $N$ with $1 \le N \le d^2$ 
such that the following properties hold.
Let ${\mathcal{U}}$ be a smooth $m$-dimensional manifold without boundary.
Then there exists a $C^2$-open $C^\infty$-dense subset ${\mathcal{O}}$ of $C^2({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$ 
such that for every system \eqref{e.proj semilin CS} with $A \in {\mathcal{O}}$,
all constant inputs of length $N$ are universally regular, except for 
those in a zero-dimensional (i.e., discrete) set.
\end{thm}

By saying that a subset ${\mathcal{O}}$ of $C^2({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$ is  $C^\infty$-dense, 
we mean that for all $r \ge 2$, the intersection of ${\mathcal{O}}$ with $C^r({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$ 
is dense in $C^r({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$.

It is remarkable that the generic dimension of the set of singular constant inputs (namely, $0$) does not depend on the dimension $m$ of the control space ${\mathcal{U}}$, neither on the dimension $d-1$ of the state space. A partial explanation for this phenomenon is the following: 
First, the obstruction to universal regularity of the input $(u,u,\dots,u)$ is the combined degeneracy of the matrix $A(u)$ and of the derivatives of $A$ at $u$.
If $m$ is small then the image of the generic map $A$ will avoid too degenerate matrices, which increases the chances of obtaining universal regularity. If $m$ is large then more degenerate matrices $A(u)$ will inevitably appear; however the large number of control parameters compensates, so universal control is still likely.

\medskip

The singular inputs that appear in \cref{t.main} are not only rare;
we also show that they are ``almost'' universally regular:

\begin{thm}[Addendum to \cref{t.main}]\label{t.addendum}
The set ${\mathcal{O}} \subset C^2({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{R}}))$ in \cref{t.main} 
can be taken with the following additional properties:
If $A \in {\mathcal{O}}$ and a constant input $(u,\dots,u)$ of length $N$ is singular then:
\begin{enumerate}
\item \label{i.addendum_1} 
There is a single direction $\xi_0 \in {\mathbb{R}\mathrm{P}}^{d-1}$ for which
the corresponding trajectory of system \eqref{e.proj semilin CS} is not regular.
\item \label{i.addendum_2}
The derivative of the map $\phi_N(\xi_0; \cdot)$ at input $(u,\dots,u)$
has corank $1$.
\end{enumerate}
\end{thm}

To sum up, for generic systems \eqref{e.proj semilin CS}, the universal regularity of constant inputs 
can fail only in the weakest possible way: there is at most one non-regular state, 
which can be moved in all directions but one.

We actually describe precisely in \cref{a.generic singular} the singular inputs that appear in \cref{t.addendum}. 
We show that these singular inputs can be unremovable by perturbations,
and therefore \cref{t.main} is optimal in the sense that there are $C^2$-open (actually even $C^1$-open) sets of maps $A$ for which the set of singular constant inputs is nonempty.
Also, by $C^1$-perturbing any $A$ in those $C^2$-open sets,
one can obtain an infinite number of singular constant inputs.
In particular, the set ${\mathcal{O}}$ in the statement of the \cref{t.main} is not $C^1$-open in general.

\subsection{Reduction to the study of the set of poor data}

The bulk of the proof of \cref{t.main} consists on the computation 
of the dimension of certain canonical sets,
as we now explain.

\medskip

We fix $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{K}})$ and consider
the projective semilinear system \eqref{e.proj semilin CS}.
By the chain rule, the universal regularity of an input 
$(u_0, u_1, \ldots, u_{N-1})$
depends only on the $1$-jets of $A$ at points $u_0$, \ldots, $u_{N-1}$,
i.e., on the first order Taylor approximations of $A$ around those points.

Let us discuss the case of constant inputs $(u_0, \ldots, u_0)$.
If we take local coordinates such that $u_0 = 0$
and replace the matrix map $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{K}})$
by its linear approximation, system \eqref{e.proj semilin CS} becomes:
\begin{equation}\label{e.proj bilin CS}
\xi_{t+1} = \left(A + \sum_{j=1}^m u_{t,i} C_j \right) \xi_t  
\, , \quad (t = 0, 1, 2, \dots) ,
\end{equation}
where $A = A(u_0)$ 
and $C_1$, \dots, $C_m$ are the partial derivatives at $u_0=0$.
This is the projectivization of a \emph{bilinear control system} (see \cite{Elliott}).
For these systems, the zero input is a distinguished one and the focus of more attention.

To study system \eqref{e.proj bilin CS} it is actually more convenient to consider
\emph{normalized derivatives} $B_j = C_j A^{-1}$,
which intrinsically take values in the Lie algebra ${\mathfrak{gl}}(d,{\mathbb{K}})$.
Consider the matrix datum ${\mathbf{A}} = (A, B_{1}, \ldots, B_{m})$.
We will explain how the universal regularity of the zero input
is expressed in linear algebraic terms.
Recall that the \emph{adjoint operator} of $A$ acts on ${\mathfrak{gl}}(d,{\mathbb{K}})$
by the formula ${\mathrm{Ad}}_A(B) = A B A^{-1}$.
Consider the linear subspace $\Lambda_N({\mathbf{A}})$ of ${\mathfrak{gl}}(d,{\mathbb{K}})$ spanned by the matrices
$$
{\mathrm{Id}} \quad \text{and} \quad
({\mathrm{Ad}}_A)^i(B_j), \quad (i=0,\ldots,n-1, \ j=1,\ldots, m).
$$
(The identity matrix appears because of the projectivization.)
This is nothing but the reachable set from $0$ 
for the linear control system $({\mathrm{Ad}}_A, {\mathrm{Id}}, B_1, \dots, B_m)$.
Then:

\begin{prop}\label{p.reg transitivity}
The constant input $(0,\dots,0)$ of length $N$ is universally regular 
for system \eqref{e.proj bilin CS}
if and only if the space $\Lambda_N({\mathbf{A}})$ is transitive.
\end{prop}

Here we say that a subspace of $d \times d$ matrices with entries in the field ${\mathbb{K}}$
is \emph{transitive} if it acts transitively in the set ${\mathbb{K}}^d_*$ of nonzero vectors.

Clearly, the spaces $\Lambda_N({\mathbf{A}})$ form a nested sequence that stabilizes 
to a space $\Lambda({\mathbf{A}})$ at some time $N \le d^2$.
If $\Lambda({\mathbf{A}})$ is transitive then the datum ${\mathbf{A}}$ is called \emph{rich}; 
otherwise it is called \emph{poor}.
Let ${\mathcal{P}}_m^{({\mathbb{K}})} = {\mathcal{P}}_{m,d}^{({\mathbb{K}})}$ denote the set of poor data.
A major part of our work is to study these sets.
We prove:

\begin{thm}\label{t.cod_data_R} 
The set ${\mathcal{P}}_{m}^{({\mathbb{R}})}$ is closed and semialgebraic,
and its codimension in ${\mathrm{GL}}(d,{\mathbb{R}}) \times ({\mathfrak{gl}}(d,{\mathbb{R}}))^m$
is $m$.
\end{thm}

\begin{thm}\label{t.cod_data_C} 
The set ${\mathcal{P}}_{m}^{({\mathbb{C}})}$ is algebraic,
and its (complex) codimension in ${\mathrm{GL}}(d,{\mathbb{C}}) \times ({\mathfrak{gl}}(d,{\mathbb{C}}))^m$
is $m$.
\end{thm}

So \cref{t.cod_data_R,t.cod_data_C} 
say how frequent universal regularity of the zero input is 
in the space of projective bilinear control systems \eqref{e.proj bilin CS}.

\subsection{Overview of the proofs}\label{ss.overview}

\Cref{t.main} follows rather directly from \cref{t.cod_data_R} 
by applying standard results from transversality theory.
More precisely, the fact that the set ${\mathcal{P}}_m^{({\mathbb{R}})}$ is semialgebraic
implies that it has a canonical stratification.
This permits us to apply Thom's jet transversality theorem and obtain \cref{t.main}.

On the other hand, \Cref{t.cod_data_R}
follows from its complex version \cref{t.cod_data_C} 
by simple abstract arguments.

Thus everything is based on \cref{t.cod_data_C}.
One part of the result is easily obtained:
we give examples of small disks of codimension $m$ formed by poor data,
so concluding that the codimension of ${\mathcal{P}}_{m}^{({\mathbb{C}})}$ is at most $m$.

To prove the other inequality, 
one could try to exhibit an explicit codimension~$m$ set containing all poor data.
For $m=1$ this task is feasible
(and we actually perform it, because with these conditions we can 
actually check universal regularity in concrete examples).
However, for $m=2$ already the task would be very laborious,
and to expect to find a general solution seems unrealistic.

Our actual approach to prove the lower bound on the codimension of ${\mathcal{P}}_{m}^{({\mathbb{C}})}$ is indirect.
Crudely speaking, after careful matrix computations,
we find some sets in the complement of ${\mathcal{P}}_{m}^{({\mathbb{C}})}$
that are reasonably ``large'' (basically in terms of dimension).
Then, by using some abstract results of algebraic geometry,
we are able to show that ${\mathcal{P}}_{m}^{({\mathbb{C}})}$ is ``small'', 
thus proving the other half of  \cref{t.cod_data_C}.

Let us give more detail about this strategy.
We decompose the set ${\mathcal{P}}_m = {\mathcal{P}}_{m}^{({\mathbb{C}})}$ into fibers: 
$$
{\mathcal{P}}_m =\bigcup_{A \in {\mathrm{GL}}(d,{\mathbb{C}})} \{A\} \times {\mathcal{P}}_m(A) , \qquad {\mathcal{P}}_m(A) \subset [{\mathfrak{gl}}(d,{\mathbb{C}})]^m.
$$
It is not very difficult to show that for generic $A$ in ${\mathrm{GL}}(d,{\mathbb{C}})$, 
the fiber ${\mathcal{P}}_m(A)$ has precisely the wanted codimension $m$.
However, for degenerate matrices $A$, the fiber ${\mathcal{P}}_m(A)$ may be much bigger.
(For example, one can show that if $A$ is an homothecy and $m \le 2d-3$ then ${\mathcal{P}}_m(A)$ is the whole $[{\mathfrak{gl}}(d,{\mathbb{C}})]^m$.)
In order to show that $\operatorname{codim} {\mathcal{P}}_m \ge m$, we need to make sure 
that those degenerate matrices do not form a large set.
More precisely, we show that:
\begin{equation}\label{e.everything}
\forall k\in\{0,\ldots,m\}, \ 
\operatorname{codim} \big\{ A \in {\mathrm{GL}}(d,{\mathbb{C}}) ; \;  \operatorname{codim} {\mathcal{P}}_m(A) \le m-k  \big\} \ge k. 
\end{equation}

Let us explain how we prove~\eqref{e.everything}.
In order to estimate the dimension of ${\mathcal{P}}_m(A)$ for any matrix $A \in {\mathrm{GL}}(d,{\mathbb{C}})$,
we consider a quantity $r = r(A)$ which is the least number such that
a rich datum of the form $(A,C_1,\ldots,C_r)$ exists.
In particular, if $r = r(A) \le m$ 
then the following affine space 
\begin{equation}\label{e.affine}
\big\{ (C_1, C_2, \dots, C_r , B_{r+1}, \dots, B_m ) ; \; B_j \in {\mathfrak{gl}}(d,{\mathbb{C}}) \big\}
\end{equation}
is contained in the complement of ${\mathcal{P}}_m(A)$.

In certain situations,  if two algebraic subsets have large enough dimensions 
then they necessarily intersect;
for example, two algebraic curves in the complex projective plane ${\mathbb{C}\mathrm{P}}^2$ always intersect.
This kind of phenomenon happens here:
the dimension of the affine space \eqref{e.affine} 
forces a lower bound for the codimension of ${\mathcal{P}}_m(A)$, namely:
\begin{equation}\label{e.algebraic}
\operatorname{codim} {\mathcal{P}}_m(A) \ge m+1-r(A).
\end{equation}

So 
we need to show that matrices $A$ with large $r(A)$ are rare.
A careful matrix analysis provides
an upper bound to $r(A)$ based on the numbers and sizes of the Jordan blocks of $A$,
and on the occasional algebraic relations between the eigenvalues.
This bound together with \eqref{e.algebraic}
implies \eqref{e.everything} and therefore concludes the proof of \cref{t.cod_data_C}.

In fact, the results of this analysis are even better,
and we conclude that the codimension inequality \eqref{e.everything}
is strict when $k \ge 1$.
This implies that poor data $(A, B_1, \dots, B_m)$ 
for which the matrix $A$ is degenerate form a subset of ${\mathcal{P}}_{m}^{({\mathbb{C}})}$ 
with strictly bigger codimension.
Thus we can show that the poor data that appear generically are well-behaved,
which leads to \cref{t.addendum}.

\subsection{Holomorphic setting}

In the case of complex matrices (i.e., ${\mathbb{K}} = {\mathbb{C}}$), we have a 
corresponding version of \cref{t.main} where the maps $A$ are holomorphic.
Given an open subset ${\mathcal{U}} \subset {\mathbb{C}}^m$, we denote by 
$\mathcal{H}({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{C}}))$ 
the set of holomorphic mappings $A \colon {\mathcal{U}}\to {\mathrm{GL}}(d,{\mathbb{C}})$ endowed with the usual topology of uniform convergence on compact sets. 

\begin{thm}\label{t.main_C}
Given integers $d\ge 2$ and $m \ge 1$, there exists an integer $N\ge 1$ with the following properties.
Let ${\mathcal{U}}\subset {\mathbb{C}}^m$ be open, and let $K\subset {\mathcal{U}}$ be compact.
Then there exists an open and dense subset ${\mathcal{O}}$ of $\mathcal{H}({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{C}}))$
such that for any $A \in {\mathcal{O}}$ the constant inputs in $K^N$ are all universally regular for the  system~\eqref{e.proj semilin CS}, except for a finite subset.
\end{thm}

We have the straightforward corollary:

\begin{corol}
Given integers $d\ge 2$ and $m \ge 1$, there exists an integer $N\ge 1$ with the following properties.
Let ${\mathcal{U}}\subset {\mathbb{C}}^m$ be an open subset. There exists a residual subset $\mathcal{R}$ of $\mathcal{H}({\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{C}}))$
such that for any $A \in \mathcal{R}$ the constant inputs in ${\mathcal{U}}^N$ are all universally regular for  the  system~\eqref{e.proj semilin CS}, except for a discrete subset.
\end{corol}

\subsection{Directions for future research}

One can also study uniform regularity of periodic inputs of higher period.
Using our results for constant inputs, it is not difficult 
to derive some (non-sharp) codimension bounds for singular periodic inputs for generic systems.
However, for highly resonant non-periodic inputs, we have no idea on how to obtain reasonable dimension estimates. 

To obtain good estimates for the codimension of \emph{non-resonant}
singular inputs for generic systems is relatively simpler from the point of view of matrix computations,
but needs more sophisticated transversality theorems (e.g., multijet transversality).
Since highly resonant inputs have large codimension themselves,
it seems possible to obtain reasonably good codimension estimates for general inputs for generic systems.

Another interesting direction of research is to consider other Lie groups of matrices.

\subsection{Organization of the paper}\label{ss.organization}

\Cref{s.prelim_poor} contains some basic results about 
transitivity of spaces of matrices and its relation with universal regularity.
We also obtain the easy parts of \cref{t.cod_data_R,t.cod_data_C}, namely
(semi)algebraicity and the upper codimension inequalities.

In \cref{s.rig} we introduce the concept of rigidity, which is related to the quantity $r(A)$
mentioned above. We state the central rigidity estimates (\cref{t.rig}), which
consist into two parts. The first and easier part is proved in the same \cref{s.rig},
while the whole \cref{s.rig proof} is devoted to the proof of the second part.

\Cref{s.cod proof} starts with some preliminaries in elementary algebraic geometry.
Then we use the rigidity estimates to prove \cref{t.cod_data_C}, following the strategy 
outlined above (\cref{ss.overview}).
\Cref{t.cod_data_R} follows easily.
We also obtain a lemma that is needed for the proof of \cref{t.addendum}.

In \cref{s.main proof} we deduce \cref{t.main}
from  previous results and standard theorems stratifications and transversality.

The paper also has some appendices:

\Cref{a.dim 1} basically reobtains the major results in the special case $m=1$,
where we actually gain additional information of practical value:
as mentioned in \cref{ss.overview}, it is possible to describe explicitly what $1$-jets
the map $A$ should avoid in order to satisfy the conclusions of \cref{t.main,t.addendum}.
The arguments necessary for the $m=1$ case are much simpler and more elementary
than those in \cref{s.rig,s.rig proof,s.cod proof}.
Therefore the \lcnamecref{a.dim 1} is also useful to give the reader some intuition about the
general problem, and as a source of examples.
\Cref{a.dim 1} is written in a slightly informal way, and it can be read after \cref{s.prelim_poor}
(though the final part requires \cref{l.sum}).

\Cref{a.algebraic} contains the proofs of necessary algebraic-geometric results,
especially the one that allows us to obtain estimate \eqref{e.algebraic}.

\Cref{a.strat_trans} reviews the necessary concepts and results on stratifications,
and proves a prerequisite transversality proposition.

In \cref{a.complex} we apply \cref{t.cod_data_C} 
to prove a version of \cref{t.main} for holomorphic mappings.

In \cref{a.generic singular} we study the singular constant inputs of generic type,
proving \cref{t.addendum} and the other assertions made at the end of \S~\ref{ss.main_statements}
concerning the sharpness of \cref{t.main}.
We also discuss the generic validity of some control-theoretic properties 
related to accessibility and regularity.

\section{Preliminary facts on the poor data}\label{s.prelim_poor}

In this section, we review some basic properties 
related to poorness, and prove the easy inequalities in \cref{t.cod_data_R,t.cod_data_C}.

\subsection{Transitive spaces}\label{ss.transitivity}

Let $E$ and $F$ be finite-dimensional vector spaces over the field ${\mathbb{K}}$.
Let $\mathcal{L}(E,F)$ be the space of linear maps from $E$ to $F$.
A vector subspace $\Lambda$ of $\mathcal{L}(E,F)$ is called \emph{transitive}
if for every $v \in E {\smallsetminus} \{0\}$, we have $\Lambda \cdot v = F$,
where $\Lambda \cdot v = \{ L(v) ; \; L \in \Lambda \}$.

Under the identification $\mathcal{L}({\mathbb{K}}^n, {\mathbb{K}}^m) = {\mathrm{Mat}}_{m \times n}({\mathbb{K}})$,
we may also speak of transitive spaces of matrices.

The following examples illustrate the concept; they will also be needed in later considerations.

\begin{example}\label{ex.toeplitz}
Recall that a \emph{Toeplitz matrix}, resp.\ a \emph{Hankel matrix}, is a matrix of the form
$$
\left(
\vcenter{
\xymatrix @-2pc {
t_0    \ar@{.}[rrrddd] & t_1 \ar@{.}[rrdd] & \cdots & t_{d-1} \\
t_{-1} \ar@{.}[rrdd]   &                   &        & \vdots  \\
\vdots                 &                   &        & t_1     \\
t_{-d+1}               & \cdots            & t_{-1} & t_0
}}
\right) ,
\quad\text{resp.}\quad
\left(
\vcenter{
\xymatrix @-2pc {
h_1       & \cdots  & h_{d-1} \ar@{.}[lldd] & h_d     \ar@{.}[lllddd] \\
\vdots    &         &                       & h_{d+1} \ar@{.}[lldd]   \\
h_{d-1}   &         &                       & \vdots                  \\
h_d       & h_{d+1} & \cdots                & h_{2d-1}
}}
\right),
$$
The set of Toeplitz matrices 
and the set of complex Hankel matrices 
constitute examples transitive subspaces of ${\mathfrak{gl}}(d,{\mathbb{K}})$.
Transitivity of the Toeplitz space is a particular case of \cref{ex.gen Toeplitz},
and transitivity of Hankel space follows from \cref{r.transitivity trick}.
For ${\mathbb{K}} = {\mathbb{C}}$, these spaces are optimal, in the sense that they have the least possible dimension; see \cite{Azoff}.
\end{example}

\begin{example}\label{ex.gen Toeplitz}
A \emph{generalized Toeplitz space} is a subspace $\Lambda$ of ${\mathrm{Mat}}_{d\times d}({\mathbb{K}})$ 
(where $d \ge 2$)
with the following property:
For any two matrix entries $(i_1,j_1)$ and $(i_2,j_2)$ which are not in the same diagonal
(i.e., $i_1-j_1 \neq i_2-j_2$), 
the linear map $(b_{i,j})_{i,j} \in \Lambda \mapsto (b_{i_1,j_1},b_{i_2,j_2}) \in {\mathbb{C}}^2$ is onto.
Equivalently, a space is generalized Toeplitz if 
it can be defined by a number of linear relations between the matrix coefficients
so that each relation involves only the entries on a same diagonal,
and so that the relations do not force any matrix entry to be zero.
We will prove later (see \cref{ss.sudoku})
that \emph{every generalized Toeplitz space is transitive}.
\end{example}

\begin{rem}\label{r.transitivity trick}
If $\Lambda$ is a transitive subspace of $\mathcal{L}(E,F)$
and $P \in \mathcal{L}(E,E)$, $Q \in \mathcal{L}(F,F)$ are invertible operators then
$P \cdot \Lambda \cdot Q := \{PLQ ; \;  L \in \Lambda\}$ 
is a transitive subspace of $\mathcal{L}(E,F)$.
\end{rem}

\medskip

Let us see that transitivity is a semialgebraic or algebraic property, according to the field.
Recall that:
\begin{itemize}
\item A subset of ${\mathbb{K}}^n$ is called \emph{algebraic} if it is expressed by 
polynomial equations with coefficients in ${\mathbb{K}}$.
\item A subset of ${\mathbb{R}}^n$ is called \emph{semialgebraic} if it is 
the union of finitely many sets, each of them defined by finitely many  real polynomial equations
and inequalities (see \cite{BR,BCR}).
\end{itemize}

\begin{prop}\label{p.NT algebraicity}
Let ${\mathcal{N}}_{m,n,k}^{({\mathbb{K}})}$ be the set of $(B_1, \dots, B_k) \in [{\mathrm{Mat}}_{m \times n}({\mathbb{K}})]^k={\mathbb{K}}^{mnk}$ such that
$\operatorname*{span}\{B_1, \dots, B_k\}$ is not transitive.
Then:
\begin{enumerate}
\item\label{i.semialgebraic}
The set ${\mathcal{N}}_{m,n,k}^{({\mathbb{R}})}$ is semialgebraic.
\item\label{i.algebraic} 
The set ${\mathcal{N}}_{m,n,k}^{({\mathbb{C}})}$ is algebraic.
\end{enumerate}
\end{prop}

\begin{proof} 
Consider the set of $(B_1, \dots, B_k, v) \in [{\mathrm{Mat}}_{m \times n}({\mathbb{K}})]^k \times {\mathbb{K}}^n_*$ 
such that 
$$
\operatorname*{span}\{B_1, \dots, B_k\}\cdot v \neq {\mathbb{K}}^m \, .
$$
For ${\mathbb{K}} = {\mathbb{R}}$, this is a semialgebraic set,
because it is expressed by the vanishing of certain determinants
plus the condition $v \neq 0$.
Projecting this set along the ${\mathbb{R}}^n_*$ fiber we obtain ${\mathcal{N}}_{m,n,k}^{({\mathbb{R}})}$;
so, by the Tarski--Seidenberg theorem (see \cite[p.~60]{BR} or \cite[p.~26]{BCR}),
this set is semialgebraic, proving part~\ref{i.semialgebraic}.

To see part~\ref{i.algebraic},
we take ${\mathbb{K}}={\mathbb{C}}$ and projectivize the ${\mathbb{C}}^n_*$ fiber, obtaining an
algebraic subset $[{\mathrm{Mat}}_{m \times n}({\mathbb{C}})]^k \times {\mathbb{C}\mathrm{P}}^{n-1}$
whose projection along the ${\mathbb{C}\mathrm{P}}^{n-1}$ fiber is  ${\mathcal{N}}_{m,n,k}^{({\mathbb{C}})}$.
So part~\ref{i.algebraic} follows from the fact that projections 
along projective fibers take algebraic sets to algebraic sets (see \cite[p.~58]{Shafa}).
\end{proof}

Complex transitivity of real matrices is a stronger property than real transitivity:

\begin{prop}\label{p.NT inclusion}
The real part of ${\mathcal{N}}_{m,n,k}^{({\mathbb{C}})}$ (that is, its intersection with $[{\mathrm{Mat}}_{m \times n}({\mathbb{R}})]^k$)
contains ${\mathcal{N}}_{m,n,k}^{({\mathbb{R}})}$.
\end{prop}

The proof is an easy exercise.

\subsection{Universal regularity for constant inputs and richness}\label{ss.characterization}

In this subsection we prove \cref{p.reg transitivity};
in fact we prove a more precise result, and also fix some notation.

Given a linear operator $H \colon E \to E$, 
where $E$ is a finite-dimensional vector space over the field ${\mathbb{K}}$,
and vectors $v_1$, \dots, $v_m \in E$, 
we denote by ${\mathfrak{R}}^N_H(v_1,\dots,v_m)$ the space spanned by 
the family of vectors $H^t(v_i)$, where $1\le i \le m$ and $0 \le t < N$.
In other words,  ${\mathfrak{R}}^N_H(v_1,\dots,v_m)$ is the reachable set from $0$ of the linear
control system 
$$
\xi_{t+1} = H\xi_t + \sum_i u_{t,i} v_i \, .
$$

The sequence of spaces ${\mathfrak{R}}^N_H(v_1,\dots,v_m)$ is nested nondecreasing, and
thus stabilize to a space ${\mathfrak{R}}_H(v_1,\dots,v_m)$ 
after $N \le \dim H$ steps.

\medskip

If $A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{C}})$ is a differentiable map
then the \emph{normalized derivative} of $A$ at a point $u$
is the linear map $T_u {\mathcal{U}} \to {\mathfrak{gl}}(d,{\mathbb{R}})$
given by 
$h \mapsto (DA(u)\cdot h)\circ A^{-1}(u)$.

Let $\phi_N(\xi_0,\hat{u})$ be the state $\xi_N\in {\mathbb{K}\mathrm{P}}^d$ of the system~\eqref{e.proj semilin CS} 
determined by the initial state $\xi_0$ and the input sequence $\hat{u}\in {\mathcal{U}}^N$. 
Let $\partial_2 \phi_N(\xi_0,\hat{u})$ be the derivative of the map $\phi_N(\xi_0, \cdot)$ at $\hat{u}$.

Fix a constant input $\hat{u}=(u,\ldots, u)\in {\mathcal{U}}^N$, 
and local coordinates on ${\mathcal{U}}$ around~$u$. 
Let $B_j$ be the normalized partial derivatives of the map $A$ at $u$ with respect to the $i^\mathrm{th}$ coordinate. 
Consider the datum ${\mathbf{A}}=(A,B_1,\ldots, B_m)$, where $A = A(u)$.
Define the following subspace of ${\mathfrak{gl}}(d,{\mathbb{K}})$:
\begin{equation}\label{e.Lambda_N}
\Lambda_N({\mathbf{A}}) =  {\mathfrak{R}}_{{\mathrm{Ad}}_A}^N ({\mathrm{Id}}, B_1, \dots, B_m) \, ,
\end{equation}
where ${\mathrm{Ad}}_A(B) = ABA^{-1}$.

\begin{prop}\label{p.ranks}
For all $\xi_0\in {\mathbb{K}\mathrm{P}}^{d-1}$ and any $x_0\in {\mathbb{K}}^d{\smallsetminus} \{0\}$ representing $\xi_0$, 
$$
\operatorname{rank} \partial_2 \phi_N (\xi_0,\hat{u}) = \dim\left[\Lambda_N({\mathbf{A}})\cdot (A^N x_0)\right]-1.
$$
\end{prop}

In particular (since $A =A(u)$ is invertible), 
the input $\hat{u}$ is universally regular 
if and only if $\Lambda_N({\mathbf{A}})$ is a transitive space,
which is the statement of \cref{p.reg transitivity}.

\begin{proof}[Proof of \cref{p.ranks}]
Let $\xi_0 = [x_0]$, where $x_0\in {\mathbb{K}}^d_*$.
Let $\psi_N(x_0,\hat{u})$ be the final state 
of the non projectivized system~\eqref{e.semilin CS}  determined by the initial state $x_0$ 
and by the sequence of controls $\hat{u}\in {\mathcal{U}}^N$.
Using local coordinates with $u$ in the origin,
we have the following first order approximation for $\hat{u}\simeq 0$:
\begin{align*}
\psi_N(x_0,\hat{u}) &\simeq A^N x_0 + 
\sum_{1\leq j\leq m \atop 0 \leq t<N}u_{t,j}A^{N-t-1}B_{j} A^{t+1} x_0 \\
&= 
\left({\mathrm{Id}} + \sum_{1\leq j\leq m \atop 0 \leq n < N} u_{N-1-n,j}{\mathrm{Ad}}_A^n(B_{j})\right) x_N \, ,
\end{align*}
where $x_N = \psi_N(x_0,0) = A^N x_0$.  
Therefore the image of 
$\partial_2 \psi_N(x_0,\hat{u})$
is the following subspace of $T_{A^N x_0} {\mathbb{K}}^d$:
\begin{align*}
V = \left(\operatorname*{span}_{1\leq j\leq m\atop 0\leq n<N}{\mathrm{Ad}}_A^n B_{j} \right) \cdot x_N ,
\end{align*}

The image of $\partial_2 \phi_N(\xi_0,\hat{u})$
equals $D\pi(x_N)(V)$, where $\pi: {\mathbb{K}}^d_* \to {\mathbb{K}\mathrm{P}}^{d-1}$ is the canonical projection.
Notice that $\operatorname{Ker} D\pi(x) = {\mathbb{K}} x$ for any $x \in {\mathbb{K}}^d_*$.
It follows that 
\begin{multline*}
\operatorname{rank} \partial_2 \phi_N(\xi_0,\hat{u})
 = \dim \left[ D\pi(x_N) (V) \right]
\\
 = \dim \left[ D\pi(x_N) \big({\mathbb{K}} x_N + V\big) \right]
 = \dim [{\mathbb{K}} x_N + V] - 1
\end{multline*}
Since ${\mathbb{K}} x_N + V = \Lambda_N({\mathbf{A}})\cdot x_N$,
the \lcnamecref{p.ranks} is proved.
\end{proof}

\subsection{The sets of poor data}\label{ss.poor_set}

For emphasis, we repeat the definition already given in the introduction:
The datum ${\mathbf{A}} = (A, B_1, \dots, B_m) \in {\mathrm{GL}}(d,{\mathbb{K}}) \times [{\mathfrak{gl}}(d,{\mathbb{K}})]^m$
is \emph{rich} if the space $\Lambda({\mathbf{A}}) = \Lambda_{d^2}({\mathbf{A}})$  is transitive,
and \emph{poor} otherwise.
The concept in fact depends on the field under consideration.
The set of such poor data is denoted by ${\mathcal{P}}_{m,d}^{({\mathbb{K}})}$.

It follows immediately from \cref{p.NT algebraicity} that 
${\mathcal{P}}_{m,d}^{({\mathbb{R}})}$ is a closed and semialgebraic subset of ${\mathrm{GL}}(d,{\mathbb{R}}) \times [{\mathfrak{gl}}(d,{\mathbb{R}})]^m$
and ${\mathcal{P}}_{m,d}^{({\mathbb{C}})}$ is an algebraic subset of ${\mathrm{GL}}(d,{\mathbb{C}}) \times [{\mathfrak{gl}}(d,{\mathbb{C}})]^m$.
This proves part of \cref{t.cod_data_R,t.cod_data_C}.

Also, by \cref{p.NT inclusion} the real poor data are contained in the real part of the complex poor data, i.e.,
\begin{equation}\label{e.RC_inclusion}
{\mathcal{P}}_{m,d}^{({\mathbb{R}})} \subset {\mathcal{P}}_{m,d}^{({\mathbb{C}})} \cap \big[ {\mathrm{GL}}(d,{\mathbb{R}}) \times [{\mathfrak{gl}}(d,{\mathbb{R}})]^m \big] \, .
\end{equation}

\medskip

For later use, we note that the sets of poor data
are saturated in the sense of the following definition: 
A set ${\mathcal{Z}} \subset [{\mathrm{Mat}}_{d\times d}({\mathbb{K}})]^{1+m}$
will be called \emph{saturated}
if $(A, B_1, \dots, B_m) \in {\mathcal{Z}}$ implies that:
$(A, B_1, \dots, B_m) \in {\mathcal{Z}}$ implies that:
\begin{itemize}
\item 
for all $P \in {\mathrm{GL}}(d,{\mathbb{K}})$ we have $(P^{-1}AP, P^{-1}B_1 P, \dots, P^{-1}B_m P) \in {\mathcal{Z}}$;
\item 
for all $Q = (q_{ij}) \in {\mathrm{GL}}(m,{\mathbb{K}})$, letting $B'_i = \sum_j q_{ij} B_j$,
we have $(A, B_1', \dots, B_m') \in {\mathcal{Z}}$.
\end{itemize}

\subsection{The easy codimension inequality of \cref{t.cod_data_R,t.cod_data_C}}
\label{ss.cod_data_easy_half}

Here we will discuss the simplest examples of poor data.

To begin, notice that if $A \in {\mathrm{GL}}(d,{\mathbb{C}})$ is diagonalizable
then so is ${\mathrm{Ad}}_A$.
Indeed, assume without loss of generality that $A = {\mathrm{Diag}}(\lambda_1, \dots, \lambda_d)$.
Consider the basis $\{E_{i,j} ; \; i,j \in \{1,\dots,d\}\} $ of ${\mathfrak{gl}}(d,{\mathbb{C}})$,
where 
\begin{equation}\label{e.Eij}
\text{$E_{i,j}$ is the matrix whose only nonzero entry is a $1$ in the $(i,j)$ position.}
\end{equation}
Then ${\mathrm{Ad}}_A (E_{i,j}) = \lambda_i \lambda_j^{-1} E_{i,j}$.
So if $f$ is a polynomial and $B = (b_{ij})$ then 
\begin{equation}\label{e.poly entry}
\text{the $(i,j)$-entry of the matrix $(f({\mathrm{Ad}}_A))(B)$ is  
$f(\lambda_i\lambda_j^{-1})b_{ij}$.}
\end{equation}

\medskip

The datum ${\mathbf{A}} = (A, B_1, \dots, B_m) \in {\mathrm{GL}}(d,{\mathbb{K}}) \times {\mathfrak{gl}}(d,{\mathbb{K}})^m$
is called \emph{conspicuously poor}
if there exists a change of bases $P \in {\mathrm{GL}}(d,{\mathbb{K}})$ such that:
\begin{itemize}
\item 
the matrix $P^{-1} A P$ is diagonal;
\item 
the matrices $P^{-1} B_k P$ have a zero entry in a common off-diagonal position;
more precisely, 
there are indices $i_0$, $j_0 \in \{1,\dots,d\}$ with $i_0 \neq j_0$ 
such that for each $k \in \{1,\dots,m\}$, the $(i_0,j_0)$ entry of the matrix $P^{-1} B_k P$ vanishes.
\end{itemize}
(As in the definition of poorness, the concept depends on the field ${\mathbb{K}}$.)

\begin{lemma}\label{l.easy_poor_data}
Conspicuously poor data are poor.
\end{lemma}

\begin{proof}
Let ${\mathbf{A}} = (A, B_1, \dots, B_m)$ be conspicuously poor.
With a change of basis we can assume that $A$ is diagonal.
Let $(e_1,\dots,e_d)$ be the canonical basis of ${\mathbb{K}}^d$.
Let $(i,j)$ be the entry position where all $B_i$'s have a zero entry. 
By \eqref{e.poly entry}, all matrices in the space 
$\Lambda({\mathbf{A}}) = {\mathfrak{R}}_{{\mathrm{Ad}}_A}({\mathrm{Id}}, B_1, \dots, B_m)$ have a zero entry in the $(i_0,j_0)$ position.
In particular, there is no $L \in \Lambda({\mathbf{A}})$ such that $L \cdot e_{j_0} = e_{i_0}$,
showing that this space is not transitive.
\end{proof}

The converse of this \lcnamecref{l.easy_poor_data} is certainly false.
(Many examples appear in \cref{a.dim 1}; see also \cref{ex.toroidal toeplitz}.)
However, we will see in \cref{ss.unconstrained} that the converse holds for generic $A$.

\medskip

We will use \cref{l.easy_poor_data} to prove the easy codimension inequalities for \cref{t.cod_data_R,t.cod_data_C};
first we need to recall the following:

\begin{prop}\label{p.eigen_smooth}
Suppose $A \in {\mathrm{Mat}}_{d\times d}({\mathbb{K}})$ is diagonalizable over ${\mathbb{K}}$ and with simple eigenvalues only.	
Then there is a neighborhood of $A$ where the eigenvalues vary smoothly, 
and where the eigenvectors can be chosen to vary smoothly.
\end{prop}

\begin{prop}[Easy half of \cref{t.cod_data_R,t.cod_data_C}]\label{p.cod_data_easy_half}
For both ${\mathbb{K}} ={\mathbb{R}}$ or ${\mathbb{C}}$, we have 
$\operatorname{codim}_{\mathbb{K}} {\mathcal{P}}^{({\mathbb{K}})}_m \le m$.
\end{prop}

\begin{proof}
Using \cref{p.eigen_smooth},
we can exhibit smoothly embedded disks of codimension $m$ 
inside ${\mathrm{GL}}(d,{\mathbb{K}}) \times {\mathfrak{gl}}(d,{\mathbb{K}})^m$
formed by conspicuously poor data.
\end{proof}

\section{Rigidity}\label{s.rig}

The aim of this section is to state \cref{t.rig} and prove its first part.
Along the way we will establish several lemmas which will be reused 
in the proof of the second part of the theorem in \cref{s.rig proof}.

\subsection{Acyclicity}\label{ss.acyclicity}

Consider a linear operator $H \colon  E \to E$, 
where $E$ is a finite-dimensional complex vector space.
The \emph{acyclicity} of $H$ is defined as the least number 
$n$ of vectors $v_1$, \dots, $v_n \in E$
such that ${\mathfrak{R}}_H(v_1, \ldots, v_n) = E$.
We denote $n = \operatorname{acyc} H$.
If $n = 1$ then $H$ is called a \emph{cyclic operator},
and $v_1$ is called a \emph{cyclic vector}.

\begin{lemma}\label{l.sum}
Let $E$ be a finite-dimensional complex vector space and let 
$H \colon E \to E$ be a linear operator.
Assume that $E_1$, \ldots, $E_k \subset E$ are $H$-invariant subspaces
and that the spectra of $A|E_i$ ($1 \le i \le k$) are pairwise disjoint.
If $v_1 \in E_1$, \ldots, $v_k \in E_k$ then  
$$
{\mathfrak{R}}_H(v_1, \ldots, v_k) = {\mathfrak{R}}_H(v_1 + \cdots + v_k) \, .
$$
\end{lemma}

\begin{proof}
View $E$ as a module over the ring of polynomials ${\mathbb{C}}[x]$ by defining $xv=H(v)$ for $v \in E$.
Then the lemma follows from \cite[Theorem~6.4]{Roman}.
\end{proof}

\medskip

The \emph{geometric multiplicity} of an eigenvalue $\lambda$ of $H$ is the 
dimension of the kernel of $H - \lambda {\mathrm{Id}}$
(or, equivalently, the number of corresponding Jordan blocks).

\begin{prop}\label{p.acyc}   
The acyclicity of an operator equals the maximum of the geometric multiplicities of its eigenvalues.
\end{prop}

\begin{proof} 
This follows from the Primary Cyclic Decomposition Theorem 
together with \cref{l.sum}.
\end{proof}

\begin{rem}\label{r.conjugacy_class}
The operators which interest us most are $H = {\mathrm{Ad}}_A$, where $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.
It is useful to observe that \emph{the geometric multiplicity of $1$ 
as an eigenvalue of ${\mathrm{Ad}}_A$ equals the codimension of the conjugacy class of $A$ inside ${\mathrm{GL}}(d,{\mathbb{C}})$}.
To prove this, consider the map $\Psi_A \colon {\mathrm{GL}}(d,{\mathbb{C}}) \to {\mathrm{GL}}(d,{\mathbb{C}})$ given by 
$\Psi_A(X) = {\mathrm{Ad}}_X(A)$.
The derivative at $X={\mathrm{Id}}$ is $H \mapsto HA-AH$; so
$\operatorname{Ker} D\Psi_A ({\mathrm{Id}}) = \operatorname{Ker} ({\mathrm{Ad}}_A - {\mathrm{id}})$.
Therefore when $X={\mathrm{Id}}$, the rank of $D\Psi_A (X)$ equals 
the geometric multiplicity of $1$ as an eigenvalue of ${\mathrm{Ad}}_A$.
To see that this is true for any $X$, notice that $\Psi_A = \Psi_{{\mathrm{Ad}}_X(A)} \circ R_{X^{-1}}$
(where $R$ denotes a right-multiplication diffeomorphism of ${\mathrm{GL}}(d,{\mathbb{C}})$).

We will see later (\cref{l.acyc and pop1}) that $1$ is the eigenvalue of ${\mathrm{Ad}}_A$
with the biggest geometric multiplicity.
By \cref{p.acyc}, we conclude that $\operatorname{acyc} {\mathrm{Ad}}_A$ equals 
the codimension of the conjugacy class of $A$.
\end{rem}

\subsection{Definition of rigidity, and the main rigidity estimate}

Let $E$ and $F$ be finite-dimensional complex vector spaces.
Let $H$ be a linear operator action on the space $\mathcal{L}(E,F)$.
We define the \emph{rigidity} of $H$, denoted $\operatorname{rig} H$,
as the least $n$ such that there exist $L_1$, \dots, $L_n \in \mathcal{L}(E,F)$ so that
${\mathfrak{R}}_H(L_1 , \dots, L_n)$ is transitive.
Therefore
$$
1 \le \operatorname{rig} H \le \operatorname{acyc} H \, .
$$

For technical reasons, 
we also define a \emph{modified rigidity} of $H$, 
denoted $\operatorname{rig}_+ H$.
The definition is the same, with the difference that if $E = F$ then 
$L_1$ is required to be the identity map in $\mathcal{L}(E,E)$.
Of course,
$$
\operatorname{rig} H \le \operatorname{rig}_+ H \le \operatorname{rig} H + 1.
$$  

\medskip

We want to give a reasonably good estimate 
of the modified rigidity of ${\mathrm{Ad}}_A$ for any fixed $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.
(This will be achieved in \cref{l.rig world}.)
We assume that $d \ge 2$; so $\operatorname{rig}_+ {\mathrm{Ad}}_A \ge 2$.
The next example shows that ``most'' matrices $A$ have the lowest possible $\operatorname{rig}_+ {\mathrm{Ad}}_A$.

\begin{example}\label{ex.unconstrained_rig}
If $A\in {\mathrm{GL}}(d,{\mathbb{C}})$ is unconstrained (see \cref{ss.unconstrained})
then $\operatorname{rig}_+ {\mathrm{Ad}}_A = 2$.
Indeed if we take a matrix $B \in {\mathfrak{gl}}(d,{\mathbb{C}})$ whose expression in the base that diagonalizes $A$
has no zeros off the diagonal then, by \cref{l.easy_fiber}, 
$\Lambda(A,B) = {\mathfrak{R}}_{{\mathrm{Ad}}_A}({\mathrm{Id}},B)$ is rich.

More generally, if $A\in {\mathrm{GL}}(d,{\mathbb{C}})$ is little constrained (see \cref{a.dim 1})
then it follows from \cref{p.rich_pair} that
$\operatorname{rig}_+ {\mathrm{Ad}}_A = 2$.
\end{example}	

\begin{example}\label{ex.toroidal toeplitz}
Consider $A = {\mathrm{Diag}} (1, \alpha, \alpha^2)$ where $\alpha = e^{2\pi i /3}$.
(In the terminology of \cref{ss.unconstrained}, 
$A$ has constraints of type 1.) 
Since ${\mathrm{Ad}}_A^3$ is the identity, we have $\dim {\mathfrak{R}}_{{\mathrm{Ad}}_A}({\mathrm{Id}},B) \le 4$ for any $B \in {\mathfrak{gl}}(3,{\mathbb{C}})$.
By the result of Azoff \cite{Azoff} already mentioned at \cref{ex.toeplitz}, 
the minimum dimension of a transitive subspace of ${\mathfrak{gl}}(3,{\mathbb{C}})$ is $5$.
This shows that $\operatorname{rig}_+ {\mathrm{Ad}}_A \ge 3$.
(Actually, equality holds, as we will see in \cref{ex.toroidal toeplitz again} below.)
\end{example}

\medskip

Let $T$ be the set of roots of unity.
Define an equivalence relation $\asymp$ on the set ${\mathbb{C}}^*$ of nonzero complex numbers by:
\begin{equation}\label{e.equiv rel}
\lambda \asymp \lambda' \ \Leftrightarrow \ \lambda / \lambda' \in T.
\end{equation}
We also say that $\lambda$, $\lambda'$ are \emph{equivalent mod~$T$}.

For $A \in {\mathrm{GL}}(d,{\mathbb{C}})$, we denote
\begin{equation}\label{e.num classes}
c(A) := \text{number of different classes mod $T$ of the eigenvalues of $A$.}
\end{equation}

We now state a technical result
which has a central role in our proofs,
as explained informally in \cref{ss.overview}:

\begin{thm}\label{t.rig}
Let $d \ge 2$ and $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.
Then:
\begin{enumerate}
\item\label{i.rig easy}
If $c(A) = d$ then $\operatorname{rig}_+ {\mathrm{Ad}}_A = 2$.
\item\label{i.rig hard}
If $c(A) < d$ then $\operatorname{rig}_+ {\mathrm{Ad}}_A \le \operatorname{acyc} {\mathrm{Ad}}_A - c(A) + 1$.
\end{enumerate}
\end{thm}

\begin{rem}
When $c(A) = d$, we have $\operatorname{acyc} {\mathrm{Ad}}_A = d$ (this will follow from \cref{l.acyc and pop1});
so the conclusion of part~\ref{i.rig hard} does not hold in this case.
\end{rem}

\begin{rem}
The conditions of $A$ being unconstrained and $A$ having $c(A)=d$ 
both mean that $A$ is ``non-degenerate''.
Both of them imply small rigidity, according to \cref{ex.unconstrained_rig}
and  part~\ref{i.rig easy} of \cref{t.rig}.
It is important, however, not to confuse the two properties;
in fact, none implies the other.
\end{rem}

\begin{example}\label{ex.toroidal toeplitz again}
Consider again $A$ as in \cref{ex.toroidal toeplitz}.
The eigenvalues of ${\mathrm{Ad}}_A$ are $1$, $\alpha$, and $\alpha^2$, each with multiplicity $3$;
so \cref{p.acyc} gives $\operatorname{acyc} {\mathrm{Ad}}_A = 3$.
So \cref{t.rig} tell us that $\operatorname{rig}_+ {\mathrm{Ad}}_A \le 3$,
which is actually sharp.
\end{example}

\medskip

The proof of part~\ref{i.rig easy} of \cref{t.rig} will be given in \cref{ss.rig easy}
after a few preliminaries (\cref{ss.sudoku,ss.preorder}).
These preliminaries are also used in the proof of the harder part~\ref{i.rig hard},
which will be given in \cref{s.rig proof}.

\subsection{A criterion for transitivity} \label{ss.sudoku}

We will show the transitivity of 
certain spaces of matrices that remotely resemble Toeplitz matrices.  

Let $t$ and $s$ be positive integers.
Let $\mathcal{R}_1$ be a partition of the interval $[1,t] = \{1, \ldots, t\}$ into intervals,
and let $\mathcal{R}_2$ be a partition of $[1,s]$ into intervals.
Let $\mathcal{R}$ be the product partition.
We will be interested in matrices of the following special form:
\begin{equation}\label{e.sudoku}
M=(m_{i,j})_{1\leq i\leq  t \atop 1\leq j\leq s }=
\left(
\begin{array}{ccc|c|ccc}
&   & &   &  &   & \\
& * & & 0 &  & 0 & \\
&   & &   &  &   & \\
\hline 
& 0 & &M_{\mathtt{R}} & & 0 & \\
\hline
&   & &   &  &   & \\
& 0 & & 0 &  & * & \\
&   & &   &  &   & 
\end{array}
\right),
\end{equation}
where ${\mathtt{R}}$ is an element of the product partition $\mathcal{R}$, and $M_{\mathtt{R}}$ is the submatrix 
$(m_{i,j})_{(i,j)\in {\mathtt{R}}}$. 

Let $\Lambda$ be a vector space of $t \times s$ matrices.
For each ${\mathtt{R}} \in \mathcal{R}$, say of size $k \times \ell$, 
we define the following space of matrices:
\begin{equation}\label{e.sudoku space}
\Lambda^{[{\mathtt{R}}]} = \big\{
N \in {\mathrm{Mat}}_{k \times \ell}({\mathbb{C}}) \; ;
\exists \ M \in \Lambda
\text{ of the form \eqref{e.sudoku} with $M_{\mathtt{R}} = N$} 
\big\}.
\end{equation}
We regard $\Lambda$ as a subspace of $\mathcal{L}({\mathbb{C}}^s,{\mathbb{C}}^t)$.
If the rectangle ${\mathtt{R}}$ is $[p, p+k-1] \times [q, q+\ell-1]$,
we regard the space $\Lambda^{[{\mathtt{R}}]}$ as a subspace of  
$$
\mathcal{L} \big( \{0\}^{q-1} \times {\mathbb{C}}^\ell \times \{0\}^{t-q-\ell+1}, \{0\}^{p-1} \times {\mathbb{C}}^k \times \{0\}^{t-p-k+1} \big).
$$ 

\begin{lemma}\label{l.sudoku}
Assume that $\Lambda^{[{\mathtt{R}}]}$ is transitive for each ${\mathtt{R}} \in \mathcal{R}$.
Then $\Lambda$ is transitive.
\end{lemma}

An interesting feature of the lemma which will be useful later is that 
it can be applied recursively.
Before giving the proof of the lemma, 
we illustrate its usefulness by showing the transitivity
of generalized Toeplitz spaces:

\begin{proof}[Proof of \cref{ex.gen Toeplitz}]
Consider the partition of $[1,d]^2$ into $1 \times 1$ ``rectangles''.
If $\Lambda$ is a generalized Toeplitz space then 
$\Lambda^{[{\mathtt{R}}]} = {\mathrm{Mat}}_{1\times 1}({\mathbb{C}}) = {\mathbb{C}}$ for each rectangle ${\mathtt{R}}$.
These are transitive spaces, so \cref{l.sudoku} implies that $\Lambda$
is transitive.
\end{proof}

Before proving \cref{l.sudoku}, notice the following dual characterization of transitivity, whose proof is immediate:

\begin{lemma}\label{l.duality}
A subspace $\Lambda \subset \mathcal{L}({\mathbb{C}}^s,{\mathbb{C}}^t)$ is transitive iff
for any non-zero vector $u \in {\mathbb{C}}^s$ and any non-zero linear functional $\phi \in ({\mathbb{C}}^t)^*$
there exists $M \in \Lambda$ such that $\phi (M \cdot u) \neq 0$.
\end{lemma}

\begin{proof}[Proof of \cref{l.sudoku}]
Take any non-zero vector $u = (u_1,\ldots,u_s)$ in ${\mathbb{C}}^s$
and a non-zero functional $\phi(v_1,\ldots,v_t) = \sum_{i=1}^t \phi_i v_i$ in $({\mathbb{C}}^t)^*$.
By \cref{l.duality}, we need to show that there exists $M = (x_{ij}) \in \Lambda$ such that 
\begin{equation}\label{e.sum}
\phi (M \cdot u) = \sum_{i=1}^t \sum_{j=1}^s \phi_i x_{ij} u_j
\end{equation}
is non-zero.

Let $j_0$ be the least index such that $u_{j_0} \neq 0$,
and let $i_0$ be the greatest index such that $\phi_{i_0} \neq 0$.
Let ${\mathtt{R}}$ be the element of $\mathcal{R}$ that contains $(i_0,j_0)$. 
Notice that if $M$ is of the form \eqref{e.sudoku} then
the $(i,j)$-entries of $M$ that are above left (resp.\ below right) of ${\mathtt{R}}$
do not contribute to the sum \eqref{e.sum}, because $u_j$ (resp.\ $\phi_i$) vanishes.
That is,
$\phi (M \cdot u)$ depends only on $M_{\mathtt{R}}$ 
and is given by $\sum_{(i,j)\in {\mathtt{R}}} \phi_i x_{ij} u_j$;
Since $\Lambda^{[{\mathtt{R}}]}$ is transitive, by \cref{l.duality}
there is a choice of a matrix $M \in \Lambda$ of the form \eqref{e.sudoku}
so that $\phi (M \cdot u) \neq 0$.
So we are done.
\end{proof}

\subsection{Preorder in the complex plane}\label{ss.preorder}

We consider the set ${\mathbb{C}}_* / T$ of equivalence classes 
of the relation~\eqref{e.equiv rel}.
Since $T$ is the torsion subgroup of ${\mathbb{C}}_*$,
the quotient ${\mathbb{C}}_* / T$ is an abelian torsion-free group.

\begin{prop}
There exists a multiplication-invariant total order $\preccurlyeq$  on ${\mathbb{C}}_* / T$. 
\end{prop}

The proposition follows from a result of Levi~\cite{Levi},
but nevertheless let us give a direct proof:

\begin{proof}
There is an isomorphism between ${\mathbb{R}} \oplus ({\mathbb{R}} / {\mathbb{Q}})$ and ${\mathbb{C}}_*/T$, namely
$(x,y) \mapsto \exp(x + 2\pi i y)$. 
So it suffices to find a multiplication-invariant order in ${\mathbb{R}}/{\mathbb{Q}}$
(and then take the lexicographic order).
Take a Hamel basis $B$ of the ${\mathbb{Q}}$-vector space ${\mathbb{R}}$ so that $1 \in B$.
Then ${\mathbb{R}}/{\mathbb{Q}}$ is a direct sum of abelian groups $\bigoplus_{x\in B, \ x \neq 1} x{\mathbb{Q}}$.
Order each $x{\mathbb{Q}}$ in the usual way and take any total order on $B$.
Then the induced lexicographic order on ${\mathbb{R}}/{\mathbb{Q}}$ is multiplication-invariant,
and the proof is concluded.
\end{proof}

Let $[z]\in {\mathbb{C}}_*/T$ denote the equivalence class of $z\in {\mathbb{C}}_*$.
Let us extend the notation, writing $z \preccurlyeq z'$ if $[z] \preccurlyeq [z']$.
Then $\preccurlyeq$ becomes a multiplication-invariant total preorder on ${\mathbb{C}}_*$ 
that induces the equivalence relation $\asymp$.
In other words, for all $z$, $z'$, $z'' \in {\mathbb{C}}_*$ we have:
\begin{itemize}
	\item $z \preccurlyeq z'$ or $z' \preccurlyeq z$;
	\item $z \preccurlyeq z'$ and $z' \preccurlyeq z$ $\Longleftrightarrow$ $z \asymp z'$;
	\item $z \preccurlyeq z'$ and $z' \preccurlyeq z''$ $\Longrightarrow$ $z \preccurlyeq z''$;
	\item $z \preccurlyeq z'$ $\Longrightarrow$ $z z'' \preccurlyeq z' z''$.
\end{itemize}
It follows that:
\begin{itemize}
	\item $z \preccurlyeq z'$ $\Longrightarrow$ $(z')^{-1} \preccurlyeq z^{-1}$.
\end{itemize}
We write $z \prec z'$ when $z \preccurlyeq z'$ and $z \not \asymp z'$.

\subsection{Proof of the easy part of \cref{t.rig}}\label{ss.rig easy}

\begin{proof}[Proof of part~\ref{i.rig easy} of \cref{t.rig}]
If $c(A)=d$ then in particular all eigenvalues are different and so the matrix $A$ is diagonalizable.
So with a change of basis we can assume that
$A = {\mathrm{Diag}}(\lambda_1, \dots, \lambda_d)$.
We can also assume that the eigenvalues are 
increasing with respect to the preorder introduced in \cref{ss.preorder}:
$$
\lambda_1 \prec \lambda_2 \prec \cdots \prec \lambda_d \, .
$$

Fix any matrix $B$ with only nonzero entries,
and consider the space $\Lambda = {\mathfrak{R}}_{{\mathrm{Ad}}_A}(B)$,
which is described by \eqref{e.poly entry}.
We will use \cref{l.sudoku} to show that $\Lambda$ is transitive.
Let $\mathcal{R}$ be the partition of $[1,d]^2$ into $1 \times 1$ rectangles.
Given a cell ${\mathtt{R}} = \{(i_0,j_0)\} \in \mathcal{R}$ and a coefficient $t \in {\mathbb{C}}$,
there exists a polynomial $f$ such that 
$f(\lambda_i \lambda_j^{-1})$
equals $t$ if $\lambda_i \lambda_j^{-1} = \lambda_{i_0} \lambda_{j_0}^{-1}$
and equals $0$ otherwise.
Because the eigenvalues are ordered,
$M = f({\mathrm{Ad}}_A)\cdot B$ is a matrix in $\Lambda$ of the form \eqref{e.sudoku}.
Also, $M_{\mathtt{R}} = (t)$.
So $\Lambda^{[{\mathtt{R}}]} = {\mathbb{C}}$, which is transitive.
This shows that $\operatorname{rig} {\mathrm{Ad}}_A = 1$, and  $\operatorname{rig}_+ {\mathrm{Ad}}_A \leq 2$. 
Thus, as $d\geq 2$, we have $\operatorname{rig}_+ {\mathrm{Ad}}_A = 2$.
\end{proof}

\section{Proof of the hard part of the rigidity estimate}\label{s.rig proof}

This section is wholly devoted to proving part~\ref{i.rig hard} of \cref{t.rig}.
In the course of the proof we need to introduce some terminology 
and to establish several intermediate results.
None of these are used in the rest of the paper, apart form a simple consequence,
which is \cref{r.jordan type and acyc}.

\subsection{The normal form}\label{ss.normal form}

Let $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.
In order to describe the estimate on $\operatorname{rig}_+ {\mathrm{Ad}}_A$, 
we need to put $A$ in a certain normal form, which we now explain. 
Fix a preorder $\preccurlyeq$ on ${\mathbb{C}}_*$ as in \cref{ss.preorder}.

\medskip

List the eigenvalues of $A$ without repetitions as
\begin{equation}\label{e.order lambdas}
\lambda_1 \preccurlyeq \cdots \preccurlyeq \lambda_r
\end{equation}
Write each eigenvalue in polar coordinates:
$$
\lambda_k = \rho_k \exp ( \theta_k \sqrt{-1}), \quad
\text{where } \rho_k > 0 \text { and } 0 \leq \theta_k <  2 \pi.
$$
Up to reordering, we may assume
$$
\left.
\begin{array}{l}
\lambda_k \asymp \lambda_\ell \\
k < \ell
\end{array}
\right\} \ \Rightarrow \ 
\theta_k < \theta_\ell \, .
$$

With a change of basis, we can assume that 
$A$ has Jordan form: 
\begin{equation}\label{e.order blocks}
A =
\begin{pmatrix}
A_1  &        &     \\
     & \ddots &     \\
     &        & A_r      
\end{pmatrix}, \quad 
A_k =
\begin{pmatrix}
J_{t_{k,1}}(\lambda_k) &        &                         \\
                       & \ddots &                         \\
                       &        & J_{t_{k,\tau_k}}(\lambda_k)      
\end{pmatrix},
\end{equation}
where $t_{k,1} + \cdots + t_{k,\tau_k} = s_k$ is the multiplicity of the eigenvalue $\lambda_k$,
and $J_t(\lambda)$ is the following $t \times t$ Jordan block:
\begin{equation}\label{e.block}
J_t(\lambda) = 
\left(
\vcenter{
\xymatrix @-1.5pc {
\lambda \ar@{.}[rrdd]  & 1 \ar@{.}[rd] &         \\
                 &               & 1       \\
                 &               & \lambda \\
}}
\right).
\end{equation}
The matrix $A$ will be fixed from now on.

\subsection{Rectangular partitions}\label{ss.geo defs}

This subsection contains several definitions that will be fundamental in all 
arguments until the end of the section.
We will define certain subregions of the set $\{1,\dots,d\}^2$ of matrix entry positions
that depend on the normal form of the matrix $A$.
Later we will see they are related to ${\mathrm{Ad}}_A$-invariant subspaces.
Those regions will be $\rm c$-rectangles, $\rm e$-rectangles, and $\rm j$-rectangles 
(where $\rm c$ stands for classes of eigenvalues, $\rm e$ for eigenvalues and $\rm j$ for Jordan blocks).
Regions will have some numerical attributes (banners and weights) coming from their geometry and from the eigenvalues of $A$ they will be associated to. Those attributes will be related to 
numerical invariants of ${\mathrm{Ad}}_A$ (eigenvalues and geometric multiplicities), but we use different names so that we remember their geometric meaning and so that they are not mistaken for the corresponding invariants of $A$. We also introduce positional attributes of the regions (arguments and latitudes) which will be useful fundamental later in the proofs of our rigidity estimates.

\medskip

Recall $A$ is a matrix in normal form as explained in \cref{ss.normal form}.
Define three partitions ${\mathcal{P}}_{\rm c}$, ${\mathcal{P}}_{\rm e}$, ${\mathcal{P}}_{\rm j}$ of the set $[1,d] = \{1,\dots,d\}$ into intervals:
\begin{itemize}
\item The partition ${\mathcal{P}}_{\rm c}$ corresponds to equivalence classes of eigenvalues 
under the relation $\asymp$, that is, 
the right endpoints of its atoms are the numbers 
$s_1 + \dots + s_k$
where $k=r$ or $k$ is such that $\lambda_k \prec \lambda_{k+1}$.
\item The partition ${\mathcal{P}}_{\rm e}$ corresponds to eigenvalues:
the right endpoints of its atoms are the numbers 
$s_1 + \dots + s_k$, where $1 \le k \le r$.
So ${\mathcal{P}}_{\rm e}$ refines ${\mathcal{P}}_{\rm c}$.
\item The partition ${\mathcal{P}}_{\rm j}$ corresponds to Jordan blocks:
the right endpoints of its atoms are the numbers 
$s_1 + \dots + s_{k-1} + t_{k,1} + \dots + t_{k,\ell}$, where $1 \le k \le r$
and $1 \le \ell \le \tau_k$.
So ${\mathcal{P}}_{\rm j}$ refines ${\mathcal{P}}_{\rm e}$.
\end{itemize}
For $*= \rm c$, $\rm e$, $\rm j$, let 
${\mathcal{P}}_*^2$ be the partition of the square $[1,d]^2$ 
into rectangles that are products of atoms of ${\mathcal{P}}_*$.
The elements of ${\mathcal{P}}_{\rm c}^2$ are called \emph{$\rm c$-rectangles}, 
the elements of ${\mathcal{P}}_{\rm e}^2$ are called \emph{$\rm e$-rectangles},
and elements of ${\mathcal{P}}_{\rm j}^2$ are called \emph{$\rm j$-rectangles}.
Thus the square $[1,d]^2$ is a disjoint union $\rm c$-rectangles,
each of them is a disjoint union of $\rm e$-rectangles,
each of them is a disjoint union of $\rm j$-rectangles.

\begin{example}\label{ex.big matrix}
Suppose $d=17$,
$A$ has $r=5$ eigenvalues 
$$
\lambda_1 =  \exp { \tfrac{1}{2}\pi i} , \quad
\lambda_2 =  \exp { \tfrac{7}{6}\pi i} , \quad
\lambda_3 =  \exp {\tfrac{11}{6}\pi i} , \quad
\lambda_4 = 2\exp { \tfrac{1}{6}\pi i} , \quad
\lambda_5 = 2\exp { \tfrac{5}{6}\pi i} 
$$
with $\lambda_1 \asymp \lambda_2 \asymp \lambda_3 \prec \lambda_4 \asymp \lambda_5$
and  respective Jordan blocks of sizes 
$4$, $2$, $1$; $3$, $2$;  $2$;  $2$; $1$. 
Then there are $4$ $\rm c$-rectangles, $25$ $\rm e$-rectangles, and $64$ $\rm j$-rectangles.
See \cref{f.big matrix}. 
\begin{figure}[hbt]  
\setlength{\unitlength}{.6cm}
\begin{picture}(17,17)(0,-1)  

\thicklines  
\put(0,-1){\framebox(17,0)} 
\put(0,2){\framebox(17,0)}
\put(0,16){\framebox(17,0)} 
\put(0,-1){\framebox(0,17)}  
\put(14,-1){\framebox(0,17)}
\put(17,-1){\framebox(0,17)} 

\thinlines
\put(0,0){\line(1,0){17}}
\put(0,4){\line(1,0){17}}
\put(0,9){\line(1,0){17}}
\put(7,-1){\line(0,1){17}}
\put(12,-1){\line(0,1){17}}
\put(16,-1){\line(0,1){17}}

\put(0,6){\dashbox{.2}(17,0)}
\put(0,10){\dashbox{.2}(17,0)}
\put(0,12){\dashbox{.2}(17,0)}
\put(4,-1){\dashbox{.2}(0,17)}
\put(6,-1){\dashbox{.2}(0,17)}
\put(10,-1){\dashbox{.2}(0,17)}

\put( 7.2,13.1){\tiny wgt.~$3$}
\put( 7.2,12.6){\tiny lat.~$0$}

\put(10.2,13.1){\tiny wgt.~$2$}
\put(10.2,12.6){\tiny lat.~$1$}

\put( 7.2,11.1){\tiny wgt.~$2$}
\put( 7.2,10.6){\tiny lat.~$-1$}

\put(10.2,11.1){\tiny wgt.~$2$}
\put(10.2,10.6){\tiny lat.~$0$}

\put( 7.2, 9.6){\tiny wgt.~$1$}
\put( 7.2, 9.1){\tiny lat.~$-2$}

\put(10.2, 9.6){\tiny wgt.~$1$}
\put(10.2, 9.1){\tiny lat.~$-1$}

\put(   0.05,15.4){{\footnotesize  $15$\,}\framebox(.5,.35){\tiny $\bullet${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(   7.05, 8.4){{\footnotesize   $9$\,}\framebox(.5,.35){\tiny $\bullet${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  12.05, 3.4){{\footnotesize   $2$\,}\framebox(.5,.35){\tiny $\bullet${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(   7.05,15.4){{\footnotesize  $11$\,}\framebox(.5,.35){\tiny $\bullet${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(  12.05, 8.4){{\footnotesize   $4$\,}\framebox(.5,.35){\tiny $\bullet${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(   0.05, 3.4){{\footnotesize   $5$\,}\framebox(.5,.35){\tiny $\bullet${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(   0.05, 2.7){{\footnotesize $\ominus$}}
\put(  12.05,15.4){{\footnotesize   $5$\,}\framebox(.5,.35){\tiny $\bullet${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(   0.05, 8.4){{\footnotesize  $11$\,}\framebox(.5,.35){\tiny $\bullet${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(   0.05, 7.7){{\footnotesize $\ominus$}}
\put(   7.05, 3.4){{\footnotesize   $4$\,}\framebox(.5,.35){\tiny $\bullet${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(   7.05, 2.7){{\footnotesize $\ominus$}}

\put(  14.05, 1.4){{\footnotesize   $2$\,}\framebox(.5,.35){\tiny $\bullet${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  16.05, -.6){{\footnotesize   $1$\,}\framebox(.5,.35){\tiny $\bullet${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  14.05, -.6){{\footnotesize   $1$\,}\framebox(.5,.35){\tiny $\bullet${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(  15.05, -.6){{\footnotesize $\ominus$}}
\put(  16.05, 1.4){{\footnotesize   $1$\,}\framebox(.5,.35){\tiny $\bullet${\sf b}}\put(-.51,0){\line(0,-1){.3}}}

\put(  14.05,15.4){{\footnotesize   $5$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  14.05,14.7){{\footnotesize $\ominus$}}
\put(  16.05,15.4){{\footnotesize   $3$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(  14.05, 8.4){{\footnotesize   $4$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(  14.05, 7.7){{\footnotesize $\ominus$}}
\put(  16.05, 8.4){{\footnotesize   $2$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  16.05, 7.7){{\footnotesize $\ominus$}}
\put(  14.05, 3.4){{\footnotesize   $2$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(  14.05, 2.7){{\footnotesize $\ominus$}}
\put(  16.05, 3.4){{\footnotesize   $1$\,}\framebox(.5,.35){\tiny $\mathord{\Uparrow}${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(  16.05, 2.7){{\footnotesize $\ominus$}}

\put(   0.05, 1.4){{\footnotesize $5$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(   0.05, -.6){{\footnotesize $3$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(   1.05, -.6){{\footnotesize $\ominus$}}
\put(   7.05, 1.4){{\footnotesize $4$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf b}}\put(-.51,0){\line(0,-1){.3}}}
\put(   7.05, -.6){{\footnotesize $2$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf a}}\put(-.51,0){\line(0,-1){.3}}}
\put(  12.05, 1.4){{\footnotesize $2$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf c}}\put(-.51,0){\line(0,-1){.3}}}
\put(  12.05, -.6){{\footnotesize $1$\,}\framebox(.5,.35){\tiny $\mathord{\Downarrow}${\sf b}}\put(-.51,0){\line(0,-1){.3}}}

\end{picture}
\caption{\footnotesize The partitions of the square $[1,d]^2$ corresponding to \cref{ex.big matrix}.
Thick (resp., thin, dashed) lines represent $\rm c$-rectangles (resp., $\rm e$-, $\rm j$-)  borders.
Weight and latitude of each $\rm j$-rectangle inside a selected $\rm e$-rectangle are indicated.
The weight of each $\rm e$-rectangle is recorded in its upper left corner, along with 
a symbolic representation of its banner.
There are three banner classes
($\bullet = [1]$, $\mathord{\Downarrow}=[2]$ and $\mathord{\Uparrow}=[1/2]$),
each of them with $3$ different banners. The $\rm e$-rectangles with negative arguments are marked with $\ominus$.
} 
\label{f.big matrix}
\end{figure}
\end{example}

For each $\rm e$-rectangle we define its \emph{row eigenvalue} and its \emph{column eigenvalue}
in the obvious way: 
If an $\rm e$-rectangle ${\mathtt{E}}$ equals $I_k \times I_\ell$ where $I_k$ and $I_\ell$
are intervals with right endpoints $s_1 + \dots + s_k$ and $s_1 + \dots + s_\ell$, respectively,
then the row eigenvalue of ${\mathtt{E}}$ is $\lambda_k$ and the column eigenvalue of ${\mathtt{E}}$ is $\lambda_\ell$.
The row and column eigenvalues of a $\rm j$-rectangle ${\mathtt{J}}$ are defined respectively
as the row and column eigenvalues of the $\rm e$-rectangle that contains it.

Let ${\mathtt{E}}$ be an $\rm e$-rectangle with row eigenvalue $\lambda_k$ and column eigenvalue $\lambda_\ell$.
The \emph{banner} of ${\mathtt{E}}$ is defined by $\lambda_k^{-1}\lambda_\ell$. The \emph{argument} of the $\rm e$-rectangle is the quantity $\theta_\ell - \theta_k \in (-2\pi,2\pi)$. 
It coincides modulo $2\pi$ with the argument of the banner, but it contains more information than the argument of the banner. 

Each $\rm j$-rectangle ${\mathtt{J}}$ has an address of the type ``$i^\text{th}$ row, $j^\text{th}$ column, $\rm e$-rectangle ${\mathtt{E}}$''; then the \emph{latitude} of the $\rm j$-rectangle ${\mathtt{J}}$ within the $\rm e$-rectangle ${\mathtt{E}}$ is defined as $j-i$. See an example in \cref{f.big matrix}.

If two $\rm e$-rectangles lie in the same $\rm c$-rectangle then their banners are equivalent mod~$T$.
Thus every $\rm c$-rectangle has a well-defined \emph{banner class} in ${\mathbb{C}}^*/T$.

If a $\rm j$-rectangle, $\rm e$-rectangle, or $\rm c$-rectangle 
intersects the diagonal $\{(1,1), \dots, (d,d)\}$
then we call it \emph{equatorial}.
Equatorial regions are always square.
Thus every equatorial $\rm e$-rectangle has banner $1$.

The \emph{weight} of a $\rm j$-rectangle is defined as the minimum of its sides.
The weight of a union $R$ of $\rm j$-rectangles in $[1,d]^2$ is defined as
the sum of the weights of those $\rm j$-rectangles. We denote it by $\operatorname{wgt} R$.
We can in particular consider the weights of $\rm e$ and $\rm c$-rectangles, and of the complete square $[1,d]^2$.
\medskip

Let us notice some facts on the location of the banners
(which will be useful to apply \cref{l.sudoku}):

\begin{lemma}\label{l.geo}
Let ${\mathtt{E}}$ be an $\rm e$-rectangle in a $\rm c$-rectangle ${\mathtt{C}}$. Consider the divisions of the square $[1,d]^2$ and the $\rm c$-rectangle ${\mathtt{C}}$ as in \cref{f.divisions}. 

\begin{figure}[hbt]  
\setlength{\unitlength}{.35cm}
\begin{picture}(27, 10)(0,0)

\linethickness{0.4mm}
\put(0,0){\framebox(10,10)} 
\put(10.5,0){$[1,d]^2$}

\put(16,1.5){\framebox(10,7.5)} 
\put(26.5,1.5){${\mathtt{C}}$}

\dottedline{.25}(6,8)(16,9)
\dottedline{.25}(6,6.5)(16,1.5)

\linethickness{0.1mm}
\put(0,0  ){\framebox(6,6.5){$ $}}  
\put(0,6.5){\framebox(6,1.5){$ $}}  
\put(0,8  ){\framebox(6,2  ){$\times$}}     
\put(6,0  ){\framebox(2,6.5){$ $}}  
\put(6,6.5){\framebox(2,1.5){${\mathtt{C}}$}}     
\put(6,8  ){\framebox(2,2  ){$ $}}  
\put(8,0  ){\framebox(2,6.5){$\times$}}     
\put(8,6.5){\framebox(2,1.5){$ $}}  
\put(8,8  ){\framebox(2,2  ){$ $}}  

\linethickness{0.1mm}
\put(16,1.5 ){\framebox(3,3.75){$ $}}  
\put(16,5.25){\framebox(3,1.5 ){$ $}}         
\put(16,6.75){\framebox(3,2.25){\large$*$}}      
\put(19,1.5 ){\framebox(2,3.75){$ $}}         
\put(19,5.25){\framebox(2,1.5 ){${\mathtt{E}}$}}       
\put(19,6.75){\framebox(2,2.25){$ $}}         
\put(21,1.5){\framebox(5,3.75){\large$*$}}        
\put(21,5.25){\framebox(5,1.5 ){$ $}}         
\put(21,6.75){\framebox(5,2.25){$ $}}   

\end{picture}
\caption{\footnotesize The divisions of $[1,d]^2$ and ${\mathtt{C}}$ in \cref{l.geo}.} 
\label{f.divisions}
\end{figure}

Let $\beta$ be the banner of the $\rm e$-rectangle ${\mathtt{E}}$, and let $[\beta]$ be the banner class of the 
$\rm c$-rectangle ${\mathtt{C}}$.
Then: 
\begin{enumerate}
\item\label{i.geo0} All the $\rm c$-rectangles with banner class $[\beta]$ are inside the rectangles marked with $\times$.
\item\label{i.geo2} If the $\rm e$-rectangle ${\mathtt{E}}$ has nonnegative (resp.\ negative) argument 
then the all the $\rm e$-rectangles with nonnegative (resp.\ negative) argument and with same banner $\beta$ are inside the rectangles marked with $*$.
\end{enumerate}
\end{lemma}

\begin{proof}
In view of the ordering of the eigenvalues \eqref{e.order lambdas},
the banner class increases strictly (with respect to the order $\prec$, of course)
when we move rightwards or upwards to another $\rm c$-rectangle.
So Claim (\ref{i.geo0}) follows.

The argument of an $\rm e$-rectangle takes values in the interval $(-2\pi,2\pi)$.
It increases strictly by moving rightwards or upwards inside ${\mathtt{C}}$. 
If two $\rm e$-rectangles in the same $\rm c$-rectangle have both nonnegative or negative argument then 
they have the same banner if and only if they have the same argument.
So Claim (\ref{i.geo2}) follows.
\end{proof}

\subsection{The action of the adjoint of $A$}\label{ss.ad_geo}

Given any $d\times d$ matrix $X = (x_{i,j})$ and 
a $\rm j$-rectangle, $\rm e$-rectangle or $\rm c$-rectangle ${\mathtt{R}} = [p,p+t-1] \times [q, q+s-1]$
we define the \emph{submatrix} of $X$ corresponding to ${\mathtt{R}}$ 
as $(x_{i,j})_{(i,j) \in {\mathtt{R}}}$.
We regard the space of ${\mathtt{R}}$-submatrices as 
$\mathcal{L} \big( \{0\}^{q-1} \times {\mathbb{C}}^s \times \{0\}^{d-q-s+1} , \{0\}^{p-1} \times {\mathbb{C}}^t \times \{0\}^{d-p-t+1} \big)$,
or as the set of $d \times d$ matrices whose entries outside ${\mathtt{R}}$ are all zero.
Such spaces are denoted by ${\mathtt{R}}^\square$, and are invariant under ${\mathrm{Ad}}_A$.
Indeed, if ${\mathtt{R}} = {\mathtt{J}}$ is a $\rm j$-rectangle 
then identifying ${\mathtt{J}}^\square$ with ${\mathrm{Mat}}_{t\times s}({\mathbb{C}})$, the action
of ${\mathrm{Ad}}_A | {\mathtt{J}}^\square$ is given by
$$
X \mapsto J_{t}(\lambda_k) \cdot X \cdot J_{s}(\lambda_\ell)^{-1},
$$
where $\lambda_k$ and $\lambda_\ell$ 
are respectively the row and the column eigenvalues of ${\mathtt{J}}$
and $J$ denotes Jordan blocks as defined by \eqref{e.block}.

\medskip

\begin{lemma}\label{l.min}
For each $\rm j$-rectangle ${\mathtt{J}}$, 
the only eigenvalue of ${\mathrm{Ad}}_A | {\mathtt{J}}^\square$ is the banner of the $\rm e$-rectangle that contains ${\mathtt{J}}$.
Moreover, the geometric multiplicity of the eigenvalue is the weight of the $\rm j$-rectangle.
\end{lemma}

\begin{proof}
The matrix of the the linear operator ${\mathrm{Ad}}_A | {\mathtt{J}}^\square$
can be described using the Kronecker product: see \cite[Lemma~4.3.1]{HJ}.
The Jordan form of this operator is then described by \cite[Theorem~4.3.17(a)]{HJ}.
The assertions of the lemma follow.
\end{proof}

Some immediate consequences are the following:
\begin{itemize}
\item The eigenvalues of ${\mathrm{Ad}}_A$ are the banners of $\rm e$-rectangles.
\item The geometric multiplicity of the eigenvalue $\beta$ for ${\mathrm{Ad}}_A$ is the total weight of $\rm e$-rectangles of banner~$\beta$.
\end{itemize}

\medskip

If ${\mathtt{R}}$ is an equatorial $\rm j$-rectangle, $\rm e$-rectangle, or $\rm c$-rectangle
we will refer to 
the $d\times d$-matrix in ${\mathtt{R}}^\square$ whose ${\mathtt{R}}$-submatrix is the identity
as the \emph{identity on ${\mathtt{R}}^\square$}.
The following observation will be useful:

\begin{lemma}\label{l.1x1}
If ${\mathtt{J}}$ is an equatorial $\rm j$-rectangle then the identity on ${\mathtt{J}}^\square$
is an eigenvector of the operator ${\mathrm{Ad}}_A | {\mathtt{J}}^\square$ corresponding to a Jordan block of size $1 \times 1$. 
\end{lemma}

\begin{proof}
Suppose ${\mathtt{J}}$ has size $t \times t$ and row (or column) eigenvalue $\lambda$.
Assume that the claim is false.
This means that there exists a matrix $X \in {\mathrm{Mat}}_{t \times t}({\mathbb{C}})$ such that $J_t(\lambda) X J_t(\lambda)^{-1} = X + {\mathrm{Id}}$, which is impossible because $X$ and $X + {\mathrm{Id}}$ have different spectra.
\end{proof}

\subsection{Rigidity estimates for $\rm j$-rectangles and $\rm e$-rectangles}

\begin{lemma}\label{l.rig district}
For any $\rm j$-rectangle ${\mathtt{J}}$, we have $\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{J}}^\square)  \le \operatorname{wgt} {\mathtt{J}}$.
\end{lemma}

\begin{proof}
By \cref{l.min} (and \cref{p.acyc}), 
${\mathrm{Ad}}_A|{\mathtt{J}}^\square$ has acyclicity $n = \operatorname{wgt} {\mathtt{J}}$,
that is, there are matrices $X_1$, \dots, $X_n \in {\mathtt{J}}^\square$ 
such that 
${\mathfrak{R}}_{{\mathrm{Ad}}_A}(X_1, \dots, X_n)$ is the whole ${\mathtt{J}}^\square$
(and, in particular, is transitive in ${\mathtt{J}}^\square$).
So $\operatorname{rig} ({\mathrm{Ad}}_A|{\mathtt{J}}^\square) \le n$,
which proves the lemma for non-equatorial $\rm j$-rectangles.

If ${\mathtt{J}}$ is an equatorial $\rm j$-rectangle then, 
by \cref{l.1x1}, ${\mathtt{J}}^\square$ splits invariantly into two subspaces, one
of them spanned by the the identity matrix on ${\mathtt{J}}^\square$.
So we can choose the matrices $X_i$ above so that $X_1$ is the identity.
This shows that $\operatorname{rig}_+ ({\mathrm{Ad}}_A|{\mathtt{J}}^\square) \le n$.
\end{proof}

In all that follows, we adopt the convention $\max {\varnothing} = 0$.

\begin{lemma}\label{l.rig city} 
For any $\rm e$-rectangle ${\mathtt{E}}$,
$$
\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}^\square) \le
\sum_{\text{\rm{$\ell$ latitude}}} 
\max_{\text{\rm{${\mathtt{J}} \subset {\mathtt{E}}$ is a $\rm j$-rectangle}} \atop \text{\rm{with latitude $\ell$}}}
\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{J}}^\square) \, .
$$
\end{lemma}

\begin{proof}
For each $\rm j$-rectangle ${\mathtt{J}}$ contained in ${\mathtt{E}}$,
let $r({\mathtt{J}}) = \operatorname{rig}_+ ({\mathrm{Ad}}_A|{\mathtt{J}}^\square)$.
Take matrices $X_{{\mathtt{J}}, 1}$, \dots, $X_{{\mathtt{J}}, r({\mathtt{J}})}$
such that 
$\Lambda_{\mathtt{J}} := {\mathfrak{R}}_{{\mathrm{Ad}}_A} \big( X_{{\mathtt{J}}, 1}, \ldots, X_{{\mathtt{J}}, r({\mathtt{J}})} \big)$
is a transitive subspace of ${\mathtt{J}}^\square$,
and $X_{{\mathtt{J}},1}$ is the identity matrix in ${\mathtt{J}}^\square$ if ${\mathtt{J}}$ is an equatorial ${\rm j}$-rectangle.
Define $X_{{\mathtt{J}}, i} = 0$ for $i>r({\mathtt{J}})$.		
For each latitude $\ell$, 
let $n_\ell$ be the maximum of $r({\mathtt{J}})$
over the $\rm j$-rectangles ${\mathtt{J}}$ of ${\mathtt{E}}$ with latitude $\ell$,
and let
$$
Y_{\ell, i} = \sum_{\text{\rm{${\mathtt{J}} \subset {\mathtt{E}}$ is a $\rm j$-rectangle}} \atop \text{\rm{with latitude $\ell$}}} 
X_{{\mathtt{J}}, i} \, ,
\quad \text{for $1 \le i \le n_\ell$.}
$$
Notice that if ${\mathtt{E}}$ is an equatorial $\rm e$-rectangle then $Y_{0,1}$ is 
the identity matrix in ${\mathtt{E}}^\square$.
Consider the space 
$$
\Delta = {\mathfrak{R}}_{{\mathrm{Ad}}_A} \big\{Y_{\ell,i} ; \; \ell \text{ is a latitude, } 1 \le i \le n_\ell \big\}.
$$

We claim that for every $\rm j$-rectangle ${\mathtt{J}}$ in ${\mathtt{E}}$ and for every $M \in \Lambda_{\mathtt{J}}$,
we can find some $N \in \Delta$ with the following properties:
\begin{itemize} 
\item the submatrix $N_{\mathtt{J}}$ equals $M$;
\item for every $\rm j$-rectangle ${\mathtt{J}}'$ in ${\mathtt{E}}$ that has a different latitude 
than ${\mathtt{J}}$, the submatrix $N_{{\mathtt{J}}'}$ vanishes.  
\end{itemize}
Indeed, if 
$M = \sum_{i=1}^{r({\mathtt{J}})} f_i({\mathrm{Ad}}_A) X_{{\mathtt{J}},i}$ for certain polynomials $f_i$,
we simply take 
$N = \sum_{i=1}^{r({\mathtt{J}})} f_i({\mathrm{Ad}}_A) Y_{\ell,i}$, where $\ell$ is the latitude of ${\mathtt{J}}$.

In notation~\eqref{e.sudoku space}, 
the claim we have just proved means that 
$\Delta^{[{\mathtt{J}}]} \supset \Lambda_{\mathtt{J}}$.
So we can apply \cref{l.sudoku} and conclude that
$\Delta$ is a transitive subspace of ${\mathtt{E}}^\square$.
Therefore $\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}^\square) \le \sum n_\ell$,
as we wanted to show.
\end{proof}

\begin{example}\label{ex.continued}
Using \cref{l.rig district,l.rig city},
we see that the $\rm e$-rectangle ${\mathtt{E}}$ 
whose $\rm j$-rectangle weights are indicated in \cref{f.big matrix}
has $\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}^\square) \le 5$. 
\end{example}

In fact, we will not use \cref{l.rig district,l.rig city} directly, 
but only the following immediate consequence:
\begin{lemma}\label{l.crude rig city}
For every $\rm e$-rectangle ${\mathtt{E}}$ we have $\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}^\square) \le \operatorname{wgt} {\mathtt{E}}$.
The inequality is strict if ${\mathtt{E}}$ has more than one row of $\rm j$-rectangles and more that one column of $\rm j$-rectangles.
\end{lemma}

\subsection{Comparison of weights}

If ${\mathtt{R}}$ is a $\rm j$-rectangle, $\rm e$-rectangle or $\rm c$-rectangle,
we define its \emph{row projection} ${\pi_\mathrm{r}}({\mathtt{R}})$
as the unique equatorial $\rm j$-rectangle, $\rm e$-rectangle or $\rm c$-rectangle (respectively)
that is in the same row as ${\mathtt{R}}$.
Analogously, we define the \emph{column projection} ${\pi_\mathrm{c}}({\mathtt{R}})$.

\begin{lemma}\label{l.city proj}
For any $\rm e$-rectangle ${\mathtt{E}}$, we have
$$
\operatorname{wgt} {\mathtt{E}} \le \frac{\operatorname{wgt} {\pi_\mathrm{r}}({\mathtt{E}}) + \operatorname{wgt} {\pi_\mathrm{c}}({\mathtt{E}})}{2} \, .
$$
Moreover, if equality holds then 
the number of rows of $\rm j$-rectangles for ${\mathtt{E}}$ equals the number of columns of $\rm j$-rectangles.
\end{lemma}

This is a clear consequence of the abstract lemma below,
taking $x_\alpha$, $\alpha\in F_0$ (resp.\ $\alpha\in F_1$) 
as the sequence of heights (resp.\ widths) of $\rm j$-rectangles in ${\mathtt{E}}$,
counting repetitions.

\begin{lemma}\label{l.combin}
Let $F$ be a nonempty finite set,
and let $x_\alpha$ be positive numbers indexed by $\alpha \in F$.
Take any partition $F = F_0 \sqcup F_1$, where $\sqcup$ stands for disjoint union.
For $\epsilon$, $\delta \in \{0,1\}$, let
$$
\Sigma_{\epsilon\delta} = \sum_{(\alpha,\beta) \in F_\epsilon \times F_\delta} \min(x_\alpha, x_\beta) \, .
$$
Then
$$
\Sigma_{01} = \Sigma_{10} \le \frac{\Sigma_{00} + \Sigma_{11}}{2} \, .
$$
Moreover, equality implies that $F_0$ and $F_1$ have the same cardinality.
\end{lemma}

\begin{proof}
We will in fact prove the stronger fact:
\begin{equation}\label{e.induction}
\Sigma_{00} - 2\Sigma_{01} + \Sigma_{11} \ge \big(|F_0| -|F_1|\big)^2 \min_{\alpha \in F} x_\alpha \, ,
\end{equation}
where $|\mathord{\cdot}|$ denotes set cardinality.
The proof is by induction on $|F|$.
It clearly holds for $|F|=1$.
Fix some $n$ and assume that \eqref{e.induction} always holds when $|F| = n$.
Take a set $F$ with $|F|=n+1$, and take positive numbers $x_\alpha$, $\alpha \in F$.
We can assume that $F=\{1, \ldots, n+1\}$ and that $x_1 \ge \cdots \ge x_{n+1}$.
Take any partition $F = F_0 \sqcup F_1$.
Without loss of generality, assume that $n+1 \in F_0$.
Apply the induction hypothesis to $F'=\{1, \ldots, n\}$, obtaining
$$
\Sigma'_{00} - 2\Sigma'_{01} + \Sigma'_{11}  \ge  \big(|F_0| - 1 - |F_1|)^2 x_n.
$$
We have 
$$
\Sigma_{00} = \Sigma_{00}' + \big( 2|F_0| -1 \big) x_{n+1} \, ,
\quad 
\Sigma_{01} = \Sigma'_{01} + |F_1| x_{n+1} \, ,
\quad \text{and} \quad
\Sigma_{11} = \Sigma'_{11} \, ,
$$
so \eqref{e.induction} follows.
\end{proof}

If ${\mathtt{R}}$ is a $\rm c$-rectangle or the entire square $[1,d]^2$, let $\operatorname{wgt}_1 {\mathtt{R}}$ denote 
the sum of the weights of the $\rm e$-rectangles in ${\mathtt{R}}$ with banner $1$.

Let us give the following useful consequence of \cref{l.city proj}:

\begin{lemma}\label{l.acyc and pop1}
$\operatorname{acyc} {\mathrm{Ad}}_A = \operatorname{wgt}_1 [1,d]^2$.
\end{lemma}

\begin{proof}
By \cref{p.acyc}, $\operatorname{acyc} {\mathrm{Ad}}_A$ 
is the maximum of the geometric multiplicities of the eigenvalues of ${\mathrm{Ad}}_A$.
Those eigenvalues are the banners $\beta$, and the geometric multiplicity of each $\beta$
is the total weight with banner $\beta$.
Thus, to prove the lemma we have to show that banner $1$ has biggest total weight.

Let $\beta$ be a banner.
Then, using \cref{l.city proj},
$$
\sum_{\text{${\mathtt{E}}$ is an $\rm e$-rectangle}\atop\text{with banner $\beta$}} \operatorname{wgt} {\mathtt{E}} 
\le \frac12 \sum_{\text{${\mathtt{E}}$ is an $\rm e$-rectangle}\atop\text{with banner $\beta$}} \operatorname{wgt} {\pi_\mathrm{r}}({\mathtt{E}})  
+   \frac12 \sum_{\text{${\mathtt{E}}$ is an $\rm e$-rectangle}\atop\text{with banner $\beta$}} \operatorname{wgt} {\pi_\mathrm{c}}({\mathtt{E}}) \, .
$$
Since no two $\rm e$-rectangles in the same row (resp.\ column) can have the same banner,
the restriction of ${\pi_\mathrm{r}}$ (resp.\ ${\pi_\mathrm{c}}$) to the set of $\rm e$-rectangles with banner $\beta$ is 
a one-to-one map. 
This allows us to conclude.
\end{proof}

\begin{rem}\label{r.jordan type and acyc}
The \emph{Jordan type} of a matrix $A \in {\mathrm{Mat}}_{d \times d}({\mathbb{C}})$ consists 
on the following data: 
\begin{enumerate}
\item The number of different eigenvalues.
\item For each eigenvalue, the number of Jordan blocks and their sizes.
\end{enumerate}
It follows from \cref{l.acyc and pop1} that these data is sufficient to
determine $\operatorname{acyc} {\mathrm{Ad}}_A$.
\end{rem}

\subsection{Rigidity estimate for $\rm c$-rectangles}

\begin{lemma}\label{l.rig island}
For any $\rm c$-rectangle ${\mathtt{C}}$,	
$$
\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{C}}^\square) \le \frac{\operatorname{wgt}_1 {\pi_\mathrm{r}}({\mathtt{C}}) + \operatorname{wgt}_1 {\pi_\mathrm{c}}({\mathtt{C}})}{2} \, .
$$
\end{lemma}

In order to prove this lemma, it is convenient to consider separately
the cases of non-equatorial and equatorial $\rm c$-rectangles.

\begin{proof}[Proof of \cref{l.rig island} when ${\mathtt{C}}$ is non-equatorial]
For each banner $\beta$ in ${\mathtt{C}}$, let $n_\beta$ (resp.\ $s_\beta$) be the maximum of 
$\operatorname{rig}_+({\mathrm{Ad}}_A | {\mathtt{E}}^\square)$ over nonnegative (resp.\ negative) argument $\rm e$-rectangles ${\mathtt{E}}$ in ${\mathtt{C}}$ with banner $\beta$.
For each $\rm e$-rectangle ${\mathtt{E}}$ with banner $\beta$, 
choose matrices $X_{{\mathtt{E}},1}$, \dots, $X_{{\mathtt{E}}, n_\beta + s_\beta} \in {\mathtt{E}}^\square$ such that:
\begin{itemize}
\item $\Lambda_{\mathtt{E}} := {\mathfrak{R}}_{{\mathrm{Ad}}_A} ( X_{{\mathtt{E}},1}, \dots, X_{{\mathtt{E}}, m})$ is a transitive subspace of ${\mathtt{E}}^\square$;
\item if ${\mathtt{E}}$ has negative argument then $X_{1} = X_{2} = \cdots = X_{n_\beta} = 0$;
\item if ${\mathtt{E}}$ has nonnegative argument then $X_{n_\beta+1} = \cdots = X_{n_\beta+s_\beta} = 0$.
\end{itemize}
Also, let $X_{{\mathtt{E}}, j} = 0$ for $j>n_\beta+s_\beta$.

Next, define
\begin{equation}\label{e.Y_j}
Y_{\beta, j} = 
\sum_{\text{${\mathtt{E}}$ is an $\rm e$-rectangle}\atop\text{of ${\mathtt{C}}$ with banner $\beta$}} X_{{\mathtt{E}}, j} 
\end{equation}
and
\begin{equation}\label{e.Z_j}
Z_j = \sum_{\text{$\beta$ banner on ${\mathtt{C}}$}} Y_{\beta, j} 
\end{equation}
Consider the space 
$$
\Delta = {\mathfrak{R}}_{{\mathrm{Ad}}_A} (Z_1, \dots, Z_m), \quad 
\text{where} \quad
m = \max_{\text{$\beta$ banner on ${\mathtt{C}}$}} (n_\beta + s_\beta)
$$
It follows from \cref{l.sum} that 
$$
\Delta = {\mathfrak{R}}_{{\mathrm{Ad}}_A} \big\{Y_{\beta,j} ; \; \beta \text{ is a banner, } 1 \le j \le n_\beta + s_\beta \big\}.
$$

Recall notation~\eqref{e.sudoku space}.
We claim that 
\begin{equation}\label{e.inclusion}
\Lambda_{\mathtt{E}} \subset \Delta^{[{\mathtt{E}}]}.
\end{equation}
Indeed, given $M \in \Lambda_{\mathtt{E}}$, write 
$M = \sum_j f_j({\mathrm{Ad}}_A) X_{{\mathtt{E}}, j}$,
where the $f_j$'s are polynomials and $f_j \equiv 0$ whenever $X_{{\mathtt{E}},j}=0$.
Consider $N = \sum_j f_j({\mathrm{Ad}}_A) Y_{\beta, j}$, where $\beta$ is the banner of ${\mathtt{E}}$.
Then it follows from \cref{l.geo} (part \ref{i.geo2}) that 
$N \in \Delta^{[{\mathtt{E}}]}$. 
This shows \eqref{e.inclusion}.
So, by \cref{l.sudoku}, $\Delta$ is a transitive subspace of ${\mathtt{C}}^\square$, showing that
$\operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{C}}^\square) \le m$.

To complete the proof of the lemma in the non-equatorial case, we show that
\begin{equation}\label{e.nonequat ineq}
m \le \frac{\operatorname{wgt}_1 {\pi_\mathrm{r}}({\mathtt{C}}) + \operatorname{wgt}_1 {\pi_\mathrm{c}}({\mathtt{C}})}{2} \, .
\end{equation}

Let $\beta$ be the banner for which $n_\beta + s_\beta$ attains the maximum $m$.
If $n_\beta>0$, let ${\mathtt{E}}_+$ be a nonnegative argument $\rm e$-rectangle in ${\mathtt{C}}$ with banner $\beta$
and $\operatorname{rig}_+({\mathrm{Ad}}_A | {\mathtt{E}}_+^\square) = n_\beta$. 
If $s_\beta>0$, let ${\mathtt{E}}_-$ be a negative argument $\rm e$-rectangle in ${\mathtt{C}}$ with banner $\beta$
and $\operatorname{rig}_+({\mathrm{Ad}}_A | {\mathtt{E}}_-^\square) = s_\beta$. 
Assume for the moment that both $\rm e$-rectangles exist.
Let ${\mathtt{E}}_1$, ${\mathtt{E}}_2$, ${\mathtt{E}}_3$, ${\mathtt{E}}_4$ be projected equatorial $\rm e$-rectangles as in \cref{f.projections1}.

\begin{figure}[htb] 
\begin{minipage}[t]{0.4\textwidth}
\centering
\setlength{\unitlength}{.25cm}
\begin{picture}(19,19)  
\thicklines  
\put(0,0){\framebox(19,19)} 
\put(.5,.5){$[1,d]^2$}

\put(1,11){\framebox(7,7)} 
\put(1.5,11.5){${\mathtt{C}}_1$}

\put(9,1){\framebox(9,9)} 
\put(9.5,1.5){${\mathtt{C}}_2$}

\put(9,11){\framebox(9,7)} 
\put(17,17){${\mathtt{C}}$}

\thinlines
\put(2.5,14){\framebox(2.5,2.5){${\mathtt{E}}_1$}} 
\put(5.5,11.5){\framebox(2,2){${\mathtt{E}}_2$}}
\put(10,5){\framebox(4,4){${\mathtt{E}}_3$}}
\put(15,2){\framebox(2,2){${\mathtt{E}}_4$}}
\put(10,11.5){\framebox(4,2){${\mathtt{E}}_-$}} 
\put(15,14){\framebox(2,2.5){ ${\mathtt{E}}_+$}}

\put(5,16.5){\dashbox{.2}(10,0)}
\put(5,14){\dashbox{.2}(10,0)}
\put(7.5,13.5){\dashbox{.2}(2.5,0)}
\put(7.5,11.5){\dashbox{.2}(2.5,0)}

\put(10,9){\dashbox{.2}(0,2.5)}
\put(14,9){\dashbox{.2}(0,2.5)}
\put(15,4){\dashbox{.2}(0,10)}
\put(17,4){\dashbox{.2}(0,10)}
\end{picture}
\caption{The case of non-equatorial $\rm c$-rectangles: ${\mathtt{E}}_1 = {\pi_\mathrm{r}}({\mathtt{E}}_+)$, ${\mathtt{E}}_2 = {\pi_\mathrm{r}}({\mathtt{E}}_-)$, ${\mathtt{E}}_3 = {\pi_\mathrm{c}}({\mathtt{E}}_-)$, ${\mathtt{E}}_4 = {\pi_\mathrm{c}}({\mathtt{E}}_+)$.} 
\label{f.projections1}
\end{minipage}
\qquad 
\qquad
\begin{minipage}[t]{0.4\textwidth}
\centering
\setlength{\unitlength}{.25cm}
\begin{picture}(19,19)  
\thicklines  
\put(0,0){\framebox(19,19)} 
\put(.5,.5){${\mathtt{C}}$}

\thinlines
\put(2.5,14){\framebox(2.5,2.5){${\mathtt{E}}_1$}} 
\put(5.5,11.5){\framebox(2,2){${\mathtt{E}}_2$}}
\put(10,5){\framebox(4,4){${\mathtt{E}}_3$}}
\put(15,2){\framebox(2,2){${\mathtt{E}}_4$}}
\put(10,11.5){\framebox(4,2){${\mathtt{E}}_+$}} 
\put(2.5,2){\framebox(2.5,2){${\mathtt{E}}_-$}}

\put(5,2){\dashbox{.2}(10,0)}
\put(5,4){\dashbox{.2}(10,0)}
\put(7.5,13.5){\dashbox{.2}(2.5,0)}
\put(7.5,11.5){\dashbox{.2}(2.5,0)}

\put(10,9){\dashbox{.2}(0,2.5)}
\put(14,9){\dashbox{.2}(0,2.5)}
\put(2.5,4){\dashbox{.2}(0,10)}
\put(5,4){\dashbox{.2}(0,10)}
\end{picture}
\caption{The case of equatorial non-exceptional $\rm c$-rectangles: ${\mathtt{E}}_1 = {\pi_\mathrm{c}}({\mathtt{E}}_-)$, ${\mathtt{E}}_2 = {\pi_\mathrm{r}}({\mathtt{E}}_+)$, ${\mathtt{E}}_3 = {\pi_\mathrm{c}}({\mathtt{E}}_+)$, ${\mathtt{E}}_4 = {\pi_\mathrm{r}}({\mathtt{E}}_-)$. It is possible that ${\mathtt{E}}_1 = {\mathtt{E}}_2$ or ${\mathtt{E}}_3 = {\mathtt{E}}_4$.} 
\label{f.projections2}
\end{minipage}
\end{figure}
Then
\begin{multline*}
m = \operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}_+) + \operatorname{rig}_+ ({\mathrm{Ad}}_A | {\mathtt{E}}_-) 
\stackrel{\text{(i)}}{\le} \operatorname{wgt} {\mathtt{E}}_+ + \operatorname{wgt} {\mathtt{E}}_-  \\
\stackrel{\text{(ii)}}{\le} \tfrac{1}{2}\big( \operatorname{wgt} {\mathtt{E}}_1 + \dots + \operatorname{wgt} {\mathtt{E}}_4 \big)  
\le \tfrac{1}{2}\big(\operatorname{wgt}_1 {\mathtt{C}}_1 + \operatorname{wgt}_1 {\mathtt{C}}_2 \big), 
\end{multline*}
where (i) and (ii) follow respectively from \cref{l.crude rig city,l.city proj}.
This proves \eqref{e.nonequat ineq} in this case.
If there is no nonnegative argument $\rm e$-rectangle or no negative argument $\rm e$-rectangle within ${\mathtt{C}}$ with banner~$1$
then the proof of \eqref{e.nonequat ineq} is easier.

So the lemma is proved for non-equatorial ${\mathtt{C}}$.
\end{proof}

We now consider equatorial $\rm c$-rectangles.
There is a special kind of $\rm c$-rectangle 
for which the proof of the rigidity estimate has to follow a different strategy.
A $\rm c$-rectangle is called \emph{exceptional} if
it has only the banners $1$ and $-1$ (so it is equatorial and has $4$ $\rm e$-rectangles),
each $\rm e$-rectangle has a single $\rm j$-rectangle,
and all $\rm j$-rectangles have the same weight.

\begin{proof}[Proof of \cref{l.rig island} when ${\mathtt{C}}$ is equatorial non-exceptional]
As in the previous case,  
let $n_\beta$ (resp.\ $s_\beta$) be the maximum of 
$\operatorname{rig}_+({\mathrm{Ad}}_A|{\mathtt{E}}^\square)$ over the nonnegative (resp.\ negative)  argument $\rm e$-rectangles ${\mathtt{E}}$ in ${\mathtt{C}}$ with banner $\beta$.

We claim that 
\begin{equation}\label{e.equat ineq}
n_\beta + s_\beta < \operatorname{wgt}_1 {\mathtt{C}} \quad \text{for all banners $\beta \neq 1$ in ${\mathtt{C}}$.}
\end{equation}
Let us postpone the proof of this inequality and see how to conclude.

Let $M=\operatorname{wgt}_1 {\mathtt{C}}$.
In view of \cref{l.crude rig city} and relation~\eqref{e.equat ineq},
for each $\rm c$-rectangle ${\mathtt{E}}$ we can take matrices $X_{{\mathtt{E}}, 1}$, \dots, $X_{{\mathtt{E}}, M} \in {\mathtt{E}}^\square$
such that:
\begin{itemize}
\item $\Lambda_{\mathtt{E}} := {\mathfrak{R}}_{{\mathrm{Ad}}_A} (X_{{\mathtt{E}}, 1}, \dots, X_{{\mathtt{E}}, M})$ is a transitive subspace of  ${\mathtt{E}}^\square$;
\item $X_{{\mathtt{E}}, M} = 0$ if ${\mathtt{E}}$ is non-equatorial;
\item $X_{{\mathtt{E}}, M}$ is the identity in ${\mathtt{E}}^\square$ if ${\mathtt{E}}$ is equatorial.
\end{itemize}
Then define matrices $Z_j$ as before: by \eqref{e.Y_j} and \eqref{e.Z_j}.
Here we have that $Z_M$ is the identity matrix in ${\mathtt{C}}^\square$.
As before, ${\mathfrak{R}}_{{\mathrm{Ad}}_A}(Z_1, \dots, Z_M)$ is a transitive subspace of ${\mathtt{C}}^\square$.
Hence $\operatorname{rig}_+({\mathrm{Ad}}_A|{\mathtt{C}}^\square) \le M = \operatorname{wgt}_1 {\mathtt{C}}$, as desired.

\medskip

Now let us prove \eqref{e.equat ineq}.
Consider a banner $\beta \neq 1$ in ${\mathtt{C}}$.
Let ${\mathtt{E}}_+$ (resp.\ ${\mathtt{E}}_-$) be a nonnegative (resp.\ negative) argument $\rm e$-rectangle within ${\mathtt{C}}$ with banner $\beta$ 
and of maximal weight;
assume for the moment that both $\rm e$-rectangles exist.
Let ${\mathtt{E}}_1$, ${\mathtt{E}}_2$, ${\mathtt{E}}_3$, ${\mathtt{E}}_4$ be projected equatorial $\rm e$-rectangles as in \cref{f.projections2}.
Then
\begin{align}
n_\beta + s_\beta  
&= \operatorname{rig}_+({\mathrm{Ad}}_A | {\mathtt{E}}_+) + \operatorname{rig}_+({\mathrm{Ad}}_A | {\mathtt{E}}_-)                \nonumber \\
&\le \operatorname{wgt} {\mathtt{E}}_+ + \operatorname{wgt} {\mathtt{E}}_-                                   \label{e.ineq1}\\
&\le  \tfrac{1}{2}\big( \operatorname{wgt} {\mathtt{E}}_1 + \dots + \operatorname{wgt} {\mathtt{E}}_4 \big)   \label{e.ineq2}\\
&\le \operatorname{wgt}_1 {\mathtt{C}}.                                                \label{e.ineq3}
\end{align}
Inequality~\eqref{e.ineq1} follows from \cref{l.crude rig city}, 
inequality~\eqref{e.ineq2} follows from \cref{l.city proj}, and
inequality~\eqref{e.ineq3} holds because the $\rm e$-rectangles ${\mathtt{E}}_1$, \dots, ${\mathtt{E}}_4$ are equatorial,
and any $\rm e$-rectangle can appear at most twice in this list.
So 
\begin{equation}\label{e.weaker}
n_\beta + s_\beta \le \operatorname{wgt}_1 {\mathtt{C}}.
\end{equation}
In the case that there is no nonnegative argument $\rm e$-rectangle or no negative argument $\rm e$-rectangle with banner $\beta$ 
(i.e., $n_\beta$ or $s_\beta$ vanishes),
a simpler argument shows that strict inequality holds in \eqref{e.weaker}.

Now assume by contradiction that \eqref{e.equat ineq} does not hold.
Then we must have equality in \eqref{e.weaker}.
By what we have just seen, both $\rm e$-rectangles ${\mathtt{E}}_+$ and ${\mathtt{E}}_-$ above exist.
Then the inequalities in \eqref{e.ineq1}--\eqref{e.ineq3} become equalities.
Since~\eqref{e.ineq3} is an equality, there must be exactly two equatorial $\rm e$-rectangles in ${\mathtt{C}}$.
So the non-equatorial banner $\beta$ satisfies $\beta^{-1} = \beta$, that is, $\beta=-1$.
Since~\eqref{e.ineq2} is an equality, it follows from \cref{l.city proj} that both non-equatorial $\rm e$-rectangles 
have the same number of $\rm j$-rectangles in each column and each row.
So there is some $\ell$ such that all four $\rm e$-rectangles in ${\mathtt{C}}$ have $\ell$ rows of $\rm j$-rectangles 
and $\ell$ columns of $\rm j$-rectangles.
Since~\eqref{e.ineq1} is an equality, \cref{l.crude rig city} implies that $\ell=1$.
That is, ${\mathtt{C}}$ is a exceptional $\rm c$-rectangle, a situation which we excluded a priori.
This contradiction proves \eqref{e.equat ineq} and \cref{l.rig island}
in the present case.
\end{proof}

Let us now deal with exceptional $\rm c$-rectangles.
In all the previous cases, the transitive subspace we found had some vaguely Toeplitz form.
For exceptional $\rm c$-rectangles, however, this strategy is not efficient. 
What we are going to do is to find a transitive space of vaguely Hankel form, namely the following:
\begin{equation}\label{e.hankel like}
\Lambda_k = \left\{ 
\begin{pmatrix}
P & M \\
M & N
\end{pmatrix};
\; \text{$M$, $N$, $P$ are $k\times k$ matrices}
\right\}.
\end{equation}
Notice that $\Lambda_k = S_k \cdot \Gamma_k$, where
$$
S_k = \begin{pmatrix} 0 & {\mathrm{Id}} \\ {\mathrm{Id}} & 0 \end{pmatrix}
\quad \text{and} \quad
\Gamma_k = \left\{ 
\begin{pmatrix}
M & N \\
P & M
\end{pmatrix};
\; \text{$M$, $N$, $P$ are $k\times k$ matrices}
\right\}.
$$
Since $\Gamma_k$ is 
a generalized Toeplitz space, 
it follows from \cref{r.transitivity trick} that $\Lambda_k$ is transitive.

\begin{proof}[Proof of \cref{l.rig island} when ${\mathtt{C}}$ is exceptional]
If ${\mathtt{C}}$ is exceptional then it has size $2k \times 2k$ for some $k$,
and the operator ${\mathrm{Ad}}_A | {\mathtt{C}}^\square$ is given by $X \mapsto {\mathrm{Ad}}_L(X)$, where
$$
L = \begin{pmatrix}  J & 0 \\ 0 & -J \end{pmatrix},
\quad \text{and $J = J_k(1)$ is the Jordan block~\eqref{e.block}.}
$$
Let $V$ be unique ${\mathrm{Ad}}_J$-invariant subspace of ${\mathrm{Mat}}_{k \times k}({\mathbb{C}})$ that 
has codimension $1$ and does not contain the identity matrix (which exists by \cref{l.1x1}).
Take matrices $X_1$, \dots, $X_k \in {\mathrm{Mat}}_{k \times k}({\mathbb{C}})$ such that
$X_1 = {\mathrm{Id}}$
and 
$V = {\mathfrak{R}}_{{\mathrm{Ad}}_J} (X_2, \dots, X_k)$.
Define $Y_1$, \dots, $Y_k \in {\mathrm{Mat}}_{2k \times 2k} ({\mathbb{C}})$ by 
$$
Y_1 = \begin{pmatrix} {\mathrm{Id}} & 0 \\ 0 & {\mathrm{Id}} \end{pmatrix},
\quad 
Y_j = \begin{pmatrix} X_j & 0       \\ 0 & 0   \end{pmatrix} \text{ for $2 \le j \le k$,}
$$
Then
$$
{\mathfrak{R}}_{{\mathrm{Ad}}_L}(Y_1,\dots,Y_k) = 
\left\{ 
\begin{pmatrix} x{\mathrm{Id}} + Z & 0 \\ 0 & x {\mathrm{Id}} \end{pmatrix} ; \;
x \in {\mathbb{C}}, \ Z \in V
\right\}.
$$
For $j=k+1$, \dots, $2k$, define
$$
Y_j = \begin{pmatrix} 0   & X_{j-k} \\ X_{j-k} & X_{j-k} \end{pmatrix}. 
$$
Then, by \cref{l.sum}, 
$$
{\mathfrak{R}}_{{\mathrm{Ad}}_L}(Y_{k+1},\dots,Y_{2k}) = 
\left\{ 
\begin{pmatrix} 0 & M \\ M & N \end{pmatrix} ; \;
M, \ N \in {\mathrm{Mat}}_{k \times k}({\mathbb{C}})
\right\}.
$$
Therefore ${\mathfrak{R}}_{{\mathrm{Ad}}_L}(Y_1,\dots,Y_{2k})$ is the transitive space given by \eqref{e.hankel like}.
Since $Y_1$ is the identity on ${\mathtt{C}}$, this shows that 
$\operatorname{rig}_+({\mathrm{Ad}}_A|{\mathtt{C}}^\square) \le 2k = \operatorname{wgt}_1 {\mathtt{C}}$,
concluding the proof of \cref{l.rig island}.
\end{proof}

\subsection{The final rigidity estimate}

Let $c = c(A)$ be the number of equivalence classes mod~$T$ of eigenvalues of $A$.

\begin{lemma}\label{l.rig world} 
If $c<d$ then
$$
\operatorname{rig}_+ {\mathrm{Ad}}_A \le  \operatorname{wgt}_1 [1,d]^2 - c + 1 \, . 
$$
\end{lemma}

\begin{proof} 
Let $m = \operatorname{wgt}_1 [1,d]^2 - c + 1$.
For each $\rm c$-rectangle ${\mathtt{C}}$, let 
$$
r({\mathtt{C}}) =  \big\lfloor \tfrac{1}{2} (\operatorname{wgt}_1 {\pi_\mathrm{r}}({\mathtt{C}}) + \operatorname{wgt}_1 {\pi_\mathrm{c}}({\mathtt{C}})) \big\rfloor.
$$
We claim that 
\begin{equation}\label{e.crude}
r({\mathtt{C}}) \le 
\begin{cases}
m   &\text{if ${\mathtt{C}}$ is an equatorial $\rm c$-rectangle,} \\
m-1 &\text{if ${\mathtt{C}}$ is a non-equatorial $\rm c$-rectangle.}
\end{cases}
\end{equation}
Let us postpone the proof of this inequality and see how it implies the lemma.

In view of \cref{l.rig island} and relation~\eqref{e.crude},
for each $\rm c$-rectangle ${\mathtt{C}}$ we can take matrices $X_{{\mathtt{C}}, 1}$, \dots, $X_{{\mathtt{C}}, m} \in {\mathtt{C}}^\square$
such that:
\begin{itemize}
\item $\Lambda_{\mathtt{C}} := {\mathfrak{R}}_{{\mathrm{Ad}}_A} (X_{{\mathtt{C}}, 1}, \dots, X_{{\mathtt{C}}, m})$ is a transitive subspace of  ${\mathtt{C}}^\square$;
\item $X_{{\mathtt{C}}, m} = 0$ if ${\mathtt{C}}$ is non-equatorial;
\item $X_{{\mathtt{C}}, m}$ is the identity in ${\mathtt{C}}^\square$ if ${\mathtt{C}}$ is equatorial.
\end{itemize}

Define matrices:
\begin{alignat*}{2}
Y_{\alpha, j} &= 
\sum_{\text{${\mathtt{C}}$ is a $\rm c$-rectangle}\atop\text{with banner class $\alpha$}} X_{{\mathtt{C}}, j}
&\qquad &(\alpha \text{ is a banner class}, \ 1 \le j \le m), \\
Z_j &= \sum_{\alpha \text{ is a banner class}} Y_{\alpha, j} 
&\qquad &(1 \le j \le m).
\end{alignat*}
So $Z_m$ is the $d \times d$ identity matrix.
Consider the space 
$$
\Delta = {\mathfrak{R}}_{{\mathrm{Ad}}_A} (Z_1, \dots, Z_m).
$$
It follows from \cref{l.sum} that 
$$
\Delta = {\mathfrak{R}}_{{\mathrm{Ad}}_A} \big\{ Y_{\alpha,j} ; \; \alpha \text{ is a banner class, } 1 \le j \le m \big\}.
$$
We claim that every $\rm c$-rectangle ${\mathtt{C}}$, 
\begin{equation}\label{e.inclusion2}
\Lambda_{\mathtt{C}} \subset \Delta^{[{\mathtt{C}}]}.
\end{equation}
Indeed, if $M \in {\mathtt{C}}$ then we can write $M = \sum_j f_j({\mathrm{Ad}}_A) X_{{\mathtt{C}},j}$,
where the $f_j$'s are polynomials.
Consider $N = \sum_j f_j({\mathrm{Ad}}_A) Y_{\alpha,j}$, where $\alpha$ is the banner class of ${\mathtt{C}}$.
It follows \cref{l.geo} (part \ref{i.geo0}) that $N \in \Delta^{[{\mathtt{C}}]}$.
This proves \eqref{e.inclusion2}.
So, by \cref{l.sudoku}, $\Delta$ is a transitive subspace of ${\mathrm{Mat}}_{d \times d}({\mathbb{C}})$, 
showing that $\operatorname{rig}_+ {\mathrm{Ad}}_A \le m$.

\medskip

To conclude the proof we have to show estimate~\eqref{e.crude}.
First consider a equatorial $\rm c$-rectangle~${\mathtt{C}}$.
Since there are $c$
equatorial $\rm c$-rectangles, and each of them has a nonzero $\operatorname{wgt}_1$ value,
we conclude that $r({\mathtt{C}}) \le m$, as claimed.

Now take a non-equatorial ${\mathtt{C}}$.
Applying what we have just proved for the equatorial $\rm c$-rectangles ${\pi_\mathrm{r}}({\mathtt{C}})$ and ${\pi_\mathrm{c}}({\mathtt{C}})$,
we conclude that $r({\mathtt{C}}) \le m$.
Now assume that \eqref{e.crude} does not hold for ${\mathtt{C}}$, that is, $r({\mathtt{C}}) = m$.
Then 
$$
\operatorname{wgt}_1 {\pi_\mathrm{r}}({\mathtt{C}}) = \operatorname{wgt}_1 {\pi_\mathrm{c}} ({\mathtt{C}}) = m = \operatorname{wgt}_1 [1,d]^2 - c + 1.
$$
Since $\operatorname{wgt}_1 [1,d]^2 \ge \operatorname{wgt}_1 {\pi_\mathrm{r}}({\mathtt{C}}) + \operatorname{wgt}_1 {\pi_\mathrm{c}} ({\mathtt{C}}) + c - 2$,
we have $m=1$ and $\operatorname{wgt}_1 [1,d]^2 = c$.
This means that $\operatorname{wgt}_1 \tilde {\mathtt{C}} = 1$ for all equatorial $\rm c$-rectangles $\tilde {\mathtt{C}}$,
which is only possible if $c=d$.
However, this case was excluded by hypothesis.

This proves \eqref{e.crude} and hence \cref{l.rig world}.
\end{proof}

\begin{example}
If $A$ is the matrix of \cref{ex.big matrix} then \cref{l.rig world}
gives the estimate $\operatorname{rig}_+ {\mathrm{Ad}}_A \le 28$.
A more careful analysis (going through the proofs of the lemmas) would give $\operatorname{rig}_+ {\mathrm{Ad}}_A \le 7$
(see \cref{ex.continued}).
\end{example}

\begin{proof}[Proof of part~\ref{i.rig hard} of \cref{t.rig}]
Apply \cref{l.rig world,l.acyc and pop1}.
\end{proof}

\section{Proof of the hard part of the codimension $m$ theorem}\label{s.cod proof}

We showed in \cref{p.cod_data_easy_half} that $\operatorname{codim} {\mathcal{P}}_m^{({\mathbb{K}})} \le m$.
In this section, we will prove the reverse inequalities.
More precisely, we will first prove \cref{t.cod_data_C}
and then deduce \cref{t.cod_data_R} from it.

\subsection{Preliminaries on elementary algebraic geometry}

\subsubsection{Quasiprojective varieties} 

An algebraic subset of ${\mathbb{C}}^n$ is also called an \emph{affine variety}.
A \emph{projective variety} is a subset of ${\mathbb{C}\mathrm{P}}^n$ that can be expressed 
as the zero set of a family of homogeneous polynomials in $n+1$ variables.
The \emph{Zariski topology} on an (affine or projective) variety $X$ is 
the topology whose closed sets are the (affine or projective) subvarieties of $X$.

An open subset $U$ of a projective variety $X$ is called a \emph{quasiprojective variety}.
We consider in $U$ the induced Zariski topology.
The affine space ${\mathbb{C}}^n$ can be identified with a quasiprojective variety,
namely its image under the embedding $(z_1, \dots, z_n) \mapsto (1: z_1 : \dots : z_n)$.

If $X$ and $Y$ are quasi-projective varieties
then the product $X \times Y$ can be identified with a quasiprojective variety, 
namely its image under the Segre embedding; see \cite[\S~5.1]{Shafa}.

Recall the following property from \cite[p.~58]{Shafa}:
 
\begin{prop}\label{p.projection}
If $X$ is a projective variety and $Y$ is a quasiprojective variety 
then the projection $p \colon X \times Y \to Y$ takes Zariski closed sets to Zariski closed sets.
\end{prop}

A quasiprojective variety is called \emph{irreducible} if it cannot be written as a nontrivial union of two quasiprojective varieties (that is, none contains the other).

\subsubsection{Dimension}
The dimension $\dim X$ of an irreducible quasiprojective variety $X$
may be defined in various equivalent ways (see for instance~\cite[p.~133ff]{Harris}). 
It will be sufficient for us to know that there exists an (intrinsically defined) 
subvariety $Y$ of the \emph{singular points of $X$}
such that in a neighborhood of each point of $X {\smallsetminus} Y$, the set 
$X$ is a complex submanifold of dimension (in the classical sense of differential geometry) $\dim X$;
moreover, each irreducible component of $Y$ has dimension strictly less than $\dim X$.

The dimension of a general quasiprojective variety is by definition the maximum of the dimensions of the irreducible components.

The following lemma is useful to estimate the codimension of an algebraic set $X$
from information about the fibers of a certain projection $\pi \colon X \to Y$.

\begin{lemma}\label{l.pret_a_porter}
Let $Y$ be a quasiprojective variety.  
Let $X \subset Y \times {\mathbb{C}\mathrm{P}}^n$ be a nonempty algebraically closed set.
Let $\pi \colon X \to Y$ be the projection along ${\mathbb{C}\mathrm{P}}^n$.
Then:
\begin{enumerate}
\item
For each $j \ge 0$, the set
$$
C_j = \{ y \in \pi(X) ; \; \operatorname{codim} \pi^{-1}(y) \le j \}
$$
is algebraically closed in $Y$.
\item
The dimension of $X$ is given in terms of the dimensions of the $C_j$'s by:
\begin{equation}\label{e.formula_pret_a_porter}
\operatorname{codim} X = \min_{j ; \; C_j \neq {\varnothing}} \big( j + \operatorname{codim} C_j \big) \, .
\end{equation}
\end{enumerate}
\end{lemma}

In the above, the codimensions of $\pi^{-1}(Y)$, $X$ and $C_j$
are taken with respect to ${\mathbb{C}\mathrm{P}}^n$, $Y \times {\mathbb{C}\mathrm{P}}^n$ and $Y$, respectively.
The proof of the lemma is given in \cref{a.algebraic}.

\begin{rem}\label{r.homogeneous}
\Cref{l.pret_a_porter} works with the same statement if ${\mathbb{C}\mathrm{P}}^{n}$ is replaced by ${\mathbb{C}}^{n+1}$,
provided one assumes that $X \subset Y \times {\mathbb{C}}^{n+1}$ is homogeneous in the second factor
(i.e., $(y,z) \in X$ implies $(y,tz)\in X$ for every $t\in {\mathbb{C}}$).
Indeed, this follows from the fact that the projection ${\mathbb{C}}^{n+1}{\smallsetminus}\{0\} \to {\mathbb{C}\mathrm{P}}^{n}$
preserves codimension of homogeneous sets.
\end{rem}

\subsubsection{Dimension estimates for sets of vector subspaces}\label{sss.sets_of_spaces}

If $M \in {\mathrm{Mat}}_{n \times m}({\mathbb{K}})$, let $\operatorname{col} M \subset {\mathbb{K}}^n$ denote the column space of $M$.
A set $X \subset {\mathrm{Mat}}_{n \times m}({\mathbb{K}})$ is called \emph{column-invariant}
if 
$$
\left.
\begin{array}{c}
M \in X \\ 
N \in {\mathrm{Mat}}_{n \times m}({\mathbb{K}})\\
\operatorname{col} M = \operatorname{col} N
\end{array}
\right\} \ \Rightarrow \ 
N \in X.
$$
So a column-invariant set $X$ is characterized by its set of column spaces.
We enlarge the latter set by including also subspaces, thus defining:
\begin{equation}\label{e.bracket_notation}
\ldbrack X \rdbrack := \big\{ E \text{ subspace of } {\mathbb{K}}^n ; \; E \subset \operatorname{col} M \text{ for some } M \in X \big\}.
\end{equation}
In \cref{a.algebraic} we prove:

\begin{thm}\label{t.schubert}
Let $X \subset {\mathrm{Mat}}_{n \times m}({\mathbb{C}})$ be an algebraically closed, column-invariant set.
Suppose $E$ is a vector subspace of ${\mathbb{C}}^n$
that does not belong to $\ldbrack X \rdbrack$.
Then
$$
\operatorname{codim} X \ge m + 1 - \dim E \, .
$$
\end{thm}

\subsubsection{The real part of an algebraic set}

Let $X$ be an algebraically closed subset of ${\mathbb{C}}^n$.
The \emph{real part} of $X$ is defined as $X \cap {\mathbb{R}}^n$.
This is an algebraically closed subset of ${\mathbb{R}}^n$. 
Indeed, generators of the corresponding ideal $f_1,\ldots, f_k$ in ${\mathbb{C}}[T_1,\ldots, T_n]$ can be replaced by the corresponding real and imaginary parts polynomials.

As in the complex case, there are many equivalent algebraic-geometric definitions of dimensions of real algebraic or semialgebraic sets. We just point out that a real algebraic or semialgebraic set admits a stratification into real manifolds such that the maximal differential-geometric dimension of the strata coincides with the algebraic-geometric dimension (see~\cite[\S~3.4]{BR} or \cite[p.~50]{BCR}).

The following is an immediate consequence of \cite[Prop.~3.3.2]{BR}:

\begin{prop}\label{p.real complex dimension}
If $X$ is an algebraically closed subset of ${\mathbb{C}}^n$ then
$\dim_{\mathbb{R}} (X \cap {\mathbb{R}}^n) \le \dim_{\mathbb{C}} X$.
\end{prop}

\subsection{Rigidity and the dimension of the poor fibers}

For simplicity of notation, let us write ${\mathcal{P}}_m = {\mathcal{P}}_m^{({\mathbb{C}})}$.
Also, for $A \in {\mathrm{GL}}(d,{\mathbb{C}})$, write:
$$
r(A) := \operatorname{rig}_+ {\mathrm{Ad}}_A  - 1  \, .
$$

We decompose the set ${\mathcal{P}}_m$ of poor data in fibers:
\begin{equation}\label{e.fiber decomp}
{\mathcal{P}}_m = \bigcup_{A \in {\mathrm{GL}}(d,{\mathbb{C}})} \{A\} \times {\mathcal{P}}_m(A),
\quad \text{where} \quad
{\mathcal{P}}_m(A) \subset {\mathfrak{gl}}(d,{\mathbb{C}})^m \, .
\end{equation}

\begin{lemma}\label{l.cod fiber}
For any $A \in {\mathrm{GL}}(d,{\mathbb{C}})$, 
the codimension of ${\mathcal{P}}_m(A)$ in ${\mathfrak{gl}}(d,{\mathbb{C}})^m$
is at least $m + 1 - r(A)$.  
\end{lemma}

The lemma follows easily from \cref{t.schubert} above:

\begin{proof} 
Fix $A \in {\mathrm{GL}}(d,{\mathbb{C}})$, and write $r=r(A)$.
We can assume that $r \le m$, otherwise there is nothing to prove.
By definition, there exists a $r$-dimensional subspace $E \subset {\mathfrak{gl}}(d,{\mathbb{C}})^m$
such that ${\mathfrak{R}}_{{\mathrm{Ad}}_A}({\mathrm{Id}} \vee E)$ is transitive.
Identify ${\mathfrak{gl}}(d,{\mathbb{C}})$ with ${\mathbb{C}}^{d^2}$ and thus
regard ${\mathcal{P}}_m(A)$ as a subset of ${\mathrm{Mat}}_{d^2 \times m} ({\mathbb{C}})$.
Since the set ${\mathcal{P}}_m$ is algebraically closed and saturated (recall \cref{ss.poor_set}),
the fiber ${\mathcal{P}}_m(A)$ is algebraically closed and column-invariant,
as required by \cref{t.schubert}.
In the notation \eqref{e.bracket_notation}, 
we have $E \not\in \ldbrack {\mathcal{P}}_m(A) \rdbrack$.
So \cref{t.schubert} gives the desired codimension estimate.
\end{proof}

\subsection{How rare is high rigidity?}

For simplicity of notation, let us write:
$$
a(A) := \operatorname{acyc} {\mathrm{Ad}}_A  \quad \text{for $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.}
$$
So \cref{t.rig} says that $r(A) \le a(A)-c(A)$ provided $c(A) < d$.

\begin{lemma}\label{l.cod rig}
For any integer $k \ge 1$, the set
$$
M_k = \big\{ A \in {\mathrm{GL}}(d,{\mathbb{C}})  ; \; r(A) \ge k \big\};
$$
is algebraically closed in ${\mathrm{GL}}(d,{\mathbb{C}})$;
moreover if $M_k \neq {\varnothing}$ then
$$
\operatorname{codim} M_k
\begin{cases}
= 0   &\text{if $k=1$,} \\
\ge k &\text{if $k \ge 2$.}
\end{cases}
$$
\end{lemma}

	

\Cref{l.cod rig} is basically a consequence of \cref{t.rig},
using the following construction:

\begin{lemma}\label{l.countable cover}
There is a family ${\mathcal{G}}(A)$ of subsets of ${\mathrm{GL}}(d,{\mathbb{C}})$, indexed by $A\in {\mathrm{GL}}(d,{\mathbb{C}})$,
such that the following properties hold:
\begin{enumerate}
\item \label{i.contains}
Each ${\mathcal{G}}(A)$ contains $A$.
\item \label{i.manifold}
Each ${\mathcal{G}}(A)$ is an immersed manifold of codimension $a(A)-c(A)$.
\item \label{i.countable}
There are only countably many different sets ${\mathcal{G}}(A)$.
\end{enumerate}
\end{lemma}
 

\begin{proof}
Fix any $A \in {\mathrm{GL}}(d,{\mathbb{C}})$.
Then $A$ is conjugate to a matrix in Jordan form:
$$
\tilde{A} =
\left(
\begin{array}{ccc}
J_{t_1}(\lambda_1) &        &                   \\[-2mm]
                   & \ddots &                   \\[-2mm]
                   &        & J_{t_n}(\lambda_n)      
\end{array}
\right), 
$$
where $J_\lambda(t)$ denotes Jordan block as in \eqref{e.block}.
Let $U$ be the set of matrices of the form  
$$
\left(
\begin{array}{ccc}
J_{t_1}(\mu_1) &        &                   \\[-2mm]
               & \ddots &                   \\[-2mm]
               &        & J_{t_n}(\mu_n)      
\end{array}
\right),
$$
where $\mu_1$, \dots, $\mu_n$ are nonzero complex numbers such that
$$
\lambda_i = \lambda_j \ \Leftrightarrow \ \mu_i = \mu_j
\qquad \text{and} \qquad
\lambda_i \asymp \lambda_j \ \Leftrightarrow \ \frac{\lambda_i}{\lambda_j} = \frac{\mu_i}{\mu_j} \, .
$$
Then $U$ is an embedded submanifold of ${\mathrm{GL}}(d,{\mathbb{C}})$ of dimension $c(A)$.
Every $Y \in U$ has the same Jordan type as $A$,
and so, by \cref{r.jordan type and acyc}, $a(Y) = a(A)$.
We define the set ${\mathcal{G}}(A)$ as the image of the map 
$\Psi = \Psi_A \colon {\mathrm{GL}}(d,{\mathbb{C}}) \times U \to {\mathrm{GL}}(d,{\mathbb{C}})$ given by $\Psi(X,Y) = {\mathrm{Ad}}_X (Y)$.
Notice that ${\mathcal{G}}(A)$ does not depend on the choice of $\tilde{A}$.
Actually ${\mathcal{G}}(A)$ is characterized by the sizes of the Jordan blocks $t_1$, \dots, $t_n$,
the pairs $(i,j)$ such that $\lambda_i \asymp \lambda_j$ and the corresponding
roots of unity;
in particular there are countably many such sets ${\mathcal{G}}(A)$.

Let us check that property~\ref{i.manifold} holds.
Let $\partial_1 \Psi$ and $\partial_2 \Psi$ denote the partial derivatives with
respect to $X$ and $Y$, respectively.
As we have seen in \cref{r.conjugacy_class},
the rank of $\partial_1 \Psi (X,Y)$ is equal to $d^2 - a(Y) = d^2 - a(A)$ 
for every $(X,Y)$.
On the other hand, $\partial_2 \Psi (X,Y)$ is one-to-one and therefore of rank $c(A)$.
We claim that
\begin{equation}\label{e.partials}
(\text{image of } \partial_1 \Psi(X,Y)) \cap (\text{image of } \partial_2 \Psi(X,Y)) = \{0\} ;
\end{equation}
To see this, consider the map $F \colon {\mathrm{Mat}}_{d\times d}({\mathbb{C}}) \to {\mathbb{C}}^d$ 
that associates to each matrix the coefficients of its characteristic polynomial.
Then $\partial_1 (F \circ \Psi)(X,Y) = 0$,
while $\partial_2 (F \circ \Psi)(X,Y)$ is one-to-one. 
So \eqref{e.partials} follows.
As a result, at every point the rank of the derivative of $\Psi$ is equal to 
the sum of the ranks of the partial derivatives, that is, $d^2 - a(A) + c(A)$.
Therefore, by the Rank Theorem, the image of $\Psi$ is an immersed manifold of codimension 
$a(A) - c(A)$.
\end{proof}

\begin{proof}[Proof of \cref{l.cod rig}]
If $k=1$ then $M_1 = {\mathrm{GL}}(d,{\mathbb{C}})$ (since $d \ge 2$), so there is nothing to prove.
Consider $k \ge 2$.
We have already shown in \cref{ss.poor_set}
that ${\mathcal{P}}_k$ is algebraic.
Since $M_k = \{ A \in {\mathrm{GL}}(d,{\mathbb{C}}) ; \; \forall \hat{X} \in {\mathfrak{gl}}(d,{\mathbb{C}})^{k} , \ (A, \hat{X}) \in {\mathcal{P}}_k \}$,
it is evident that $M_k$ is algebraically closed as well. 
We are left to estimate its dimension.

Take a nonsingular point $A_0$ of $M_k$ where the local dimension is maximal.
Let $D$ be the intersection of $M_k$ 
with a small neighborhood of $A_0$; it is an embedded disk. 
Each $A \in D$ has $r(A) \ge 2$;
therefore by (both parts of) \cref{t.rig},
we have $a(A) - c(A) \ge r(A) \ge k$. 
So, in terms of the sets from \cref{l.countable cover},
$$
D \subset \bigcup_{A \text{ s.t.\ } a(A) - c(A) \ge k} {\mathcal{G}}(A).
$$
The right hand side is a countable union of immersed manifolds of codimension at least $k$.
It follows (e.g.\ by Baire Theorem)
that $D$ (and hence $M_k$) has codimension at least $k$. 
\end{proof}

\subsection{Proof of \cref{t.cod_data_C,t.cod_data_R}}

Now we apply \cref{l.cod fiber,l.cod rig} to prove one of our major results:

\begin{proof}[Proof of \cref{t.cod_data_C}]
The set ${\mathcal{P}}_m \subset {\mathrm{GL}}(d,{\mathbb{C}}) \times [{\mathfrak{gl}}(d,{\mathbb{C}})]^m$
is homogeneous in the second factor.
Using \cref{l.pret_a_porter} together with \cref{r.homogeneous}, we
obtain that the sets 
\begin{equation}\label{e.C_j}
C_j = \big \{A \in {\mathrm{GL}}(d,{\mathbb{C}}) ; \; \operatorname{codim}  {\mathcal{P}}_m (A) \le j \big\}
\end{equation}
are algebraically closed in ${\mathrm{GL}}(d,{\mathbb{C}})$, and
$$
\operatorname{codim} {\mathcal{P}}_m = \min_{j ; \; C_j \neq {\varnothing}} \big( j + \operatorname{codim} C_j \big) \, .
$$
By \cref{l.cod fiber}, we have $C_j \subset M_{m+1-j}$.
Therefore, by \cref{l.cod rig},
\begin{equation}\label{e.ultraimportant}
C_j \neq {\varnothing} \quad \Rightarrow \quad
\operatorname{codim} C_j 
\begin{cases}
\ge 0      &\text{if $j=m$,} \\
\ge m-j+1  &\text{if $j \le m-1$.}
\end{cases}
\end{equation}
So $\operatorname{codim} {\mathcal{P}}_m \ge m$, as we wanted to show.
\end{proof}

The proof above only used that $\operatorname{codim} C_j \ge m-j$.
On the other hand, using the full power of \eqref{e.ultraimportant} we obtain: 

\begin{scho}\label{scholium}
The set of poor data in ``fat fibers'', namely
$$
{\mathcal{F}}_m := \big\{ (A, B_1, \dots, B_m)  \in {\mathcal{P}}_m^{({\mathbb{C}})} ; \; \operatorname{codim} {\mathcal{P}}_m(A) \le m-1 \big\},
$$
has codimension at least $m+1$ in ${\mathrm{GL}}(d,{\mathbb{C}}) \times [{\mathfrak{gl}}(d,{\mathbb{C}})]^m$.
\end{scho}

\begin{proof}
The projection of ${\mathcal{F}}_m$ on ${\mathrm{GL}}(d,{\mathbb{C}})$ is $C_{m-1}$.
Use \cref{l.pret_a_porter} (together with \cref{r.homogeneous}) and \eqref{e.ultraimportant}.
\end{proof}

Next, let us consider the real case:

\begin{proof}[Proof of \cref{t.cod_data_R}]
The real part of ${\mathcal{P}}^{({\mathbb{C}})}_m$ is a real algebraic set
which, in view of \cref{p.real complex dimension}, has codimension at least $m$.
Recall from \cref{ss.poor_set} that this set contains the semialgebraic set ${\mathcal{P}}^{({\mathbb{R}})}_m$,
which therefore has codimension at least $m$.
Since we already knew from \cref{p.cod_data_easy_half} that $\operatorname{codim} {\mathcal{P}}^{({\mathbb{R}})}_m \le m$,
the theorem is proved.
\end{proof}

\section{Proof of the main result} \label{s.main proof}
We now use \cref{t.cod_data_R} and transversality theorems to prove our main result.
For precise definitions and statements on the objects used in this section, see \cref{a.strat_trans}.

A \emph{stratification} is a filtration by closed subsets of a smooth manifold $X$
$$
{\Sigma} = {\Sigma}_n \supset {\Sigma}_{n-1} \supset \cdots \supset {\Sigma}_0
$$ 
such that for each $i$, the set $\Gamma_i= {\Sigma}_i {\smallsetminus} {\Sigma}_{i-1}$ (where $\Sigma_{-1}:={\varnothing}$) is a smooth submanifold of $X$ without boundary, and the dimension of $\Gamma_i$ decreases strictly with increasing $i$. 

We say that a $C^1$-map is {\em transverse} to that stratification if it is transverse to each of the submanifolds $\Gamma_i$. 
There are explicit, so-called {\em Whitney conditions} that guarantee that a stratification behaves nicely with respect to transversality, as the next proposition shows. A stratification satisfying those conditions is called a {\em Whiney stratification}.
By the classical~\cref{t.canonical_stratif} stated in \cref{a.strat_trans} (see for instance~\cite{GWPL}), any semi-algebraic subset of an affine space admits a canonical Whitney stratification.

We refer the reader to~\cref{a.strat_trans} for the definitions of jets, jet extensions and for a proof of the following: 
\begin{prop}\label{p.stratifiedtransversality}
Let $X$, $Y$ be $C^\infty$-manifolds without boundary. 
Let ${\Sigma}$ be a Whitney stratified closed subset of the set of $1$-jets from $X$ to $Y$. 
Then the set of maps $f \in C^2(X,Y)$ whose $1$-jet extension $j^1f$ is transverse to ${\Sigma}$ is $C^2$-open and $C^\infty$-dense in $C^2(X,Y)$ (i.e., its intersection with $C^r(X,Y)$ is $C^r$-dense, for every $2\le r\le \infty$).
\end{prop}

By \cref{t.cod_data_R}, ${\mathcal{P}}_m^{({\mathbb{R}})}$ is a closed semialgebraic subset of ${\mathrm{GL}}(d,{\mathbb{R}}) \times {\mathfrak{gl}}(d,{\mathbb{R}})^m$ of codimension $m$.
The closure $\overline{{\mathcal{P}}_m^{({\mathbb{R}})}}$ of ${\mathcal{P}}_m^{({\mathbb{R}})}$ in  $[{\mathrm{Mat}}_{d\times d}({\mathbb{R}})]^{1+m}$ is a closed semialgebraic set of the affine space  $[{\mathrm{Mat}}_{d\times d}({\mathbb{R}})]^{1+m}$. As mentioned above, it admits a canonical Whitney stratification
$$
\overline{{\mathcal{P}}_m^{({\mathbb{R}})}} = \hat \Gamma_n \supset \cdots \supset  \hat \Gamma_0 \, .
$$
The differentiable codimension of that stratification is also $m$. By locality of the Whitney conditions (see \cref{p.whitney_properties} of \cref{a.strat_trans}), this stratification restricts to a Whitney stratification of codimension~$m$:
\begin{equation}\label{e.poor_data_stratif}
{\mathcal{P}}_{m}^{({\mathbb{R}})} = \Gamma_n \supset \cdots \supset \Gamma_0 \, .
\end{equation}
Since that stratification of $\overline{{\mathcal{P}}_m^{({\mathbb{R}})}}$ is canonical, 
the stratification \eqref{e.poor_data_stratif} is invariant under 
polynomial automorphisms of ${\mathrm{GL}}(d,{\mathbb{R}}) \times {\mathfrak{gl}}(d,{\mathbb{R}})^m$ that preserve ${\mathcal{P}}_{m}^{({\mathbb{R}})}$.

\begin{proof}[Proof of Theorem~\ref{t.main}]
Let ${\mathcal{U}}$ be a smooth manifold without boundary and of dimension $m$. 
Given local coordinates on an open set $U\subset {\mathcal{U}}$, the set $J^1(U,{\mathrm{GL}}(d,{\mathbb{R}}))$ of $1$-jets from $U$ to ${\mathrm{GL}}(d,{\mathbb{R}})$ may be identified with the set $$U \times {\mathrm{GL}}(d,{\mathbb{R}}) \times {\mathfrak{gl}}(d,{\mathbb{R}})^m.$$ 
Indeed, a jet ${\mathbf{J}}$ represented by a pair $(u,A)$ can be identified with the point 
$$(u,A(u),B_1, \dots ,B_m)\in U \times {\mathrm{GL}}(d,{\mathbb{R}}) \times  {\mathfrak{gl}}(d,{\mathbb{R}})^m,$$
where $B_i\in {\mathrm{Mat}}_{d \times d}({\mathbb{R}})$ is the normalized derivative 
of $A$ at $u$, 
along the $i^\text{th}$ coordinate. 
Let us say that the $1$-jet ${\mathbf{J}}$ is \emph{rich}
if the datum ${\mathbf{A}} = (A(u),B_1, \dots ,B_m)$ is rich,
or equivalently, if for sufficiently large $N$,
the input $(u,\ldots,u)\in {\mathcal{U}}^N$ is universally regular for the system \eqref{e.proj semilin CS}.
If the jet is not rich then it is called \emph{poor}.

Define a filtration 
\begin{equation}\label{e.poor_jets_stratif}
\Sigma_n  \supset \cdots \supset \Sigma_0
\end{equation}
of the set of poor jets from ${\mathcal{U}}$ to ${\mathrm{GL}}(d,{\mathbb{R}})$ as follows:
a jet ${\mathbf{J}}$ represented as above in local coordinates by $(u,A(u),B_1, \dots ,B_m)$
belongs to $\Sigma_i$ if and only if $(A(u), B_1, \dots ,B_m)$ belongs to 
the set $\Gamma_i$ in \eqref{e.poor_data_stratif}.
We need to check that this definition does not depend on the choice of the local coordinates.
Indeed, this follows from ${\mathcal{P}}_m^{({\mathbb{R}})}$ being a saturated set
(see \cref{ss.poor_set}) and from the invariance of \eqref{e.poor_data_stratif} by polynomial automorphisms.

We claim that the filtration \eqref{e.poor_jets_stratif} is a Whitney stratification 
of codimension $m$.
Indeed, the intersection of the filtration 
with the open subset $J^1(U,{\mathrm{GL}}(d,{\mathbb{R}}))$ of $J^1({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{R}}))$
is identified (through a smooth diffeomorphism) with the filtration 
$$
U \times \Gamma_n \supset \cdots \supset U \times \Gamma_0.
$$
Such a filtration is still a Whitney stratification (see~\cref{p.whitney_properties} of  \cref{a.strat_trans}) of codimension $m$ in $J^1(U,{\mathrm{GL}}(d,{\mathbb{R}}))\approx U \times {\mathrm{GL}}(d,{\mathbb{R}}) \times {\mathfrak{gl}}(d,{\mathbb{R}})^m$. Covering ${\mathcal{U}}$ by open sets $U$, we deduce that \eqref{e.poor_jets_stratif} is a Whitney stratification of codimension $m$ in $J^1({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{R}}))$.

Applying Proposition~\ref{p.stratifiedtransversality}, we obtain a $C^2$-open
$C^\infty$-dense set ${\mathcal{O}} \subset C^2({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}}))$ formed by maps $A$ 
that are transverse to the stratification \eqref{e.poor_jets_stratif} 
of the set of poor jets.
Since the codimension of the stratification equals the dimension of ${\mathcal{U}}$,
if $A \in {\mathcal{O}}$ then the points $u$ for which $j^1 A(u)$ is poor form a $0$-dimensional set.
This proves~\cref{t.main}.
\end{proof}

\appendix  

\section{The case of one-dimensional input}\label{a.dim 1}

As we explained in \cref{ss.overview},
this appendix contains a basically independent discussion of the case where $m = \dim {\mathcal{U}}$ equals $1$.
The prerequisites are all contained in \cref{s.prelim_poor,ss.acyclicity}. 

\medskip

\subsection{Elementary constraints}\label{ss.unconstrained}
The material of this subsection is also used in \cref{a.generic singular}.

\medskip

An \emph{elementary constraint} in the variables $\lambda_1$, \dots, $\lambda_d$ 
is a relation $p=0$ where
$p$ is an irreducible factor of a polynomial 
of the form $\lambda_i \lambda_\ell - \lambda_j \lambda_k$.
Every elementary constraint can be written, after a permutation of the indices $1,\dots,d$, 
as one of the following:
\begin{equation}\label{e.canonical_constr}
\lambda_1 \lambda_3 = \lambda_2^2 , \qquad
\lambda_1 \lambda_4 = \lambda_2 \lambda_3, \qquad  
\lambda_1 = -\lambda_2, \qquad 
\lambda_1 = \lambda_2 \, ,
\end{equation}
which will be called the \emph{canonical constraints} respectively of type $1$, $2$, $3$, $4$.
The \emph{type} of elementary constrained is defined as 
the (unique) type of the associated canonical constraint.

We say that a matrix $A\in {\mathrm{GL}}(d,{\mathbb{R}})$ is \emph{unconstrained} if its eigenvalues, counted with multiplicity, satisfy no elementary constraint.
(Equivalently, ${\mathrm{Ad}}_A$ has the maximal possible number of distinct eigenvalues, namely, $d^2-d+1$.)
 

Let us see that the converse of \cref{l.easy_poor_data} holds for unconstrained matrices:

\begin{lemma}\label{l.easy_fiber}
Suppose that the datum 
${\mathbf{A}} = (A, B_1, \dots, B_m) \in {\mathrm{GL}}(d,{\mathbb{K}}) \times {\mathfrak{gl}}(d,{\mathbb{K}})^m$ is poor
and that the matrix $A$ is unconstrained.
Then ${\mathbf{A}}$ is conspicuously poor.
\end{lemma}

\begin{proof}
Suppose $A$ is unconstrained.
In particular, $A$ has simple spectrum.
With a change of basis we can assume that $A$ is diagonal.

Now suppose that ${\mathbf{A}} = (A, B_1, \dots, B_m)$ is not conspicuously poor.
This means that for each off-diagonal position there is at least of the matrices $B_k$
that has a non-zero entry in that position.
(Notice that this fact does not depend on the change of basis chosen before.)

Since $A$ is unconstrained, the values $\lambda_i \lambda_j^{-1}$, 
where $(i,j)$ runs on the matrix positions outside the diagonal, are pairwise different, 
and all different from $1$. 
Recall that one can always (using Lagrange formula) find a polynomial 
whose values at finitely many different points are prescribed.
Restricting to polynomials $f$ such that $f(1)=0$,
it follows from \eqref{e.poly entry}
that the space $\Lambda({\mathbf{A}})$ contains all matrices $(y_{ij})$ 
with only zeros in the diagonal.
Since, by definition, $\Lambda({\mathbf{A}})$ also contains the identity matrix,
it contains all Toeplitz matrices.
So $\Lambda({\mathbf{A}})$ is transitive, i.e., ${\mathbf{A}}$ is not poor.
This proves the \lcnamecref{l.easy_fiber}.
\end{proof}

\subsection{Effective richness criteria for the case $m=1$}
We will describe an explicit set of rich data $(A,B)$ whose complement has codimension $1$.
In order to avoid technicalities, we will be sometimes informal, especially regarding questions of transversality.

\medskip

Let us say that a matrix $A\in {\mathrm{GL}}(d,{\mathbb{R}})$ is \emph{$(i)$-constrained}, where $1\leq i\leq 4$, if:
\begin{itemize}
	\item its eigenvalues, counted with multiplicity, satisfy exactly one elementary constraint, which is a type $i$ constraint,
	\item if there is a type $4$ constraint between the eigenvalues, then the matrix $A$ is \emph{not} diagonalizable.
\end{itemize}
Suppose that there is no $i$ for which the matrix $A$ is $(i)$-constrained; then:
\begin{itemize}
\item either $A$ is unconstrained, i.e., its eigenvalues (with multiplicity) satisfy no elementary constraint;
\item or the eigenvalues of $A$ satisfy at least two elementary constraints;
\item or $A$ has a (multiple) eigenvalue corresponding to at least two Jordan blocks.
\end{itemize}
If either of the last two cases hold, we say that $A$ is \emph{multiconstrained}.

\begin{prop}\label{p.cod_constraints}
\begin{enumerate}
\item The complement of the set of unconstrained matrices has codimension $1$ in ${\mathrm{GL}}(d,{\mathbb{R}})$.
\item The set of multiconstrained matrices has codimension $2$ in ${\mathrm{GL}}(d,{\mathbb{R}})$.
\end{enumerate}
\end{prop}

\begin{proof}[Informal proof]
Matrices that are not unconstrained have at least one constraint on their eigenvalues, so the corresponding set has codimension~$1$.

Matrices that are multiconstrained either have at least two constraints on their eigenvalues, or 
are derogatory, i.e., have an eigenvalue corresponding to at least two Jordan blocks.
In both cases, the corresponding set has codimension~$2$.
\end{proof}

Let us define \emph{adapted bases} for matrices $A$ that are not multiconstrained:
\begin{itemize}
\item If $A$ is unconstrained then an adapted basis is a basis of eigenvectors.
\item If $A$ is $(i)$-constrained, for $i = 1$, $2$, or $3$ 
then an adapted basis is an (ordered) basis of eigenvectors such that
the corresponding eigenvectors $\lambda_1,\ldots \lambda_d$ 
satisfy the canonical type $i$ constraint.
\item If $A$ is $(4)$-constrained then an \emph{adapted basis} for $A$ 
is a basis in which $A$ is written in the following \emph{modified Jordan form} 
$$
\left(
\begin{array}{cc|ccc}
\lambda_1 & \lambda_1 &          &       &           \\
0         & \lambda_1 &          &       &           \\
\hline
          &           &\lambda_3 &       &           \\
          &           &          &\ddots &           \\
          &           &          &       &\lambda_d  
\end{array}
\right).
$$
\end{itemize}
Obviously, such adapted bases always exist.

If a matrix $A$ is $(i)$-constrained then we say that a 
$d \times d$ matrix $B$ is a \emph{good match} for $A$, 
if there is an adapted basis for $A$ in which 
it writes as $B = (b_{ij})$, 
where all nondiagonal entries $b_{ij}$ are nonzero and 
if $b_{11}\neq b_{22}$, in the particular case where $A$ is $3$-constrained.

The usefulness of this definition is explained by the following 
\cref{p.rich_pair,p.cod_RH}.
(Actually, the definition of a good match matrix is stronger than necessary for the validity of the propositions below. But in order to avoid complications, we chose a condition that works for all types of constraints.)

\begin{prop}\label{p.rich_pair}
If $A$ is not multiconstrained and $B$ is a good match for $A$ then the pair $(A,B)$ is rich.
\end{prop}

In other words, ${\mathcal{P}}^{({\mathbb{C}})}_1$ is contained in the following set:
\begin{multline}\label{e.explicit}
{\mathcal{E}}:=\big\{ (A,B) \in {\mathrm{GL}}(d,{\mathbb{C}}) \times {\mathfrak{gl}}(d,{\mathbb{C}}) ; \; 
\text{either $A$ is multiconstrained} \\
\text{or $A$ is not multiconstrained but $B$ is not a good match for $A$} \big\}.
\end{multline}

\begin{prop}\label{p.cod_RH}
\begin{enumerate}
\item The set ${\mathcal{E}}$ has codimension $1$.
\item The set $\{(A,B) \in {\mathcal{E}}; \; A \text{ is not unconstrained}\}$ has codimension $2$.
\end{enumerate}
\end{prop}

\begin{proof}[Informal proof]
\Cref{p.cod_RH} follows from \cref{p.cod_constraints}
and from the fact that for each matrix $A$ that is not multiconstrained,
the set of $B$'s that are not good matches for $A$
has positive codimension in ${\mathfrak{gl}}(d,{\mathbb{C}})$.
\end{proof}

\cref{t.cod_data_C} in the case $m=1$ follows from the propositions above.
Therefore the other main results (\cref{t.cod_data_R,t.main,t.addendum,t.main_C}) in the $m=1$ 
case also follow from  the propositions.
For any of these results, the propositions give extra information of practical value: 
with the explicit definition of the set ${\mathcal{E}}$ in \eqref{e.explicit}, we know 
which $1$-jets should be avoided in \cref{t.main}, for example.
The discussion given in \cref{a.generic singular}
also applies; it gives explicit conditions on the $2$-jet extension of the map 
$A \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{R}})$ that ensure that $A$ satisfies the conclusions of \cref{t.main,t.addendum}.
 

\begin{proof}[Proof of \cref{p.rich_pair}]
Let $A$ and $B$ satisfy the hypotheses.
We need to show that $\Lambda(A,B) = {\mathfrak{R}}_{{\mathrm{Ad}}_A}({\mathrm{Id}},B)$ is a transitive subspace of ${\mathfrak{gl}}(d,{\mathbb{C}})$.
Let $\Gamma = {\mathfrak{R}}_{{\mathrm{Ad}}_A}(B)$,
so that $\Lambda(A,B) = \{{\mathrm{Id}}\} \vee \Gamma$.

The matrix $A$ is not multiconstrained and so has an adapted basis as above.
We change the basis so that $A$ and $B$ are ``canonical''.

The proof is divided in cases according to the type of constraint.
Except for the $(4)$-constrained case, the matrix $A$ is diagonal,
and so the space $\Gamma$ is described by \eqref{e.poly entry}.

\medskip\noindent\emph{Unconstrained case:}
It follows from \cref{l.easy_fiber}
that if $A$ is unconstrained and diagonal then the only way for the pair $(A,B)$ to be poor
is that $B$ has an off-diagonal zero entry.
(The reader should review the proof of \cref{l.easy_fiber}.)

\medskip\noindent\emph{$(1)$-constrained case:}
We see that the adjoint ${\mathrm{Ad}}_A$ has two eigenvalues (different from $1$) 
of multiplicity $2$, namely
$\lambda_1 \lambda_2^{-1} = \lambda_2 \lambda_3^{-1}$ and 
$\lambda_2 \lambda_1^{-1} = \lambda_3 \lambda_2^{-1}$.
By the same reasoning as in the unconstrained case, it follows that $\{{\mathrm{Id}}\} \vee \Gamma$ contains the space
$$
\big \{ (y_{ij}) \in {\mathfrak{gl}}(d,{\mathbb{C}}) ; \;  y_{11} = \cdots = y_{dd}\, , \;
b_{12}^{-1} y_{12} = b_{23}^{-1} y_{23}\, , \;  
b_{21}^{-1} y_{21} = b_{32}^{-1} y_{32}
\big \}.
$$
This is a generalized Toeplitz space, 
and so by \cref{ex.gen Toeplitz} it is transitive.

\medskip\noindent\emph{$(2)$-constrained case:}
The reasoning is very similar to that of the $(1)$-constrained case,
but now the adjoint has four eigenvalues (different from $1$) of multiplicity $2$.
The space $\Lambda(A,B)$ contains the following subspace:
\begin{align*}
\big \{ (y_{ij}) \in {\mathfrak{gl}}(d,{\mathbb{C}}) ; \;	
&y_{11} = \cdots = y_{dd}\, , \; 
 b_{13}^{-1} y_{13} = b_{24}^{-1} y_{24}\, , \\
&b_{12}^{-1} y_{12} = b_{34}^{-1} y_{34}\, , \; 
 b_{21}^{-1} y_{21} = b_{43}^{-1} y_{43}\, , \;
 b_{31}^{-1} y_{31} = b_{34}^{-1} y_{34}
\big \}.
\end{align*}
Again, this is a generalized Toeplitz space, 
and so it is transitive.

\medskip\noindent\emph{$(3)$-constrained case:}
This case is a little different from the two previous ones.
The adjoint has an eigenvalue $-1$ of multiplicity $2$.
Recalling that $b_{11}$ and $b_{22}$ are different, and making use of the identity matrix,
we see that $\Lambda(A,B)$ contains the following subspace:
$$
\tilde \Gamma = 
\big \{ (y_{ij}) \in {\mathfrak{gl}}(d,{\mathbb{C}}) ; \;  y_{33} = \cdots = y_{dd}\, , \;
b_{12}^{-1} y_{12} = b_{21}^{-1} y_{21}
\big \}.
$$
This is not a generalized Toeplitz space.
However, consider the linear automorphism $S$ that swaps the first two elements of the canonical basis of ${\mathbb{C}}^n$, and fixes the others.
Then
$$
S \cdot \tilde \Gamma = 
\big \{ (z_{ij}) \in {\mathfrak{gl}}(d,{\mathbb{C}}) ; \;  z_{33} = \cdots = z_{dd}\, , \;
b_{12}^{-1} z_{22} = b_{21}^{-1} z_{11}
\big \}
$$
is a generalized Toeplitz space.
By \cref{r.transitivity trick}, the space $S \cdot \tilde \Gamma$ is transitive, and so are $\tilde \Gamma$ and $\Lambda(A,B)$.

\medskip\noindent\emph{$(4)$-constrained case:}
This case is more involved because the operator ${\mathrm{Ad}}_A$ is not diagonalizable.
We will explain its Jordan form.
Let us explain visually how ${\mathrm{Ad}}_A$ acts: given any matrix, decompose it into blocks $C_{ij}$ as in the following picture $$\left(
\begin{array}{cc|c|c|ccc|c}
C_{22}& &\!\!C_{23}\!\!\! &\!\!C_{24}\!\!\! &  &\hdots & &\!\! C_{2d}\!\!\!\!\\
& & & & & & &\\
\hline C_{32} &&\!\!C_{33}\!\!\!& & &\hdots  &\\
\hline C_{42} & & &\!\!C_{44}\!\!\! & &\hdots  &\\
\hline & & & & & & &\\
\vdots& &\vdots &\vdots & &\ddots & &\\
& & & & & & &\\
\hline C_{d2}& & & & &\hdots & &\!\!C_{dd}\!\!\!\!\\
\end{array}
\right)
$$
where the block $C_{22}$ is a $2\times 2$ matrix, the blocks $C_{2j}$ are $2\times 1$, the blocks $C_{i2}$ are $1\times 2$ and the  others are $1\times 1$. Then, the operator ${\mathrm{Ad}}_A$ leaves invariant the space $\Gamma_{ij}$ of matrices whose nonzero coefficients lie inside the block $C_{ij}$.

Let us use notations $J_t(\lambda)$ from \eqref{e.block} and  $E_{i,j}$ from \eqref{e.Eij}.
It is easily computed that the operator ${\mathrm{Ad}}_A$ has the following properties:
\begin{itemize}
	
\item the matrix of ${{\mathrm{Ad}}_A}{|\Gamma_{11}}$ with respect to the basis formed by 
$M_1 = -2 E_{12}$, $M_2 = E_{11}-E_{12}-E_{22}$, $M_3 = E_{21}$, $M_4 = E_{11}+E_{22}$
is $\begin{pmatrix}J_3(1) & 0 \\ 0 & 1 \end{pmatrix}$.

\item For any $j\geq 3$, the matrix of ${{\mathrm{Ad}}_A}{|\Gamma_{2j}}$ 
with respect to the basis formed by $\lambda_2 \lambda_j^{-1}E_{1,j}$ and $E_{2,j}$
(where we use the notation $E_{i,j}$ from \eqref{e.Eij})
is $J_2(\lambda_2 \lambda_j^{-1})$.

\item For any $i\geq 3$, the matrix of ${{\mathrm{Ad}}_A}{|\Gamma_{i2}}$ 
with respect to the basis formed by $-\lambda_i \lambda_2^{-1}E_{i,1}$ and $E_{i,2}$
is $J_2(\lambda_i \lambda_2^{-1})$.

\item For $3\leq i,j\leq d$, the matrix of ${{\mathrm{Ad}}_A}{|\Gamma_{ij}}$ 
with respect to the basis formed by the single vector $E_{ij}$ is $(\lambda_i\lambda_j^{-1})$.
 
\item The spaces $\Gamma_{ij}$, for $2\leq i,j\leq d$ have  respective spectra $\{\lambda_i\lambda_j^{-1}\}$, which for $i\neq j$ are pairwise disjoint and different from $\{1\}$. 
\end{itemize}

The concatenation of the bases described above gives a Jordan basis for ${{\mathrm{Ad}}_{A}}$. 
Now take a matrix $B$ that is a good match for $A$,
and consider its expression as a linear combination of the elements of that Jordan basis.
One easily checks that all coefficients in this linear combination are nonzero, except possibly
the coefficients of the vectors $M_1$, $M_2$, $M_4$ and the vectors $E_{ii}$, for all $3\leq i\leq d$.
Consider now the splitting ${\mathrm{Mat}}_{d \times d}({\mathbb{C}}) =  V \oplus \Delta$, where $\Delta$ is the subspace ${\mathbb{C}} M_4 \oplus E_{33} \oplus \ldots \oplus E_{dd}$ of the space of diagonal matrices, and 
$V$ is the space spanned by all other elements of the above Jordan basis. Note that
\begin{align*}
V=({\mathbb{C}} M_1+{\mathbb{C}} M_2+{\mathbb{C}} M_3)\oplus \left(\bigoplus_{2\leq i,j\leq d\atop i\neq j}\Gamma_{ij}\right)
\end{align*}
is a decomposition of $V$ into ${\mathrm{Ad}}_A$-invariant subspaces with pairwise disjoint spectra. 
Let $\pi$ be the projection onto $V$ along $\Delta$.
Using \cref{l.sum}, we see that $\pi(B)$ is a cyclic vector for ${\mathrm{Ad}}_A | V$.
So, using the ${\mathrm{Ad}}_A$-invariance of the spaces $V$ and $\Delta$, we have
$$
\pi (\Gamma) = 
\pi \big( {\mathfrak{R}}_{{\mathrm{Ad}}_A} (B) \big) = 
{\mathfrak{R}}_{{\mathrm{Ad}}_A} (\pi(B)) = V. 
$$
Note that $V$ contains the matrices $E_{ij}$, for all $i\neq j$, hence $\{{\mathrm{Id}}\} \vee V$ is a generalized Toeplitz space. As $\pi$ projects along a subspace of diagonal matrices,  $\{{\mathrm{Id}}\} \vee \Gamma$ is again a generalized Toeplitz space and in particular is a transitive space.

\medskip

We have considered the four types, and \cref{p.rich_pair} is proved.
\end{proof}

\section{Some general facts on dimensions of algebraic sets}\label{a.algebraic}

In this appendix we prove \cref{l.pret_a_porter,t.schubert},
which were used in \cref{s.cod proof}.
\Cref{l.pret_a_porter} is a simple consequence of standard theorems in algebraic geometry, 
but for the reader's convenience let us spell out the details. 
\Cref{t.schubert} follows from intersection theory of the Grassmannians (``Schubert calculus'').
We tried to make the exposition the least technical as possible, to make it accessible to non-experts (like ourselves).

\subsection{Fiberwise dimension estimate}

\begin{proof}[Proof of \cref{l.pret_a_porter}]
In what follows, all topologies are Zariski.
We will prove the equivalent ``dual form'' of the lemma, namely, that
the sets 
$$
Y_k = \big\{ y \in \pi(X) ; \; \dim \pi^{-1}(y) \ge k \big\}
$$
are algebraically closed in $Y$,
and 
\begin{equation}\label{e.dual_formula}
\dim X = \max_{k ; \;  Y_k \neq {\varnothing}}
\big( k + \dim Y_k \big).
\end{equation}

First, the sets $X_k = \{x \in X ; \; \dim \pi^{-1}(\pi(x)) \ge k\}$ are closed.
(see \cite[Thrm.~11.12]{Harris}).
So, by \cref{p.projection}, $Y_k = \pi(X_k)$ is closed.

For each $k$ with $X_k \neq {\varnothing}$, 
let $X_{k,i}$ denote the irreducible components of $X_k$.
Let 
$$
\mu(k,i) = \min_{x \in X_{k,i}} \dim \pi^{-1}(\pi(x)) \, .
$$
Then, by \cite[Thrm.~11.12]{Harris} (and the fact that 
taking closures does not affect dimension) 
we have 
$$
\dim X_{k,i} = \mu(k,i) + \dim \pi(X_{k,i}) \, .
$$
By definition, $\mu(k,i) \ge k$;
moreover equality holds unless $X_{k,i} \subset X_{k+1}$.
So 
$$
X_{k,i} \not \subset X_{k+1} \ \Rightarrow \ 
\dim X_{k,i} 
=  k + \dim \pi(X_{k,i})  
\le k+ \dim Y_k \, .
$$
Since $X = \bigcup_{X_{k,i} \not \subset X_{k+1}} X_{k,i}$,
this proves the $\le$ inequality in \eqref{e.dual_formula}.

To prove the converse inequality, fix any $k$ with $Y_k \neq {\varnothing}$.
Find $i$ such that $\dim \pi(X_{k,i}) = \dim Y_k$.
Then
$$
\dim X \ge \dim X_{k,i} =  \mu(k,i) + \dim Y_k \ge k + \dim Y_k.
$$
This proves \eqref{e.dual_formula} and hence the \lcnamecref{l.pret_a_porter}.
\end{proof}

\subsection{A particular case of \cref{t.schubert}}\label{ss.particular}

Let us begin the proof of \cref{t.schubert}.
For the reader's convenience we recall the notations and the statement.

If $M \in {\mathrm{Mat}}_{n \times m}({\mathbb{C}})$, let $\operatorname{col} M \subset {\mathbb{C}}^n$ denote the column space of $M$.
A set $X \subset {\mathrm{Mat}}_{n \times m}({\mathbb{C}})$ is called \emph{column-invariant}
if 
$$
\left.
\begin{array}{c}
M \in X \\ 
N \in {\mathrm{Mat}}_{n \times m}({\mathbb{C}})\\
\operatorname{col} M = \operatorname{col} N
\end{array}
\right\} \ \Rightarrow \ 
N \in X.
$$
So a column-invariant set $X$ is characterized by its set of column spaces.
We enlarge the latter set by including also subspaces, thus defining:
$$
\ldbrack X \rdbrack := \big\{ E \text{ subspace of } {\mathbb{C}}^n ; \; E \subset \operatorname{col} M \text{ for some } M \in X \big\}.
$$
Then we have:

\begin{repeatedthm} 
Let $X \subset {\mathrm{Mat}}_{n \times m}({\mathbb{C}})$ be an algebraically closed, column-invariant set.
Suppose $E$ is a vector subspace of ${\mathbb{C}}^n$
that does not belong to $\ldbrack X \rdbrack$.
Then
$$
\operatorname{codim} X \ge m + 1 - \dim E \, .
$$
\end{repeatedthm}

It is obvious that the algebraicity hypothesis is indispensable. 

\medskip

Define
\begin{equation}\label{e.R_k}
R_k := \big \{ A \in {\mathrm{Mat}}_{n \times m}({\mathbb{C}}) ; \; \operatorname{rank} A \le k \big\} \, .
\end{equation}
We recall (see \cite[Prop.~12.2]{Harris}) that 
this is an irreducible algebraically closed set of codimension
\begin{equation}\label{e.cod_R_k}
\operatorname{codim} R_k = (m-k)(n-k) \qquad \text{if } 0 \le k \le \min(m,n).
\end{equation}

\begin{proof}[Proof of \cref{t.schubert} in the case $E = {\mathbb{C}}^n$]
If $E = {\mathbb{C}}^n$ then the hypothesis ${\mathbb{C}}^n \not\in \ldbrack X \rdbrack$
means that $X \subset R_{n-1}$.
We can assume that $n-1 \le m$, otherwise the conclusion of the 
\lcnamecref{t.schubert} is vacuous.
Thus $\operatorname{codim} X \ge \operatorname{codim} R_{n-1} = m + 1 - n$, as we wanted to show.
\end{proof}

\subsection{Reduction to a property of Grassmannians} \label{ss.reduction}

As we will see, to prove \cref{t.schubert} 
it is sufficient to prove a dimension estimate (\cref{t.schubert2} below) 
for certain subvarieties of a Grassmaniann.

\subsubsection{Grassmannians}

Given integers $n > k \ge 1$, the \emph{Grassmanniann} $G_k({\mathbb{C}}^n)$ 
is the set of the vector subspaces of ${\mathbb{C}}^{n}$ of 
dimension $k$.

The Grassmannian can be interpreted as a subvariety of a higher dimensional complex projective space
using the \emph{Pl\"ucker embedding} $G_k({\mathbb{C}}^n) \to P(\bigwedge^k {\mathbb{C}}^n)$,
which maps each $V \in G_k({\mathbb{C}}^n)$ to $[v_1 \wedge \cdots \wedge  v_k]$,
where $\{v_1, \dots, v_k\}$ is any basis of $V$/
This is clearly an one-to-one map.
It can be shown (see e.g.~\cite[p.~61ff]{Harris}) that the image is 
an algebraically closed subset 
of $P(\bigwedge^k {\mathbb{C}}^n)$.
Its dimension is 
\begin{equation}\label{e.dim_G}
\dim G_k({\mathbb{C}}^n) = k(n-k).
\end{equation}

If $E \subset {\mathbb{C}}^n$ is a vector space with $\dim E = e \le k$ then 
we consider the following subset of $G_k({\mathbb{C}}^n)$: 
\begin{equation}\label{e.special schubert}
S_k(E) := \big\{V \in G_k({\mathbb{C}}^n) ; \; V \supset E \big\}.
\end{equation}
(This is a Schubert variety of a special type, as we will see later.)
Since any $V \in S_k(E)$ can be written as $E \oplus W$ for some $V \subset W^\perp$,
we see that $S_k(E)$ is homeomorphic to $G_{k-e}({\mathbb{C}}^{n-e})$.

We will 
show that an algebraic set that avoids $S_k(E)$ cannot be too large:

\begin{thm}\label{t.schubert2}
Fix integers $1 \le e \le k < n$. 
Suppose that $Y$ is an algebraically closed subset of $G_k({\mathbb{C}}^n)$
that is disjoint from $S_k(E)$,
for some $e$-dimensional subspace $E \subset {\mathbb{C}}^n$.
Then $\operatorname{codim} Y \ge k + 1 - e$.
\end{thm}

\subsubsection{Proof of \cref{t.schubert} assuming \cref{t.schubert2}} \label{sss.reduction}

Assuming \cref{t.schubert2} for the while, let us see how it yields \cref{t.schubert}.

Recalling notation \eqref{e.R_k}, define the quasiprojective variety
$$
\hat{R}_k := R_k {\smallsetminus} R_{k-1} \, .
$$
We define a map $\pi_k \colon \hat{R}_k \to G_k({\mathbb{C}}^n)$
by $A \mapsto \operatorname{col} A$.

\begin{lemma}\label{l.projection}
If $X$ is an algebraically closed column-invariant subset of $\hat{R}_k$
then $Y = \pi_k(X)$ is algebraically closed subset of $G_k({\mathbb{C}}^n)$,
and the codimension of $Y$ inside $G_k({\mathbb{C}}^n)$ is the same as the codimension of $X$ inside $\hat{R}_k$.
\end{lemma}

\begin{proof}
First, let us see that $\pi_k \colon \hat{R}_k \to G_k({\mathbb{C}}^n)$ is a regular map.
We identify $G_k({\mathbb{C}}^n)$ with the image of the Pl\"ucker embedding.
In a Zariski neighborhood of each  matrix $A \in \hat{R}_k$, 
the map $\pi_k$ can be defined as $A \mapsto [a_{j_1} \wedge \dots \wedge a_{j_k}]$
for some $j_1 < \dots < j_k$, where $a_j$ is the $j^\text{th}$ column of $A$.
This shows regularity.
	
Next, let us see that $Y = \pi_k (X)$ is closed with respect to the classical (not Zariski)
topology.
Consider the subset $K$ of $X$ formed by the matrices $A \in \hat{R}_k$ 
whose first $k$ columns form an orthonormal set, and whose $m-k$ remaining columns are zero.
Then $K$ is compact (in the classical sense), and thus so is $\pi_k(K)$.
But column-invariance of $X$ implies that $\pi_k(K) = Y$, so $Y$ is closed (in the classical sense).

It follows (see e.g.~\cite[p.39]{Harris}) 
from regularity of $\pi_k$ is regular that the set $Y$ is constructible, i.e., 
it can be written as
$$
Y = \bigcup_{i=1}^{p} Z_i {\smallsetminus} W_i \, ,
$$
where $Z_i \varsupsetneq W_i$ are algebraically closed subsets of $G_k({\mathbb{C}}^n)$.
We can assume that each $Z_i$ is irreducible.
It follows from \cite[Thrm.~2.33]{Mumford} that $\overline{Z_i {\smallsetminus} W_i} = Z_i$,
where the bar denotes closure in the classical sense.
In particular, $Y = \overline{Y} = \bigcup_{i=1}^{p} Z_i$,
showing that $Y$ is algebraically closed.

We are left to show the equality between codimensions.
Since the codimension of an algebraically closed set equals the minimum of the codimensions of its components, we can assume that $X$ is irreducible.

By column-invariance of $X$,
for each $y\in Y$, the whole fiber $\pi^{-1}(y)$ is contained in $X$.
All those fibers have the same dimension $\mu = km$.
By \cite[Thrm.~11.12]{Harris}, $\dim X = \dim Y + km$.
By \eqref{e.cod_R_k} and \eqref{e.dim_G}, we have $\dim \hat{R}_k - \dim G_k = km$,
so the claim about codimensions follows.
\end{proof}

\begin{proof}[Proof of \cref{t.schubert}]
Let $X \subset {\mathrm{Mat}}_{n \times m}({\mathbb{C}})$ be a nonempty algebraically closed, column-invariant set.
Suppose $E$ is a vector subspace of ${\mathbb{C}}^n$ that does not belong to $\ldbrack X \rdbrack$.
Let $e = \dim E$.
We can assume $e > 0$ (otherwise the result is vacuously true), 
and $e<n$ (because the case $e=n$ was already considered in \S~\ref{ss.particular}).

Notice that $X \subset R_{n-1}$.
Let 
$$
X_k := X \cap \hat{R}_k \quad \text{and} \quad 
Y_k := \pi_k(X_k) , \quad \text{for } 0 \le k \le \min(m,n-1).
$$
For every $k$ with $e \le k < n$, the set $Y_k$ is disjoint from 
the set $S_k(E)$ defined by \eqref{e.special schubert}.
In view of \cref{l.projection} and \cref{t.schubert2}, we have
$$
\operatorname{codim}_{\hat{R}_k} X_k =  \operatorname{codim} Y_k \ge k + 1 - e \, .
$$
So the codimension of $X_k$ as a subset of ${\mathrm{Mat}}_{n\times m}({\mathbb{C}})$ is
\begin{align*}
\operatorname{codim} X_k &=   \operatorname{codim} \hat{R}_k + \operatorname{codim}_{\hat{R}_k} X_k \\
           &\ge (m-k)(n-k) + k + 1 - e =: f(k) \, .
\end{align*}
The function $f(k)$ is decreasing on the interval $0 \le k \le \min(m,n-1)$.
Therefore:
\begin{multline*}
\operatorname{codim} X 
=   \min_{0 \le k \le \min(m,n-1)} \operatorname{codim} X_k  
\ge \min_{0 \le k \le \min(m,n-1)} f(k) \\
= f(\min(m,n-1)) 
= m + 1 - e,
\end{multline*}
as claimed.
This proves \cref{t.schubert} modulo \cref{t.schubert2}.
\end{proof}

The proof of \cref{t.schubert2} will be given in \S~\ref{ss.end},
after we explain the necessary tools in \S\S~\ref{ss.schubert}, \ref{ss.intersection}.

\subsection{Schubert calculus} \label{ss.schubert}

Here we will outline some facts about the intersection of Schubert varieties.
The readable expositions \cite{Blasiak,Vakil} contain more information.

\medskip

A (complete) flag 
in ${\mathbb{C}}^{n}$ is a sequence of subspaces $F_0 \subset F_1 \subset \cdots \subset F_{n}$
with $\dim F_j = j$. We denote $F_\bullet = \{F_i\}$.

Given $V \in G_k ({\mathbb{C}}^n)$, 
its \emph{rank table} (with respect to the flag $F_\bullet$)
is the datum $\dim (V \cap F_j)$, $j=0,\dots,n$.
The \emph{jumping numbers} are
the indexes $j \in \{1,\dots,n\}$ such that 
$\dim (V \cap F_j) - \dim (V \cap F_{j-1})$ is positive (and thus equal to $1$).
Of course, if one knows the jumping numbers, one know the rank table and vice-versa.
Let us define a third way to encode this information:
Consider a rectangle of height $m$ and width $n-m$, divided in $1 \times 1$ squares.
We form a path of square edges:
Start in the northeast corner of the rectangle.
In the $j^\text{th}$ step ($1 \le j \le n$),
if $j$ is a jumping number then we move one unit in the south direction,
otherwise we move one unit in the west direction.
Since there are exactly $k$ jumping numbers, 
the path ends at the southwest corner of the rectangle.
The \emph{Young diagram} of $V$ with respect to the flag $F_\bullet$ is 
the set of squares in the rectangle that lie northwest of the path.
We denote a Young diagram by $\lambda = (\lambda_1, \lambda_2, \dots, \lambda_k)$,
where $\lambda_i$ is the number of squares in the $i^\text{th}$ row (from north to south).
Its \emph{area} $\lambda_1+\cdots+\lambda_k$ is denoted by $|\lambda|$.

\begin{example}\label{ex.Young}
Here is a possible rank table with $k=5$, $n=12$;
the jumping numbers are underlined:
\begin{center}
\begin{tabular}{rrrrrrrrrrrrrr}
$j = $                 & 0 & 1 & 2 & \underline{3} & 4 & 5 & \underline{6} & 7 & \underline{8} & \underline{9} & 10 & \underline{11} & 12 \\
$\dim (W \cap F_j)=$ & 0 & 0 & 0 &       1 & 1 & 1 &       2 & 2 &        3 &      4 &  4 &        5 &  5 \\
\end{tabular}
\end{center}
The associated path in the rectangle is:
\setlength{\unitlength}{.4cm}
\begin{center}
\raisebox{-3\unitlength}{
\begin{picture}(7,5)
\thinlines
\put(0,0){\grid(7,5)(1,1)}
\thicklines
\put(7,5){\vector(-1,0){1}} 
\put(6,5){\vector(-1,0){1}} 
\put(5,5){\vector(0,-1){1}} 
\put(5,4){\vector(-1,0){1}} 
\put(4,4){\vector(-1,0){1}} 
\put(3,4){\vector(0,-1){1}} 
\put(3,3){\vector(-1,0){1}} 
\put(2,3){\vector(0,-1){1}} 
\put(2,2){\vector(0,-1){1}} 
\put(2,1){\vector(-1,0){1}} 
\put(1,1){\vector(0,-1){1}} 
\put(1,0){\vector(-1,0){1}} 
\end{picture}
}
\end{center}
and so the Young diagram is $$\lambda=\tiny{\yng(5,3,2,2,1)} = (5,3,2,2,1).$$
\end{example}

In general, we have:
\begin{itemize}
\item $\lambda = (\lambda_1, \dots, \lambda_k)$ is a possible Young diagram if and only if
$n-k \ge \lambda_1 \ge \dots \ge \lambda_k \ge 0$.
\item If $j_1 < \dots < j_k$ are the jumping numbers then $\lambda_i = n-k-j_i+i$.
\end{itemize}

The set of $V \in G_k({\mathbb{C}}^n)$ that have a given Young diagram 
$\lambda$ 
is called a 
\emph{Schubert cell}, denoted by $\Omega(\lambda)$ or $\Omega(\lambda,F_\bullet)$.
Each Schubert cell is a topological disk of real codimension $2|\lambda|$.
The Schubert cells (for a fixed flag) give a CW decomposition of the space $G_k({\mathbb{C}}^n)$.
The closure of $\Omega(\lambda)$ (in either classical or Zariski topologies) is
the set of $V \in G_k({\mathbb{C}}^n)$ such that $\dim (V \cap F_{j_i}) \ge i$ for each $i=1,\ldots,n$
(where $j_1 < \dots < j_k$ are the jumping numbers associated to $\lambda$).
These sets are closed irreducible varieties,
called \emph{Schubert varieties}. (See e.g.~\cite[\S9.4]{Fulton}.)

\begin{example}\label{ex.special schubert}
If $E \subset {\mathbb{C}}^n$ is a subspace with $\dim E = e \le k$ then 
the set $S_k(E)$ defined by \eqref{e.special schubert} is 
a Schubert variety $\bar\Omega(\lambda,F_\bullet)$,
where $F_\bullet$ is any flag with $F_e = E$ and
\setlength{\unitlength}{.25cm}
\begin{equation}\label{e.special young}
\lambda = \big( \underbrace{n-k,\dots,n-k}_{e \text{ times}}, \underbrace{0,\dots,0}_{k-e \text{ times}} \big) = 
\raisebox{-4\unitlength}{
\begin{picture}(12,8)
\thinlines
\put(0,0){\grid(12,8)(1,1)}
\multiput(0,5)(0,1){3}{\multiput(0,0)(1,0){12}{\drawline(.5,0)(1,.5)\drawline(0,0)(1,1)\drawline(0,.5)(.5,1)}}
\end{picture}
}
\end{equation}
\end{example}

Let $A^*(k,n)$ denote the set of formal linear combinations with integer coefficients of
Young diagrams in the $k \times (n-k)$ rectangle.
This is by definition an abelian group.

\begin{prop}
There is a second binary operation called the \emph{cup product} and denoted by the symbol ${\smallsmile}$ 
that makes $A^*(k,n)$ a commutative ring, and 
is characterized by the following properties:

If $\lambda$ and $\mu$ are Young diagrams with respective areas $r$ and $s$
then their cup product is of the form:
$$
\lambda {\smallsmile} \mu = \nu_1 + \cdots + \nu_N \, .
$$
where $\nu_1$, \dots, $\nu_N$ are Young diagrams with area $r+s$
(possibly with repetitions, possibly $N=0$).
Moreover, there are flags $F_\bullet$, $G_\bullet$, $H^{(i)}_\bullet$ such that
the manifolds $\bar\Omega(\lambda,F_\bullet)$ and $\bar\Omega(\mu,G_\bullet)$ are transverse
and their intersection is $\bigcup \bar\Omega(\nu_i,H^{(i)}_\bullet)$. 
\end{prop}

\begin{example}
Working in $A^*(2,4)$,
let us compute the products of the Young diagrams 
$\lambda = {\tiny \yng(2)}$ and $\mu={\tiny \yng(1,1)}$.
Fix a flag $F_\bullet$.
Then
$\bar\Omega(\lambda, F_\bullet)$ is the set of $W \in G_2({\mathbb{C}}^4)$ that contain $F_1$,
and $\bar\Omega(\mu, F_\bullet)$ is the set of $W \in G_2({\mathbb{C}}^4)$ that are contained in $F_3$.
Take another flag $G_\bullet$ which is in general position with respect to $F_\bullet$,
that is $F_i \cap G_{4-i} = \{0\}$.
Then:
\begin{itemize}
\item The set
$\bar\Omega(\lambda, F_\bullet) \cap \bar\Omega(\lambda, G_\bullet)$ 
contains a single element, namely $F_1 \oplus G_1$,
and thus equals $\bar\Omega((2,2),H_\bullet) = \{H_2\}$ for an appropriate flag $H_\bullet$.
This shows that
$\lambda {\smallsmile} \lambda = {\tiny \yng(2,2)}$.

\item The space $F_3 \cap G_3$ is $2$-dimensional and thus
is the single element of 
$\bar\Omega(\mu, F_\bullet) \cap \bar\Omega(\mu, G_\bullet)$.
So
$\mu {\smallsmile} \mu = {\tiny \yng(2,2)}$.

\item The set 
$\bar\Omega(\lambda, F_\bullet) \cap \bar\Omega(\mu, G_\bullet)$
is empty, thus $\lambda {\smallsmile} \mu = 0$.
\end{itemize}
However, if we work in $A^*(4,8)$  
then it can be shown that:
$$
{\tiny \yng(2)}   {\smallsmile} {\tiny \yng(2)}   = {\tiny \yng(2,2)} + {\tiny \yng(4)} + {\tiny \yng(3,1)}, \quad
{\tiny \yng(1,1)} {\smallsmile} {\tiny \yng(1,1)} = {\tiny \yng(2,2)} + {\tiny \yng(2,1,1)} + {\tiny \yng(1,1,1,1)}, \quad
{\tiny \yng(2)}   {\smallsmile} {\tiny \yng(1,1)} = {\tiny \yng(3,1)} + {\tiny \yng(2,1,1)}.
$$
If we drop the terms that do not fit in a $2 \times 2$ rectangle, we reobtain the results for $G_2({\mathbb{C}}^4)$. 
\end{example}

The general computation of the product $\lambda {\smallsmile} \mu$ is 
not simple and can be done in various ways. 
For our purposes, however, it will be sufficient to know when 
the product is zero or not.
The answer is provided by the following simple \lcnamecref{l.overlap}: 

\begin{lemma}[\cite{Fulton}, p.~148--149]\label{l.overlap}
Let $\lambda$ and $\mu$ be Young diagrams in the $k \times (n-k)$ rectangle.
The following two conditions are equivalent:
\begin{enumerate}

\item\label{i.nonzero} 
$\lambda {\smallsmile} \mu \neq 0$.

\item\label{i.nonoverlap} 
If one draws inside the $k \times (n-k)$ rectangle
the Young diagrams of $\lambda$ and $\mu$,
being the later rotated by $180^{\circ}$ and put in the southeast corner,
then the two figures do not overlap
(see \cref{f.nooverlap}). 
Equivalently, $\lambda_i + \mu_{k+1-i} \le n-k$ for every $i=1, \ldots, n$.
\end{enumerate}
\end{lemma}

\begin{figure}[hbt]  
\setlength{\unitlength}{.4cm}
\begin{picture}(7,5)

\thicklines
\put(0,0){\grid(7,5)(1,1)}

\thinlines
\put(0,0){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(0,1){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(1,1){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(0,2){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(1,2){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(0,3){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(1,3){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(2,3){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(0,4){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(1,4){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(2,4){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(3,4){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}
\put(4,4){\drawline(.75,0)(1,.25)\drawline(.5,0)(1,.5)\drawline(.25,0)(1,.75)\drawline(0,0)(1,1)\drawline(0,.25)(.75,1)\drawline(0,.5)(.5,1)\drawline(0,.75)(.25,1)}

\thinlines
\put(2,0){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(3,0){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(4,0){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(5,0){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(6,0){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(2,1){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(3,1){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(4,1){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(5,1){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(6,1){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(3,2){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(4,2){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(5,2){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(6,2){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(5,3){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\put(6,3){\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}

\end{picture}
\caption{{\footnotesize 
Consider $k=5$, $n=12$, $\lambda=(5,3,2,2,1)$, and $\mu = (5,5,4,2,0)$.
The picture shows that the non-overlap condition~(\ref{i.nonoverlap}) from \cref{l.overlap}
is satisfied, and in particular $\lambda {\smallsmile} \mu \neq 0$. (This example is reproduced from 
\cite[p.~150]{Fulton}.)}}
\label{f.nooverlap}
\end{figure}

\subsection{Intersection of subvarieties of the Grassmannian}\label{ss.intersection}

Next we explain how the Schubert calculus sketched above can be used to obtain information
about intersection of general subvarieties of the Grassmannian,
by means of cohomology and Poincar\'{e} duality.
See \cite[Appendix~B]{Fulton} and \cite{Hutchings} for further details.

\medskip

Any topological space $X$ has singular homology groups $H_i X$ and cohomology groups $H^i X$ (here taken always with integer coefficients). With the cup product $H^i X \times H^j X \to H^{i+j} X$,
the cohomology $H^* X = \bigoplus H^i X$ has a ring structure.

If $X$ is a real compact oriented manifold of dimension $d$ then 
the homology group $H_d X$ is canonically isomorphic to ${\mathbb{Z}}$, with a generator $[X]$
called the \emph{fundamental class} of $X.$
In addition, there is \emph{Poincar\'{e} duality isomorphism}
$H^i X \to H_{d-i} X$, which is given by 
$\alpha \mapsto \alpha \smallfrown [X]$
(taking the cap product with the fundamental class).
Let us denote by $\omega \mapsto \omega^*$ the inverse isomorphism.

Next suppose $Y$ and $Z$ are compact oriented submanifolds of $X$, of codimensions $i$ and $j$ respectively.
Also suppose that $Y$ and $Z$ have transverse intersection
$Y \cap Z$, which therefore is either empty or a compact submanifold of codimension $i + j$,
which is oriented in a canonical way.
The images of the fundamental classes of $Y$, $Z$, and $Y\cap Z$ under the inclusions into $X$
define homology classes that we denote (with a slight abuse of notation) by
$[Y] \in H_{d-i} X$, $[Z] \in H_{d-j} X$, $[Y\cap Z] \in H_{d-i-j} X$.
Then their Poincar\'e duals 
$[Y]^*\in H^i X$, $[Z]^* \in H^j X$, and $[Y \cap Z]^* \in H^{i+j} X$
are related by: 
$$
[Y]^* {\smallsmile} [Z]^* = [Y \cap Z]^* \, .
$$
That is, \emph{cup product is Poincar\'{e} dual to intersection.}

Now consider the case where $X$ is a projective nonsingular (i.e., smooth) complex variety,
and $Y$ and $Z$ are irreducible  subvarieties of $X$.
Obviously, the fundamental class $[X]$ makes sense, because $X$ is a compact manifold
with a canonical orientation induced from the complex structure.
A deeper fact (see \cite[Appendix~B]{Fulton})
is that fundamental classes $[Y]$ and $[Z]$ can also be canonically associated to 
the (possibly singular) subvarieties $Y$ and $Z$,
and the Poincar\'{e} duality between cup product and intersection 
works in this situation.
More precisely,
suppose that $Y$ and $Z$ are transverse in the algebraic sense:
$Y \cap Z$ is a union of subvarieties $W_1$, \dots, $W_\ell$
whose codimensions are the sum of the codimensions of $Y$ and $Z$,
and for each $i=1,\dots,\ell$, the tangent spaces
$T_w Y$ and $T_w Z$ are transverse 
for all $w$ in a Zariski-open subset of $W_i$.
Then each $W_i$ has its canonical fundamental class, 
and the following duality formula holds:
$$
[Y]^* {\smallsmile} [Z]^* = [W_1]^* + \cdots + [W_\ell]^* \, .
$$

\medskip

In our application of this machinery, $X$ will be the Grassmannian $G_k({\mathbb{C}}^n)$.
In this case:
\begin{itemize}
\item The fundamental classes of the Schubert varieties $[\bar\Omega(\lambda, F_\bullet)]$
do not depend on the flag $F_\bullet$.
\item Let $\sigma_\lambda$ denote the Poincar\'e dual of $[\bar\Omega(\lambda, F_\bullet)]$.
Then $H^{2r} G_k({\mathbb{C}}^n)$ is a free abelian group and the elements $\sigma_\lambda$
with $|\lambda| = r$ form a set of generators.
(The cohomology groups of odd codimension are zero.)
\item The cup product on cohomology agrees with the ``cup'' product of Young diagrams 
explained in the previous section.
\end{itemize}

\subsection{End of the proof}\label{ss.end}

We are now able to prove \cref{t.schubert2}. 

\begin{proof}[Proof of \cref{t.schubert2}]
Let $1 \le e \le k < n$.
Let $E \subset {\mathbb{C}}^n$ be a subspace of dimension $e$,
and consider the set $S_k(E)$ defined by \eqref{e.special schubert}.
Recall from \cref{ex.special schubert} that 
this is the Schubert variety for the Young diagram $\lambda$ given by \eqref{e.special young}.

Now consider a (nonempty) subvariety $Y \subset G_k({\mathbb{C}}^n)$
that is disjoint from $S_k(E)$.
We want to give a lower bound for the codimension $c$ of $Y$.
We can of course assume that $Y$ is irreducible.

Let $[Y]^*$ be the dual of fundamental class of $Y$.
This is a nonzero element of $H^{2c} G_k({\mathbb{C}}^n)$.
It can be expressed as  
$\sum n_i \sigma_{\mu_i}$,
where $\mu_i$ are Young diagrams with area $|\mu_i|=c$,
and $n_i$ are nonzero integers.
In fact we have $n_i>0$, because of the canonical
orientations induced by complex structure. 

Since the intersection between $S_k(E)$ and $Y$ is empty (and in particular transverse),
Poincar\'{e} duality gives $[S_k(E)]^* {\smallsmile} [Y]^* = 0$.
Therefore we have $\sigma_\lambda {\smallsmile} \sigma_{\mu_i} = 0$ for each $i$.

By \cref{l.overlap},
if we draw the Young diagram of $\mu_i$
rotated by $180^{\circ}$ and put in the southeast corner
of the $k \times (n-k)$ rectangle,
then it overlaps the Young diagram $\lambda$ pictured in \eqref{e.special young}.
This is only possible if $c \ge k-e+1$;
indeed the Young diagram $\mu$ with least area such that $\lambda {\smallsmile} \mu \neq 0$ is
$$
\mu = \big( \underbrace{1,\dots,1}_{k-e+1 \text{ times}}, \underbrace{0,\dots,0}_{e-1 \text{ times}} \big), 
$$
for which the overlapping picture becomes:
\setlength{\unitlength}{.25cm}
\begin{center}
\begin{picture}(12,8)
\thinlines
\put(0,0){\grid(12,8)(1,1)}
\multiput(0,5)(0,1){3}{\multiput(0,0)(1,0){12}{\drawline(.5,0)(1,.5)\drawline(0,0)(1,1)\drawline(0,.5)(.5,1)}}
\multiput(11,0)(0,1){6}{\drawline(.25,0)(.25,.25)\drawline(.75,.25)(.75,.5)\drawline(.25,.5)(.25,.75)\drawline(.75,.75)(.75,1)\put(0,0){\grid(1,1)(1,.25)}}
\thicklines
\put(11,5){\grid(1,1)(1,1)}
\end{picture}
\end{center}
This concludes the proof of  \cref{t.schubert2}.
\end{proof}

As explained in \S~\ref{sss.reduction}, \cref{t.schubert} follows.

\section{Stratifications and transversality}\label{a.strat_trans}

\subsection{Stratifications}
This appendix contains fundamental for the understanding of \cref{s.main proof}. We recall a few notions about stratifications and transversality, and prove \cref{p.stratifiedtransversality}.
We refer the reader to \cite{GWPL,Mather_71} for more details and proofs.

\medskip

Let $X$ be a smooth (i.e., $C^\infty$) manifold. 
A \emph{smooth stratification} of a closed subset ${\Sigma}\subset X$ is a filtration by closed subsets 
$$
{\Sigma} = {\Sigma}_n \supset {\Sigma}_{n-1} \supset \cdots \supset {\Sigma}_0
$$ 
such that for each $i$, the set $\Gamma_i = {\Sigma}_i {\smallsetminus} {\Sigma}_{i-1}$ (where $\Sigma_{-1}:={\varnothing}$) is a smooth submanifold of $X$ without boundary and the dimension of $\Gamma_i$ decreases strictly with increasing $i$. 
Each connected component of $\Gamma_i$ is called a \emph{stratum}. 
The \emph{codimension} in $X$ of a stratification is the codimension of the stratum of largest dimension. 
A stratification of  a set ${\Sigma}$ is not unique, but this codimension in $X$ does not depend on the choice of the stratification.

Actually, apart for discrete subsets ${\Sigma}\subset X$, if there is one smooth stratification, 
then there are infinitely many others. 
However, the subsets we deal with 
are endowed with certain \emph{canonical} stratifications:

\begin{otherthm}[Existence of canonical stratifications]\label{t.canonical_stratif}
Any algebraic set ${\Sigma}\subset {\mathbb{C}}^N$ admits a canonical smooth stratification
whose strata are complex submanifolds of ${\mathbb{C}}^N$.
Any closed semialgebraic set ${\Sigma}\subset {\mathbb{R}}^N$ admits a canonical smooth stratification
whose strata are semialgebraic submanifolds of ${\mathbb{R}}^N$.
\end{otherthm}

In the case of an irreducible algebraic set ${\Sigma}\subset {\mathbb{C}}^n$, 
the canonical stratification can be obtained as follows:
The connected components of the set of regular (i.e., non-singular) points form the higher-dimensional 
strata; then one decomposes the set of singular points of ${\Sigma}$ into irreducible components and proceeds by 
induction.

In any case, those canonical stratifications are uniquely characterized by a certain minimality property.
In particular, the canonical stratifications are equivariant under polynomial automorphisms of 
the ambient space. 

Another important property of the canonical stratifications 
is that they satisfy the so-called \emph{Whitney conditions $(a)$ and $(b)$}: 
\medskip

For any  sequence of points $x_n$ in a stratum $\Gamma$ of dimension $i$ converging to a point $y$ in a stratum $\Delta$ of dimension $<i$, if the sequence of tangent spaces $T_{x_n}\Gamma$ converges to an $i$-space $E\subset T_yX$, then we have
\begin{itemize}
\item[(a)] $E$ contains $T_y\Delta$,

\item[(b)] in a local chart, if a sequence $y_n\in \Delta$ converges to $y$ and if the lines $x_ny_n$ converge to a line $L\subset T_y\Delta$,  then $L\subset E$.
\end{itemize}

\medskip

A smooth stratification that satisfies the Whitney conditions is called a \emph{Whitney stratification}. Let us write down some properties.

\begin{prop}[Basic properties of Whitney stratifications]\label{p.whitney_properties} 
Let $X$, $Y$ be smooth manifolds.
Let 
\begin{equation}\label{e.filtration}
\Sigma_n \supset \cdots \supset \Sigma_0
\end{equation}
be a filtration of a set $\Sigma \subset X$.
Then:
\begin{enumerate}
\item \label{i.whitney_local}
Being a Whitney stratification is a local property of a filtration:
So if \eqref{e.filtration} is a Whitney stratification then
$\Sigma_n \cap U \supset \cdots \supset \Sigma_0 \cap U$ 
is a Whitney stratification, 
and conversely if each point in $\Sigma$ has an open neighborhood $U \subset X$ such that
$\Sigma_n \cap U \supset \cdots \supset \Sigma_0 \cap U$ is a Whitney stratification
then \eqref{e.filtration} is a Whitney stratification.
\item \label{i.whitney_product}
If  \eqref{e.filtration} is a Whitney stratification of codimension $m$ in $X$,
then $\Sigma_n \times Y \supset \cdots \supset \Sigma_0 \times Y$  
is a Whitney stratification of codimension $m$ in $X \times Y$.
\item \label{i.whitney_invariance}
If  \eqref{e.filtration} is a Whitney stratification 
and $f \colon X \to Y$ is a smooth diffeomorphism 
then $f(\Sigma_n) \supset \cdots \supset f(\Sigma_0)$ 
is a Whitney stratification in $Y$. 
\end{enumerate}
\end{prop}

Let us now discuss how stratifications behave with respect to transversality. 
Let $f \colon X \to Y$ be a $C^1$ map.
Let ${\Sigma}={\Sigma}_d \supset\cdots \supset{\Sigma}_0$ be a stratification of a closed subset ${\Sigma}$ of $Y$. 
One says that $f$ is \emph{transverse} to that stratification 
(in symbols, $f{\;\;\makebox[0pt]{$\top$}\makebox[0pt]{\small $\cap$}\;\;} {\Sigma}$)
if it is transverse to each of its strata.
Transversality to a general stratification is not an open condition.
However, we obtain openness if the stratification is Whitney:

\begin{prop}[Transversality is open]\label{p.transversality open}
Let $X$, $Y$ be $C^\infty$ manifolds without boundary. Let
${\Sigma}={\Sigma}_d \supset\cdots \supset{\Sigma}_0$ be a Whitney stratification of a closed subset of $Y$. 
Then the set ${\mathcal{O}} = \{f \in C^1(X,Y) ; \;  f{\;\;\makebox[0pt]{$\top$}\makebox[0pt]{\small $\cap$}\;\;} {\Sigma}\}$ is open in $C^1(X,Y)$ (with respect to the 
strong topology). 
\end{prop}

Actually, only Whitney condition $(a)$ is necessary here
(use the (1)$\Rightarrow$(3) implication of Trotman's theorem \cite{Trotman_79}).

\subsection{Jets and jet transversality}

We recall the basic notions on jets 
and state the transversality theorems we will need;
see~\cite{Hirsch} for details.

Let $X$, $Y$ be smooth manifolds without boundary. 
If $1 \le r < \infty$, an \emph{$r$-jet from $X$ to $Y$} is an equivalence class of pairs $(x,f)$, where $x\in X$, $f$ is a $C^r$ map from a neighborhood of $x$ to $Y$, and where $(x,f)$ is equivalent to $(x',f')$ if $x=x'$ and $f$ and $f'$ have same derivatives at $x$ up to order $r$. 
We denote by $J^r(X,Y)$ the space of $r$-jets from $X$ to $Y$. 
It is a smooth manifold.

For all $1 \leq s \leq \infty$, we denote by $C^s(X,Y)$ the space of $C^s$-maps from $X$ to
$Y$, endowed with the 
strong topology.

Given $1\leq r < s \leq \infty$ and a map $g\in C^s(X,Y)$, 
the \emph{$r$-jet extension} is the map $j^r g \colon X \to J^r(X,Y)$ that sends $x$ 
to the equivalence class $j^r g(x)$ of $(x,g)$. 
Then the mapping 
$$
j^r\colon C^s(X,Y) \to C^{s-r}\left( X, J^r(X,Y) \right)
$$
is continuous.

\begin{otherthm}[Jet transversality]\label{t.jtransversality}
Let $1\leq r < s \leq \infty$.
Let $X$ and $Y$ be $C^\infty$ manifolds without boundary.
Let $W\subset J^r(X,Y)$ be a $C^\infty$ submanifold without boundary. 
Then the $C^s$-maps
$g\colon X\to Y$ for which the $r$-jet 
extension $j^r g$ is transverse to $W$
form a residual subset of $C^s(X,Y)$.
\end{otherthm}

We finally prove the proposition stated in \S~\ref{s.main proof}: 

\begin{proof}[Proof of Proposition~\ref{p.stratifiedtransversality}]
By Proposition~\ref{p.transversality open}, the set $\{F\colon X\to j^1(X,Y) ;\; F{\;\;\makebox[0pt]{$\top$}\makebox[0pt]{\small $\cap$}\;\;}{\Sigma}\}$ is open in $C^1\left(X,J^1(X,Y)\right)$. Hence the set ${\mathcal{O}} := \{f\colon X\to Y ;\;j^1f{\;\;\makebox[0pt]{$\top$}\makebox[0pt]{\small $\cap$}\;\;}{\Sigma}\}$ is open in $C^2(X,Y)$. 
	
Fix $r \ge 2$.
Given a Whitney stratification $\Sigma_n \supset \cdots \supset \Sigma_0$ of $\Sigma$,
let $Z_i = \Sigma_i {\smallsetminus} \Sigma_{i-1}$ be the corresponding decomposition into smooth submanifolds. By the jet transversality theorem (\cref{t.jtransversality}),
each set $\mathcal{R}_i = \{f\in C^r(X,Y) ; \; j^1 f {\;\;\makebox[0pt]{$\top$}\makebox[0pt]{\small $\cap$}\;\;} Z_i\}$ is residual. 
Thus ${\mathcal{O}} \cap C^r(X,Y) = \bigcap_i\mathcal{R}_i$ is $C^r$-dense. 
This concludes the proof.
\end{proof}

\section{Proof of the result in the holomorphic setting}\label{a.complex}

\begin{proof}[Proof of~\cref{t.main_C}]
Let ${\mathcal{U}}\subset {\mathbb{C}}^m$ be an open subset. We may identify the set of $1$-jets from ${\mathcal{U}}$ to ${\mathrm{GL}}(d,{\mathbb{C}})$ with 
$$
{\mathcal{U}}\times {\mathrm{GL}}(d,{\mathbb{C}}) \times  {\mathfrak{gl}}(d,{\mathbb{C}})^m.
$$
As we did in \cref{s.main proof}, 
and using \cref{t.cod_data_C} instead of \cref{t.cod_data_R}, we obtain that the set of poor $1$-jets from ${\mathcal{U}}$ to ${\mathrm{GL}}(d,{\mathbb{C}})$ is the algebraic subset ${\mathcal{U}}\times{\mathcal{P}}_m^{({\mathbb{C}})}$ of the space of $1$-jets. Hence it admits a stratification
$$
{\mathcal{U}}\times{\mathcal{P}}_m^{({\mathbb{C}})}={\mathcal{U}}\times {\Sigma}_n\supset \cdots \supset{\mathcal{U}}\times {\Sigma}_0.
$$
Write ${\mathcal{U}}\times{\mathcal{P}}_m^{({\mathbb{C}})}$ as the disjoint union $\bigsqcup_{0\leq i\leq n} X_i$ where each $X_i$ is a smooth submanifold  of dimension $i$ in the jet space $J^1\left({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}})\right)$, and $X_n$ has codimension $m$.

Fix now a map $A\in \mathcal{H}({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}}))$. For all $v=(a,b_1,\ldots b_m)\in {\mathbb{C}}^{m+1}$ and $u=(u_1, \dots ,u_m)\in {\mathbb{C}}^m$, write 
 $$P_{v}(u)=a+\sum_{i=1}^m b_ku_k.$$
For all $\mathtt{v}= (v_{i,j})_{1\leq i,j\leq d}\in\left({\mathbb{C}}^{m+1}\right)^{d^2}$, write $P_\mathtt{v}=\left[P_{v_{i,j}}\right]_{1\leq i,j\leq d}$ and define the map $\Phi_\mathtt{v}= A+P_\mathtt{v}$.
One can write the $1$-jet extension $j^1 A$ at the point $u\in {\mathcal{U}}$ as 
$$j^1 A(u)= \left[u,A(u),B_1, \dots ,B_m\right]\in {\mathcal{U}} \times {\mathrm{GL}}(d,{\mathbb{C}}) \times \left[\mathrm{Mat}_{d \times d}({\mathbb{C}})\right]^m.$$
The same way, if we put $v_{i,j}=(a_{i,j},b_{1,i,j},\ldots,b_{m,i,j})$, we have
$$j^1P_\mathtt{v}(u)=\left[u,P_\mathtt{v}(u),(b_{1,i,j})_{1\leq i,j\leq d},\ldots,(b_{m,i,j})_{1\leq i,j\leq d}\right].$$
Define the map $F\colon \mathtt{v} \mapsto F_\mathtt{v}=j^1\Phi_\mathtt{v}$. The evaluation map of $F$ is:
$$F^\mathrm{ev}\colon \begin{cases} \left({\mathbb{C}}^{m+1}\right)^{d^2} \times {\mathcal{U}} &\to \; {\mathcal{U}}\times \mathrm{Mat}_{d \times d}({\mathbb{C}}) \times \left[\mathrm{Mat}_{d \times d}({\mathbb{C}})\right]^m\\
                                                          (\mathtt{v},u) &\mapsto \; F_\mathtt{v}(u)
                                     \end{cases}.$$ 
Hence,
\begin{align*}
F^\mathrm{ev}(\mathtt{v},u)&=j^1(A+P_\mathtt{v})\\
&=\left[u,\left(A+P_\mathtt{v}\right)(u),\left(b_{1,i,j}\right)_{1\leq i,j\leq d},\ldots,\left(b_{m,i,j}\right)_{1\leq i,j\leq d}\right]
\end{align*}

\begin{claim} For all $u$, the map $F^\mathrm{ev}$ restricts to a submersion from the $(\cdot,u)$-fiber to the $\left[u,\cdot\right]$-fiber. 
\end{claim}

\begin{proof}
We want to prove that $$\mathtt{v}\mapsto \left[(A+P_\mathtt{v})(u),\left(b_{1,i,j}\right)_{1\leq i,j\leq d},\ldots,\left(b_{m,i,j}\right)_{1\leq i,j\leq d}\right]$$
is a submersion, or equivalently that
$$\mathtt{v}\mapsto \left[P_\mathtt{v}(u),\left(b_{1,i,j}\right)_{1\leq i,j\leq d},\ldots,\left(b_{m,i,j}\right)_{1\leq i,j\leq d}\right]$$
is a submersion. Noting that $\mathtt{v}=(a_{i,j},b_{k,i,j})_{1\leq i,j\leq d \atop 1\leq k\leq m}$, this comes easily from the fact that $(a_{i,j})\mapsto P_\mathtt{v}(u)$ is a submersion, for any fixed set of coefficients $(b_{k,i,j})_{1\leq i,j\leq d \atop 1\leq k\leq m}$.
\end{proof}

That claim immediately implies that $F^\mathrm{ev}$ is a submersion. In particular it is transverse to each $X_i$. By the parametric transversality theorem (see \cite[p.~79]{Hirsch}), 
there is a residual subset of parameters $\mathtt{v}$ in $\left({\mathbb{C}}^{m+1}\right)^{d^2}$ such that $F_\mathtt{v}=j^1\Phi_\mathtt{v}$ is transverse to $X_i$, for all $i$.

When $\mathtt{v}$ goes to $0$, $\Phi_\mathtt{v}$ tends to $A$ in $\mathcal{H}\left({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}})\right)$. 
This shows the denseness in $\mathcal{H}\left({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}})\right)$ of the maps $\hat{A}$ such that $j^1\hat{A}$ is transverse to $X_i$, for all $i$.
Take such a map $\hat{A}$: for all $i$, the image of $j^1\hat{A}$ does not intersect $X_0\sqcup  \dots  \sqcup X_{n-1}$ and intersects $X_n$ (which has codimension $m$) only in a discrete subset.

Fix $K'\subset {\mathcal{U}}$ a compact set that contains $K$ in its interior. The image $j^1\hat{A}$ restricted to $K'$ can only intersect $X_n$ in a finite set $\Gamma$: indeed, any accumulation point of that intersection set would have to be in $X_0\sqcup  \dots  \sqcup X_{n-1}$, since $X_0\sqcup\ldots\sqcup X_n$ is closed, and this would contradict the fact that $j^1\hat{A}$ does not intersect $X_0\sqcup  \dots  \sqcup X_{n-1}$.

By the choice of our topology, a small perturbation $\tilde{A}$ of $\hat{A}$ is $C^0$ close to $\hat{A}$ by restriction to $K'$. By Cauchy's formula, the map $\tilde{A}$ is $C^2$ close to $\hat{A}$ over the set $K$. Hence, the (compact) image of $j^1\tilde{A}$ restricted to $K$ is still far from $X_0\sqcup  \dots  \sqcup X_{n-1}$, and intersects $X_n$ transversally in some $\epsilon$-neighborhood of $\Gamma$ inside $X_n$. Thus it also has to intersect $X_n$ only on a finite set. 

So we have found an open and dense subset of holomorphic maps whose $1$-jets above $K$ intersect the set of $N$-poor jets only on a finite number of points. As a consequence, for such maps,
there are only finitely many constant singular inputs  in $K^N$ for the system~\ref{e.proj semilin CS}. 
This concludes the proof of Theorem~\ref{t.main_C}.
\end{proof}

\section{Singular constant inputs of generic type} \label{a.generic singular}

In this appendix we prove \cref{t.addendum}
and the other assertions made at the end of \S~\ref{ss.main_statements}.
We also discuss other control-theoretic properties of generic semilinear systems
that are related to universal regularity.

\subsection{The poor data of generic type}\label{ss.additional}

Recall from \S~\ref{ss.unconstrained} the definition of 
an unconstrained matrix.
Let $(e_1, \dots, e_d)$ denote the canonical basis of ${\mathbb{C}}^d$.

\begin{lemma}\label{l.good_poor}
Suppose that the datum ${\mathbf{A}} = (A, B_1, \dots, B_m) \in {\mathrm{GL}}(d,{\mathbb{C}}) \times {\mathfrak{gl}}(d,{\mathbb{C}})^m$
has the following properties:
\begin{enumerate}
\item\label{i.good_hyp_1}
$A$ is an unconstrained diagonal matrix;
\item\label{i.good_hyp_2}
there are indices $i_0$, $j_0 \in \{1,\dots,d\}$ with $i_0 \neq j_0$ 
such that for each $k \in \{1,\dots,m\}$, the $(i_0,j_0)$ entry of the matrix $B_k$ vanishes;
\item\label{i.good_hyp_3}
the off-diagonal vanishing entry position $(i_0,j_0)$ above is unique.
\end{enumerate}
Then:
\begin{enumerate}
\item\label{i.good_conclusion_1}
There is a single direction $[v] \in {\mathbb{C}\mathrm{P}}^{d-1}$ such that $\Lambda({\mathbf{A}}) \cdot v \neq {\mathbb{C}}^d$,
namely~$[e_{j_0}]$.
\item\label{i.good_conclusion_2}
The space $\Lambda({\mathbf{A}}) \cdot e_{j_0}$ has codimension $1$; 
in fact, it equals $\operatorname*{span} \{e_i ; \; i \neq i_0\}$.
\end{enumerate}
\end{lemma}

If the datum ${\mathbf{A}}$ satisfies the assumptions of the lemma
then it is conspicuously poor (see \cref{ss.cod_data_easy_half})
and thus the constant input $(0,\dots,0)$ of length $d^2$
for the associated bilinear control system on ${\mathbb{C}\mathrm{P}}^{d-1}$
is not universally regular.
However, the conclusions of the lemma say that this universal regularity fails in 
the weakest possible way: there is exactly one non-regular state,
which can be moved in all directions but one.
We will show in \cref{l.bad_set} below that the generic poor data
satisfy the hypotheses of \cref{l.good_poor} after a change of basis.

\begin{proof}[Proof of \cref{l.good_poor}]
By arguments as in the proof of \cref{l.easy_fiber},
we see that 
$$
\Lambda({\mathbf{A}}) \supset
\big\{ (y_{ij}) \in {\mathfrak{gl}}(d,{\mathbb{C}}) ; \; y_{11} = \cdots = y_{dd}, \ y_{i_0 j_0} = 0 \big\}.
$$	
The conclusions follow easily.
\end{proof}

\medskip

Recall from \cref{ss.poor_set} that a set ${\mathcal{Z}} \subset [{\mathrm{Mat}}_{d\times d}({\mathbb{K}})]^{1+m}$
is called \emph{saturated} if
$(A, B_1, \dots, B_m) \in {\mathcal{Z}}$ implies that:
\begin{itemize}
\item 
for all $P \in {\mathrm{GL}}(d,{\mathbb{K}})$ we have $(P^{-1}AP, P^{-1}B_1 P, \dots, P^{-1}B_m P) \in {\mathcal{Z}}$;
\item 
for all $Q = (q_{ij}) \in {\mathrm{GL}}(m,{\mathbb{K}})$, letting $B'_i = \sum_j q_{ij} B_j$,
we have $(A, B_1', \dots, B_m') \in {\mathcal{Z}}$.
\end{itemize}

\begin{rem}\label{r.saturation_properties}
\begin{enumerate}
\item\label{i.saturation_properties_1}
A subset $[{\mathrm{Mat}}_{d\times d}({\mathbb{K}})]^{1+m}$ is saturated if and only if it is invariant under a certain action of the group ${\mathrm{GL}}(d,{\mathbb{K}})\times{\mathrm{GL}}(m,{\mathbb{K}})$.
\item\label{i.saturation_properties_2}
The real part of a complex saturated set is saturated (in the real sense).
\end{enumerate}
\end{rem}

\begin{lemma}\label{l.bad_set}
There exists a saturated algebraically closed set 
${\mathcal{S}}_m^{({\mathbb{C}})} \subset {\mathrm{GL}}(d,{\mathbb{C}})\times [{\mathrm{Mat}}_{d\times d}({\mathbb{C}})]^m$        
of codimension at least $m+1$
such that for all $(A, B_1, \dots, B_m) \in {\mathcal{P}}_m^{({\mathbb{C}})} {\smallsetminus} {\mathcal{S}}_m^{({\mathbb{C}})}$, the following properties hold:
\begin{enumerate}
\item\label{i.good_1}
$A$ is unconstrained;
\item\label{i.good_2}
if $P\in {\mathrm{GL}}(d,{\mathbb{C}})$ is such that $P^{-1} A P$ is a diagonal matrix then
there are indices $i_0$, $j_0 \in \{1,\dots,d\}$ with $i_0 \neq j_0$ 
such that for each $k \in \{1,\dots,m\}$, the $(i_0,j_0)$ entry of the matrix $P^{-1} B_k P$ vanishes;
\item\label{i.good_3}
for each choice of $P$ above,
the off-diagonal vanishing entry position $(i_0,j_0)$ is unique.
\end{enumerate}
\end{lemma}

\medskip

In order to prove the \lcnamecref{l.bad_set},
we begin by checking algebraicity of the constraints:

\begin{lemma}\label{l.alg_constraint}
The set $K \subset {\mathrm{GL}}(d,{\mathbb{C}})$ of constrained matrices
is an algebraically closed subset of codimension $1$.
\end{lemma}

\begin{proof}
Multiply all constraints, obtaining a polynomial in the variables $\lambda_1$, \dots, $\lambda_d$.
This polynomial is symmetric, 
and therefore (see e.g.\ \cite[Thrm.~IV.6.1]{Lang})
can be written as a polynomial function of the elementary symmetric polynomials in the variables 
$\lambda_1$, \dots, $\lambda_d$.
Now substitute each elementary symmetric polynomial in this expression by the corresponding
coefficient of the characteristic polynomial of the matrix $A$.
This gives a polynomial function on the entries of the matrix $A$
that vanishes if and only if $A$ is constrained.
It is obvious that the corresponding algebraic set $K$ has codimension $1$.
\end{proof}

Now we check algebraicity of double vanishing:

\begin{lemma}\label{l.double_zero}
There exists a saturated algebraically closed subset 
${\mathcal{D}}$ of ${\mathrm{GL}}(d,{\mathbb{C}})\times [{\mathrm{Mat}}_{d\times d}({\mathbb{C}})]^m$ 
such that
if $(A, B_1, \dots, B_m) \in {\mathcal{D}}$ and $A$ has simple spectrum then 
property \ref{i.good_2} from \cref{l.bad_set} is satisfied, 
but property \ref{i.good_3} is not.
\end{lemma}

\begin{proof}
First, consider the subset $X \subset [{\mathrm{Mat}}_{d\times d}({\mathbb{C}})]^{1+m} \times ({\mathbb{C}\mathrm{P}}^{d-1})^2$
formed by tuples $(A,B_1,\dots,B_m,[v],[w])$ such that
$$
[Av] = [v], \quad [A^* w] = [w], \quad w^* v = 0, \quad w^* B_k v = 0 \text{ for each $k=1,\dots,m$,}
$$
where $v$ and $w$ are regarded as column-vectors and the star denotes transposition.
The set $X$ is obviously algebraic; thus, by \cref{p.projection}, so is its projection
$Y$ on $[{\mathrm{Mat}}_{d\times d}({\mathbb{C}})]^{1+m}$.

Let $A$ be a matrix with simple spectrum.
Then $(A, B_1, \dots, B_m)$
belongs to $Y$ if and only if property \ref{i.good_2} from \cref{l.bad_set} is satisfied.
In particular, the fiber of $Y$ over $A$ is 
a union of affine subspaces of $[{\mathrm{Mat}}_{d\times d}({\mathbb{C}})]^m$.
Intersections of those affine spaces correspond to points where the uniqueness 
property \ref{i.good_3} is not satisfied.
These points of intersection are singular points of $Y$.
Conversely, it is clear that the variety $Y$ is smooth at the points on the fiber over $A$
where property \ref{i.good_3} is satisfied.

So let $Z$ be the (algebraically closed) set of singular points of $Y$.
It is straightforward to see that the set $Y$ is saturated.
Recalling \cref{r.saturation_properties} (part~\ref{i.saturation_properties_1})
and the fact that a group acting on a variety preserves singular points,
we see that the set $Z$ is saturated as well.

We define ${\mathcal{D}}$ as the set $Z$ minus the tuples $(A, B_1, \dots, B_m)$ with $\det A = 0$.
Then ${\mathcal{D}}$ has all the required properties.
\end{proof}

Now we combine the facts above with \cref{scholium}
to prove \cref{l.bad_set}:

\begin{proof}[Proof of \cref{l.bad_set}]
For simplicity of writing we will omit the $m$ subscripts and the $({\mathbb{C}})$ superscripts.

Let $\pi: {\mathcal{P}} \to {\mathrm{GL}}(d,{\mathbb{C}})$ be the projection on the first matrix.
Define
$$
{\mathcal{S}} = \pi^{-1}(K) \cup ({\mathcal{D}} \cap {\mathcal{P}}),
$$
where $K$ and ${\mathcal{D}}$ come respectively from \cref{l.alg_constraint,l.double_zero}.
Then ${\mathcal{S}}$ is a saturated algebraically closed subset of ${\mathcal{P}}$. 
If ${\mathbf{A}} = (A, B_1, \dots, B_m) \in {\mathcal{P}} {\smallsetminus} {\mathcal{S}}$ then:
\begin{itemize}
\item $A \not\in K$, which is property~\ref{i.good_1};
\item since ${\mathbf{A}} \in {\mathcal{P}}$, it follows from \cref{l.easy_fiber} that ${\mathbf{A}}$ is conspicuously poor,
and so property~\ref{i.good_2} holds;
\item since ${\mathbf{A}} \not\in {\mathcal{D}}$, property~\ref{i.good_3} also holds.
\end{itemize}

To complete the proof of the \lcnamecref{l.bad_set}, we need to show that $\operatorname{codim} {\mathcal{S}} \ge m+1$.
We will use the following inclusion:
\begin{equation}\label{e.union}
{\mathcal{S}} \subset {\mathcal{F}} \cup \underbrace{\big( \pi^{-1}(K) {\smallsetminus} {\mathcal{F}} \big)}_{{\mathcal{F}}'} \cup 
\underbrace{\big( ({\mathcal{D}} \cap {\mathcal{P}}) {\smallsetminus} \pi^{-1}(K)\big)}_{{\mathcal{F}}''} \, .
\end{equation}
where ${\mathcal{F}}$ comes from \cref{scholium}.
Recall that ${\mathcal{F}}$ equals $\pi^{-1}(C_{m-1})$, where $C_j$ is given by \eqref{e.C_j},
and it has codimension at least $m+1$.

We apply \cref{l.pret_a_porter,r.homogeneous}
to the set ${\mathcal{F}}' \subset Y' \times [{\mathfrak{gl}}(d,{\mathbb{C}})]^m$, 
where $Y' = {\mathrm{GL}}(d,{\mathbb{C}}) {\smallsetminus} C_{m-1}$.
Since $K$ has codimension at least $1$ in $Y'$,
and the fibers of ${\mathcal{F}}'$ all have codimension at least $m$, 
we conclude that that $\operatorname{codim} {\mathcal{F}}' \ge m+1$.

Next, we want to apply \cref{l.pret_a_porter,r.homogeneous}
to the set ${\mathcal{F}}'' \subset Y'' \times [{\mathfrak{gl}}(d,{\mathbb{C}})]^m$, 
where $Y'' = {\mathrm{GL}}(d,{\mathbb{C}}) {\smallsetminus} K$.
For each $A \in Y''$, it follows from \cref{l.double_zero}
that the fiber of ${\mathcal{F}}''$ over $A$ (which is the same as the fiber of ${\mathcal{D}}$ over $A$)
has codimension $2m$ in $[{\mathfrak{gl}}(d,{\mathbb{C}})]^m$,
corresponding to the $2m$ different matrix entries that must vanish.
We conclude that $\operatorname{codim} {\mathcal{F}}'' \ge 2m$.

We have seen that each of the three sets on the right-hand side of \eqref{e.union}
has codimension at least $m+1$.
So the same is true for ${\mathcal{S}}$, as we wanted to prove.
\end{proof}

\subsection{Proof of the addendum to the Main \cref{t.main}}

\begin{proof}[Proof of \cref{t.addendum}]
Consider the set ${\mathcal{S}}_m^{({\mathbb{C}})}$ given by \cref{l.bad_set},
and let ${\mathcal{S}}_m^{({\mathbb{R}})}$ be its real part.
This is an algebraically closed saturated subset of ${\mathrm{GL}}(d,{\mathbb{R}}) \times [{\mathfrak{gl}}(d,{\mathbb{R}})]^m$
which, by \cref{p.real complex dimension}, has codimension at least $m+1$.

Consider the set $\tilde \Gamma$ of $1$-jets ${\mathbf{J}} \in J^1({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}}))$ 
that have a local expression $(u, A(u),B_1,\dots,B_m)$
with $(A(u),B_1,\dots,B_m) \in {\mathcal{S}}_m^{({\mathbb{R}})}$.
This does not depend on the choice of the local coordinates, because ${\mathcal{S}}_m^{({\mathbb{R}})}$ is saturated.
By the same arguments as in the proof of \cref{t.main}, 
the set $\tilde \Gamma$ admits a Whitney stratification.
Its codimension is at least $m+1$.
Applying Proposition~\ref{p.stratifiedtransversality}, we obtain a $C^2$-open
$C^\infty$-dense set $\tilde {\mathcal{O}} \subset C^2({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{C}}))$ formed by maps $A$ 
that are transverse to the stratification.

Let ${\mathcal{O}}$ be the set provided by \cref{t.main}.
and consider a map $A \in {\mathcal{O}} \cap \tilde {\mathcal{O}}$.
Then whenever a jet $j^1 A (u)$ is poor,
it does not belong to $\tilde \Gamma$.
Recalling \cref{l.bad_set}, we see that the local expression of 
$j^1 A (u)$ satisfies (after a change of basis) the hypotheses of \cref{l.good_poor}.
Therefore parts \ref{i.addendum_1} and \ref{i.addendum_2} of the theorem
follow respectively from conclusions \ref{i.good_conclusion_1} and \ref{i.good_conclusion_2}
of the \lcnamecref{l.good_poor}.
\end{proof}

\begin{rem}\label{r.addendum_of_addendum}
The proof of \cref{t.addendum} also gives more information
about the $1$-jets that appear generically for singular constant inputs $(u,\ldots,u)$:
any associated matrix datum is conspicuously poor 
and the matrix $A(u)$ is unconstrained. 
\end{rem}

\begin{rem}
Properties \ref{i.addendum_1} and \ref{i.addendum_2} in \cref{t.addendum}
are in fact dual to each other.
If ${\mathbf{A}}$ is the datum representing the $1$-jet of $A$ at $u$,
and $\Lambda = \Lambda({\mathbf{A}})$,
then property \ref{i.addendum_1} means that there is an unique direction 
$[v] \in {\mathbb{R}\mathrm{P}}^{d-1}$ such that $\Lambda \cdot v \neq {\mathbb{C}}^d$.
Then property \ref{i.addendum_2} means that there is an unique direction 
$[w] \in {\mathbb{R}\mathrm{P}}^{d-1}$ such that $\Lambda^* \cdot w \neq {\mathbb{C}}^d$,
where $\Lambda$ is the set of the transposes of the matrices in $\Lambda$.
This fact can be proved easily using the dual characterization of \cref{l.duality}. 
\end{rem}

\subsection{Local persistence of singular inputs}

Let $A \in C^r( {\mathcal{U}}, {\mathrm{GL}}(d,{\mathbb{R}}))$, $r \ge 1$.
We will work upon \cref{l.easy_poor_data} in order to 
obtain a more practical way to detect that the $1$-jet of $A$ at a point
corresponds to a conspicuously poor datum
(which as mentioned in \cref{r.addendum_of_addendum} is the only type of poor data that appear generically).
For example, in the $m=1$, $d=2$ case, we will see that conspicuous poorness means that
the angular velocity of one of the eigendirections vanishes (see \cref{r.speed} below).

\medskip

Suppose that $u_0 \in {\mathcal{U}}$ is such that the matrix $A(u_0)$ is diagonalizable over ${\mathbb{R}}$ 
and with simple eigenvalues only.
By \cref{p.eigen_smooth}, there is a neighborhood ${\mathcal{U}}_0$ of $u_0$
 and $C^r$-maps $\lambda_1$, \dots, $\lambda_d\colon {\mathcal{U}}_0\to {\mathbb{C}}$ such that for all $u\in {\mathcal{U}}_0$, the complex numbers $\lambda_i(u)$ are all distinct, and form the spectrum of $A(u)$; moreover there exist
a $C^r$ map $P \colon {\mathcal{U}}_0 \to {\mathrm{GL}}(d,{\mathbb{R}})$ 
such that for all $u \in {\mathcal{U}}_0$,
\begin{equation}\label{e.diagonalize}
A(u) = P(u) \, \Delta(u) \, P^{-1}(u) \text{ , where }
\Delta(u) = {\mathrm{Diag}} (\lambda_1(u), \dots, \lambda_d(u)).
\end{equation}

For simplicity, 
let us consider first case where ${\mathcal{U}}$ is an interval in ${\mathbb{R}}$ (in particular $m=1$).
Then the normalized derivative of $A$ at a point $u$ 
can be identified with $N(u):= A'(u) \, A^{-1}(u)$.
Consider the expression of $N(u)$ in the basis that diagonalizes $A(u)$, that is,
$B(u) := P^{-1}(u) \, N(u) \, P(u)$.
Since
$\frac{\mathrm{d}}{\mathrm{d}u}P^{-1}(u) = - P^{-1}(u) \, P'(u) \, P^{-1}(u)$, 
we compute that
$$
B(u) = \Delta'(u) \, \Delta^{-1}(u) 
+ Q(u) - \Delta(u) \, Q(u) \, \Delta^{-1}(u) \, , 
$$ 
where
$$
Q(u) := P^{-1}(u) \, P'(u) \, .
$$
So the off-diagonal entries of the matrices $B(u)$ and $Q(u)$ are related by
$$
b_{ij}(u) = \big(1 - \lambda_i(u) / \lambda_j(u)\big) \, q_{ij}(u)
\quad (i\neq j).
$$
In view of \cref{l.easy_poor_data}, we conclude the following:
if for some $u_* \in {\mathcal{U}}_0$
\begin{equation}\label{e.easy_poor_cond}
\text{there is an off-diagonal entry position $(i,j)$ such that $q_{ij}(u_*) = 0$}
\end{equation}
then the $1$-jet $j^1 A(u_*)$ is poor.

\begin{rem}\label{r.speed}
Let us give a geometrical interpretation of condition~\eqref{e.easy_poor_cond}.
The columns of $P$ form a basis $(v_1,\dots,v_d)$ of eigenvectors of $A$,
and the rows of $P^{-1}$ form a basis $(f_1,\dots,f_d)$ of eigenfunctionals of $A$
(in the sense that $f_i \circ A = \lambda_i f_i$); 
these two bases are related by $f_i(v_j) = \delta_{ij}$.
So $q_{ij} = f_i \left(\frac{\mathrm{d} v_j}{\mathrm{d} u}\right)$
is the component of the velocity of $v_j$ in the direction of $v_i$.
For example, for $d=2$,  condition~\eqref{e.easy_poor_cond} means 
that one of the eigendirections of $A$ has zero angular speed at instant $u=u_*$.
\end{rem}

It is trivial to adapt the previous calculations to the higher dimensional case and then conclude the following:

\begin{prop}\label{p.easy_poor_derivative}
Let $(u_1, \dots, u_m)$ be coordinates in a chart domain ${\mathcal{U}}_0\subset {\mathcal{U}}$
where expression~\eqref{e.diagonalize} holds. 
Consider matrices
\begin{equation}\label{e.Q_k}
Q_k (u) := P^{-1}(u) \, \frac{\partial P}{\partial u_k}(u) \, .
\end{equation}
If for some $u_* \in {\mathcal{U}}_0$ there is an off-diagonal entry position $(i,j)$
such that
\begin{equation}\label{e.easy_poor_cond_multdim}
\text{for each $k=1,\dots,m$, the $(i,j)$-entry of the matrix $Q_k(u_*)$ vanishes}
\end{equation} 
then the $1$-jet $j^1 A(u_*)$ is poor, that is, 
the constant input $(u_*,\dots,u_*)$ (of any length) is singular.
\end{prop}

In the situation of \cref{p.easy_poor_derivative},
assume additionally that the map
\begin{equation}\label{e.2nd_derivative}
\Phi\colon \begin{cases}
{\mathcal{U}}_0& \to {\mathrm{Im}\;} \Phi \subset {\mathbb{K}}^m\\
u&\mapsto \left[ \text{the $(i,j)$-entry of } Q_k (u)
\right]_{1\leq k \leq m}
\end{cases} \mbox{ is a diffeomorphism. } 
\end{equation}
In that case, the existence of a poor jet is persistent in the following way:
If $\tilde A$ is sufficiently $C^2$-close to $A$
then by \cref{p.eigen_smooth} we can express 
$\tilde A (u) = \tilde P(u) \, \tilde \Delta(u) \, \tilde P^{-1}(u)$
for $u$ close to $u_*$, where $\tilde P$ and $\tilde \Delta$ are $C^2$-close to $P$ and $\Delta$ respectively, and $\tilde \Delta$ is diagonal.
The corresponding matrices $\tilde Q_k = \tilde P^{-1} \, \frac{\partial \tilde P}{\partial u_k} $ are $C^1$-close to $Q_k$ and the map 
\begin{align*}
\tilde \Phi \colon u \mapsto \left[ \text{the $(i,j)$-entry of } \tilde Q_k (u)
\right]_{1\leq k \leq m}
\end{align*}
is $C^1$-close to $\Phi$. By~\eqref{e.2nd_derivative} the fact that $\Phi(u_*)=(0,...,0)$,  there is $\tilde u$ close to $u_*$ such that $\tilde \Phi(u)=(0,...,0)$.
In particular the $1$-jet $j^1 \tilde A(\tilde u)$ is poor.

Now, concerning existence:
It is evident that a domain ${\mathcal{U}}_0$ and $2$-jets $j^2 P(u_*)$ satisfying conditions \eqref{e.easy_poor_cond_multdim} and \eqref{e.2nd_derivative} actually exist; moreover we can always find a map $P \colon {\mathcal{U}} \to {\mathrm{GL}}(d,{\mathbb{R}})$ with a prescribed $2$-jet at a point $u_*$. In view of the discussion above, we conclude the following:

\begin{prop}[Persistence of singular inputs]\label{p.persistent_poorness}
For any $d\geq 1$ and any $d$-dimensional smooth manifold ${\mathcal{U}}$,
there exists a $C^2$-open nonempty subset of maps $A\in C^2({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{R}}))$
with the following property:
there exists $u \in {\mathcal{U}}$ such that the constant inputs $(u,\dots,u)$ of any length are all singular for the system~\eqref{e.proj semilin CS}.
\end{prop}

That is, one cannot improve \cref{t.main} replacing ``discrete set'' by ``empty set''.

Given any map $A$ such that \eqref{e.easy_poor_cond_multdim} holds at some point,
we can $C^1$-perturb $A$ (by $C^0$-perturbing $P$) in a way such that \eqref{e.easy_poor_cond_multdim} now holds for a non-discrete set of points.
This shows that the statement of \cref{t.main} with ``$C^2$-open'' replaced by ``$C^1$-open'' is not true.
Using the same idea and Baire's theorem, one can also show that the conclusion of \cref{t.main} is not true for $C^1$-generic maps $A$; actually for $C^1$-generic $A$, the points $u \in {\mathcal{U}}$ corresponding to singular constant controls form a perfect set.

\subsection{Other control-theoretic properties}

We now introduce a few control-theoretic notions related to accessibility and regularity,
and discuss the validity of statements similar to \cref{t.main} for these notions.

Consider a general control system \eqref{e.general CS}.
Fix a time length $N$, and let $\phi_N$ denote the response map as in \eqref{e.final state}.
We say that a trajectory determined by $(x_0; u_0, \dots, u_{N-1})$ is:
\begin{itemize}
\item \emph{locally accessible} 
if for every neighborhood $V$ of $(u_0, \dots, u_{N-1})$ in ${\mathcal{U}}^N$,
the set $\phi_N(\{x_0\} \times V)$ has nonempty interior.
\item \emph{strongly locally accessible} if for every neighborhood $V$ of $(u_0, \dots, u_{N-1})$ in ${\mathcal{U}}^N$, the set $\phi_N(\{x_0\} \times V)$ contains in its interior the final state $\phi_N(x_0;u_0,\dots,u_{N-1})$.
\end{itemize}
The following implications are immediate:
$$
\text{regular $\Rightarrow$ strongly locally accessible $\Rightarrow$ locally accessible.}
$$
We say that an input $(u_0, \dots, u_{N-1})$ is \emph{universally locally accessible} 
(resp.\ \emph{universally strongly locally accessible}) if the trajectory determined by  
$(x_0; u_0, \dots, u_{N-1})$ is locally accessible (resp.\ strongly locally accessible). 

Now we come back to the context of 
projective semilinear control systems \eqref{e.proj semilin CS}.
A (relatively weak) corollary of \cref{t.main} is
that for generic maps~$A$, universal local accessibility holds at all 
constant inputs:

\begin{prop}\label{p.local acc}
Let $N \in {\mathbb{N}}$ and ${\mathcal{O}} \subset C^2({\mathcal{U}},{\mathrm{GL}}(d,{\mathbb{R}}))$ be as in \cref{t.main}.
For any $A \in {\mathcal{O}}$, every constant input sequence of length $N$ is universally locally accessible.
\end{prop}

\begin{proof}
If $A\in {\mathcal{O}}$ then for every constant input sequence of length $N$
we can find a regular input sequence nearby.
\end{proof}

As we have shown in \cref{p.persistent_poorness}, 
it is not possible to improve \cref{p.local acc} by replacing ``local accessible'' by ``regular''.
Neither it is possible to replace ``local accessible'' by ``strongly local accessible'',
as the following simple example (in dimensions $m=1$, $d=2$) shows:

\begin{example}
For $u \in {\mathbb{R}}$, define
$$
P(u) = \begin{pmatrix} 1 & u \\ u^2 & 1 \end{pmatrix}, \quad \Delta(u) = {\mathrm{Diag}}(2,1).
$$
Let ${\mathcal{U}}$ be an small open interval containing $0$,
and define $A \colon {\mathcal{U}} \to {\mathrm{GL}}(2,{\mathbb{R}})$ by \eqref{e.diagonalize}.
Let $\xi_0 \in {\mathbb{R}}{\mathbb{P}}^1$ correspond to the direction of the vector $(1,0)$.
Then for any subinterval $V \ni 0$, and any $N>0$, the set 
$$
\phi_N (\{\xi_0\} \times V^N) = \big\{ A(u_{n-1})\cdots A(u_0)\cdot \xi_0 \; u_i \in V \big\}
$$
is an interval of ${\mathbb{R}}{\mathbb{P}}^1$ containing $\xi_0 = \phi_N (\xi_0; 0,\dots,0)$ in its boundary.
Therefore the input $(0,\dots,0)$ is not universally strongly locally accessible.
A similar situation occurs for any $C^2$-perturbation of $A$.
\end{example}

\bigskip

\begin{ack}
We are grateful for the hospitality of Institute Mittag--Leffler, 
where this work begun to take form.
We thank R.~Potrie, L.~San~Martin, S.~Tikhomirov, and C.~Tomei for valuable discussions.
We thank the referees for corrections, references to the literature,
and other suggestions that helped to improve the exposition.
\end{ack}

\begin{thebibliography}{GWPL}
	  

{\@ifnextchar[\my@lbibitem\my@bibitem}[Az]{0858467}{Azoff}
\textsc{Azoff, E.A.}
On finite rank operators and preannihilators.
\textit{Mem.\ Amer.\ Math.\ Soc.\ }64, no.~357 (1986).

{\@ifnextchar[\my@lbibitem\my@bibitem}[BR]{1070358}{BR}
\textsc{Benedetti, R.; Risler, J.-J.}
\textit{Real algebraic and semi-algebraic sets.}
Hermann, Paris, 1990.

\bibitem[Bl]{Blasiak}
\textsc{Blasiak, J.}
\textit{Cohomology of the complex Grassmannian.}
\href{http://www-personal.umich.edu/~jblasiak/grassmannian.pdf}{\url{www-personal.umich.edu/~jblasiak/grassmannian.pdf}}

{\@ifnextchar[\my@lbibitem\my@bibitem}[BCR]{1659509}{BCR}
\textsc{Bochnak, J.; Coste, M.; Roy, M.-F.}
\textit{Real algebraic geometry.}
Springer--Verlag, Berlin, 1998.

{\@ifnextchar[\my@lbibitem\my@bibitem}[CK1]{1235040}{CK_93}
\textsc{Colonius, F.; Kliemann, W.}
Linear control semigroups acting on projective space.
\textit{J.\ Dynam.\ Differential Equations} 5 (1993), no.~3, 495--528. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[CK2]{1752730}{CK_book}
\bysame.
\textit{The dynamics of control.}
Birkh\"auser, Boston, MA, 2000.

{\@ifnextchar[\my@lbibitem\my@bibitem}[El]{2509466}{Elliott}
\textsc{Elliott, D.L.}
\textit{Bilinear control systems.}
Springer, Dordrecht, 2009.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Fu]{1464693}{Fulton}
\textsc{Fulton, W.}
\textit{Young tableaux. With applications to representation theory and geometry.}
Cambridge Univ.\ Press, Cambridge, 1997.

{\@ifnextchar[\my@lbibitem\my@bibitem}[GWPL]{0436203}{GWPL}
\textsc{Gibson, C.G.; Wirthm\"{u}ller, K.; du~Plessis, A.A.; Looijenga, E.J.N.}
\textit{Topological stability of smooth mappings.}
Lecture Notes in Mathematics, Vol.~552. 
Springer--Verlag, Berlin -- New York, 1976.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Ha]{1182558}{Harris}
\textsc{Harris, J.}
\textit{Algebraic geometry: a first course.}
Springer--Verlag, New York, 1992.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Hi]{0448362}{Hirsch}
\textsc{Hirsch, M.W.}
\textit{Differential Topology.}
Springer--Verlag, New York -- Heidelberg, 1976.

{\@ifnextchar[\my@lbibitem\my@bibitem}[HJ]{1288752}{HJ}
\textsc{Horn, R.A.; Johnson, C.R.}
\textit{Topics in matrix analysis.}
Corrected reprint of the 1991 original. 
Cambridge University Press, Cambridge, 1994.

\bibitem[Hu]{Hutchings}
\textsc{Hutchings, M.}
\textit{Cup product and intersection.} 
\href{http://math.berkeley.edu/~hutching/teach/215b-2011/cup.pdf}{\url{math.berkeley.edu/ ~hutching/teach/215b-2011/cup.pdf}}

{\@ifnextchar[\my@lbibitem\my@bibitem}[La]{1878556}{Lang}
\textsc{Lang, S.}
\textit{Algebra.}
Revised 3rd edition.
Springer--Verlag, New York, 2002.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Le]{0007779}{Levi}
\textsc{Levi, F.W.}
Ordered groups.
\textit{Proc.\ Indian Acad.\ Sci.\ }16 (1942), 256--263.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Ma]{0368064}{Mather_71}
\textsc{Mather, J.N.}
Stratifications and mappings. 
\textit{Dynamical systems (Proc.~Sympos., Univ.~Bahia, Salvador, 1971)}, 195--232. 
Academic Press, New York, 1973. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[Mu]{0453732}{Mumford}
\textsc{Mumford, D.}
\textit{Algebraic geometry. I. Complex projective varieties.}
Springer--Verlag, Berlin -- New York, 1976.

{\@ifnextchar[\my@lbibitem\my@bibitem}[Ro]{2344656}{Roman}
\textsc{Roman, S.}
\textit{Advanced linear algebra.}
3rd edition. 
Springer, New York, 2008. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[Sh]{1328833}{Shafa}
\textsc{Shafarevich, I.G.}
\textit{Basic algebraic geometry. Vol~1.}
2nd edition.
Springer--Verlag, Berlin, 1994.

{\@ifnextchar[\my@lbibitem\my@bibitem}[So]{1180510}{Sontag_92}
\textsc{Sontag, E.D.}
Universal nonsingular controls. 
\textit{Systems Control Lett.\ }19 (1992), no.~3, 221--224.
Errata: Ibid, 20 (1993), no.~1, 77. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[SW]{1607810}{Sontag_Wirth_98}
\textsc{Sontag, E.D.; Wirth. F.R.}
Remarks on universal nonsingular controls for discrete-time systems.
\textit{Systems Control Lett.\ }33 (1998), no.~2, 81--88. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[Tr]{0520929}{Trotman_79} 
\textsc{Trotman, D.J.A.}
Stability of transversality to a stratification implies Whitney (a)-regularity. 
\textit{Invent. Math.} 50 (1978/79), no. 3, 273--277. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[Va]{2247964}{Vakil}
\textsc{Vakil, R.}
A geometric Littlewood-Richardson rule.
\textit{Ann.\ of Math.\ }164 (2006), no.~2, 371--421. 

{\@ifnextchar[\my@lbibitem\my@bibitem}[Wi]{1616502}{Wirth_98}
\textsc{Wirth, F.}
Dynamics of time-varying discrete-time linear systems: spectral theory and the projected system.
\textit{SIAM J.\ Control Optim.\ }36 (1998), no.~2, 447--487. 

\end{thebibliography}

\end{document}

