
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
\RequirePackage{fix-cm}
\documentclass[smallextended]{svjour3}       
\smartqed  
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algorithmic}



\begin{document}

\title{Quasi-linear compressed sensing via non-convex fraction function penalty}


\author{Angang Cui$^{1}$ \and
        Jigen Peng$^{\ast,1}$ \and
        Haiyang Li$^{2}$ \and
        Qian Zhang$^{2}$
        
}


\institute{$\ast$Corresponding author\\
           Jigen Peng \at
            \email{jgpengxjtu@126.com}\\
           1 School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 710049, China \\
           2 School of Science, Xi'an Polytechnic University, Xi'an, 710048, China
}

\date{Received: date / Accepted: date}


\maketitle

\begin{abstract}
In this paper, a non-convex fraction function $\rho_{a}$ is studied in quasi-linear compressed sensing and the iterative fraction thresholding algorithm
is proposed to solve the regularization problem $(QP_{a}^{\lambda})$ for all $a>0$. With the change of parameter $a>0$, our algorithm could get a
promising result, which is one of the advantages for our algorithm comparing with other algorithms. Numerical experiments show that our method
performs much better comparing with some state-of-art methods.
\keywords{Quasi-linear compressed sensing\and Sparse\and Non-convex fraction function\and Iterative fraction thresholding algorithm}
\subclass{34A34\and 78M50\and 93C10}
\end{abstract}

\section{Introduction}\label{intro}
In compressed sensing [1,2], a basic problem is to reconstruct a sparse signal under a few linear measurements far less than the
dimension of the ambient space of the signal, and this basic problem can be viewed as the following mathematical form
\begin{equation}\label{r1}
(P_{0})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}}\|x\|_{0}\ \ \mathrm{subject}\ \mathrm{to}\ \ Ax=b
\end{equation}
where $A\in \mathcal{R}^{m\times n}$ is a $m\times n$ real matrix of full row rank with $m<n$, $b\in \mathcal{R}^{m}$ is
a nonzero real vector of $m$-dimension, and $\|x\|_{0}$ is the $\ell_{0}$-norm of the real vector $x$, which counts
the number of the non-zero entries in $x$ [3-5].

However, many real-life applications in physics and biomedical sciences carry some strongly nonlinear structures [6], so
that the linear model (1) is no longer suitable. We consider a map $A: \mathcal{R}^{n}\rightarrow \mathcal{R}^{m}$,
which is no longer necessarily linear, and reconstruct a sparse vector $x\in \mathcal{R}^{n}$ from the
measurements $b\in \mathcal{R}^{m}$ given by
\begin{equation}\label{r2}
A(x)=b.
\end{equation}
In order to get a stability properties which convenient to sparse signal recovery, we set the most of the
nonlinear models have a smooth quasi-linear nature. By this mean, there exists a Lipschitz map
\begin{equation}\label{r3}
F: \mathcal{R}^{n}\rightarrow \mathcal{R}^{m\times n}
\end{equation}
such that
\begin{equation}\label{r4}
A(x)=F(x)x
\end{equation}
for all $x\in \mathcal{R}^{n}$.

The reason for the restriction to quasi-linear case and thus not too strong nonlinear character of the problem is hope that the
convergence of sparse recovery techniques can still be guaranteed requiring similar conditions as in the linear case.
The sparse signals recovered under the quasi-linear case can be mathematical viewed as the following form
\begin{equation}\label{r5}
(QP_{0})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}}\|x\|_{0}\ \ \mathrm{subject}\ \mathrm{to}\ \ F(x)x=b.
\end{equation}
Usually, solving the minimization problem $(QP_{0})$ could be transformed into solving the following regularization problem
\begin{equation}\label{r6}
(QP_{0}^{\lambda})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}} \Big\{\|F(x)x-b\|_{2}^{2}+\lambda\|x\|_{0})\Big\}
\end{equation}
where $\lambda>0$, which is called the regularization parameter, represents a tradeoff between error and sparsity.
Similar to the linear circumstance, the quasi-linear compressed sensing is also combinatorial and NP-hard because of the
discrete and discontinuous nature of the $\ell_{0}$-norm.

In general, the relaxation methods replace $\ell_{0}$-norm by a continuous sparsity promoting penalty functions $P(x)$ and the minimization
takes the form
\begin{equation}\label{r7}
\min_{x\in \mathcal{R}^{n}}P(x)\ \ \mathrm{subject}\ \mathrm{to}\ \ F(x)x=b
\end{equation}
for the constrained problem and
\begin{equation}\label{r8}
\min_{x\in \mathcal{R}^{n}} \Big\{\|F(x)x-b\|_{2}^{2}+\lambda P(x)\Big\}
\end{equation}
for the regularization problem.

Convex relaxation uniquely selects $P(x)$ as the $\ell_{1}$-norm [6,7], and the minimization for quasi-linear compressed sensing has the
following form
\begin{equation}\label{r9}
(QP_{1})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}}\|x\|_{1}\ \ \mathrm{subject}\ \mathrm{to}\ \ F(x)x=b.
\end{equation}
for the constrained problem and
\begin{equation}\label{r10}
(QP_{1}^{\lambda})\ \ \ \ \ \ \min_{x\in \mathcal{R}^{n}} \Big\{\|F(x)x-b\|_{2}^{2}+\lambda\|x\|_{1}\Big\}
\end{equation}
for the regularization problem, where $\|x\|_{1}=\sum_{i=1}^{n}|x_{i}|$ is called $\ell_{1}$-norm of vector $x$.

In linear compressed sensing, a lot of excellent theoretical work [8-12], together with some empirical
evidence [13], has shown that, provided some conditions are met, such as assuming the restricted isometric
property (RIP), the $\ell_{1}$-norm minimization can really make an exact recovery.

In this paper, we replace $\|x\|_{0}$ by a continuous sparsity promoting penalty function
\begin{equation}\label{r11}
P(x)=P_{a}(x)=\sum_{i=1}^{n}\rho_{a}(x_{i}),\ \ \ a>0
\end{equation}
where
\begin{equation}\label{r12}
\rho_{a}(t)=\frac{a|t|}{a|t|+1}
\end{equation}
is the fraction function, called "strictly non-interpolating" in[14], good performance in image restoration.

In [14], the non-convex fraction function gave rise to a step-shaped estimate from ramp-shaped data. In [15], Nikolova demonstrated
that for almost all data, the strongly homogeneous zones recovered by the non-convex fraction function were preserved constant under
any small perturbation of the data.

Clearly, $\rho_{a}(t)$ is increasing and concave in $t\in[0,+\infty]$, and with the change of parameter $a>0$,
the non-convex fraction function $\rho_{a}(t)$ interpolates the $\ell_{0}$-norm
\begin{equation}\label{r13}
\lim_{a\rightarrow+\infty}\rho_{a}(t)=\left\{
    \begin{array}{ll}
      0, & {\ \ \mathrm{if} \ t=0;} \\
      1, & {\ \ \mathrm{if} \ t\neq 0.}
    \end{array}
  \right.
\end{equation}
\begin{figure}[h!]
 \centering
 \includegraphics[width=0.75\textwidth]{fractiontu.eps}
\caption{The behavior of the fraction function $\rho_{a}(t)$ for various values of $a>0$.}
\label{fig:1}       
\end{figure}

By this transformation, the minimization problem $(QP_{0})$ could be translated into the following minimization problem
\begin{equation}\label{r14}
(QP_{a})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}} P_{a}(x)\ \ \mathrm{subject}\ \mathrm{to}\ \ F(x)x=b
\end{equation}
for the constrained problem and
\begin{equation}\label{r15}
(QP_{a}^{\lambda})\ \ \ \ \ \min_{x\in \mathcal{R}^{n}}\Big\{\|F(x)x-b\|_{2}^{2}+\lambda P_{a}(x)\Big\}.
\end{equation}
for the regularization problem.

In [16], we proved that in every underdetermined linear system $Ax=b$ there corresponds a constant $a^{\ast}(A,b)>0$ such
that every solution to the minimization problem $(QP_{a})$ with non-convex penalty $P_{a}(x)$ also solves $(QP_{0})$ whenever $a>a^{\ast}(A,b)$.
Moreover, we also demonstrated that the uniqueness of global minimizer of $(QP_{a})$ equivalences to $(P_{0})$ if the
sensing matrix $A$ satisfies a restricted isometry property (RIP) and if $a>a^{\ast}$, where $a^{\ast}$ depends only on matrix $A$.

The rest of this paper is organized as follows. In Section 2, we propose an iterative fraction thresholding algorithm to
solve the regularization problem $(QP_{a}^{\lambda})$. In Section 3, we present some numerical experiments to demonstrate
the effectiveness of our algorithm. The concluding remarks are presented in Section 4.

\section{Iterative fraction thresholding algorithm (IFTA) for solving $(QP_{a}^{\lambda})$}
In this section, the iterative fraction thresholding algorithm (IFTA) is proposed to solve the regularization problem $(QP_{a}^{\lambda})$
for all $a>0$.

Before we embark to this discussion, we hope that the regularization problem $(QP_{a}^{\lambda})$ with the non-convex penalty
$P_{a}(x)$ might preferably has sparse solution. In fact, it is a standard result, but we explicitly state this and prove it for
the sake of completeness.

As a matter of convenience, we let
\begin{equation}\label{r16}
\hat{x}_{\lambda}:=\arg\min_{x\in \mathcal{R}^{n}}\mathcal{T}_{\lambda}(x)
\end{equation}
where
$$\mathcal{T}_{\lambda_{n}}(x)=\|F(x)x-b\|_{2}^{2}+\lambda_{n} P_{a}(x),\ \ \ \lambda_{n}>0.$$

\begin{proposition}\label{pro1}
Let the map $F:x\mapsto F(x)$ be continuous, and suppose that $\{\lambda_{n}\}_{n=1}^{+\infty}$ is a sequence that converges
toward 0. If $\{\hat{x}_{\lambda_{n}}\}_{n=1}^{+\infty}$ is any sequence of unique minimizers of $\mathcal{T}_{\lambda_{n}}$, then it contains a
subsequence that converges towards the optimal solution of $(QP_{a})$.
\end{proposition}

\begin{proof}
Let $\hat{x}$ be the optimal solution of $(QP_{a})$, we have
\begin{eqnarray*}
P_{a}(\hat{x}_{\lambda_{n}})&\leq& \frac{1}{\lambda_{n}}\mathcal{T}_{\lambda_{n}}(\hat{x}_{\lambda_{n}})\\
&\leq&\frac{1}{\lambda_{n}}\mathcal{T}_{\lambda_{n}}(\hat{x})\\
&=&\frac{1}{\lambda_{n}}(\|F(\hat{x})\hat{x}-b\|_{2}^{2}+\lambda_{n} P_{a}(\hat{x}))\\
&=&\frac{1}{\lambda_{n}}(\lambda_{n} P_{a}(\hat{x}))\\
&=&P_{a}(\hat{x}).
\end{eqnarray*}
Hence, the sequence $\{P_{a}(\hat{x}_{\lambda_{n}})\}_{n=1}^{+\infty}$ is bounded.

It is very easy to verifies that there exists a convergent subsequence $\{\hat{x}_{\lambda_{\tilde{n}_{j}}}\}_{j\in N^{+}}$
converges to $\bar{x}\in \mathcal{R}^{n}$ for $j\rightarrow \infty$ is the optimal solution of $(QP_{a})$.

Since
\begin{equation}\label{r17}
\begin{array}{llll}
\|F(\hat{x})\hat{x}-b\|_{2}^{2}&=&\displaystyle\lim_{j\rightarrow\infty}\mathcal{T}_{\lambda_{\tilde{n}_{j}}}(\hat{x})\\\\
&\geq&\displaystyle\lim_{j\rightarrow\infty}\mathcal{T}_{\lambda_{\tilde{n}_{j}}}(\hat{x}_{\lambda_{\tilde{n}_{j}}})\\\\
&=&\|F(\bar{x})\bar{x}-b\|_{2}^{2}.
\end{array}
\end{equation}
We can conclude that
$$0=\|F(\hat{x})\hat{x}-b\|_{2}^{2}\geq\|F(\bar{x})\bar{x}-b\|_{2}^{2}$$
which implies that
\begin{equation}\label{r18}
F(\bar{x})\bar{x}=b.
\end{equation}

Moreover, combined with the upper bound of the sequence $\{P_{a}(\hat{x}_{\lambda_{n}})\}_{n=1}^{+\infty}$, we have
\begin{equation}\label{r19}
P_{a}(\bar{x})=\lim_{j\rightarrow\infty}P_{a}(\hat{x}_{\lambda_{\tilde{n}_{j}}})\leq P_{a}(\hat{x}),
\end{equation}
and we can deduce that
$$\bar{x}\in\arg\min_{x\in \mathcal{R}^{n}} \ P_{a}(x)\ \ s.t. \ \  F(x)x=b.$$
Hence, $\bar{x}$ must be the optimal solution of $(QP_{a})$.\ \ \ \ \ \ \ $\sharp$
\end{proof}

Some results need to be expressed before IFTA is proposed to solve the regularization problem $(QP_{a}^{\lambda})$.

\begin{lemma}\label{le1} {\rm [16]}
Define three threshold values
$$t_{1}^{\ast}=\frac{\sqrt[3]{\frac{27}{8}\lambda a^{2}}-1}{a}, \ \ \ \ t_{2}^{\ast}=\frac{\lambda}{2}a, \ \ \ \ t_{3}^{\ast}=\sqrt{\lambda}-\frac{1}{2a}$$
for any positive parameters $\lambda$ and $a$, then the inequalities $t_{1}^{\ast}\leq t_{3}^{\ast}\leq t_{2}^{\ast}$ hold. Furthermore,
they are equal to $\frac{1}{2a}$ when $\lambda=\frac{1}{a^{2}}$.
\end{lemma}

We define a function of $\beta\in \mathcal{R}$ as
$$f_{\lambda}(\beta)=(\beta-\gamma)^{2}+\lambda\cdot\rho_{a}(\beta)$$
and
$$\beta^{\ast}=\arg\min_{\beta\in \mathcal{R}}f_{\lambda}(\beta).$$
\begin{lemma}\label{le2} {\rm [16]}
The optimal solution to $\beta^{\ast}=\displaystyle \arg\min_{\beta\in \mathcal{R}} f_{\lambda}(\beta)$ is the threshold function defined as
\begin{equation}\label{r20}
\beta^{\ast}=\left\{
    \begin{array}{ll}
      g_{a,\lambda}(\gamma), & \ \ \mathrm{if} \ {|\gamma|> t^{\ast};} \\
      0, & \ \ \mathrm{if} \ {|\gamma|\leq t^{\ast}.}
    \end{array}
  \right.
\end{equation}
where $g_{a,\lambda}(\gamma)$ is defined as
\begin{equation}\label{r21}
g_{a,\lambda}(\gamma)=sign(\gamma)\bigg(\frac{\frac{1+a|\gamma|}{3}(1+2\cos(\frac{\phi(\gamma)}{3}-\frac{\pi}{3}))-1}{a}\bigg),
\end{equation}
$$\phi(\gamma)=\arccos\Big(\frac{27\lambda a^{2}}{4(1+a|\gamma|)^{3}}-1\Big)$$
and the threshold value satisfies
\begin{equation}\label{r22}
t^{\ast}=\left\{
    \begin{array}{ll}
      t_{2}^{\ast}, & \ \ \mathrm{if} \ {\lambda\leq \frac{1}{a^{2}};} \\
      t_{3}^{\ast}, & \ \ \mathrm{if} \ {\lambda>\frac{1}{a^{2}}.}
    \end{array}
  \right.
\end{equation}
\end{lemma}
The more detailed accounts of Lemmas 1 and 2 can be seen in [16].

\begin{proposition}\label{pro2}
The threshold function $g_{a,\lambda}$ defined in Lemma 2 is monotone.
\end{proposition}

\begin{proof}
For different $\gamma_{1}$ and $\gamma_{2}$, let
$$\beta_{1}\in\displaystyle\arg\min_{\beta\in \mathcal{R}}\Big\{(\beta-\gamma_{1})^{2}+\lambda\cdot\rho_{a}(\beta)\Big\},$$
$$\beta_{2}\in\displaystyle\arg\min_{\beta\in \mathcal{R}}\Big\{(\beta-\gamma_{2})^{2}+\lambda\cdot\rho_{a}(\beta)\Big\}.$$
Then, we have
$$(\beta_{2}-\gamma_{1})^{2}+\lambda\cdot \rho_{a}(\beta_{2})\geq(\beta_{1}-\gamma_{1})^{2}+\lambda\cdot \rho_{a}(\beta_{1}),$$
$$(\beta_{1}-\gamma_{2})^{2}+\lambda\cdot \rho_{a}(\beta_{1})\geq(\beta_{2}-\gamma_{2})^{2}+\lambda\cdot \rho_{a}(\beta_{2}).$$
By adding them, we have
$$(\beta_{2}-\gamma_{1})^{2}+(\beta_{1}-\gamma_{2})^{2}\geq(\beta_{1}-\gamma_{1})^{2}+(\beta_{2}-\gamma_{2})^{2}$$
and
$$(\beta_{1}-\beta_{2})(\gamma_{1}-\gamma_{2})\geq0.$$
That is
$$(g_{a,\lambda}(\gamma_{1})-g_{a,\lambda}(\gamma_{2}))(\gamma_{1}-\gamma_{2})\geq0$$
for any $\gamma_{1}, \gamma_{2}>t^{\ast}$, and the thresholding function $g_{a,\lambda}$ defined in Lemma 2 is monotone. \ \ \ \ \ \ \ $\sharp$
\end{proof}

\begin{definition}\label{de1}
The iterative thresholding operator $G_{\lambda, P}$ is a diagonally nonlinear analytically expressive operator, and can be defined by
\begin{equation}\label{r23}
G_{\lambda, P}(x)=\Big(g_{a,\lambda}(x_{1}), \cdots, g_{a,\lambda}(x_{n})\Big)^{T}
\end{equation}
where $g_{a,\lambda}(x_{i})$ is defined in Lemma 2.
\end{definition}

\begin{figure}[h!]
  \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{a=1.eps}
  \end{minipage}
  \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{a=2.eps}
  \end{minipage}
   \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{a=3.eps}
  \end{minipage}
   \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{a=5.eps}
  \end{minipage}
  \caption{The plots of the threshold function $g_{a,\lambda}$ for a=1, 2, 3, 5, and $\lambda=0.25$.} \label{fig:2}
\end{figure}

\begin{figure}[h!]
  \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{huatusoft.eps}
  \end{minipage}
  \begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\textwidth]{huatuhard.eps}
  \end{minipage}
  \caption{The plots of the Soft/Hard (left/right) threshold functions withe $\lambda=0.25$.} \label{fig:3}
\end{figure}

Inspired by Lemma 2, we will show that the optimal solution to $(QP_{a}^{\lambda})$ could also be expressed as a threshold
function defined as (20).

For any fixed positive parameters $\lambda>0$, $\mu>0$, $a>0$ and $x\in \mathcal{R}^{n}$, let
\begin{equation}\label{r24}
C_{1}(x)=\|F(x)x-b\|_{2}^{2}+\lambda P_{a}(x)
\end{equation}
and
\begin{equation}\label{r25}
\begin{array}{llll}
C_{2}(x, y)&=&\mu\|F(y)x-b\|_{2}^{2}+\lambda\mu P_{a}(x)\\
&&-\mu\|F(y)x-F(y)y\|_{2}^{2}+\|x-y\|_{2}^{2}.
\end{array}
\end{equation}
Clearly, $C_{2}(x,x)=\mu C_{1}(x)$.
\begin{theorem}\label{th1}
For any fixed positive parameters $\lambda>0$, $\mu>0$ and $y\in \mathcal{R}^{n}$, $\displaystyle\min_{x\in \mathcal{R}^{n}}C_{2}(x,y)$
equivalents to
\begin{equation}\label{r26}
\min_{x\in \mathcal{R}^{n}}\Big\{\|x-B_{\mu}(y)\|_{2}^{2}+\lambda\mu P_{a}(x)\Big\}
\end{equation}
where $B_{\mu}(y)=y+\mu F(y)^{\ast}(b-F(y)y)$.
\end{theorem}
\begin{proof}
By the definition, $C_{2}(x,y)$ can be rewritten as
\begin{eqnarray*}
C_{2}(x,y)&=&\|x-(y-\mu F(y)^{\ast}F(y)y+\mu F(y)^{\ast}b)\|_{2}^{2}+\lambda\mu P_{a}(x)+\mu\|b\|_{2}^{2}+\|y\|_{2}^{2}\\
&&-\mu\|F(y)y\|_{2}^{2}-\|y-\mu F(y)^{\ast}F(y)y+\mu F(y)^{\ast}b\|_{2}^{2}\\
&=&\|x-B_{\mu}(y)\|_{2}^{2}+\lambda\mu P_{a}(x)+\mu\|b\|_{2}^{2}+\|y\|_{2}^{2}-\mu\|F(y)y\|_{2}^{2}-\|B_{\mu}(y)\|_{2}^{2}
\end{eqnarray*}
which implies that $\displaystyle\min_{x\in \mathcal{R}^{n}}C_{2}(x,y)$ for any fixed positive parameters $\lambda>0$, $\mu>0$ and
$y\in \mathcal{R}^{n}$ equivalents to
$$\min_{x\in \mathcal{R}^{n}}\Big\{\|x-B_{\mu}(y)\|_{2}^{2}+\lambda\mu P_{a}(x)\Big\}. \ \ \ \ \ \ \ \sharp $$
\end{proof}

\begin{theorem}\label{th2}
For any fixed positive parameter $\lambda>0$ and $0<\mu<L_{\ast}^{-1}$ with $\|F(x^{\ast})x-F(x^{\ast})x^{\ast}\|_{2}^{2}\leq L_{\ast}\|x-x^{\ast}\|_{2}^{2}$.
If $x^{\ast}$ is the optimal solution of $\displaystyle\min_{x\in \mathcal{R}^{m\times n}}C_{1}(x)$, then $x^{\ast}$ is also the optimal solution of
$\displaystyle\min_{x\in \mathcal{R}^{m\times n}}C_{2}(x,x^{\ast})$, that is
$$C_{2}(x^{\ast},x^{\ast})\leq C_{2}(x,x^{\ast})$$
for any $x\in \mathcal{R}^{m\times n}$.
\end{theorem}
\begin{proof}
By the definition of $C_{2}(x, y)$, we have
\begin{eqnarray*}
C_{2}(x,x^{\ast})&=&\mu\|F(x^{\ast})x-b\|_{2}^{2}+\lambda\mu P_{a}(x)\\
&&-\mu\|F(x^{\ast})x-F(x^{\ast})x^{\ast}\|_{2}^{2}+\|x-x^{\ast}\|_{2}^{2}\\
&\geq&\mu\|F(x^{\ast})x-b\|_{2}^{2}+\lambda\mu P_{a}(x)\\
&\geq&\mu C_{1}(x^{\ast})\\
&=&C_{2}(x^{\ast},x^{\ast}). \ \ \ \ \ \ \ \sharp
\end{eqnarray*}
\end{proof}

Theorem 2 shows that $x^{\ast}$ is the optimal solution of $\displaystyle\min_{x\in \mathcal{R}^{n}}C_{2}(x,x^{\ast})$ if
and only if $x^{\ast}$ solves $\displaystyle\min_{x\in \mathcal{R}^{n}}C_{1}(x)$. Moreover, combined with Lemma 2 and Theorem 1,
we can immediately conclude that the thresholding representation of $(QP_{a}^{\lambda})$ can be given by
\begin{equation}\label{r28}
x_{i}^{\ast}=g_{a,\lambda\mu}(B_{\mu}(x^{\ast})_{i})
\end{equation}
where the threshold function $g_{a,\lambda\mu}$ is obtained in Lemma 2 by replacing $\lambda$ with $\lambda\mu$.

Moreover, according to Lemma 2 and the thresholding representation (27), we can get that the following result.

\begin{corollary}\label{co1}
For any fixed $\lambda>0$, $\mu>0$ and vector $x^{\ast}\in \mathcal{R}^{n}$, let $x^{\ast}=G_{\lambda\mu, P}(B_{\mu}(x^{\ast}))$, then
\begin{equation}\label{r28}
x^{\ast}_{i}=\left\{
    \begin{array}{ll}
      g_{a,\lambda\mu}(|B_{\mu}(x^{\ast})_{i}|), & \ \ \mathrm{if} \ {|B_{\mu}(x^{\ast})_{i}|> t^{\ast};} \\
      0, & \ \ \mathrm{if} \ {|B_{\mu}(x^{\ast})_{i}|\leq t^{\ast}.}
    \end{array}
  \right.
\end{equation}
\end{corollary}

With the thresholding representations (27) and (28), the IFTA for solving the regularization problem $(QP_{a}^{\lambda})$ can be naturally defined as
\begin{equation}\label{r29}
x_{i}^{k+1}=G_{\lambda\mu, P}(B(x^{k}))_{i}=g_{a,\lambda\mu}(B(x^{k})_{i})
\end{equation}
where $B_{\mu}(x^{k})=x^{k}+\mu F(x^{k})^{\ast}(b-F(x^{k})x^{k})$.

\begin{algorithm}[h!]
\caption{: Iterative fraction thresholding algorithm (IFTA)-Scheme 1}
\label{alg:A}
\begin{algorithmic}
\STATE {Initialize: Given $x^{0}\in \mathcal{R}^{n}$, $\mu_{0}=\frac{1-\epsilon}{\|F(x^{0})\|_{2}^{2}}$ $(0<\epsilon<1)$ and $a>0$;}
\STATE {\textbf{while} not converged \textbf{do}}
\STATE \ \ \ \ \ \ \ {$z^{k}:=B_{\mu_{k}}(x^{k})=x^{k}+\mu_{k} F(x^{k})^{\ast}(b-F(x^{k})x^{k}))$;}
\STATE \ \ \ \ \ \ \ {$\lambda=\lambda_{0}\ \mathrm{and}\ \mu_{k}=\frac{1-\epsilon}{\|F(x^{k})\|_{2}^{2}}$;}
\STATE \ \ \ \ \ \ \ {if\ $\lambda\leq\frac{1}{a^{2}\mu_{k}}$\ then}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {$t=\frac{\lambda\mu_{k} a}{2}$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {for\ $i=1:\mathrm{length}(x)$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {1.\ $|z^{k}_{i}|>t$, then $x^{k+1}_{i}=\displaystyle\arg\min_{x\in \mathcal{R}}C_{2}(x_{i},x_{i}^{k})=g_{a,\lambda\mu_{k}}(z^{k}_{i})$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {2.\ $|z^{k}_{i}|\leq t$, then $x^{k+1}_{i}=0$}
\STATE \ \ \ \ \ \ \ {else}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {$t=\sqrt{\lambda\mu_{k}}-\frac{1}{2a}$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {for\ $i=1:\mathrm{length}(x)$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {1.\ $|z^{k}_{i}|>t$, then $x^{k+1}_{i}=\displaystyle\arg\min_{x\in \mathcal{R}}C_{2}(x_{i},x_{i}^{k})=g_{a,\lambda\mu_{k}}(z^{k}_{i})$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {2.\ $|z^{k}_{i}|\leq t$, then $x^{k+1}_{i}=0$}
\STATE \ \ \ \ \ \ \ {end}
\STATE \ \ \ \ \ \ \ {$k\rightarrow k+1$}
\STATE{\textbf{end while}}
\STATE{\textbf{return}: $x^{k+1}$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{: Iterative fraction thresholding algorithm (IFTA)-Scheme 2}
\label{alg:A}
\begin{algorithmic}
\STATE {Initialize: Given $x^{0}\in \mathcal{R}^{n}$, $\mu_{0}=\frac{1-\epsilon}{\|F(x^{0})\|_{2}^{2}}$ $(0<\epsilon<1)$ and $a>0$;}
\STATE {\textbf{while} not converged \textbf{do}}
\STATE \ \ \ \ \ \ \ {$z^{k}:=B_{\mu_{k}}(x^{k})=x^{k}+\mu_{k} F(x^{k})^{\ast}(y-F(x^{k})x^{k})$;}
\STATE \ \ \ \ \ \ \ {$\lambda_{1,k}=\frac{2|B_{\mu_{k}}(x^{k})|_{r+1}}{a\mu_{k}}$, $\lambda_{2,k}=\frac{(2a|B_{\mu_{k}}(x^{k})|_{r}+1)^{2}}{4a^{2}\mu_{k}}$, $\mu_{k}=\frac{1-\epsilon}{\|F(x^{k})\|_{2}^{2}}$;}
\STATE \ \ \ \ \ \ \ {if\ $\lambda_{1,k}\leq\frac{1}{a^{2}\mu_{k}}$\ then}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {$\lambda=\lambda_{1,k}$; $t=\frac{\lambda\mu_{k} a}{2}$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {for\ $i=1:\mathrm{length}(x)$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {1.\ $|z^{k}_{i}|>t$, then $x^{k+1}_{i}=\displaystyle\arg\min_{x\in \mathcal{R}}C_{2}(x_{i},x_{i}^{k})=g_{a,\lambda\mu_{k}}(z^{k}_{i})$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {2.\ $|z^{k}_{i}|\leq t$, then $x^{k+1}_{i}=0$}
\STATE \ \ \ \ \ \ \ {else}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {$\lambda=\lambda_{2,k}$; $t=\sqrt{\lambda\mu_{k}}-\frac{1}{2a}$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ {for\ $i=1:\mathrm{length}(x)$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {1.\ $|z^{k}_{i}|>t$, then $x^{k+1}_{i}=\displaystyle\arg\min_{x\in \mathcal{R}}C_{2}(x_{i},x_{i}^{k})=g_{a,\lambda\mu_{k}}(z^{k}_{i})$}
\STATE \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {2.\ $|z^{k}_{i}|\leq t$, then $x^{k+1}_{i}=0$}
\STATE \ \ \ \ \ \ \ {end}
\STATE \ \ \ \ \ \ \ {$k\rightarrow k+1$}
\STATE{\textbf{end while}}
\STATE{\textbf{return}: $x^{k+1}$}
\end{algorithmic}
\end{algorithm}

\emph{In each iteration step of Algorithm 1 and Algorithm 2, we minimize the surrogate function $C_{2}(x,x^{k})$ in the first variable
having the second one fixed with the previous iteration, which requires only a simple iterative fraction thresholding.}\\\\

It is fairly well known that the quantity of the solution of a regularization problem depends seriously on the setting of the regularization
parameter. Here, the cross-validation method is accepted to select the proper regularization parameter. Nevertheless, when some prior information
is known for a regularization problem, this selection is more reasonably and intelligently. When doing so, the IFTA will be adaptive and free from
the choice of the regularization parameter.

To make this selection clear, we suppose that the vector $x^{\ast}$ of sparsity $r$ is the optimal solution of the regularization problem $(QP_{a}^{\lambda})$,
without loss of generality, we suppose that
$$|B_{\mu}(x^{\ast})|_{1}\geq|B_{\mu}(x^{\ast})|_{2}\geq\cdots\geq|(B_{\mu}(x^{\ast})|_{r}\geq\cdots\geq|(B_{\mu}(x^{\ast})|_{n}.$$
By Corollary 1, the following inequalities hold
$$|B_{\mu}(x^{\ast})|_{i}>t^{\ast}\Leftrightarrow i\in\{1,2,\cdots,r\},$$
$$|B_{\mu}(x^{\ast})|_{i}\leq t^{\ast}\Leftrightarrow i\in\{r+1,r+2,\cdots,n\}$$
where $t^{\ast}$ is the threshold value defined in (22) obtained by replacing $\lambda$ with $\lambda\mu$.

According to $t_{3}^{\ast}\leq t_{2}^{\ast}$, we have
\begin{equation}\label{r30}
\left\{
  \begin{array}{ll}
   |B_{\mu}(x^{\ast})|_{r}\geq t^{\ast}\geq t_{3}^{\ast}=\sqrt{\lambda\mu}-\frac{1}{2a}; \\
   |B_{\mu}(x^{\ast})|_{r+1}<t^{\ast}\leq t_{2}^{\ast}=\frac{\lambda\mu}{2}a,
  \end{array}
\right.
\end{equation}
which implies
\begin{equation}\label{r31}
\frac{2|B_{\mu}(x^{\ast})|_{r+1}}{a\mu}\leq\lambda\leq\frac{(2a|B_{\mu}(x^{\ast})|_{r}+1)^{2}}{4a^{2}\mu}.
\end{equation}
Above estimation helps to set the optimal regularization parameter $\lambda^{\ast}$. For convenience, we denote by
$\lambda_{1}$ and $\lambda_{2}$ the left and the right of above inequality respectively.
$$
\left\{
  \begin{array}{ll}
   \lambda_{1}=\frac{2|B_{\mu}(x^{\ast})|_{r+1}}{a\mu}; \\
   \lambda_{2}=\frac{(2a|B_{\mu}(x^{\ast})|_{r}+1)^{2}}{4a^{2}\mu},
  \end{array}
\right.
$$

A choice of $\lambda^{\ast}$ is
$$\lambda^{\ast}=\left\{
            \begin{array}{ll}
              \lambda_{1}=\frac{2|B_{\mu}(x^{\ast})|_{r+1}}{a\mu}, & \ \ {\mathrm{if}\ \lambda_{1}=\frac{2|B_{\mu}(x^{\ast})|_{r+1}}{a\mu}\leq\frac{1}{a^{2}\mu};} \\
              \lambda_{2}=\frac{(2a|B_{\mu}(x^{\ast})|_{r}+1)^{2}}{4a^{2}\mu}, &\ \ {\mathrm{if}\ \lambda_{1}=\frac{2|B_{\mu}(x^{\ast})|_{r+1}}{a\mu}>\frac{1}{a^{2}\mu}.}
            \end{array}
          \right.
$$
In practice, we approximate $B_{\mu}(x^{\ast})_{i}$ by $B_{\mu}(x^{k})_{i}$ in (31), and we can take
\begin{equation}\label{r39}
\begin{array}{llll}
\lambda_{k}=\left\{
            \begin{array}{ll}
              \lambda_{1,k}=\frac{2|B_{\mu}(x^{k})|_{r+1}}{a\mu},  & \ \ {\mathrm{if}\ \lambda_{1,k}=\frac{2|B_{\mu}(x^{k})|_{r+1}}{a\mu}\leq\frac{1}{a^{2}\mu};} \\
              \lambda_{2,k}=\frac{(2a|B_{\mu}(x^{k})|_{r}+1)^{2}}{4a^{2}\mu},  & \ \ {\mathrm{if}\ \lambda_{1,k}=\frac{2|B_{\mu}(x^{k})|_{r+1}}{a\mu}>\frac{1}{a^{2}\mu}.}
            \end{array}
          \right.
\end{array}
\end{equation}
in applications.

Incorporated with different parameter-setting strategies, defines different implementation schemes of the IFTA. For example,
we can have the following
\begin{itemize}
  \item Scheme 1: $\mu=\mu_{0}$, $\lambda=\lambda_{0}$ and $a=a_{0}$.
  \item Scheme 2: $\mu=\mu_{0}$, $\lambda=\lambda_{k}$ defined in (32) and $a=a_{0}$.
\end{itemize}

In particular, there is one more thing needed to be mentioned that the threshold value
\begin{equation}\label{r39}
\begin{array}{llll}
t^{\ast}=\left\{
            \begin{array}{ll}
              \frac{\lambda_{1,k}\mu}{2}a,  & \ \ {\mathrm{if}\ \lambda_{1,k}=\frac{2|B_{\mu}(x^{k})|_{r+1}}{a\mu}\leq\frac{1}{a^{2}\mu};} \\
              \sqrt{\lambda_{2,k}\mu}-\frac{1}{2a},  & \ \ {\mathrm{if}\ \lambda_{1,k}=\frac{2|B_{\mu}(x^{k})|_{r+1}}{a\mu}>\frac{1}{a^{2}\mu}.}
            \end{array}
          \right.
\end{array}
\end{equation}
in Scheme 2.

Note that (33) is valid for any $\mu>0$ satisfying $0<\mu\leq\|F(x_{k})\|_{2}^{-2}$. In general, we can take $\mu=\mu_{0}=\frac{1-\epsilon}{\|F(x_{k})\|_{2}^{2}}$
with any small $\epsilon\in(0,1)$ below.

\section{Numerical experiments} \label{applications-sec}
In the section, we carry out a series of simulations to demonstrate the performance of IFTA (Scheme 2) for solving the regularization
problem $(QP_{a}^{\lambda})$.

In our numerical experiments, we set
\begin{equation}\label{34}
F(x)=A_{1}+\epsilon f(\|x-x_{0}\|_{2}) A_{2}
\end{equation}
where $A_{1}\in\mathcal{R}^{m\times n}$ is a fixed Gaussian random matrix satisfying the RIP of order $k$ with $0<\delta_{k}<1$
(The random matrices satisfy the RIP of order $k$ with high probability [17]), $x_{0}\in \mathcal{R}^{n}$ is a reference vector,
$f:[0,\infty)\rightarrow \mathcal{R}$ is a positive and smooth Lipschitz continuous function with $f(t)=\ln(t+1)$, $\epsilon$ is
a sufficiently small scaling factor (we set $\epsilon=0.003$), and $A_{2}\in \mathcal{R}^{m\times n}$ is a fixed matrix with
every entry equal to 1. Then the form of nonlinearity considered in (34) is a quasi-linear, and the more detailed accounts of
the setting in form (34) can be seen in [6,17].

We consider a random matrix $A_{1}$ of size $30\times100$, with entries independently drawn by random from a Gaussian distribution of zero
mean and unit variance, $N(0, 1)$. By randomly generating such sufficiently sparse vectors $x_{0}$ (choosing the non-zero locations
uniformly over the support in random, and their values from $N(0,1))$, we generate vectors $b$. This way, we know the sparsest solution
to $F(x_{0})x_{0} = b$, and we are able to compare this to algorithmic results. The success is measured by the computing
$$\mathrm{relative\ error}=\frac{\|x^{\ast}-x_{0}\|_{2}}{\|x_{0}\|_{2}}\leq \mathrm{Tol}.$$

In our all experiments, the setting of the regularization parameter $\lambda$ are all conducted by applying (33) in Scheme 2, and we set
$\mathrm{Tol}=10^{-4}$ to indicate a perfect recovery of the original sparse vector $x_{0}$.

Among all of the experiments, we compare the performances of IFTA with some state-of-art methods (iterative soft thresholding algorithm (ISTA)[6,17],
iterative hard thresholding algorithm (IHTA)[6,17]). And for each experiment, we repeatedly perform 30 tests and present average results and take $a=1$.

\begin{figure}[h!]
 \centering
 \includegraphics[width=0.75\textwidth]{succes.eps}
\caption{The success rate of three algorithms in the recovery of a sparse signal with different cardinality with $a=1$.}
\label{fig:3}       
\end{figure}

\begin{figure}[h!]
 \centering
 \includegraphics[width=0.75\textwidth]{relative.eps}
\caption{The relative error between the solution $x^{\ast}$ and the given signal $x_{0}$ with $a=1$.}
\label{fig:4}       
\end{figure}

The graphs presented in Fig.3 and Fig.4 show the performance of ISTA, IHTA and IFTA in recovering the true (sparsest) solution. From
Fig.3, we can see that IFTA performs best, with ISTA as the second. From Fig.4, we can see that IFTA always has the smallest relative
error value and the error values of IHTA and ISTA increase rapidly with sparsity growing.

\section{Conclusions}
In this paper, a non-convex fraction function $P_{a}(x)$ is studied to replace the $\ell_{0}$-norm $\|x\|_{0}$ in quasi-linear compressed sensing.
Inspired by our former work in compressed sensing under the linear circumstance, an iterative fraction thresholding algorithm is proposed to
solve the regularization problem $(QP_{a}^{\lambda})$. Numerical experiments show that our algorithm performs much better comparing with some
state-of-art methods. However, the convergence of IFTA does not proved theoretically in this paper, and it is our future work.

\begin{acknowledgements}
The work was supported by the National Natural Science Foundations of China (11131006, 11271297) and the Science
Foundations of Shaanxi Province of China (2015JM1012).
\end{acknowledgements}


\begin{thebibliography}{}

\bibitem{1}
E. Candes, J. Romberg, T. Tao. Stable signal recovery from incomplete and inaccurate measurements.
Communications on Pure and Applied Mathematics, 59(8): 1207-1223 (2006)

\bibitem{2}
D. L. Donoho. Compressed sensing. IEEE Transaction on Information Theory, 52(4): 1289-1306 (2006)

\bibitem{3}
A. M. Bruckstein, D. L. Donoho, and M. Elad. From sparse solutions of systems of equations to sparse modelling
of signals and images. SIAM Review, 51(1): 34-81 (2009)

\bibitem{4}
M. Elad. Sparse and Redundant Representations: from Theory to Applications in Signal and Image Processing. Springe, New York, 2010.

\bibitem{5}
S. Theodoridis, Y. Kopsinis, and K. Slavakis. Sparsity-aware learning and compressed sensing: an overview.
arXiv: 1211.5231v1 [cs. IT] 22 Nov 2012.

\bibitem{6}
M. Ehler, M. Fornasier, and J. Sigl. Quasi-linear compressed sensing. Multiscale Modeling and Simulation, 12(2): 725-754 (2014)

\bibitem{7}
J. Sigl. Quasilinear compressed sensing, Master's thesis, Technische University M\"{u}nchen, Munich, Germay, 2013.

\bibitem{8}
E. Candes, T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12): 4203-4215 (2005).

\bibitem{9}
D. L. Dohoho, X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47(7): 2845-2862 (2001).

\bibitem{10}
D. L. Donoho, J. Tanner. Sparse nonnegative solution of underdetermined linear equations by linear programming. PNAS, 102(27): 9446-9451 (2005).

\bibitem{11}
D. L. Donoho, M. Elad. Optimally sparse representation in general (nonorthoganal) dictionaries via l1 minimization. Proceedings of the National Academy of Sciences of the United States of America, 100(5), 2197-2202 (2003).

\bibitem{12}
R. Gribonval, M. Nielson. Sparse representations in unions of bases. IEEE Transactions on Information Theory, 49(12): 3320-3325 (2003).

\bibitem{13}
S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomosition by basic pursuit. SIAM Journal of Science Computing, 20(1): 33-61 (1999).

\bibitem{14}
D. Geman and G. Reynolds. Constrained restoration and recovery of discontinuities. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(3), 367-383 (1992)

\bibitem{15}
M. Nikolova. Local strong homogeneity of a regularized estimator. SIAM Journal on Applied Mathematics, 61(2), 633-658 (2000)

\bibitem{16}
H. Li, Q. Zhang, A. Cui and J. Peng. Minimization of fraction function penalty in compressed sensing. arXiv:1705.06048v1 [math.OC] 17 May 2017

\bibitem{17}
S. Foucart, H. Rauhut. A mathematic introduction to compressive sensing. Springe, New York, 2013.

\end{thebibliography}

\end{document}


