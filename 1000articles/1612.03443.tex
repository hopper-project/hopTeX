
\documentclass[11pt,reqno]{amsart} 

\usepackage{graphicx}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{xcolor}
\usepackage[labelfont=bf,font=sf]{caption}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{polynom}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{boldline}
\usepackage{placeins}
\usepackage{afterpage}
\usepackage{xifthen}
\usepackage{framed}
\usepackage[font={sf,small},labelfont=md]{subfig}
\usepackage{hyperref, upref}
	\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black}
	

\numberwithin{equation}{section} 

\makeatletter
\setlength{\@fptop}{0pt plus 1fil}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{conjecture}[thm]{Conjecture}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{remark}[thm]{Remark}

 
 
 

 

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

 

\begin{document}
\bibliographystyle{plain}

\title{The endpoint distribution of directed polymers}

\subjclass[2010]{ 
60K37, 
82B26, 
82B44, 
82D60} 

\keywords{Directed polymer, free energy, disordered system, phase transition}

\author{Erik Bates}
\thanks{Erik Bates's research was partially supported by NSF grant DGE-114747} 
\address{\newline Department of Mathematics \newline Stanford University \newline 450 Serra Mall, Bldg 380 \newline Stanford, CA 94305 \newline \textup{\tt ewbates@stanford.edu}}

\author{Sourav Chatterjee}
\thanks{Sourav Chatterjee's research was partially supported by NSF grants DMS-1441513 and DMS-1608249}

\address{\newline Department of Statistics \newline Stanford University\newline Sequoia Hall, 390 Serra Mall \newline Stanford, CA 94305\newline \textup{\tt souravc@stanford.edu}}

 \setcounter{footnote}{0}
\begin{abstract}
Probabilistic models of directed polymers in random environment have received considerable attention  in recent years. Much of this attention has focused on integrable models. In this paper, we introduce some new computational tools that do not require integrability. We begin by defining a new kind of abstract limit object, called ``partitioned subprobability measure'', to describe the limits of endpoint distributions of directed polymers. Inspired by a recent work of Mukherjee and Varadhan on large deviations of the occupation measure of Brownian motion,  we define a suitable topology on the space of partitioned subprobability measures and prove that this topology is compact. Then using a variant of the cavity method from the theory of spin glasses, we  show that any limit law of a sequence of endpoint distributions must satisfy a fixed point equation on this abstract space, and that the limiting free energy of the model can be expressed as the solution of a variational problem over the set of fixed points.  As a first application of the theory, we prove that in an environment with finite exponential moment, the endpoint distribution is asymptotically purely atomic if and only if the system is in the low temperature phase. The analogous result for a heavy-tailed environment was proved by Vargas in~2007.
As a second application, we prove a subsequential version of the longstanding conjecture that in the low temperature phase, the endpoint distribution is asymptotically localized in a region of stochastically bounded diameter. All  our results hold in arbitrary dimensions, and make no use of integrability. 
\end{abstract}
\maketitle

 \setcounter{footnote}{0}

\tableofcontents

\section{Introduction}
The model of directed polymers in random environment was introduced in the physics literature by Huse and Henley \cite{huse-henley85} to represent the phase boundary of the Ising model in the presence of random impurities. It was later mathematically reformulated as a model of random walk in random potential by Imbrie and Spencer~\cite{imbrie-spencer88}. Over the last thirty years, the directed polymer model has played an important role as a source of many fascinating problems in the probability literature, culminating in the amazing recent developments in integrable polymer models. However, in spite of the wealth of information now available for integrable models, our knowledge about the general case is fairly limited, especially in spatial dimension greater than one. The goal of this paper is to introduce an abstract machine that allows computations for polymer models that are not integrable. Very briefly, the main ideas and results may be summarized as follows:
\begin{itemize}
\item The probabilistic model of $(d+1)$-dimensional directed polymers of length $n$ assigns a random probability measure to the set of random walk paths of length $n$ in ${\mathbb{Z}}^d$ that start at the origin. The precise mathematical model is defined in Section \ref{model} below. The ``endpoint distribution'' is the probability distribution of the final vertex reached by the random walk. Note that this is a random probability measure on ${\mathbb{Z}}^d$, which we will denote $f_n$. Understanding the  behavior of $f_n$ is the main goal of this article. Although a number of results about $f_n$ were known prior to this work (reviewed in  Section \ref{disorder_background}), this is the first paper that puts forward a comprehensive theoretical framework for analyzing the asymptotic properties of~$f_n$.
\item One problem with $f_n$ is that it is not expected to have a limit. Rather, it keeps fluctuating. To understand the asymptotic behavior of $f_n$, we therefore consider the {\it empirical measure} of the endpoint distribution,
{\begin{align*} {
\mu_n := \frac{1}{n+1}\sum_{i=0}^n \delta_{f_i},
} \end{align*}}
where $\delta_{f_i}$ is the Dirac point mass at $f_i$, considering $f_i$ as a random element of the space of probability measures on ${\mathbb{Z}}^d$. Note that $\mu_n$ is therefore a random probability measure on the space of probability measures on ${\mathbb{Z}}^d$. 
\item To obtain an adequate understanding of $\mu_n$, it turns out that we need to move to a larger space. The reason for this will be explained later, in Section \ref{results}. We first define a non-standard compactification ${\mathcal{S}}$ of the space of probability measures on ${\mathbb{Z}}^d$, and then consider ${\mathcal{P}}^1({\mathcal{S}})$, the space of probability measures on ${\mathcal{S}}$ with the Wasserstein metric. We consider $\mu_n$ as an element of ${\mathcal{P}}^1({\mathcal{S}})$. The elements of ${\mathcal{S}}$ are called ``partitioned subprobability measures'' in this article.
\item We define a certain continuous function ${\mathcal{T}} :{\mathcal{P}}^1({\mathcal{S}})\to {\mathcal{P}}^1({\mathcal{S}})$, called the ``update map'' in this paper, and let ${\mathcal{K}}$ be the set of fixed points of ${\mathcal{T}}$. Furthermore, we define a linear functional ${\mathcal{R}}$ on ${\mathcal{P}}^1({\mathcal{S}})$, and let ${\mathcal{M}}$ be the subset of ${\mathcal{K}}$ where ${\mathcal{R}}$ is minimized. Our first main result is that ${\mathcal{M}}$ is nonempty and the distance between $\mu_n$ and the set ${\mathcal{M}}$ converges almost surely to zero as $n\to \infty$. 
\item As a consequence of the above result, we obtain a new variational formula for the limiting free energy of directed polymers, namely, that the limit free energy equals the value of ${\mathcal{R}}$ on ${\mathcal{M}}$. 
\item Like many statistical mechanical models, directed polymers have a low temperature phase and a high temperature phase. One of the most striking features of directed polymers is that in the low temperature phase, $f_n$ has ``atoms'' whose weights do not decay to zero as $n\to \infty$. This is in stark contrast with the endpoint distribution of simple random walk, where the most likely site has mass of order $n^{-d/2}$. The precise statement of this well-known localization phenomenon will be discussed in Section~\ref{disorder_background}. One of the main applications of the abstract machinery developed in this paper is to show that in the low temperature regime, the atoms account for {\it all} of the mass --- that is, there is no part of the mass that diffuses out. Such a result was proved earlier for directed polymers in heavy-tailed environment. We prove it under finite exponential moments, where the behavior is somewhat unexpected.
\item Our second main application is to show that in the low temperature regime, there is almost surely a subsequence of positive density along which the endpoint distribution concentrates mass $> 1-\delta$ on a set of diameter $\leq K(\delta)$, where $\delta$ is arbitrary and $K(\delta)$ is a deterministic constant that depends only on $\delta$ and some features of the model. In other words, not only does the mass localize on atoms, but the atoms also localize in a set of bounded diameter. This proves a subsequential version of a longstanding conjecture about the endpoint distribution. Prior to this work, the only case where a similar statement could be proved was for an integrable model.
\end{itemize}
We will now begin a more detailed presentation by defining the model below. This is followed by a discussion of the known results about polymer models, and then a general overview of the results proved in this paper and the ideas involved in the proofs. 

\subsection{The model of directed polymers in random environment} \label{model}
Take any integer $d\ge 1$. The probabilistic model of $(d+1)$-dimensional \textit{directed polymers in random environment} is defined as follows. We begin with a simple random walk $\omega = (\omega_i)_{i \geq 0}$ on ${\mathbb{Z}}^d$, letting $P$ denote the law of the walk when started at the origin.
More precisely, if $\Omega_p := \{(\omega_i)_{i \geq 0} : \omega_i \in {\mathbb{Z}}^d\}$ is the \textit{path space} and ${\mathcal{F}}_{\text{c}}$ is the cylindrical $\sigma$-algebra on $\Omega_p$, then $P$ is the unique probability measure on $(\Omega_p,{\mathcal{F}}_\text{c})$ under which $\omega_0 = 0$ with probability 1, and $\omega_1 - \omega_0, \omega_2 - \omega_1, \dots$ are i.i.d.~random variables taking values $\pm e_j$ with probability $(2d)^{-1}$.
Here $e_j$ is the $j$-th standard basis vector in ${\mathbb{Z}}^d$.
Let $E$ denote expectation according to $P$. 

Next let ${\mathbb{N}} := \{1,2,\dots\}$ and introduce a collection of i.i.d.~random variables $(X_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ called the \textit{random environment}, defined on some probability space $(\Omega_e,{\mathcal{F}},{\mathbf{P}})$.
We will write ${\mathbf{E}}$ for expectation according to ${\mathbf{P}}$. In what follows, it will not be problematic to define all random variables on the abstract probability space $(\Omega_e,{\mathcal{F}},{\mathbf{P}})$.
Unless stated otherwise, ``almost sure" statements are made with respect to ${\mathbf{P}}$. 

Let $\beta > 0$ be a parameter, called the \textit{inverse temperature}.
Let $\lambda$ denote the common law of the $X_u$, often called the \textit{disorder distribution}. 
We will assume the logarithmic moment generating function for $\lambda$ satisfies
{\begin{align} \begin{split} {
c(\alpha) := \log ({\mathbf{E}}\, e^{\alpha X_u}) < \infty \quad \text{for all $\alpha \in [-2\beta,2\beta]$}. \label{mgf}
} \end{split} \end{align}}
We now have the notation to define the model of $(d+1)$-dimensional directed polymers in random environment.
The \textit{quenched polymer measure} of length $n \geq 0$, denoted $\rho_n$, is the Gibbs measure for $P$ with Hamiltonian
{\begin{align*} {
H_n(\omega) := -\sum_{i = 1}^n X_{i,\, \omega_i}.
} \end{align*}}
That is,
{\begin{align} \begin{split} {
\rho_n({\mathrm{d}}\omega) = \frac{1}{Z_n}e^{-\beta H_n(\omega)}\ P({\mathrm{d}}\omega), \label{rho_def}
} \end{split} \end{align}}
where the normalization constant
{\begin{align*} {
Z_n := E(e^{-\beta H_n(\omega)})
} \end{align*}}
is called the \textit{quenched partition function}.
Explicitly,
{\begin{align} \begin{split} {
Z_n = \frac{1}{(2d)^n} \sum_{\gamma} \exp\bigg(\beta\sum_{i = 1}^n X_{i,\, \gamma(i)}\bigg), \label{Zn_def}
} \end{split} \end{align}}
where the sum is over the $(2d)^n$ nearest-neighbor paths $\gamma : \{0,1,\dots,n\} \to {\mathbb{Z}}^d$ of length $|\gamma| = n$, starting at the origin ($\gamma(0) = 0$).
We can equivalently write
{\begin{align*} {
Z_n = \frac{1}{(2d)^n} \sum_{x \in {\mathbb{Z}}^d} Z_n(x),
} \end{align*}}
where
{\begin{align} \begin{split} {
Z_n(x) := \sum_{\gamma\, :\, \gamma(n) = x} \exp\bigg(\beta\sum_{i = 1}^n X_{i,\, \gamma(i)}\bigg) \label{pointwise_Zn}
} \end{split} \end{align}}
is often referred to as a \textit{point-to-point partition function}.
When $d = 1$, convention often replaces $Z_n$ by $(2d)^n Z_n$, which is correspondingly called the \textit{point-to-line partition function}.
So that the model does not reduce to a simple random walk (which is the case when $\beta = 0$), we assume the $X_u$ are non-degenerate random variables.
That is, $\lambda$ is not supported on a single point.

This directed polymer model first appeared in physics literature \cite{huse-henley85,kardar85,huse-henley-fisher85,kardar-nelson85,kardar-zhang87}.
A typical physical interpretation of the setup explains its nomenclature.
Each vertex in ${\mathbb{Z}}^d$ is a possible location of a monomer, and a polymer is a chain of monomers occupying neighboring vertices.
A random walk $(\omega_i)_{i \geq 0}$ chosen according to $P$ represents a polymer growing without preference for particular directions; that is, growing in a purely uniform environment.
When random impurities are introduced, the polymer may exhibit markedly different behavior because of attraction to or repulsion from certain impurities.
A simple choice of environment to model this scenario is a Bernoulli random environment:
{\begin{align*} {
X_u = \begin{cases}
-1 &\text{with probability $p > 0$,} \\
+1 &\text{with probability $1 - p > 0$}.
\end{cases}
} \end{align*}}
In this case, the energy of a sample path would increase by traversing a $-1$ site, thus decreasing its likelihood under $\rho_n$.
Another standard choice is a Gaussian random environment, where each $X_u$ is a standard normal random variable.

\subsection{An overview of known results about general polymer models} \label{disorder_background}
In this section we will review the results that are known in arbitrary dimensions, with usually mild assumptions on the distribution of the environment.
Indeed, our paper works in this general setting, and so these results will be most relevant to the present study.
One will find a more detailed review in the forthcoming notes by Comets \cite{comets17}.

\subsubsection{High and low temperature phases}
The qualitative behavior of directed polymers depends on the disorder distribution $\lambda$, the inverse temperature $\beta$, and the traversal dimension $d$.
Much of this dependence can be observed through the system's \textit{quenched free energy}\footnote{The quantity $F_n$ might be more appropriately called the free energy \textit{per monomer}, but we will not be concerned with the unscaled free energy, $\log Z_n$.}, given by
{\begin{align*} {
F_n := \frac{\log Z_n}{n}.
} \end{align*}}
Like $(\rho_n)_{n \geq 0}$ and $(Z_n)_{n \geq 0}$, the sequence $(F_n)_{n \geq 0}$ is a random process with respect to the filtration $({\mathcal{F}}_n)_{n \geq 0}$, where
{\begin{align} \begin{split} {
{\mathcal{F}}_n := \sigma(X_{i,\,x} : 1 \leq i \leq n,\, x \in {\mathbb{Z}}^d).\label{gndef}
} \end{split} \end{align}}
When the randomness of the environment is averaged out, one obtains the \textit{averaged quenched free energy},
{\begin{align*} {
{\mathbf{E}}\, F_n = \frac{1}{n}\, {\mathbf{E}} \log Z_n.
} \end{align*}}
In general, this quantity is distinct from the \textit{annealed free energy}, which is
{\begin{align*} {
\frac{1}{n} \log ({\mathbf{E}}\, Z_n) = \log({\mathbf{E}}\, e^{\beta X_u}) = c(\beta).
} \end{align*}}
Indeed, Jensen's inequality 
gives the comparison
{\begin{align*} {
\log({\mathbf{E}}\, e^{\beta X_u}) > {\mathbf{E}}\, F_n,
} \end{align*}}
where the inequality is strict because $F_n$ is not an almost sure constant.
A superadditivity argument  (see e.g.~\cite{carmona-hu02}, proof of Proposition 1.4) 
shows that ${\mathbf{E}}\, F_n$ is non-decreasing in $n$
and thus converges to some value
{\begin{align} \begin{split} {
p(\beta) := \lim_{n \to \infty} {\mathbf{E}}\, F_n = \sup_{n \geq 0} {\mathbf{E}}\, F_n. \label{avgFn_lim}
} \end{split} \end{align}}
The difference between this limit and the annealed energy,
{\begin{align} \begin{split} {
\Lambda(\beta) := c(\beta) - p(\beta) = \log({\mathbf{E}}\, e^{\beta X_u}) - p(\beta) \geq 0, \label{lyapunov_def}
} \end{split} \end{align}}
is known as the \textit{Lyapunov exponent} of the system.
Using the FKG inequality, Comets and Yoshida~\cite{comets-yoshida06} identified a phase transition:

\begin{thm}[\cite{comets-yoshida06}, Theorem 3.2]\label{critical_temperature}
There exists a critical inverse temperature $\beta_c = \beta_c(\lambda,d) \in [0,\infty]$ such that
{\begin{align*} {
0 \le \beta \leq \beta_c \quad &\Rightarrow \quad \Lambda(\beta) = 0 \\
\beta > \beta_c \quad &\Rightarrow \quad \Lambda(\beta) > 0.
} \end{align*}}
\end{thm}

We refer to the region $0 \le \beta < \beta_c$ as the ``high temperature phase'', while $\beta > \beta_c$ defines the ``low temperature phase".
Roughly speaking, high temperatures reduce the influence of the random environment, and so polymer growth resembles a simple random walk, while at low temperatures the random impurities force a wholly different behavior.
This distinction has been most frequently made in terms of the \textit{endpoint distribution} $\rho_n(\omega_n \in \cdot) $, which is a random probability measure on ${\mathbb{Z}}^d$.
For instance, one striking result first proved by Carmona and Hu \cite{carmona-hu02} (for a Gaussian enviornment) 
and then by Comets, Shiga, and Yoshida \cite{comets-shiga-yoshida03} (in the general case) 
is the following: 
If $\beta > \beta_c$, then the polymer endpoint observes so-called \textit{strong localization}:
{\begin{align*} {
(\operatorname{SL}): \quad \exists\ c > 0, \quad \limsup_{n \to \infty} \max_{x \in {\mathbb{Z}}^d} \rho_{n}(\omega_n = x) \geq c \quad \text{a.s.}
} \end{align*}}
That is, the polymer has ``favorite sites" at which its endpoint distribution concentrates infinitely often.
While examining the high and low temperature regimes is very natural, the mathematical development of directed polymers in random environment has often followed an ostensibly different route.

\subsubsection{Localization and diffusivity}
Since the work of Bolthausen \cite{bolthausen89}, analysis of the directed polymer model has frequently focused on the \textit{normalized partition function},
{\begin{align*} {
{\widetilde{{Z}}}_n := Z_n\, e^{-nc(\beta)}, 
} \end{align*}}
with ${\widetilde{{Z}}}_0 = Z_0 = 1$ and $c(\beta)$ defined as in \eqref{mgf}.
It is not difficult to check that ${\widetilde{{Z}}}_n$ is a positive martingale adapted to the filtration $({\mathcal{F}}_n)_{n \geq 0}$ defined in \eqref{gndef}.
In particular, ${\mathbf{E}}\, {\widetilde{{Z}}}_n = 1$ for all $n$, and the martingale convergence theorem 
implies that there is an ${\mathcal{F}}$-measurable random variable ${\widetilde{{Z}}}_\infty$ such that
{\begin{align} \begin{split} {
\lim_{n \to \infty} {\widetilde{{Z}}}_n = {\widetilde{{Z}}}_\infty \quad \text{a.s.} \label{Ztilde_def}
} \end{split} \end{align}}
Furthermore, we necessarily have ${\widetilde{{Z}}}_\infty \geq 0$ almost surely, and the event of positivity
$\{{\widetilde{{Z}}}_\infty > 0\}$ is measurable with respect to the tail $\sigma$-algebra,
{\begin{align*} {
\bigcap_{n = 1}^\infty \sigma(X_{i,\, x} : i \geq n,\, x\in {\mathbb{Z}}^d).
} \end{align*}}
By Kolmogorov's zero-one law 
we have either \textit{weak disorder},
{\begin{align*} {
(\operatorname{WD})&: \quad {\widetilde{{Z}}}_\infty > 0 \quad \text{a.s.},
\intertext{or \textit{strong disorder},}
(\operatorname{SD})&: \quad {\widetilde{{Z}}}_\infty = 0 \quad \text{a.s.}
} \end{align*}}
Carmona and Hu \cite{carmona-hu02} 
and Comets, Shiga, and Yoshida \cite{comets-shiga-yoshida03} 
gave the following characterization of the two phases.
It says that there is strong disorder exactly when the overlap of two independent polymers has infinite expectation.

\begin{thm}[\cite{carmona-hu02}, Proposition 5.1 and \cite{comets-shiga-yoshida03}, Theorem 2.1]\label{disorder_equiv} 
\label{sd_equals_wl}
Strong disorder $(\operatorname{SD})$ is equivalent to \textit{weak localization},
{\begin{align*} {
(\operatorname{WL}):\quad \sum_{n = 0}^\infty \rho_n^{\otimes 2}(\omega_n = \omega_n') = \infty \quad \mathrm{a.s.},
} \end{align*}}
where $(\omega_n)_{n \geq 0}$ and $(\omega_n')_{n \geq 0}$ are i.i.d.~directed polymers supported on the product space $(\Omega_p \times \Omega_p, {\mathcal{F}}_c \times {\mathcal{F}}_c)$,  whose finite dimensional distributions are given by the polymer measures $(\rho_n)_{n \geq 0}$.
\end{thm}

As with the high and low temperature regimes, there is a phase transition (once again, see  \cite{comets-yoshida06} for a proof).

\begin{thm}[\cite{comets-yoshida06}, Theorem 12.1]
There exists a critical value ${\widetilde{{\beta}}}_c = {\widetilde{{\beta}}}_c(\lambda,d) \in [0,\infty]$ such that
{\begin{align*} {
0 \le \beta < {\widetilde{{\beta}}}_c \quad &\Rightarrow \quad (\operatorname{WD}) \\
\beta > {\widetilde{{\beta}}}_c \quad &\Rightarrow \quad (\operatorname{SD}).
} \end{align*}}
\end{thm}

Determining the behavior of ${\widetilde{{Z}}}_\infty$ at ${\widetilde{{\beta}}}_c$ is an open problem. 
(For analogous models on self-similar trees, it is known from work of Kahane and Peyri\`ere \cite{kahane-peyriere76} that strong disorder occurs at ${\widetilde{{\beta}}}_c$ for $d \geq 3$.)
Interestingly, in dimensions $d = 1$ and $d = 2$, there is strong disorder at all finite temperatures (i.e.~${\widetilde{{\beta}}}_c = 0$), while in higher dimensions weak disorder occurs at sufficiently high temperatures (${\widetilde{{\beta}}}_c > 0$).
This is the content of the following theorem. Recall the logarithmic moment generating function $c(\cdot)$ defined in \eqref{mgf}. 

\begin{thm}[see~\cite{comets-shiga-yoshida04, denHollander09} and other references given below] \label{original_characterization}
Define
{\begin{align*} {
\pi_d := P(\omega_i = 0 \textup{ for some } i > 0) \begin{cases}
= 1 &d = 1,2 \\
< 1 &d \geq 3
\end{cases}
} \end{align*}}
to be the probability that simple random walk on ${\mathbb{Z}}^d$ returns to the origin. 
\begin{itemize}
\item[(a)] If $d \geq 3$ and
{\begin{align} \begin{split} {
c(2\beta) - 2c(\beta) < \log \frac{1}{\pi_d}, \label{wd_condition}
} \end{split} \end{align}}
then $(\operatorname{WD})$ holds.\footnote{Upon observing that $c(2\beta) - 2c(\beta)$ is continuous in $\beta$ and $c(0) = 0$, we see that \eqref{wd_condition} is satisfied at sufficiently high temperatures, for $d \geq 3$.}
\item[(b)] If $d = 1$ or $2$, or $d \geq 3$ and
{\begin{align} \begin{split} {
\beta c'(\beta) - c(\beta) > \log(2d), \label{sd_condition}
} \end{split} \end{align}}
then $(\operatorname{SD})$ holds.\footnote{For bounded environments that assume their maximum value with positive probability, it is possible that \eqref{sd_condition} does not hold for any $\beta > 0$.
Moreover, if that probability is sufficiently high, then \eqref{wd_condition} holds for every $\beta > 0$, in which case (WD) can be concluded even at low temperatures.}
\end{itemize}
\end{thm}

A proof of Theorem \ref{original_characterization} as stated above can be found in the review \cite{comets-shiga-yoshida04} 
or in Chapter 12 of \cite{denHollander09}, 
but the full statement is the culmination of results in \cite{imbrie-spencer88,bolthausen89,sinai95,albeverio-zhou96,song-zhou96,kifer97,carmona-hu02,comets-shiga-yoshida03}.
It was these works that justified the abstract definitions of (WD) and (SD) discussed here, for it was known that \eqref{wd_condition} implies diffusive behavior,
namely
{\begin{align} \begin{split} {
\lim_{n \to \infty} \int \frac{|\omega_n|^2}{n}\ \rho_n({\mathrm{d}} \omega) = 1 \quad \text{a.s.} \label{diffusive}
} \end{split} \end{align}}
as well as delocalized behavior,
{\begin{align} \begin{split} {
\sum_{n = 0}^\infty \max_{x \in {\mathbb{Z}}^d} \rho_n(\omega_n = x)^2 < \infty \quad \text{a.s.}\label{delocalized}
} \end{split} \end{align}}
Notice that the rate of diffusion in \eqref{diffusive} matches that of simple random walk on ${\mathbb{Z}}^d$.

Carmona and Hu \cite{carmona-hu02} 
and Comets, Shiga, and Yoshida \cite{comets-shiga-yoshida03} 
found that \eqref{sd_condition} actually implies strong localization (SL), which is stronger than (SD).
Indeed, it is apparent from the inequality
{\begin{align*} {
\rho_n^{\otimes 2}(\omega_n = \omega_n') \leq \max_{x \in {\mathbb{Z}}^d} \rho_n(\omega_n = x)
} \end{align*}}
that $(\operatorname{SL}) \Rightarrow (\operatorname{WL})$, and Theorem \ref{sd_equals_wl} tells us $(\operatorname{WL}) \Leftrightarrow (\operatorname{SD})$.
In fact, Carmona and Hu \cite{carmona-hu06} proved that in a continuous-time model with an environment of i.i.d.~Brownian motions --- the parabolic Anderson model --- that the converse is also true: $(\operatorname{SL}) \Leftarrow (\operatorname{WL})$. 
It is believed that the two notions are equivalent in general.

In the weak disorder regime, Comets and Yoshida  \cite{comets-yoshida06} showed that the diffusivity in \eqref{diffusive} always holds, not only when \eqref{wd_condition} is satisfied.
More generally, they established the following central limit theorem. 

\begin{thm}[\cite{comets-yoshida06}, Theorem 1.2]
Assume $d \geq 3$ and weak disorder.
Then for any bounded continuous function ${\varphi} : {\mathbb{W}}^d \to {\mathbb{R}}$ on the Wiener space
{\begin{align*} {
{\mathbb{W}}^d := \{W \in C([0,1] \to {\mathbb{R}}^d) : W(0) = 0\}
} \end{align*}}
with the uniform norm $\|W\| := \sup_{0 \leq t \leq 1} |W(t)|$ and Wiener measure $P_{{\mathbb{W}}^d}$, we have
{\begin{align*} {
\int {\varphi}(\omega^{(n)})\ \rho_n({\mathrm{d}} \omega) \to \int {\varphi}(W/\sqrt{d})\ P_{{\mathbb{W}}^d}({\mathrm{d}} W) \quad \text{in probability, as $n \to \infty$,}
} \end{align*}}
where $\omega^{(n)}$ is the linear interpolation of $(\omega_{i}/\sqrt{n})_{0 \leq i \leq n}$.
\end{thm}

A major challenge is to reconcile the notion of high and low temperature with the notion of weak and strong disorder.
The first is more natural from the perspective of statistical physics, 
because phase transitions, in the standard sense, are defined in terms of non-analyticities of limiting free energies. Since the annealed free energy is typically analytic everywhere, the transition of the Lyapunov exponent from zero to nonzero denotes the first point of non-analyticity of the limiting free energy $p(\beta)$.
On the other hand, the more abstract conditions of $(\operatorname{WD})$ and $(\operatorname{SD})$ have lead to wealth of mathematical results.
It is conjectured (see \cite{comets-yoshida06,carmona-hu06}) that $\beta_c = {\widetilde{{\beta}}}_c$, which would mean
{\begin{align*} {
0 \leq \beta < \beta_c \quad \Rightarrow \quad (\operatorname{WD}) \qquad \text{and} \qquad
\beta > \beta_c \quad \Rightarrow \quad (\operatorname{SD}).
} \end{align*}}
Evidence for this belief includes the result by Comets and Vargas  \cite{comets-vargas06} that $\beta_c = 0$ universally in $d = 1$, and the subsequent proof by Lacoin  \cite{lacoin10} that $\beta_c = 0$ in $d = 2$.
For $d \geq 3$, only the second implication above is known in general (i.e.~$\beta_c \geq {\widetilde{{\beta}}}_c$).
Therefore, it has become common to say that in the low temperature phase, there is \textit{very strong disorder},
{\begin{align*} {
(\operatorname{VSD}): \quad \Lambda(\beta) > 0.
} \end{align*}}
To see that $(\operatorname{VSD}) \Rightarrow (\operatorname{SD})$, suppose for the moment that
{\begin{align} \begin{split} {
\lim_{n \to \infty} F_n = \lim_{n \to \infty} {\mathbf{E}}\, F_n = p(\beta) \quad \mathrm{a.s.} \label{Fn_lim}
} \end{split} \end{align}}
Then upon rewriting
{\begin{align*} {
\Lambda(\beta) &= \log ({\mathbf{E}}\, e^{\beta X_u}) - \lim_{n \to \infty}\frac{1}{n} \log Z_n\\
&= \log({\mathbf{E}}\, e^{\beta X_u}) - \lim_{n \to \infty} \frac{1}{n} \log\big(({\mathbf{E}}\, e^{\beta X_u})^n\, {\widetilde{{Z}}}_n\big)
= - \lim_{n \to \infty} \frac{\log {\widetilde{{Z}}}_n}{n},
} \end{align*}}
we easily see from \eqref{Ztilde_def} that $\Lambda(\beta) > 0 \Rightarrow (\operatorname{SD})$.
Fortunately, \eqref{Fn_lim} has been proved under very mild assumptions.
We record the result below so that it may be quoted later in the paper.
Note that the hypotheses \eqref{finiteness_assumption1} and \eqref{finiteness_assumption2} are implied by \eqref{mgf}.

\begin{thm}[\cite{vargas07}, Theorem 3.1]  \label{Fn_converges}
Let $F$ denote the distribution function of $\lambda$, and assume
{\begin{align} \begin{split} {
\int_{-\infty}^0 F(x)^{\frac{1}{d+1}}\, {\mathrm{d}} x < \infty \quad \text{and} \quad
\int_0^\infty (1 - F(x))^{\frac{1}{d+1}}\, {\mathrm{d}} x < \infty, \label{finiteness_assumption1}
} \end{split} \end{align}}
as well as
{\begin{align} \begin{split} {
\int |x|\, \lambda({\mathrm{d}} x) < \infty \label{finiteness_assumption2}.
} \end{split} \end{align}}
Then
{\begin{align} \begin{split} {
\lim_{n \to \infty} F_n = p(\beta) \quad \mathrm{a.s.}\text{~and in $L^1$.} \label{quenched_limit}
} \end{split} \end{align}}
\end{thm}

In recent work, Rassoul-Agha, Sepp\"al\"ainen and Yilmaz \cite{rassoul-seppalainen-yilmaz13,rassoul-seppalainen-yilmaz15} 
expressed the limit \eqref{quenched_limit} in terms of several variational formulas.\footnote{In fact, they did the same for the annealed free energy and then gave a variational characterization of weak disorder, namely the existence of a minimizer to their expression.
The minimizer was shown to be unique, equal to ${\widetilde{{Z}}}_\infty$, and interestingly \textit{not} bounded away from 0.}
A method for finding minimizers to these formulas was proposed in \cite{georgiou-rassoul-seppalainen16}.
It has been noted (\cite{rassoul-seppalainen-yilmaz15}, Remark 2.12) that they may not admit minimizers when $\beta > \beta_c$.
These results suggest the possibility of a more general correspondence between the disorder regimes and the existence of variational minimizers (see \cite{rassoul-seppalainen-yilmaz15}, Conjecture 2.13).

Another central task in the theory of directed polymers is determining if and when weak localization results imply stronger ones.
The following analog to Theorem \ref{disorder_equiv} shows that $(\operatorname{VSD}) \Rightarrow (\operatorname{SL})$, which means the strongest type of localization occurs throughout the low temperature regime.
Therefore, if $\beta_c = {\widetilde{{\beta}}}_c$, then all notions of disorder and localization are equivalent, except possibly at the critical temperature.

\begin{thm}[\cite{comets-shiga-yoshida03}, Corollary 2.2 and Theorem 2.3(a)\footnote{This reference also includes the result that $\eqref{sd_condition} \Rightarrow (\operatorname{VSD})$.}] \label{characterization0}
There is very strong disorder $(\operatorname{VSD})$ if and only if there exists $c > 0$ such that
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}^{\otimes 2}(\omega_i = \omega_i') \geq c \quad \mathrm{a.s.,}
} \end{align*}}
or, equivalently, there exists $c > 0$ such that
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1}  \max_{x \in {\mathbb{Z}}^d} \rho_{i}(\omega_i = x) \geq c \quad \mathrm{a.s.} \label{similar_apa}
} \end{split} \end{align}}
\end{thm}

Strong localization $(\operatorname{SL})$ or the stronger property \eqref{similar_apa} captures the tendency of the endpoint distribution to localize mass when $\beta > \beta_c$. 
In this scenario, it is natural to ask whether the entire mass localizes, or if some positive proportion of the mass remains delocalized.  In \cite{vargas07}, Vargas proposed the following definition for complete localization, or as Vargas called it, ``asymptotic pure atomicity''. Let
{\begin{align*} {
{\mathcal{B}}_i^{\varepsilon} := \{x \in {\mathbb{Z}}^d : \rho_{i}(\omega_i = x) > {\varepsilon}\}, \quad i \geq 0,\ {\varepsilon} > 0,
} \end{align*}} 
and say that the sequence $(\rho_{i}(\omega_i \in \cdot))_{i \geq 0}$ is \textit{asymptotically purely atomic} if for every sequence $({\varepsilon}_i)_{i \geq 0}$ tending to 0 as $i \to \infty$, we have
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \to 1 \quad \text{in probability, as $n \to \infty$.}
} \end{align*}}
The following result was obtained in \cite{vargas07}. 

\begin{thm}[\cite{vargas07}, Theorem 3.2 and Corollary 3.3] \label{vargas_apa}
If $c(\beta) = \infty$, then $(\rho_{i}(\omega_i \in \cdot))_{i \geq 0}$ is asymptotically purely atomic.
\end{thm}

Actually, Vargas showed that $(\rho_{i-1}(\omega_i \in \cdot))_{i \geq 1}$ is asymptotically purely atomic, with the set ${\mathcal{B}}_i^{\varepsilon}$ replaced by ${\mathcal{A}}_i^{\varepsilon} := \{x \in {\mathbb{Z}}^d : \rho_{i-1}(\omega_i = x) > {\varepsilon}\}$.  Our Corollary \ref{apa_either_way} in Section \ref{characterize_proof} shows that the two notions are equivalent under the assumption \eqref{mgf}. Furthermore, we show that ``in probability" can be replaced with ``almost surely". One of the main results of this paper, Theorem \ref{total_mass}, asserts that the conclusion of Theorem \ref{vargas_apa} continues to hold even if $c(\beta)$ is finite, as long as $\beta > \beta_c$ and~\eqref{mgf} holds. This is discussed in greater detail in Section \ref{results} below.

\subsubsection{A remark about the definition of endpoint distribution}
All the results of Section \ref{disorder_background} are normally stated in the literature using the measure $\rho_{n-1}(\omega_n \in \cdot)$, as opposed to $\rho_n(\omega_n \in \cdot)$. 
The reason is that the former arises naturally out of the martingale analysis for ${\widetilde{{Z}}}_n$ (for instance, see the proof of Theorem~3.3.1 in \cite{comets-shiga-yoshida04}), but in all cases, the statements are equivalent when the latter is used instead.
This equivalence can be seen by writing
{\begin{align} \begin{split} {
\rho_{n-1}(\omega_n = x) = \frac{1}{2d} \sum_{\|x - y\|_1 = 1} \rho_{n-1}(\omega_{n-1} = y), \label{next_step}
} \end{split} \end{align}}
(where $\|\cdot\|_1$ denotes the $\ell^1$ norm on ${\mathbb{Z}}^d$), which implies
{\begin{align*} {
\frac{1}{2d}\, \max_{y \in {\mathbb{Z}}^d} \rho_{n-1}(\omega_{n-1} = y) \leq \max_{x \in {\mathbb{Z}}^d} \rho_{n-1}(\omega_n = x) \leq \max_{y \in {\mathbb{Z}}^d} \rho_{n-1}(\omega_{n-1} = y).
} \end{align*}}
Similarly, we have
{\begin{align*} {
\frac{1}{2d}\, \rho^{\otimes 2}_{n-1}(\omega_{n-1} = \omega_{n-1}') \leq \rho^{\otimes 2}_{n-1}(\omega_{n} = \omega_{n}') \leq \rho^{\otimes 2}_{n-1}(\omega_{n-1} = \omega_{n-1}'),
} \end{align*}}
where the first inequality follows from the observation that if $\omega_{n-1} = \omega_{n-1}'$, then $\omega_n = \omega_n'$ with probability $(2d)^{-1}$ under $\rho_{n-1}$.
The second inequality is due to Cauchy--Schwarz applied to \eqref{next_step}:
{\begin{align*} {
\rho^{\otimes 2}_{n-1}(\omega_n = \omega_n') 
= \sum_{x \in {\mathbb{Z}}^d} \rho_{n-1}(\omega_n = x)^2
&\leq \sum_{x \in {\mathbb{Z}}^d} \frac{1}{2d} \sum_{\|x - y\|_1 = 1} \rho_{n-1}(\omega_{n-1} = y)^2 \\
&= \sum_{y \in {\mathbb{Z}}^d} \rho_{n-1}(\omega_{n-1} = y)^2
= \rho^{\otimes 2}_{n-1}(\omega_{n-1}=\omega_{n-1}').
} \end{align*}}
As $\rho_n(\omega_n \in \cdot)$ will be the more natural object for our purposes, we reserve the term ``endpoint distribution" for this measure.
Its mass function on ${\mathbb{Z}}^d$ will be denoted $f_n(\cdot) := \rho_n(\omega_n = \cdot)$.

\subsection{Related results in integrable models} \label{solvability_background}
Until recently, no directed polymer model offered the possibility of exact asymptotic calculations.
This was in contrast to the integrable last passage percolation (LPP) models in $1+1$ dimensions, for which specific choices of the passage time distribution (namely geometric or exponential) allow one to use representation theory to derive exact formulas for the distribution of passage times, which are the LPP analogs of partition functions.
Such was the approach advanced in the seminal work of Johansson \cite{johansson00}, which showed that asymptotic fluctuations of passage times follow the Tracy--Widom distributions at scale $n^{1/3}$ : GUE for point-to-point passage times, and GOE for the point-to-line passage time.
These results place the LPP models within the KPZ universality class (see \cite{corwin12, borodin-petrov14}). 
For a review of related works, including results verifying spatial fluctuations at scale $n^{2/3}$, we refer the reader to \cite{quastel-remenik14} and references therein.

In the zero-temperature limit $\beta \to \infty$, the polymer measure $\rho_n$ concentrates on the path that is most likely given the random environment.
When $\beta = \infty$, the directed polymer model is equivalent to LPP.
The natural conjecture, therefore, is that directed polymers in strong disorder obey the same KPZ scaling relations.
In particular, models in $1+1$ dimensions should have energy fluctuations of order $n^{1/3}$, and endpoint fluctuations of order $n^{2/3}$.
The first case permitting exact calculations was the integrable log-gamma model introduced by Sepp{\"a}l{\"a}inen \cite{seppalainen12}, whose breakthrough work proved the conjectured exponents.
Subsequent studies showed that free energy fluctuations were again of Tracy--Widom type \cite{corwin-oconnell-seppalainen-zygouras14,borodin-corwin-remenik13}, gave a large deviation rate function \cite{georgiou-seppalainen13}, and computed the limiting value $p(\beta)$ \cite{georgiou-rassoul-seppalainen-yilmaz15}.

Regarding spatial fluctuations, Comets and Nguyen \cite{comets-nguyen15} found an explicit limiting endpoint distribution in the equilibrium case of the log-gamma model.
More precisely, they showed that the endpoint distribution, when re-centered around the most likely site, converges in law to a certain random distribution on ${\mathbb{Z}}$. 
While the boundary conditions they enforce prevent the correct exponent of $2/3$ from appearing, which would mean annealed spatial fluctuations occur on the order of $n^{2/3}$, their result implies that the \textit{quenched} endpoint distributions concentrate all their mass around a single favorite site, moreover in a microscopic region (i.e.~of size ${\mathcal{O}}(1)$).
In the notation of this paper, if $x_n = \arg \max \rho_n(\omega_n = \cdot)$, then the result of \cite{comets-nguyen15} says
{\begin{align} \begin{split} {
\lim_{K \to \infty} \liminf_{n \to \infty} \rho_n(\|\omega_n - x_n\|_1 \leq K) = 1 \quad \mathrm{a.s.}\label{mode}
} \end{split} \end{align}}
This provides an affirmative case of the so-called ``favorite region" conjecture, which speculates that in strong disorder, directed polymers on the lattice localize their endpoint to a region of fixed diameter. 
We emphasize once more that this is a statement about quenched endpoint distributions.
While little is known on annealed distributions in the general case, related progress in LPP models can be found in \cite{flores-quastel-remenik13}, which expands on earlier work of Johansson \cite{johansson03}.
For further literature on integrable directed polymers, see \cite{corwin-seppalainen-shen15,barraquand-corwin15,thiery-doussal15} and references therein.

 \setcounter{thm}{0}
\numberwithin{thm}{section}

\subsection{An overview of results proved in this paper} \label{results}
The endpoint distribution $\rho_n(\omega_n \in \cdot)$ of length $n$ polymers is a random probability measure on $\mathbb{Z}^d$. The main goal of this paper is to understand the behavior of this object as $n\to\infty$.
The majority of our work is in building an ``abstract machine" designed to compute limits of endpoint distributions, or more to the point, of \textit{functionals} on the endpoint distributions.
Broadly speaking, the construction consists of three components: (i) a compactification of measures on ${\mathbb{Z}}^d$; (ii) a map whose fixed points are the possible limits of the endpoint distribution; and (iii) a functional that distinguishes those fixed points with minimal energy.

\subsubsection{Compactification of measures on ${\mathbb{Z}}^d$}
One key obstacle to studying the endpoint distribution is that the standard topology of weak convergence of probability measures is inadequate for understanding its limiting behavior. Even the weaker topology of vague convergence does not provide an adequate description of the localization phenomenon that is of central interest.
In the recent work of Mukherjee and Varadhan \cite{mukherjee-varadhan14}, this issue was addressed by exploiting translation invariance inherent in their problem to pass to a certain compactification of probability measures on ${\mathbb{R}}^d$.
We have drawn inspiration from their methods, using a similar construction we will soon describe.
Ultimately, this approach is enabled by the ``concentration compactness" phenomenon.
Before we discuss this topic further, let us motivate the discussion with two elementary examples.

Roughly speaking, the difficulty of using standard weak or vague convergence is that the endpoint distribution may manifest itself as multiple localized ``blobs'' that escape to infinity in different directions as $n\to \infty$. Additionally, some part of the mass may just ``diffuse to zero''. As a first example, consider the following sequence of probability mass functions on $\mathbb{Z}$:
\[
q_n(x) :=
\begin{cases}
1/2 &\text{if $x=n$,}\\
1/(2n) &\text{if $0\le x< n$,}\\
0 &\text{else.}
\end{cases}
\]
This sequence fails to converge weakly, and its vague limit is the subprobability measure of mass zero. 
However, to understand the true limiting behavior of $q_n$ it seems more appropriate to take the vague limit after translating the measure by $n$; that is, taking ${\widetilde{{q}}}_n(x) := q_n(x-n)$. 
The limit of the sequence ${\widetilde{{q}}}_n$ is the subprobability measure that puts mass $1/2$ at the origin and zero elsewhere, which gives a better picture of the true limiting behavior of $q_n$. 

The situation is more complicated when multiple blobs escape to infinity in different directions. For example, consider the following sequence of probability mass functions on $\mathbb{Z}$:
\[
r_n(x) :=
\begin{cases}
1/5 &\text{if $x \in \{-2n, -n, n, n+1\}$,}\\
1/(5n) &\text{if $0\le x<n$,}\\
0 &\text{else.}
\end{cases}
\]
Like $q_n$, the sequence $r_n$ also converges vaguely 
to the subprobability measure of mass zero. But in this case, no sequence of translates of $r_n$ can fully capture its limiting behavior, because a translate can only capture the mass at one of the three blobs but not all of them simultaneously.  The only recourse, it seems, is to express the limit not as a single subprobability measure, but a union of three subprobability measures on three distinct copies of $\mathbb{Z}$. Two of these will put mass $1/5$ at the origin in their respective copies of ${\mathbb{Z}}$, and the third will put mass $1/5$ at $0$ and $1/5$ at $1$ in its copy of ${\mathbb{Z}}$. 
The first two are the vague 
limits of $r_n(\cdot +2n)$ and $r_n(\cdot + n)$, and the third is the vague 
limit of $r_n(\cdot - n)$. One can view this object jointly as a subprobability measure of total mass $4/5$ on the set $\{1,2,3\}\times {\mathbb{Z}}$. Of course, it is not important which copy of ${\mathbb{Z}}$ gets which part of the measure, nor does it matter if a translation is applied to the subprobability measure on one of the copies. Thus, the limit object is an equivalence class of subprobability measures on $\{1,2,3\}\times {\mathbb{Z}}$ rather than a single subprobability measure.

Generalizing the above idea, we can consider equivalence classes of subprobability measures on ${\mathbb{N}} \times {\mathbb{Z}}^d$. First, define the ${\mathbb{N}}$-support of a subprobability measure $f$ on ${\mathbb{N}}\times {\mathbb{Z}}^d$ to be the set of all $n\in {\mathbb{N}}$ such that $f$ puts nonzero mass on $\{n\}\times {\mathbb{Z}}^d$. Next, declare two subprobability measures $f$ and $g$ on ${\mathbb{N}} \times {\mathbb{Z}}^d$ to be equivalent if there is a bijection $\sigma$ between their ${\mathbb{N}}$-supports and a number $x_n\in {\mathbb{Z}}^d$ for each $n$ in the ${\mathbb{N}}$-support of $f$ such that 
\[
g(\sigma(n), \cdot) = f(n, x_n +\cdot).
\]
It is easy to verify that this is indeed an equivalence relation. The equivalence classes are called \textit{partitioned subprobability measures} on ${\mathbb{Z}}^d$ in this paper, and the set of all equivalence classes is denoted by ${\mathcal{S}}$. The set ${\mathcal{S}}$ is formally defined and studied in Section \ref{compactness}.

The equivalence relation we have just defined creates a translation invariance that prevents so-called blobs from escaping to infinity.
Without this invariance, such escape can be thought of as an obstruction to compactness, a property we would like to have in studying localization behavior.
One hopes this is the only obstruction; that this is indeed the case is called ``concentration compactness".
The concept of concentration compactness builds on the notion of the concentration functions of probability measures, defined by L\'evy~\cite{levy54}.  The idea of using L\'evy's concentration functions for constructing compactifications of non-compact spaces was introduced in  Parthasarathy, Ranga Rao and Varadhan~\cite{parthasarathy-rangarao-varadhan62}, and  developed into a powerful tool by Lions \cite{lions84I,lions84II,lions85I,lions85II}. It has been applied broadly in the study of calculus of variations and PDEs. It holds in a wide array of function spaces and gives rise to a ``profile decomposition" (for a friendly discussion, see \cite{tao08,tao10}).
In the present setting, this decomposition states that within any sequence of probability mass functions on ${\mathbb{Z}}^d$, one can find a subsequence $s_n$ for which there exists a countable collection $(p_j)_{j \in {\mathbb{N}}}$ of fixed \textit{sub}probability mass functions, and sequences $x_{j,\, n}$ in ${\mathbb{Z}}^d$ such that the following three statements hold:
{\begin{align*} {
&\sum_{j \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d} p_j(x) \leq 1, \\
&\lim_{n \to \infty} \|x_{j,\, n} - x_{k,\, n}\|_1 = \infty \quad \text{if $j \neq k$}, \\
&\lim_{n \to \infty} \bigg\|s_n(\cdot) - \sum_{j \in {\mathbb{N}}} p_j(\cdot - x_{j,\, n})\bigg\|_\infty = 0.
} \end{align*}}
The $p_j$ are called \textit{profiles} and play the role of the vague limits in the examples given above.
In the first example, there was only one nonzero profile, while in the second there were three.
Our motivation for defining the set ${\mathcal{S}}$, then, is to record these profiles as a single function.

As mentioned before, a similar construction in a continuous setting has appeared recently in the paper~\cite{mukherjee-varadhan14}.
Specifically, its authors first identify probability distributions on ${\mathbb{R}}^d$ that are translations of one another (that is, they define an equivalence relation), and then embed the resulting quotient space within a set (which is analogous to ${\mathcal{S}}$) of countable sequences of subprobability measures on ${\mathbb{R}}^d$, again identified by translations.
Upon defining a metric on this set under which the image of the embedding is dense, they prove that the resulting topology is compact.
This fact is used to establish a large deviation result for occupation measures on ${\mathbb{R}}^d$ induced by Brownian motion.
In comparison, our construction is built to study the endpoint measures on ${\mathbb{Z}}^d$ induced by directed polymers, and we follow a similar program to obtain a compact topology on the space of interest: the set ${\mathcal{S}}$, in our case.
One key difference is that a measure on ${\mathbb{Z}}^d$ can be thought of as a function on ${\mathbb{Z}}^d$, and so we can define a metric on ${\mathcal{S}}$ permitting explicit calculations. 
This bypasses the use of an abstract family of test functions and allows us to directly check the continuity of certain functionals on (equivalence classes of) measures.
Let us now outline the approach.

We will view probability measures on ${\mathbb{Z}}^d$ (or more precisely, their mass functions) as elements of ${\mathcal{S}}$ supported on a single copy of ${\mathbb{Z}}^d$.
In particular, we have a natural embedding of endpoint distributions into ${\mathcal{S}}$.
In Section \ref{topology-definitions}, we define a metric $d$ on ${\mathcal{S}}$ (not to be confused with the dimension $d$).
Due to the somewhat complicated nature of the metric, we will not reproduce the definition in this overview section, but it is constructed to ensure that certain functionals on ${\mathcal{S}}$ are continuous or at least semi-continuous.
In this study, these functionals are related to either localization or the free energy of the system.
One of the first nontrivial results of this paper is that $({\mathcal{S}}, d)$ is a compact metric space, again in analogy with the construction in \cite{mukherjee-varadhan14}. 
This is Theorem~\ref{compactness_result} in Section~\ref{compactness}.

After defining the metric space $({\mathcal{S}},d)$ and proving its compactness, we present in Section \ref{compare_topologies} a formal comparison between our construction and the one of Mukherjee and Varadhan.
In particular, we first introduce the relevant ideas from their paper \cite{mukherjee-varadhan14} for measures on ${\mathbb{Z}}^d$, and check that in this discrete setting one can still define a metric $D$ on ${\mathcal{S}}$ by using translation invariant test functions.
Heuristics and motivating examples suggest correctly that this alternative metric $D$ is equivalent to $d$;
the equivalence is stated as Proposition \ref{equal_topologies}, in terms of convergent sequences.
This result is satisfying in that it shows a consistency of theory that might be generalized.
Nevertheless, its proof is quite involved.
In fact, our argument requires the compactness of $d$ to even establish the equivalence of topologies, meaning a separate proof that $({\mathcal{S}},D)$ is compact would not be sufficient to show that $({\mathcal{S}},d)$ is compact.
In any case, the entirety of Section \ref{compare_topologies} is supplementary and tangential to the subject of directed polymers.
The reader may choose to skip this part or return to it after reading later sections.

\subsubsection{The update map}
In Section \ref{transformation} we resume developing our abstract machinery by defining an ``update'' map $T:{\mathcal{S}} \to {\mathcal{P}}^1({\mathcal{S}})$,  where ${\mathcal{P}}^1({\mathcal{S}})$ is the set of all probability measures on ${\mathcal{S}}$ equipped with the Kantorovich--Rubinstein--Wasserstein  distance  ${\mathcal{W}}^1$. The map $T$ has the property that if $f_n(\cdot) := \rho_n(\omega_n = \cdot)$ is the endpoint mass function of the length $n$ polymers (considered as an element of ${\mathcal{S}}$ supported on a single copy of ${\mathbb{Z}}^d$), then $Tf_n$ is the law of $f_{n+1}$ given ${\mathcal{F}}_n$ (recall the definition \eqref{gndef} of ${\mathcal{F}}_n$). Considerable work is done in Section \ref{transformation-continuity} to show that $T$ is a continuous map.  This is the conclusion of Proposition \ref{continuous1}. The explicit nature of the metric $d$ is particularly important in the proof of this result.
Finally, the map $T$ lifts to a map ${\mathcal{T}}: {\mathcal{P}}^1({\mathcal{S}}) \to {\mathcal{P}}^1({\mathcal{S}})$, defined as
{\begin{align*} {
{\mathcal{T}} \nu({\mathrm{d}} g) := \int Tf({\mathrm{d}} g)\ \nu({\mathrm{d}} f).
} \end{align*}}
The continuity of $T$ implies that ${\mathcal{T}}$ is also continuous.

In Section \ref{free_energy} we study the following random element of ${\mathcal{P}}^1({\mathcal{S}})$:
{\begin{align*} {
\mu_n := \frac{1}{n+1}\sum_{i = 0}^n \delta_{f_i}.
} \end{align*}}
Here $\delta_{f_i}$ is the unit point mass at the $i$-th endpoint mass function $f_i$, considered as an element of ${\mathcal{S}}$ as before. In words, $\mu_n$ is the \textit{empirical measure of the endpoint distributions up to time $n$}. Let 
\[
{\mathcal{K}} := \{\nu \in {\mathcal{P}}^1({\mathcal{S}}) : {\mathcal{T}}\nu = \nu\}
\]
be the set of fixed points of ${\mathcal{T}} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathcal{P}}^1({\mathcal{S}})$.
The first main result of Section \ref{free_energy} is Corollary \ref{close_probability}, which says that
{\begin{align} \begin{split} {
\lim_{n \to \infty} \inf_{\nu \in {\mathcal{K}}} {\mathcal{W}}^1(\mu_n,\nu) = 0 \quad \mathrm{a.s.} \label{going_to_K}
} \end{split} \end{align}}
In this way, our abstract framework helps replace the algebraic structure of $(1+1)$-dimensional integrable models (for instance, see \cite{seppalainen12,corwin-seppalainen-shen15,barraquand-corwin15,thiery-doussal15}).
Such models work by identifying a disorder distribution and boundary conditions such that the system is stationary under a group of spatial translations.
Loosely speaking, our approach similarly recovers a stationary measure \textit{in the limit}, even without explicit calculations.
This is enabled by a topology on endpoint distributions that is rich enough to capture the desired localization, yet sufficiently ``compressed" to be compact.

\subsubsection{The energy functional}
Given the convergence \eqref{going_to_K}, the next key observation is that the free energy $F_n$ can be expressed in terms of the empirical measure $\mu_n$ (see \eqref{Zfrac}, \eqref{Fn_cesaro}--\eqref{full_expectation}, and \eqref{R_def2}).
We thus define a functional ${\mathcal{R}} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ so that we may concisely write
{\begin{align*} {
{\mathbf{E}}\, F_n = {\mathbf{E}}\, {\mathcal{R}}(\mu_{n-1}).
} \end{align*}}
This and Corollary \ref{close_probability} lead to the following variational formula for the limiting free energy, given in Theorem \ref{upper_bound}. For any $\beta$ such that \eqref{mgf} holds,
{\begin{align*} {
p(\beta) = \lim_{n \to \infty} {\mathbf{E}}\, F_n = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu).
} \end{align*}}
Moreover, this computation allows us to improve Corollary \ref{close_probability}, giving Theorem \ref{close_M}, which says that if 
\[
{\mathcal{M}} := \Big\{\nu_0 \in {\mathcal{K}} : {\mathcal{R}}(\nu_0) = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)\Big\},
\]
then
{\begin{align*} {
\lim_{n \to \infty} \inf_{\nu \in {\mathcal{M}}} {\mathcal{W}}^1(\mu_n,\nu) = 0 \quad \mathrm{a.s.}
} \end{align*}}
In Section \ref{empirical_limits} we study the minimizing set ${\mathcal{M}}$. In particular, Theorem \ref{characterization} says that either ${\mathcal{M}}$ consists of the single element of total mass zero (which happens when $0\le \beta\le \beta_c$, where $\beta_c$ is the critical inverse temperature of Theorem \ref{critical_temperature}), or every element of ${\mathcal{M}}$ has total mass one (which happens when $\beta>\beta_c$).
This result is similar to the technique of identifying a phase transition as the critical point at which a recursive distributional equation begins to have a nontrivial solution; for an account of this method, we refer the reader to the survey of Aldous and Bandyopadhyay~\cite{aldous-bandyopadhyay05} and references therein.

From a different perspective, the limit law of the empirical measure can be viewed as an ``order parameter'' for the model, whose behavior distinguishes between the high and low temperature regimes. Such order parameters arise frequently in the study of disordered systems. A prominent example is the Sherrington--Kirkpatrick model of spin glasses, where the limiting distribution of the overlap serves as the order parameter (see Panchenko~\cite{panchenko13}). Interestingly, the limiting free energy of the Sherrington--Kirkpatrick model can also be expressed as the solution of a variational problem involving the order parameter. This is the famous Parisi formula proved by Talagrand~\cite{talagrand06}. The analogy with spin glasses goes even further: The partitioned subprobability measures are close cousins of the random overlap structures introduced by Aizenman, Sims and Starr~\cite{ass07} for understanding the limiting behavior of mean-field spin glass models, and the update map is the analog of similar stabilizing maps arising out of the cavity method for spin glasses and related competing particle systems, that were studied by Aizenman and Contucci~\cite{aizenman-contucci98}, Ruzmaikina and Aizenman~\cite{ruzmaikina-aizenman05} and Arguin and Chatterjee~\cite{arguin-chatterjee13}.

\subsubsection{Applications}
Theorem \ref{characterization} yields the following concrete application of our abstract machinery. Recall the notations and terminologies related to Theorem~\ref{critical_temperature} and Theorem~\ref{vargas_apa}. Assuming \eqref{mgf}, Theorem \ref{total_mass} says: 
\begin{itemize}
\item[(a)] If $\beta > \beta_c$, then for every sequence $({\varepsilon}_i)_{i \geq 0}$ tending to 0 as $i \to \infty$,
{\begin{align*} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) = 1 \quad \mathrm{a.s.}
} \end{align*}}
\item[(b)] If $0\le \beta \le \beta_c$, then there exists a sequence $({\varepsilon}_i)_{i \geq 0}$ tending to 0 as $i \to \infty$ such that
{\begin{align*} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) = 0 \quad \mathrm{a.s.}
} \end{align*}}
\end{itemize}
This generalizes Theorem \ref{vargas_apa}, where an ``in probability" version of (a) was proved under the condition $c(\beta)=\infty$. 

Finally, in Section \ref{main_thm2}, we apply our techniques to go further than atomic localization by considering ``geometric localization".
In the low temperature phase, the endpoint distribution can not only concentrate mass on a few likely sites, but moreover have those sites close together.
We make this phenomenon precise in the following manner.
Let $f_i(\cdot) = \rho_i(\omega_i = \cdot)$ be the mass function for the $i$-th endpoint distribution.
For $\delta \in (0,1)$ and a nonnegative number $K$, let ${\mathcal{G}}_{\delta,K}$ denote the set of probability mass functions on ${\mathbb{Z}}^d$ that assign measure greater than $1-\delta$ to some subset of ${\mathbb{Z}}^d$ having diameter at most $K$.
We will say that the sequence $(f_i)_{i \geq 0}$ is \textit{geometrically localized} if for every $\delta$, there is $K< \infty$ such that
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{\delta,K}\} = 1 \quad \mathrm{a.s.}, \label{geometric_localization}
} \end{split} \end{align}}
where ${\mathbb{I}}\{\cdot\}$ denotes the indicator of the event $\{\cdot \}$.
That is, there are endpoint distributions with limiting density $1$ that place mass greater than $1 - \delta$ on a set of bounded diameter.
We will say $(f_i)_{i \geq 0}$ is \textit{subsequentially geometrically localized} if for every $\delta$, there is $K < \infty$ and $\theta > 0$ such that
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i\in {\mathcal{G}}_{\delta,K}\} \geq \theta \quad \mathrm{a.s.}
} \end{align*}}
In this weaker case, there is still a positive lower density $\theta$, but that density may be less than~$1$. The main result of Section \ref{main_thm2}, Theorem \ref{localized_subsequence}, says that under \eqref{mgf},  subsequential geometric localization is equivalent to the low temperature condition $\beta > \beta_c$. Moreover, the numbers $K$ and $\theta$ are deterministic quantities that depend only on the choice of $\delta$, the disorder distribution $\lambda$, the parameter $\beta$, and the dimension $d$.

As mentioned in Section \ref{solvability_background}, the only case where a version of geometric localization has been proved is an integrable $(1+1)$-dimensional model, where Comets and Nguyen \cite{comets-nguyen15} proved localization and moreover computed the limit distribution of the endpoint. Similar results for one-dimensional random walk in random environment were proved by Sinai~\cite{sinai82}, Golosov~\cite{golosov84}, and Gantert, Peres and Shi~\cite{gantert-peres-shi10}.

\subsubsection{The single copy condition}
In addition to the above unconditional results, we also prove a few conditional statements, which hold under the condition that every $\nu \in {\mathcal{M}}$ puts all its mass on those $f \in {\mathcal{S}}$ that are supported on a single copy of ${\mathbb{Z}}^d$. We call this the ``single copy condition''. 

One consequence of the single copy condition is full geometric localization, as defined in equation~\eqref{geometric_localization}. Part (b) of Theorem \ref{localized_subsequence} proves this conditional claim. 
A second consequence of the single copy condition is Proposition \ref{localization_thm}, which gives the following Ces\`aro form of \eqref{mode}. For each $i\ge 0$ and $K\ge 0$, let ${\mathcal{C}}_{i}^K$ be the set of all $x\in {\mathbb{Z}}^d$ that are at distance $\le K$ from {\it every} mode of the endpoint mass function $f_i$. Note that ${\mathcal{C}}_i^K$ has diameter $\le 2K$. Then, assuming \eqref{mgf} and the single copy condition, Proposition~\ref{localization_thm} asserts that 
{\begin{align*} {
\lim_{K \to \infty} \liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i\in {\mathcal{C}}_{i}^K) = 1 \quad \mathrm{a.s.}
} \end{align*}}
In view of the result \eqref{mode} of Comets and Nguyen~\cite{comets-nguyen15}, it seems plausible that the single copy condition holds for the log-gamma polymer in $1+1$ dimensions. Unfortunately, we have been unable to determine whether or not the single copy condition holds in general. Preliminary investigations (not reported in this paper) indicate that at least for directed polymers on binary trees, the single copy condition does not hold, and full geometric localization is not valid. 

\section{Partitioned subprobability measures} \label{topology}
In this section and in the remainder of this manuscript, we will  always assume \eqref{mgf}. Also, throughout, $f_i(\cdot)$ will denote the endpoint probability mass function $\rho_i(\omega_i=\cdot)$. 
 The goals of this section are to (i) define a space of functions which contains endpoint distributions (i.e.~probability measures on ${\mathbb{Z}}^d$) and their localization limits (subprobability measures on ${\mathbb{N}} \times {\mathbb{Z}}^d$); (ii) equip said space with a metric; and (iii) prove that the induced metric topology is compact.

\subsection{Definition and properties} \label{topology-definitions}
Let us restrict our attention to the set of functions
{\begin{align*} {
S := \{f : {\mathbb{N}} \times {\mathbb{Z}}^d \to {\mathbb{R}} : f \geq 0,\, \|f\| \leq 1\},
} \end{align*}}
where
{\begin{align} \begin{split} {
\|f\| := \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u). \label{norm_def}
} \end{split} \end{align}}
Since we regard distant point masses as nearly existing on separate copies of ${\mathbb{Z}}^d$, we consider
differences between elements of ${\mathbb{N}} \times {\mathbb{Z}}^d$ by extending the natural group operation in ${\mathbb{Z}}^d$ as follows:
{\begin{align*} {
(n,x) - (m,y) := \begin{cases}
x - y &\text{if } n = m, \\
\infty &\text{else}.
\end{cases}
} \end{align*}}
It then makes sense to write, for $u = (n,x)$ and $v = (m,y)$ in ${\mathbb{N}} \times {\mathbb{Z}}^d$,
{\begin{align*} {
\|u - v\|_1 := \begin{cases}
\|x - y\|_1 &\text{if } n = m, \\
\infty &\text{else}.
\end{cases}
} \end{align*}}
If $u = (n,x)$ and $y \in {\mathbb{Z}}^d$, then $u \pm y$ will be understood to mean $(n,x\pm y)$.
Of interest will be certain injective functions on ${\mathbb{N}} \times {\mathbb{Z}}^d$.
Given a set $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$, we call a map $\phi: A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ an \textit{isometry} of degree $m \geq 1$ if
{\begin{align} \begin{split} {
\phi(u) - \phi(v) = u - v 
\quad \text{whenever} \quad 
\|u - v\|_1 < m
\quad \text{or} \quad
\|\phi(u) - \phi(v)\|_1 < m. \label{isometry}
} \end{split} \end{align}}
The maximum $m$ for which \eqref{isometry} holds is called the \textit{maximum degree} of $\phi$, denoted by $\deg(\phi)$.
To say that $\phi$ is an isometry of degree $1$ simply means $\phi$ is injective.
If \eqref{isometry} holds for every $m \in {\mathbb{N}}$, then $\deg(\phi) = \infty$, meaning $\phi$ acts by translations. That is, each copy of ${\mathbb{Z}}^d$ intersecting the domain $A$ is translated and moved to some copy of ${\mathbb{Z}}^d$ in the range, with no two copies in the domain going to the same copy in the range.
Note that an isometry is necessarily injective and thus has an inverse defined on its image.

\begin{lemma} \label{infinite_degree}
If $\phi: A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ is an isometry, then $\phi^{-1}: \phi(A) \to {\mathbb{N}} \times {\mathbb{Z}}^d$ is also an isometry with $\deg(\phi^{-1}) = \deg(\phi)$.
\end{lemma}

\begin{proof}
Let $\phi$ be an isometry of degree $m$.
One necessarily has that $\phi^{-1}$ is injective, and for $\phi(u),\phi(v) \in \phi(A)$,
{\begin{align*} {
\phi^{-1}(\phi(u)) - \phi^{-1}(\phi(v)) = u - v = \phi(u) - \phi(v),
} \end{align*}}
where the first equality is trivial, and the second holds whenever
{\begin{align*} {
\|\phi(u) - \phi(v)\|_1 < m
\quad \text{or} \quad
\|u - v\|_1 = \|\phi^{-1}(\phi(u)) - \phi^{-1}(\phi(v))\|_1 < m.
} \end{align*}}
Indeed, $\phi^{-1}$ is an isometry of degree $m$.
As this argument holds for every $m$, one concludes $\deg(\phi^{-1}) \geq \deg(\phi)$.
Of course, by symmetry one also has $\deg(\phi) \geq \deg(\phi^{-1})$.
\end{proof}

Another useful property of isometries is closure under composition, which is defined in the next lemma.
It is important, especially for the proof below, to note that an isometry is permitted to have empty domain.
That is, there exists the empty isometry $\phi: \varnothing \to {\mathbb{N}} \times {\mathbb{Z}}^d$, which is its own inverse and has $\deg(\phi) = \infty$.

\begin{lemma} \label{composition}
Let $\phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ and $\psi : B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ be isometries.
Define $A' := \{a \in A : \phi(a) \in B\}$.  Then $\theta: A' \to {\mathbb{N}} \times {\mathbb{Z}}^d$ defined by $\theta(u) = \psi(\phi(u))$ is an isometry with $\deg(\theta) \geq \min(\deg(\phi),\deg(\psi))$.
\end{lemma}

\begin{proof}
Let $m := \min(\deg(\phi),\deg(\psi))$ so that $\phi$ and $\psi$ are each isometries of degree $m$.
For any $a_1,a_2 \in A'$,
{\begin{align*} {
\|a_1 - a_2\|_1 < m \quad &\Rightarrow \quad \phi(a_1) - \phi(a_2) = a_1 - a_2 \\
&\Rightarrow \quad \|\phi(a_1) - \phi(a_2)\|_1 < m \\
&\Rightarrow \quad \psi(\phi(a_1)) - \psi(\phi(a_2)) = \phi(a_1) - \phi(a_2) = a_1 - a_2,
} \end{align*}}
as well as
{\begin{align*} {
\|\theta(a_1) - \theta(a_2)\|_1 < m \quad &\Rightarrow \quad \phi(a_1) - \phi(a_2) = \psi(\phi(a_1)) - \psi(\phi(a_2)) \\
&\Rightarrow \quad \|\phi(a_1) - \phi(a_2)\|_1 < m \\
&\Rightarrow \quad a_1 - a_2 = \phi(a_1) - \phi(a_2) = \psi(\phi(a_1)) - \psi(\phi(a_2)).
} \end{align*}}
Indeed, $\theta$ is an isometry of degree $m$.
\end{proof}

A final observation about isometries is that they obey a certain extension property, which is proved below:
By allowing the maximum degree of an isometry to be lowered by at most 2, we can expand its domain  by one unit in every direction.
If the maximum degree is infinite (i.e.~$\phi$ acts by translations), then the extension can be repeated \textit{ad infinitum} to recover the translation on all of ${\mathbb{Z}}^d$, for any copy of ${\mathbb{Z}}^d$ intersecting the domain.

\begin{lemma} \label{extension}
Suppose that $\phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ is an isometry of degree $m \geq 3$.
Then $\phi$ can be extended to an isometry $\Phi: B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ of degree $m-2$, where
{\begin{align*} {
B := \{v \in {\mathbb{N}} \times {\mathbb{Z}}^d : \|u - v\|_1 \leq 1 \textup{ for some $u \in A$}\} \supset A.
} \end{align*}}
\end{lemma}

\begin{proof}
For $v \in B$ such that $\|u - v\|_1 \leq 1$ with $u \in A$, define
{\begin{align*} {
\Phi(v) := \phi(u) + (v - u).
} \end{align*}}
If $u' \in A$ also satisfies $\|u' - v\|_1 \leq 1$, then
{\begin{align*} {
\|u - u'\|_1 \leq 2 \quad &\Rightarrow \quad \phi(u) - \phi(u') = u - u'
\quad \Rightarrow \quad \phi(u) + (v - u) = \phi(u') + (v - u').
} \end{align*}}
So $\Phi$ is well-defined; in particular, $\Phi(u) = \phi(u)$ for all $u \in A$.
To see that $\Phi$ is an isometry of degree $m-2$, consider $v,v' \in B$ and $u,u' \in A$ such that $\|u - v\|_1 \leq 1$ and $\|u' - v'\|_1 \leq 1$.
If $\|v - v'\|_1 < m - 2$, then
{\begin{align*} {
\|u - u'\|_1 < m \quad &\Rightarrow \quad \phi(u) - \phi(u') = u - u' \\
&\Rightarrow \quad \Phi(v) - \Phi(v') = [\phi(u) + (v - u)] - [\phi(u') + (v' - u')] = v - v'.
} \end{align*}}
Alternatively, if $\|\Phi(v) - \Phi(v')\|_1 < m -2$, then
{\begin{align*} {
\|\phi(u) - \phi(u')\| < m
\quad &\Rightarrow \quad u - u' = \phi(u) - \phi(u') \\
&\Rightarrow \quad v - v' = [u + (\Phi(v) - \phi(u))] - [u' + (\Phi(v') - \phi(u'))] = \Phi(v) - \Phi(v').
} \end{align*}}
Indeed, $\deg(\Phi) \geq m-2$.
\end{proof}

We can now define the desired metric on functions.
Roughly speaking, an isometry allows for the comparison of the large values of two functions. 
The size of the isometry's domain reflects how many of their large values are similar, while the degree of the isometry captures how similar their relative positioning is.
The metric is constructed in stages.

First, given an isometry $\phi: A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ with finite domain $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$ and two functions $f,g \in S$, define
{\begin{align} \begin{split} {
d_\phi(f,g) := 2\sum_{u \in A} |f(u) - g(\phi(u))| + \sum_{u \notin A} f(u)^{2} + \sum_{u \notin \phi(A)} g(u)^{2} + 2^{-\deg(\phi)}. \label{d_def}
} \end{split} \end{align}}
Next define
{\begin{align*} {
d(f,g) := \inf_{\phi} d_\phi(f,g),
} \end{align*}}
where the infimum is taken over all isometries $\phi$ with finite domain.
Since $\deg(\phi^{-1}) = \deg(\phi)$, it is easy to see that $d_{\phi^{-1}}(g,f) = d_\phi(f,g)$, and so the function $d$ is symmetric:
{\begin{align} \begin{split} {
d(f,g) = d(g,f) \quad \text{for all } f,g \in S. \label{symmetry}
} \end{split} \end{align}}
In fact, $d$ also satisfies the triangle inequality on $S$.

\begin{lemma}
For any $f,g,h \in S$,
{\begin{align} \begin{split} {
d(f,h) \leq d(f,g) + d(g,h). \label{triangle}
} \end{split} \end{align}}
\end{lemma}

\begin{proof}
Fix ${\varepsilon} > 0$, and choose isometries $\phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ and $\psi : B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that
{\begin{align*} {
d_\phi(f,g) < d(f,g) + {\varepsilon} \quad \text{and} \quad
d_\psi(g,h) < d(g,h) + {\varepsilon}.
} \end{align*}}
Define $\theta : A' \to {\mathbb{N}} \times {\mathbb{Z}}^d$ as in Lemma \ref{composition}.
We have
{\begin{align} \begin{split} {
d_\theta(f,h) = 2\sum_{u \in A'} |f(u) - h(\theta(u))| + \sum_{u \notin A'} f(u)^2 + \sum_{u \notin \theta(A')} h(u)^2 + 2^{-\deg(\theta)}. \label{theta_distance}
} \end{split} \end{align}}
The first sum above can be bounded as
{\begin{align} \begin{split} {
2\sum_{u \in A'} |f(u) - h(\theta(u))| &\leq 2\sum_{u \in A'} \bigl(|f(u) - g(\phi(u))| + |g(\phi(u)) - h(\psi(\phi(u)))|\bigr)  \\
&= 2\sum_{u \in A'} |f(u) - g(\phi(u))| + 2\sum_{u \in B \cap \phi(A)} |g(u) - h(\psi(u))|. \label{tri_bound_1}
} \end{split} \end{align}}
Now, if $f(u) < g(v)$, then trivially one has $f(u)^2 \leq g(v)^2 \leq 2|f(u) - g(v)| + g(v)^2$. 
If $f(u) \geq g(v)$, then one again has
{\begin{align*} {
f(u)^2 - g(v)^2 &= (f(u)+g(v))(f(u) - g(v))  \\
&\leq 2(f(u) - g(v))  \\
\Rightarrow \quad f(u)^2 &\leq 2|f(u) - g(v)| + g(v)^2.
} \end{align*}}
As a result, the second sum in \eqref{theta_distance} satisfies
{\begin{align} \begin{split} {
\sum_{u \notin A'} f(u)^{2} &= \sum_{u \in A \setminus A'} f(u)^2 + \sum_{u \notin A} f(u)^2  \\
&\leq \sum_{u \in A \setminus A'}\bigl( 2|f(u) - g(\phi(u))| + |g(\phi(u))|^2\bigr) + \sum_{u \notin A} f(u)^2  \\
&\leq 2\sum_{u \in A \setminus A'} |f(u) - g(\phi(u))| + \sum_{u \notin B} g(u)^2 + \sum_{u \notin A} f(u)^2. \label{tri_bound_2}
} \end{split} \end{align}}
Similarly, the third sum satisfies
{\begin{align} \begin{split} {
\sum_{u \notin \theta(A')} h(u)^2 &= \sum_{u \in \psi(B) \setminus \theta(A')} h(u)^2 + \sum_{u \notin \psi(B)} h(u)^{2}  \\
&\leq \sum_{u \in B \setminus \phi(A)} \bigl(2|h(\psi(u)) - g(u)| + g(u)^2\bigr) + \sum_{u \notin \psi(B)} h(u)^{2}  \\
&\leq 2\sum_{u \in B \setminus \phi(A)} |g(u) - h(\psi(u))| + \sum_{u \notin \phi(A)} g(u)^{2}+ \sum_{u \notin \psi(B)} h(u)^{2}. \label{tri_bound_3}
} \end{split} \end{align}}
Finally, Lemma \ref{composition} guarantees 
{\begin{align} \begin{split} {
\deg(\theta) &\geq \min(\deg(\phi),\deg(\psi))  \\
\quad \Rightarrow 2^{-\deg(\theta)} &\leq 2^{-\deg(\phi)} + 2^{-\deg(\psi)}. \label{tri_bound_4}
} \end{split} \end{align}}
Using \eqref{tri_bound_1}--\eqref{tri_bound_4} in \eqref{theta_distance}, we find
{\begin{align*} {
d(f,h) \leq d_\theta(f,h) \leq d_\phi(f,g) + d_\psi(g,h) < d(f,g) + d(g,h) + 2{\varepsilon}.
} \end{align*}}
As ${\varepsilon}$ is arbitrary, \eqref{triangle} follows.
\end{proof}

From \eqref{symmetry} and \eqref{triangle}, we see that $d$ is a pseudometric on $S$.
It does not, however, separate points.
Nevertheless, one can construct a metric space $(\mathcal{S},d)$ by taking the quotient of $S$ with respect to the equivalence relation
{\begin{align*} {
f \equiv g \quad \Leftrightarrow \quad d(f,g) = 0.
} \end{align*}}
We shall write without confusion the symbol $f$ for both the equivalence class in ${\mathcal{S}}$ and the representative chosen from $S$.
When $f$ is evaluated at a particular element $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$, an explicit representative has been chosen.
In the sequel, it will be important that certain properties of elements of $S$ are invariant within the equivalence classes and thus well-defined in ${\mathcal{S}}$.
In checking that this is the case, the following technical result will be critical.

\begin{prop} \label{superisometry}
Two functions $f,g \in S$ satisfy $d(f,g) = 0$ if and only if
there is a set $B\subset {\mathbb{N}} \times {\mathbb{Z}}^d$ and a map $\psi : B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that 
\begin{itemize}
\item[(i)] $f(u) = g(\psi(u))$ for all $u \in B$,
\item[(ii)] $f(u) = 0$ for all $u \notin B$,
\item[(iii)] $g(u) = 0$ for all $u \notin \psi(B)$, and
\item[(iv)] $\psi(u) - \psi(v) = u - v$ for all $u,v \in B$.
\end{itemize}
\end{prop}

\begin{proof}
The ``if" direction is straightforward to prove.
Suppose such a map $\psi$ exists.
For any ${\varepsilon} > 0$, properties (ii) and (iii) allow us to take $A \subset B$ to be finite but large enough that
{\begin{align*} {
\sum_{u \notin A} f(u)^2 + \sum_{u \notin \psi(A)} g(u)^2 < {\varepsilon}. 
} \end{align*}}
Let $\phi: A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ be the restriction of $\psi$ to $A$.
Then (i) says $f(u) - g(\phi(u)) = 0$ for all $u \in A$, and (iv) shows $\deg(\phi) = \infty$.
Therefore, 
{\begin{align*} {
d(f,g) \leq d_\phi(f,g) = \sum_{u \notin A} f(u)^2 + \sum_{u \notin \phi(A)} g(u)^2 < {\varepsilon}.
} \end{align*}}
As ${\varepsilon}$ is arbitrary, we must have $d(f,g) = 0$.

Now we prove the converse.
Assume $d(f,g) = 0$.
Let $\alpha_1 > \alpha_2 > \cdots > \alpha_p$ be the possibly infinite (in which case $p = \infty$) list of distinct positive values of $f$, and let
$\beta_1 > \beta_2 > \cdots > \beta_q$ be the distinct positive values of $g$.
If $p < \infty$, let $\alpha_{p+1} := 0$. 
Similarly, if $q < \infty$, let $\beta_{q+1} := 0$. 
For each $j \leq p$ and $k \leq q$, consider the finite sets
{\begin{align*} {
A_j := \{u \in {\mathbb{N}} \times {\mathbb{Z}}^d : f(u) = \alpha_j\}, \qquad
B_k := \{u \in {\mathbb{N}} \times {\mathbb{Z}}^d : g(u) = \beta_k\}.
} \end{align*}}
Fix finite, positive integers $M \leq p$ and $N \leq q$. 
Next choose $m$ so large that for every $u,v \in \big(\bigcup_{j = 1}^{M} A_j\big) \cup \big(\bigcup_{k = 1}^{N} B_k\big)$, either $\|u - v\|_1 < m$ or $\|u - v\|_1 = \infty$.
We define\footnote{The reason for including $j = M+1$ and $k = N+1$ in \eqref{C_set} is to obtain the penultimate inequality in \eqref{finalcontradiction}.}
{\begin{align} \begin{split} {
C := \{\alpha_j : 1 \leq j \leq M+1\} \cup \{\beta_k : 1 \leq k \leq N+1\} \cup \{0\}. \label{C_set}
} \end{split} \end{align}}
If $C = \{0\}$, then $f$ and $g$ are both identically 0, in which case the claim is trivial.
Otherwise, we set
{\begin{align*} {
{\varepsilon} := \min\{|a - b| : a, b \in C,\ a \neq b\} > 0.
} \end{align*}}
Since $d(f,g) = 0$, there is an isometry $\phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that
{\begin{align} \begin{split} {
d_\phi(f,g) = 2\sum_{u \in A} |f(u) - g(\phi(u))| + \sum_{u \notin A} f(u)^2 + \sum_{u \notin \phi(A)} g(u)^2 + 2^{-\deg(\phi)} < \min\{2^{-m},{\varepsilon},{\varepsilon}^2\}. \label{3_inequalities}
} \end{split} \end{align}}
For any $j \leq M$ and any $u \in A_j$,
{\begin{align*} {
f(u)^2 \geq {\varepsilon}^2 > d_\phi(f,g) \quad \Rightarrow \quad u \in A.
} \end{align*}}
Similarly, for any $k \leq N$ and any $u \in B_k$, we must have $u \in \phi(A)$.

Without loss of generality (since $d_\phi(f,g) = d_{\phi^{-1}}(g,f)$), assume that $\alpha_1 \geq \beta_1$.
Consider any $u \in A_1 \subset A$.
If $\alpha_1 > \beta_1$, then we would have
{\begin{align*} {
d_\phi(f,g) \geq |f(u) - g(\phi(u))| \geq \alpha_1 - \beta_1 \geq {\varepsilon} > d_\phi(f,g).
} \end{align*}}
This contradiction to \eqref{3_inequalities} forces $\alpha_1 = \beta_1$.
Moreover, $g(\phi(u))$ must equal $f(u)$, since otherwise
{\begin{align*} {
d_\phi(f,g) \geq |f(u) - g(\phi(u))| \geq \alpha_1 - \beta_2 \geq {\varepsilon} > d_\phi(f,g).
} \end{align*}}
We conclude that $\phi(A_1) \subset B_1$.
Furthermore, since $B_1 \subset \phi(A)$ and $\beta_1 = \alpha_1 \geq \alpha_2 + {\varepsilon}$, a parallel argument shows $\phi^{-1}(B_1) \subset A_1$, and so in fact $\phi(A_1) = B_1$.

Next we proceed by induction on $j \leq \min(M,N)$.
The inductive assumption is that $\alpha_i = \beta_i$ and $\phi(A_i) = B_i$ for all $i < j$.
Assuming without loss of generality that $\alpha_j \geq \beta_j$, we consider $u \in A_j \subset A$.
As above, $\alpha_j > \beta_j$ would yield a contradiction:
Since $\phi(u) \notin B_i$ for any $i < j$, we have $\phi(u) \leq \beta_j$.
Hence
{\begin{align*} {
d_\phi(f,g) \geq |f(u) - g(\phi(u))| \geq \alpha_j - \beta_j \geq {\varepsilon} > d_\phi(f,g).
} \end{align*}}
Indeed, this contradiction to \eqref{3_inequalities} forces $\alpha_j = \beta_j$, and so either $f(u) = g(\phi(u))$ or
{\begin{align} \begin{split} {
d_\phi(f,g) \geq |f(u) - g(\phi(u))| \geq \alpha_j - \beta_{j+1} \geq {\varepsilon} > d_\phi(f,g), \label{finalcontradiction}
} \end{split} \end{align}}
another contradiction.
This argument shows $\phi(A_j) \subset B_j$.
Of course, the reverse containment follows from considering $d_{\phi^{-1}}(g,f)$.

Suppose $p < q$.
Then we could have taken $M = p$ and $N > p$, and
the above argument would show $\phi(A_j) = B_j$ for $j = 1,2,\dots,p$.
But $f(u) = 0$ for all $u \notin \bigcup_{j = 1}^p A_j$, and so
{\begin{align*} {
u \in B_{p+1} \subset \phi(A) \quad &\Rightarrow \quad f(\phi^{-1}(u)) = 0 \\
\quad &\Rightarrow \quad d_\phi(f,g) \geq |f(\phi^{-1}(u)) - g(u)| = g(u) = \beta_{p+1} \geq {\varepsilon} > d_\phi(f,g),
} \end{align*}}
a contradiction.
Therefore, it cannot be the case that $p < q$, and for a symmetric reason it cannot be that $q < p$.
Consequently, $p = q$ and we may take $M = N$. 

With $M = N$ above, we now let $\psi_M$ be the restriction of $\phi$ to the set $\bigcup_{j = 1}^M A_j$.
Notice that $d_\phi(f,g) < 2^{-m}$ implies $m \leq \deg(\phi) \leq \deg(\psi_M)$.
By choice of $m$ and the domain of $\psi_M$, we actually have $\deg(\psi_M) = \infty$.

If $p = q$ is finite, then $\psi_{p}$ has the desired properties (i)--(iv).
So we may assume $p = \infty$.
Naturally, $\psi_M$ induces a family of bijections $\psi_{M,\, k} : A_k \to B_k$, $1 \leq k \leq M$ that are compatible:
{\begin{align} \begin{split} {
u - v = \psi_{M,\, j}(u) - \psi_{M,\, k}(v) \quad \text{for all $j,k \leq M$, $u \in A_j$, $v \in A_k$}. \label{compatible}
} \end{split} \end{align}}
The finiteness of $|A_j| = |B_j|$ ensures there are only finitely many possibilities for $\psi_{M,\, k}$, irrespective of $M$.
We are now left to make a standard diagonalization argument:
There must be some $\psi_1 : A_1 \to B_1$ such that $\psi_1 = \psi_{M,\, 1}$ for infinitely many $M$.
In turn, there is $\psi_2 : A_2 \to B_2$ such that $\psi_2 = \psi_{M,\, 2}$ for infinitely many $M$ such that $\psi_{M,\, 1} = \psi_1$.
Proceeding in this way, we can define $\psi_k : A_k \to B_k$ for every $k$, such that $\psi_k$ is compatible with all $\psi_j$, $j \leq k$, in the sense of~\eqref{compatible}.
Letting $\psi$ act as $\psi_k$ on $A_k$, we obtain a map $\psi : \bigcup_{k = 1}^\infty A_k \to {\mathbb{N}} \times {\mathbb{Z}}^d$ satisfying (i)--(iv) with $B :=  \bigcup_{k = 1}^\infty A_k$.
\end{proof}

A more transparent description of equivalence under $d$ was given in the introductory Section~\ref{results}.
We restate it here, and prove equivalence to the conditions of Proposition \ref{superisometry}.
Recall that the ${\mathbb{N}}$-support of $f \in S$ is the set
{\begin{align*} {
H_f := \{n \in {\mathbb{N}} : f(n,x) > 0 \text{ for some $x \in {\mathbb{Z}}^d$}\}.
} \end{align*}}

\begin{cor} \label{better_def_cor}
Let $f,g \in S$ have ${\mathbb{N}}$-supports denoted by $H_f$ and $H_g$, respectively.
Then $d(f,g) = 0$ if and only if there is a bijection $\sigma : H_f \to H_g$ and vectors $(x_n)_{n \in H_f} \subset {\mathbb{Z}}^d$ such that 
{\begin{align} \begin{split} {
g(\sigma(n),x) = f(n,x+x_n) \quad \text{for all $n \in H_f$, $x \in {\mathbb{Z}}^d$.} \label{better_def}
} \end{split} \end{align}}
\end{cor}

\begin{proof}
First assume $d(f,g) = 0$, and take $\psi : B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ as in Proposition \ref{superisometry} so that properties (i)--(iv) hold.
By repeatedly applying Lemma \ref{extension},
one may assume that the domain $B$ is a union of copies of ${\mathbb{Z}}^d$; that is, $B = H \times {\mathbb{Z}}^d$.
By property (ii), we can take $H = H_f$, while property (iv) shows first that for every $n \in H_f$, there is $\sigma(n) \in {\mathbb{N}}$ and $x_n \in {\mathbb{Z}}^d$ satisfying
{\begin{align*} {
\psi(n,x) = (\sigma(n),x-x_n) \quad \text{for all $x \in {\mathbb{Z}}^d$,}
} \end{align*}}
and second that $n \mapsto \sigma(n)$ is injective.
Then (i) leads to \eqref{better_def}, while (iii) guarantees that $\sigma(H_f) = H_g$. Conversely, assume $\sigma : H_f \to H_g$ and $(x_n)_{n \in H_f}$ satisfy \eqref{better_def}.
Then define $\psi : H_f \times {\mathbb{Z}}^d \to H_g \times {\mathbb{Z}}^d$ by $\psi(n,x) := (\sigma(n),x-x_n)$.
Since the domain of $\psi$ is $H_f \times {\mathbb{Z}}^d$, and its range is $H_g \times {\mathbb{Z}}^d$, this map satisfies (ii) and (iii).
At the same time, (iv) holds by construction, and (i) follows from~\eqref{better_def}.
\end{proof}

We can now state a clear geometric condition for a function on $S$ to be well-defined on ${\mathcal{S}}$.
For the functions that will be of interest to us, it will be obvious that the hypotheses of the following corollary are satisfied.

\begin{cor} \label{defined_pspm}
Suppose $L$ is a function on $S$ that satisfies the following:
\begin{itemize}
\item[(i)] L is invariant under shifts of ${\mathbb{Z}}^d$: If there are vectors $(x_n)_{n \in {\mathbb{N}}}$ in ${\mathbb{Z}}^d$ such that $f(n,x) = g(n,x-x_n)$, then $L(f) = L(g)$.
\item[(ii)] L is invariant under permutations of ${\mathbb{N}}$: If there is a bijection $\sigma : {\mathbb{N}} \to {\mathbb{N}}$ such that $f(n,x) = g(\sigma(n),x)$, then $L(f) = L(g)$.
\item[(iii)] L is invariant under zero-padding: If there is an increasing sequence $(n_k)_{k \geq 1}$ in ${\mathbb{N}}$ such that
{\begin{align*} {
f(n,x) = \begin{cases}
g(k,x) &\text{if } n = n_k, \\
0 &\text{else},
\end{cases}
} \end{align*}}
then $L(f) = L(g)$.
\end{itemize}
Then $L$ is well-defined on ${\mathcal{S}}$ by evaluating at any representative in $S$.
\end{cor}

\begin{proof}
Suppose $f,g \in S$ are such that $d(f,g) = 0$.
We wish to show that $L(f) = L(g)$.
Let $H_f$ and $H_g$ be the ${\mathbb{N}}$-supports of $f$ and $g$ respectively, and let $\sigma : H_f \to H_g$ be a bijection satisfying \eqref{better_def} with vectors $(x_n)_{n \in H_f}$.
If $H_f$ is finite, then $\sigma$ can be trivially extended to a bijection on ${\mathbb{N}}$, and then invariance properties (i) and (ii) are sufficient to show $L(f) = L(g)$.
If $H$ is infinite, then we enumerate the sets
{\begin{align*} {
H_f = \{n_1 < n_2 < \cdots\} \qquad H_g = \{m_1 < m_2 < \cdots\},
} \end{align*}}
and define the functions $h_f \in S$ by $h_f(k,x) = f(n_k,x)$ and $h_g(\ell,x) = g(m_\ell,x)$.
By (iii), we have $L(f) = L(h_f)$ and $L(g) = L(h_g)$.
Furthermore, $\sigma$ induces a bijection $\sigma' : {\mathbb{N}} \to {\mathbb{N}}$ by
{\begin{align*} {
\sigma : n_k \mapsto m_\ell \quad \Leftrightarrow \quad \sigma' : k \mapsto \ell.
} \end{align*}}
We now have
{\begin{align*} {
h_f(k,x) = f(n_k,x) = g(\sigma(n_k),x - x_n) = h_g(\sigma'(k),x - x_n),
} \end{align*}}
so that (i) and (ii) give $L(h_f) = L(h_g)$.
So $L(f) = L(g)$ in this case as well.
\end{proof}

In the next lemma we discuss our first example of a function $L$ satisfying the hypotheses of Corollary \ref{defined_pspm}.
This function will be important in defining the ``update procedure" of Section~\ref{transformation}. 
The proof of Lemma \ref{norm_equivalence} below is similar to arguments appearing throughout the rest of the manuscript.

\begin{lemma} \label{norm_equivalence}
The map $\|\cdot\| : {\mathcal{S}} \to [0,1]$ defined by \eqref{norm_def} is lower semi-continuous and thus measurable.
\end{lemma}

\begin{proof}
It is clear that $\|\cdot\| : S \to [0,1]$ satisfies (i)--(iii) in Corollary \ref{defined_pspm}, and so the map $f \mapsto \|f\|$ is well-defined on ${\mathcal{S}}$.
To prove lower semi-continuity, it suffices to fix $f \in {\mathcal{S}}$, let ${\varepsilon} > 0$ be arbitrary, and find $\delta > 0$ sufficiently small that
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad \|g\| > \|f\| - {\varepsilon}.
} \end{align*}}
Upon selecting a representative $f \in S$, we can find $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$ finite but large enough that
{\begin{align*} {
\sum_{u \notin A} f(u) < \frac{\varepsilon}{2}.
} \end{align*}}
By possibly omitting some elements of $A$, we may assume that $f$ is strictly positive on $A$ (if $f$ is the constant zero function, this results in $A = \varnothing$).
Then take $0 < \delta < \inf_{u \in A} f(u)^2$, where an infimum over the empty set is $\infty$.
We will further assume $\delta < {\varepsilon}$.
If $d(f,g) < \delta$, then there is a representative $g \in S$ and an isometry $\phi : C \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $d_\phi(f,g) < \delta$.
It follows that $A \subset C$, since otherwise we would have $d_\phi(f,g) \geq f(u)^2 > \delta$ for some $u \in A \setminus C$.
Hence
{\begin{align*} {
\|g\| \geq \sum_{u \in \phi(A)} g(u)
&\geq \sum_{u \in A} f(u) - \sum_{u \in A} |f(u) - g(\phi(u))| + \sum_{u \notin A} f(u) - \sum_{u \notin A} f(u) \\
&> \|f\| - \frac{d_\phi(f,g)}{2} - \frac{\varepsilon}{2} > \|f\| - \frac{\delta}{2} - \frac{\varepsilon}{2} > \|f\| - {\varepsilon}.
} \end{align*}}
\end{proof}

\subsection{Compactness} \label{compactness}
We now state and prove compactness for the metric space $({\mathcal{S}},d)$.

\begin{thm} \label{compactness_result}
In any sequence $(f_n)_{n \geq 1}$ in $({\mathcal{S}},d)$ there exists a converging subsequence.  
That is, there is $f \in {\mathcal{S}}$ and a subsequence $(f_{n_k})_{k \geq 1}$ such that
{\begin{align*} {
\lim_{k \to \infty} d(f_{n_k},f) = 0.
} \end{align*}}
\end{thm}

\begin{proof}
For each $n$, fix a representative $f_n \in S$. 
The elements of ${\mathbb{N}} \times {\mathbb{Z}}^d$ can be ordered so that $f_n$ attains its $k$-th largest value at the $k$-th element on the list.
Ties are broken according to some arbitrary but fixed rule (e.g.~assigning an order to ${\mathbb{Z}}^d$ and following the lexicographic ordering induced on ${\mathbb{N}} \times {\mathbb{Z}}^d$).
This information is recorded by defining
{\begin{align*} {
u_{n,\, k} := u \in {\mathbb{N}} \times {\mathbb{Z}}^d \text{ at which $f_n$ attains its $k$-th largest value}.
} \end{align*}}
Of interest will be the quantities
{\begin{align*} {
t_n(k,\ell) := u_{n,\, k} - u_{n,\, \ell} \in {\mathbb{Z}}^d \cup \{\infty\}.
} \end{align*}}
Here we consider ${\mathbb{Z}}^d \cup \{\infty\}$ as the one-point compactification of ${\mathbb{Z}}^d$ (cf.~Steen and Seebach \cite{seebach-steen78}, page 63).
Since this space is second countable, compactness is equivalent to sequential compactness (\cite{seebach-steen78}, page 22).
Consequently, by passing to a subsequence, we may assume that for every $k,\ell \in {\mathbb{N}}$, there is $t(k,\ell) \in {\mathbb{Z}}^d \cup \{\infty\}$ so that
{\begin{align*} {
t_n(k,\ell) \to t(k,\ell) \quad \text{as } n \to \infty.
} \end{align*}}
Because convergence takes place in the discrete space ${\mathbb{Z}}^d \cup \{\infty\}$, $t(k,\ell) \neq \infty$ only if $t_n(k,l) = t(k,\ell) < \infty$ for all $n$ sufficiently large.
On the other hand, $t(k,\ell) = \infty$ when either $t_n(k,\ell)$ is finite infinitely often but $\liminf_n \|t_n(k,\ell)\|_1 = \infty$, or $t_n(k,\ell) = \infty$ for all $n$ sufficiently large.

Since $\|f_n\| \leq 1$, we necessarily have
{\begin{align} \begin{split} {
0 \leq f_n(u_{n,\, k}) \leq \frac{1}{k} \quad \text{for all } n,k. \label{f_n_bound}
} \end{split} \end{align}}
So by passing to a further subsequence, it may be assumed that
{\begin{align} \begin{split} {
{\varepsilon}_k := \lim_{n\to\infty} f_n(u_{n,\, k}) \quad \text{exists for all } k, \label{eps_convergence}
} \end{split} \end{align}}
where ${\varepsilon}_k$ satisfies
{\begin{align} \begin{split} {
0 \leq {\varepsilon}_k \leq \frac{1}{k}. \label{eps_bound}
} \end{split} \end{align}}
Next, inductively define
{\begin{align*} {
\ell(1) &:= 1 \\
\ell(r) &:= \min\{k > \ell(r-1)\ :\ t(k,\ell(s)) = \infty \text{ for all } s \leq r - 1\}, \quad r \geq 2.
} \end{align*}}
It may be the case that only finitely many $\ell(r)$ can be defined, if the set considered in the definition is empty for some $r$.
In any case, let
{\begin{align*} {
R := \text{number of $r$ for which $\ell(r)$ is defined} \leq \infty.
} \end{align*}}
Clearly each $\ell(r)$ is distinct.
In words, $\ell(r)$ is the next integer past $\ell(r-1)$ such that the distance between $u_{n,\, \ell(r)}$ and each of $u_{n,\, \ell(1)}, u_{n,\, \ell(2)}, \dots ,u_{n,\, \ell(r-1)}$ is tending to $\infty$ with $n$.

Consider any fixed $k \in {\mathbb{N}}$.
There is some $r$ such that $t(k,\ell(r)) \neq \infty$.
For instance, if $k = \ell(r)$, then $t(k,\ell(r)) = t(\ell(r),\ell(r)) = 0$.
If $\ell(r) < k < \ell(r+1)$, then $t(k,\ell(s))$ is finite for some $s \leq r$, by the definition of $\ell(r+1)$.
Similarly, if $R < \infty$ and $k > \ell(R)$, then $t(k,\ell(s))$ is finite for some $s \leq R$.
Now given the existence of $r$ with $t(k,\ell(r))$ finite, we claim there is a unique such $r$.
Indeed, if $t(k,\ell(r))$ and $t(k,\ell(r'))$ are both finite, then
{\begin{align*} {
t(\ell(r),\ell(r')) 
= \lim_{n \to \infty} (u_{n,\, \ell(r)} - u_{n,\, \ell(r')})
&= \lim_{n \to \infty} \bigl(-(u_{n,\, k} - u_{n,\, \ell(r)}) + (u_{n,\, k} - u_{n,\, \ell(r')}) \bigr)\\
&= -t(k,\ell(r)) + t(k,\ell(r'))
\ne \infty,
} \end{align*}}
which forces $r = r'$.
This discussion shows it is possible to define
{\begin{align*} {
r_k := \text{unique $r$ such that } t(k,\ell(r)) \neq \infty.
} \end{align*}}
Let us summarize the construction thus far.
For each $k \in {\mathbb{N}}$, consider the point
{\begin{align*} {
v_k := \Big(r_k, t\big(k,\ell(r_k)\big)\Big) \in {\mathbb{N}} \times {\mathbb{Z}}^d.
} \end{align*}}
That is, $v_k$ is in the $r_k$-th copy of ${\mathbb{Z}}^d$, at the finite limit point of $u_{n,\, k} - u_{n,\, \ell(r_k)}$.
Moreover, $r_k$ is the unique $r$ such that $u_{n,\, k} - u_{n,\, \ell(r)}$ converges to a finite limit.
So there are $R$ copies of ${\mathbb{Z}}^d$ populated by the $v_k$, the $r$-th copy containing those $v_k$ for which $u_{n,\, k}$ and $u_{n,\, \ell(r)}$ remain close as $n$ grows large.
The actual point in that copy of ${\mathbb{Z}}^d$, at which $v_k$ is located, encodes the limiting difference $t(k,\ell(r))$, prescribing exactly how those two locations are separated for large $n$.
More precisely, there is some $N_k$ such that
{\begin{align} \begin{split} {
u_{n,\, k} - u_{n,\, \ell(r_k)} = t\big(k,\ell(r_k)\big) < \infty \quad \text{for all $n \geq N_k$}. \label{relative_finite}
} \end{split} \end{align}}
Finally, recall that these locations give the order statistics on the values of $f_n$.
The first coordinate of $v_k$ indicates that the location of the $k$-th largest value of $f_n$ remains close to the location of the $\ell(r_k)$-th largest value of $f_n$, as $n$ tends to infinity.
The $R$ different locations of the $\ell(r)$-th largest values of $f_n$ may not converge, but they serve as moving reference points to which all other locations remain close.

With the definitions made above, it is now possible to construct the limit function $f$.
Set
{\begin{align*} {
f(u) := \begin{cases} 
{\varepsilon}_k &\text{if } u = v_k, \\
0 &\text{else.}
\end{cases}
} \end{align*}}
In order for $f$ to be well-defined, it must be checked that the $v_k$ are distinct.
Suppose $v_k = v_{k'}$.
That is, $r_{k} = r_{k'}$ and $t\big(k,\ell(r_k)\big) = t\big(k',\ell(r_{k'})\big)$, meaning that for all $n \geq \max\{N_k,N_{k'}\}$,
{\begin{align} \begin{split} {
u_{n,\, k} - u_{n,\, \ell(r_k)} 
= t\big(k,\ell(r_k)\big)
= t\big(k',\ell(r_{k'})\big)
= u_{n,\, k'} - u_{n,\, \ell(r_{k'})} 
= u_{n,\, k'} - u_{n,\, \ell(r_k)}.
} \end{split} \end{align}}
It follows that $u_{n,\, k} = u_{n,\, k'}$ for all sufficiently large $n$.
That this holds for even one value of $n$ implies $k = k'$.

In order for $f$ to be an element of $S$, it must be checked that $\|f\| \leq 1$
($f$ is nonnegative due to \eqref{eps_bound}), which is a straightforward task:
Let $K \in {\mathbb{N}}$ be fixed.
Given ${\varepsilon} > 0$, from \eqref{eps_convergence} we may choose $N$ large enough that
{\begin{align*} {
|f_N(u_{N,\, k}) - {\varepsilon}_k| \leq\frac{\varepsilon}{K} \quad \text{for } k = 1,2,\dots,K.
} \end{align*}}
Then
{\begin{align*} {
\sum_{k = 1}^K {\varepsilon}_k \leq \sum_{k = 1}^K \bigg(f_N(u_{N,\, k}) + \frac{\varepsilon}{K}\bigg) \leq \|f_N\| + {\varepsilon} \leq 1 + {\varepsilon}.
} \end{align*}}
As ${\varepsilon}$ is arbitrary, it follows that
{\begin{align*} {
\sum_{k = 1}^K f(v_k) = \sum_{k=1}^K {\varepsilon}_k \leq 1.
} \end{align*}}
Letting $K$ tend to infinity yields the bound $\|f\| \leq 1$.

In some sense, the remaining goal of the proof is to show that $f_n(v_k) \to {\varepsilon}_k$.
Of course, this is not true.
Rather, the pointwise convergence of $(f_n)_{n \geq 1}$ is given by \eqref{eps_convergence}.
Nevertheless, we have the convergence \eqref{relative_finite} of \textit{relative} coordinates.
Furthermore, for any $k$ and $k'$ such that $r_k \neq r_{k'}$, we have $t(k,k') = \infty$. 
Therefore, for any $M > 0$, there is $N_{k,\, k'}$ such that
{\begin{align} \begin{split} {
\|u_{n,\, k} - u_{n,\, k'}\|_1 \geq M \quad \text{for all $n \geq N_{k,\, k'}$.} \label{relative_infinite}
} \end{split} \end{align}}
So the desired convergence will hold after pre-composing $f$ with a suitable isometry.
The correct choice of isometry is described below.

Temporarily fix $K \in {\mathbb{N}}$.
For each $n \in {\mathbb{N}}$, define $\phi_{n,\, K} : \{u_{n,\, 1},u_{n,\, 2},\dots,u_{n,\, K}\} \to {\mathbb{N}} \times {\mathbb{Z}}^d$ by
{\begin{align*} {
\phi_{n,\, K}(u_{n,\, k}) := v_k, \quad k = 1,2,\dots,K.
} \end{align*}}
Consider the case when $n \geq N_k$ for all $k = 1,2,\dots,K$.
Then \eqref{relative_finite} guarantees that $u_{n,\, k} - u_{n,\, \ell(r_k)} = t\big(k,\ell(r_k)\big)$ for each $k = 1,2,\dots,K$.
Consequently, for $1 \leq k,k' \leq K$ we have
{\begin{align*} {
r_k = r_{k'} \quad &\Rightarrow \quad u_{n,\, k} - u_{n,\, k'} = t\big(k,\ell(r_k)\big) - t\big(k',\ell(r_{k'})\big)
= v_k - v_{k'} 
= \phi_{n,\, K}(u_{n,\, k}) - \phi_{n,\, K}(u_{n,\, k'}).
\intertext{If it is further the case that $n \geq N_{k,\, k'}$ for all $1 \leq k,k' \leq K$ with $r_k \neq r_{k'}$, then \eqref{relative_infinite} shows}
r_k \neq r_{k'} \quad &\Rightarrow \quad 
\|u_{n,\, k} - u_{n,\, k'}\|_1 \geq M \text{ and } \|\phi_{n,\, K}(u_{n,\, k}) - \phi_{n,\, K}(u_{n,\, k'})\|_1 = \|v_k - v_{k'}\|_1 = \infty.
} \end{align*}}
Together, the previous two displays ensure $\deg(\phi_{n,\, K}) \geq M$ for large $n$.
Moreover, since $M$ in \eqref{relative_infinite} can be chosen arbitrarily large, we conclude that for fixed $K$,
{\begin{align} \begin{split} {
\lim_{n \to \infty} \deg(\phi_{n,\, K}) = \infty. \label{degtoinf}
} \end{split} \end{align}}
One can now verify convergence of $f_n$ to $f$ under the metric $d$.
To do so, one must exhibit, for every ${\varepsilon} > 0$, some $N \in {\mathbb{N}}$ satisfying the following condition:
For every $n \geq N$, there is some finite $A_n \subset {\mathbb{N}} \times {\mathbb{Z}}^d$ and some isometry $\phi_n : A_n\to {\mathbb{N}} \times {\mathbb{Z}}^d$ satisfying
{\begin{align*} {
d_{\phi_n}(f_n,f) < {\varepsilon}.
} \end{align*}}
So fix ${\varepsilon} > 0$.
For each $K \in {\mathbb{N}}$, write $A_{n,\, K} = \{u_{n,\, 1},u_{n,\, 2},\dots,u_{n,\, K}\}$ for the domain of $\phi_{n,\, K}$.
Choose $K$ large enough that
{\begin{align*} {
\sum_{k > K} \frac{1}{k^2} < \frac{\varepsilon}{4}.
} \end{align*}}
It follows from \eqref{f_n_bound} that
{\begin{align} \begin{split} {
\sum_{k > K} f_n(u_{n,\, k})^2
< \frac{\varepsilon}{4} \quad \text{for all $n$.} \label{final_bound1}
} \end{split} \end{align}}
Similarly, from \eqref{eps_bound} we have
{\begin{align} \begin{split} {
\sum_{k > K} f(v_k)^2
= \sum_{k > K} {\varepsilon}_k^2
< \frac{\varepsilon}{4}. \label{final_bound2}
} \end{split} \end{align}}
Finally, in light of \eqref{eps_convergence} and \eqref{degtoinf}, it is possible to choose $N$ large enough that
{\begin{align} \begin{split} {
\sum_{k = 1}^K |f_n(u_{n,\, k}) - {\varepsilon}_k| < \frac{\varepsilon}{8} \qquad \text{for all } n \geq N,\label{final_bound3}
} \end{split} \end{align}}
and that
{\begin{align} \begin{split} {
2^{-\deg(\phi_{n,\, K})} < \frac{\varepsilon}{4} \quad \text{for all $n \geq N$}. \label{final_bound4}
} \end{split} \end{align}}
Using \eqref{final_bound1}--\eqref{final_bound4}, one arrives at
{\begin{align*} {
d_{\phi_{n,\, K}}(f_n,f) &=
2\sum_{u \in A_{n,K}} |f_n(u) - f(\phi_{n,\, K}(u))|
+ \sum_{u \notin A_{n,\, K}} f_n(u)^2 + \sum_{v \notin \phi_{n,\, K}(A_{n,\, K})} f(v)^2 + 2^{-\deg(\phi_{n,\, K})} \\
&= 2\sum_{k=1}^K |f_n(u_{n,\, k}) - \underbrace{f(v_k)}_{=\, {\varepsilon}_k}|
+ \sum_{k > K} f_n(u_{n,\, k})^2 + \sum_{k > K} f(v_k)^2 + 2^{-\deg(\phi_{n,\, K})}  \\
&< \frac{\varepsilon}{4} + \frac{\varepsilon}{4} + \frac{\varepsilon}{4} + \frac{\varepsilon}{4} = {\varepsilon}
} \end{align*}}
for all $n \geq N$.
In particular,
{\begin{align*} {
d(f_n,f) \leq d_{\phi_{n,\, K}}(f_n,f) < {\varepsilon} \quad \text{for all } n \geq N.
} \end{align*}}
One concludes $d(f_n,f) \to 0$ as $n \to \infty$, as desired.
\end{proof}

The sequential compactness just shown will be used to establish results in a certain metric space of probability measures.
For now, let us keep the notation general.
Let $({\mathcal{X}},\tau)$ be a Polish (metric) space\footnote{In particular, $({\mathcal{S}},d)$ is Polish, since a compact metric space is Polish (see \cite{seebach-steen78}, pages 66 and 67).}, and consider the space ${\mathcal{P}}^p({\mathcal{X}})$ of Borel probability measures on ${\mathcal{X}}$ with finite $p$-norm, where $p \geq 1$ (see Villani \cite{villani09}, Definition 6.4):
{\begin{align*} {
{\mathcal{P}}^p({\mathcal{X}}) := \bigg\{\text{Borel probability measure } \mu: \int_{\mathcal{X}} \tau(x,x_0)^p\, \mu({\mathrm{d}} x) < \infty \text{ for some } x_0 \in {\mathcal{X}}\bigg\}.
} \end{align*}}
Equip this space with the \textit{Wasserstein metric} (\cite{villani09}, Definition 6.1):
{\begin{align*} {
{\mathcal{W}}^p(\mu,\nu) := \bigg(\inf_{\pi \in \Pi(\mu,\nu) }\int_{{\mathcal{X}} \times {\mathcal{X}}} \tau(x,y)^p\, \pi({\mathrm{d}} x,{\mathrm{d}} y)\bigg)^{1/p},
} \end{align*}}
where $\Pi(\mu,\nu)$ denotes the set of probability measures on ${\mathcal{X}} \times {\mathcal{X}}$ having $\mu$ and $\nu$ as marginals.

Below we collect some general facts on convergence in ${\mathcal{P}}^p({\mathcal{X}})$ that are quoted in later sections.
In particular, Lemma \ref{wasserstein_compact}(c) shows ${\mathcal{P}}^p({\mathcal{S}})$ is compact under $\mathcal{W}^p$.
Concerning Lemma \ref{wasserstein_compact}(a), it will be sufficient in our subsequent analysis to work in ${\mathcal{P}}^1({\mathcal{S}})$, and so we may use the equivalent definition \eqref{kantorovich} of ${\mathcal{W}}^1$ when it is convenient.
First we recall the Portmanteau theorem.

\begin{lemma}[Portmanteau, see Billingsley \cite{billingsley99}, Theorem 21] \label{portmanteau}
Let $({\mathcal{X}},\tau)$ be a metric space.
Let $\mu_1,\mu_2,\dots$ and $\mu$ be Borel probability measures on ${\mathcal{X}}$.
The following statements are equivalent:
\begin{itemize}
\item[(a)] For any bounded, continuous function $F : {\mathcal{X}} \to {\mathbb{R}}$, 
{\begin{align*} {
\lim_{n \to \infty} \int F(x)\ \mu_n({\mathrm{d}} x) = \int F(x)\ \mu({\mathrm{d}} x).
} \end{align*}}
\item[(b)] For any lower semi-continuous function $F: {\mathcal{X}} \to {\mathbb{R}}$ bounded from below,
{\begin{align*} {
\liminf_{n \to \infty} \int F(x)\ \mu_n({\mathrm{d}} x) \geq \int F(x)\ \mu({\mathrm{d}} x).
} \end{align*}}
\item[(c)] For any upper semi-continuous function $F: {\mathcal{X}} \to {\mathbb{R}}$ bounded from above,
{\begin{align*} {
\limsup_{n \to \infty} \int F(x)\ \mu_n({\mathrm{d}} x) \leq \int F(x)\ \mu({\mathrm{d}} x).
} \end{align*}}
\item[(d)] For any open set $U \subset {\mathcal{X}}$,
{\begin{align*} {
\liminf_{n \to \infty} \mu_n(U) \geq \mu(U).
} \end{align*}}
\item[(e)] For any closed set $C \subset {\mathcal{X}}$,
{\begin{align*} {
\limsup_{n \to \infty} \mu_n(C) \leq \mu(C).
} \end{align*}}
\end{itemize}
\end{lemma}

If one of the equivalent conditions listed in Lemma \ref{portmanteau} is satisfied, we say $\mu_n$ \textit{converges weakly} to $\mu$ and write $\mu_n \rightharpoonup \mu$.
If a sequence $(\mu_n)_{n \geq 1}$ in ${\mathcal{P}}^p({\mathcal{X}})$ satisfies ${\mathcal{W}}^1(\mu_n,\mu) \to 0$ as $n \to \infty$ for some $\mu \in {\mathcal{P}}^p({\mathcal{X}})$, then we write $\mu_n \to \mu$.
Notice that if ${\mathcal{X}}$ is compact, then \textit{any} continuous function $F : {\mathcal{X}} \to {\mathbb{R}}$ is bounded, and any lower (upper) semi-continuous function is bounded from below (above).

\begin{lemma}[see \cite{villani09}, Theorem 5.10 and equation (5.11), Theorem 6.9, and Remark 6.19] \label{wasserstein_compact}
Let $({\mathcal{X}},\tau)$ be a Polish space.
Then the following statements hold:
\begin{itemize}
\item[(a)] When $p = 1$, ${\mathcal{W}}^1$ has a dual representation due to Kantorovich~\cite{kantorovich42}:
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu,\nu) = \sup_{\varphi} \biggl(\int_{\mathcal{X}} {\varphi}(x)\ \mu({\mathrm{d}} x) - \int_{\mathcal{X}} {\varphi}(y)\ \nu({\mathrm{d}} y)\biggr), \label{kantorovich}
} \end{split} \end{align}}
where the supremum is taken over all Lipschitz continuous functions ${\mathcal{X}} \to {\mathbb{R}}$ with minimal Lipschitz constant at most 1.
\item[(b)] Let $(\mu_n)_{n \geq 1}$ be a sequence in ${\mathcal{P}}^p({\mathcal{X}})$. For $\mu \in {\mathcal{P}}^p({\mathcal{X}})$, ${\mathcal{W}}^p(\mu_n,\mu) \to 0$ as $n \to \infty$ if and only if $\mu_n \rightharpoonup \mu$ and\footnote{Because of Lemma \ref{portmanteau}(a), \eqref{tau_converge} is implied by $\mu_n \rightharpoonup \mu$ whenever $({\mathcal{X}},\tau)$ is compact, as $\tau$ is trivially continuous and thus bounded when ${\mathcal{X}}$ is compact.  So $\mu_n \to \mu$ is equivalent to $\mu_n \rightharpoonup \mu$ in this case.}
{\begin{align} \begin{split} {
\lim_{n \to \infty} \int_{\mathcal{X}} \tau(x,x_0)^{p}\, \mu_n({\mathrm{d}} x) = \int_{\mathcal{X}} \tau(x,x_0)^{p}\ \mu({\mathrm{d}} x) \quad \text{for some $x_0 \in {\mathcal{X}}$}. \label{tau_converge}
} \end{split} \end{align}}
\item[(c)] If $({\mathcal{X}},\tau)$ is compact, then $({\mathcal{P}}^p({\mathcal{X}}),{\mathcal{W}}^p)$ is compact.
\end{itemize}
\end{lemma}

We conclude this section with some observations that will be needed in later arguments.

\begin{lemma} \label{trivial_bound}
For any $f,g \in {\mathcal{S}}$, $d(f,g) \leq 2$.
\end{lemma}

\begin{proof}
Pick representatives $f,g \in S$ and let $\phi : \varnothing \to {\mathbb{N}} \times {\mathbb{Z}}^d$ be the empty isometry.
Then
{\begin{align*} {
d(f,g) \leq d_\phi(f,g) = \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u)^2 + \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} g(u)^2 \leq 2.
} \end{align*}}
\end{proof}

The next lemma concerns measure-theoretic properties of the spaces $S$ and ${\mathcal{S}}$.
In particular, $S$ is considered as a subset of
{\begin{align*} {
L^1({\mathbb{N}} \times {\mathbb{Z}}^d) = \{f : {\mathbb{N}} \times {\mathbb{Z}}^d \to {\mathbb{R}} : \|f\| < \infty\},
} \end{align*}}
on which there is the standard $L^1$ metric induced by the norm $\|\cdot\|$ extending \eqref{norm_def}:
{\begin{align*} {
\|f\| := \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} |f(u)|, \quad f \in L^1({\mathbb{N}} \times {\mathbb{Z}}^d).
} \end{align*}}

\begin{lemma} \label{S_meas}
Consider the metric spaces $L^1({\mathbb{N}} \times {\mathbb{Z}}^d)$ and ${\mathcal{S}}$ with their Borel $\sigma$-algebras.
Then the following statements hold: 
\begin{itemize}
\item[(a)] $S$ is a measurable subset of $L^1({\mathbb{N}} \times {\mathbb{Z}}^d)$ and is thus itself a measurable space with the subspace $\sigma$-algebra.
\item[(b)] The quotient map $\iota: S \to {\mathcal{S}}$ that sends $f \in S$ to its equivalence class in ${\mathcal{S}}$ is measurable.
\end{itemize}
\end{lemma}

\begin{proof}
To show $S \subset L^1({\mathbb{N}} \times {\mathbb{Z}}^d)$ is measurable, we express $S$ as the countable intersection of measurable sets:
{\begin{align*} {
S = \{f \in L^1({\mathbb{N}} \times {\mathbb{Z}}^d) : \|f\| \leq 1\} \cap \bigg(\bigcap_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \{f \in L^1({\mathbb{N}} \times {\mathbb{Z}}^d) : f(u) \geq 0\}\bigg).
} \end{align*}}
To next show $\iota$ is measurable, it suffices to verify that the inverse image of any open ball is measurable.
For $f \in {\mathcal{S}}$ and $r > 0$, we write
{\begin{align*} {
B_r(f) := \{g \in {\mathcal{S}} : d(f,g) < r\}.
} \end{align*}}
Notice that
{\begin{align*} {
\iota^{-1}(B_r(f)) = \bigcup_{\phi}\ \{g \in S : d_\phi(f,g) < r\},
} \end{align*}}
where the union is over isometries with \textit{finite} domains.
The union occurs, therefore, over a countable set.
For each $\phi$, it is clear from \eqref{d_def} that $d_\phi(f,\, \cdot\, )$ is a measurable function on $S$, and so each set in the union is measurable.
Being a countable union of measurable sets, $\iota^{-1}(B_r(f))$ is measurable.
\end{proof}

\section{Comparison with the Mukherjee--Varadhan topology} \label{compare_topologies}
This section accomplishes two goals: (i) adapt the compactification technique of Mukherjee and Varadhan \cite{mukherjee-varadhan14} to measures on ${\mathbb{Z}}^d$, and (ii) prove that the metric in this adaptation, which is defined in terms of suitable test functions, is equivalent to the metric $d$.
None of the facts proved here are needed in the rest of our study, and the reader will not encounter any lapse of presentation by skipping to Section \ref{transformation}.  
Rather, the results of this section are included to verify that our methods may achieve the same effect as those initiated in \cite{mukherjee-varadhan14}, while offering a more tractable metric with which to work.
Indeed, this is one way our approach capitalizes on the countability of ${\mathbb{Z}}^d$, although
the discussion that follows underscores the possibility that the abstract machinery can be made more general.

\subsection{Adaptation to the lattice}
In this preliminary section, we convert the Mukherjee--Varadhan setup to the discrete setting.
Aside from the proof of Proposition \ref{not_pseudo}, the construction is completely parallel to the one in \cite{mukherjee-varadhan14}.
For the sake of the ambitious reader, we mirror the notation of that manuscript as closely as possible.
We will use boldface ${{\boldsymbol {x}}} = (x_1,x_2,\dots,x_k)$ to denote a vector in $({\mathbb{Z}}^d)^k$.
For ${{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k$ and $z \in {\mathbb{Z}}^d$, we use the notation
{\begin{align*} {
{{\boldsymbol {x}}}+z := (x_1+z,x_2+z,\ldots,x_k+z).
} \end{align*}}
For an integer $k \ge 2$, call a function $W : ({\mathbb{Z}}^d)^k \to {\mathbb{R}}$ \textit{translation invariant} if
{\begin{align} \begin{split} { \label{trans_invariant}
W({{\boldsymbol {x}}}+z) = W({{\boldsymbol {x}}}) \quad \text{for all ${{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k,\ z \in {\mathbb{Z}}^d$}.
} \end{split} \end{align}}
We will say such a function $W$ \textit{vanishes at infinity} if
{\begin{align} \begin{split} { \label{vanish_inf}
\lim_{\max_{i \neq j} \|x_i - x_j\|_1 \to \infty} W({{\boldsymbol {x}}}) = 0.
} \end{split} \end{align}}
Now let ${\mathcal{I}}_k$ denote the set of functions $W : ({\mathbb{Z}}^d)^k \to {\mathbb{R}}$ that are both translation invariant and vanishing at infinity.
The space ${\mathcal{I}}_k$ is naturally equipped with a metric by the uniform norm,
{\begin{align*} {
\|W\|_\infty := \sup_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}).
} \end{align*}}
The condition \eqref{trans_invariant} means that $W$ depends only on the $k-1$ variables $x_2-x_1,x_3-x_1,\dots,x_{k}-x_{1}$.
That is,
{\begin{align*} {
W(x_1,x_2,\dots,x_k) = w(x_2-x_1,x_3-x_1,\dots,x_{k}-x_{1}),
} \end{align*}}
where $w : ({\mathbb{Z}}^d)^{k-1} \to {\mathbb{R}}$ is given by
{\begin{align*} {
w(y_1,y_2,\dots,y_{k-1}) = W(0,y_1,y_2,\dots,y_{k-1}).
} \end{align*}}
Then \eqref{vanish_inf} is equivalent to
{\begin{align} \begin{split} { \label{vanish_inf_2}
\lim_{\max_{1 \le i \le k-1} \|y_i\|_1 \to \infty} w({{\boldsymbol {y}}}) = 0.
} \end{split} \end{align}}
Since the space of functions $w : ({\mathbb{Z}}^d)^{k-1} \to {\mathbb{R}}$ satisfying \eqref{vanish_inf_2} is separable, it follows that ${\mathcal{I}}_k$ is separable.

The space of test functions will be 
{\begin{align*} {
{\mathcal{I}} := \bigcup_{k \ge 2} {\mathcal{I}}_k.
} \end{align*}}
As each ${\mathcal{I}}_k$ is separable, we can find
a countable dense subset $(W_r)_{r \in {\mathbb{N}}}$ of ${\mathcal{I}}$,
where $W_r \in {\mathcal{I}}_{k_r}$.
Notice that for any $W \in {\mathcal{I}}_k$ and any $f \in {\mathcal{S}}$, the quantity
{\begin{align*} {
I(W,f) := \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
} \end{align*}}
does not depend on the representative $f$ chosen from $S$.
(For the sake of exposition, we note that $I(W,f)$ is simply the sum of countably many integrals of $W$, the $n$-th integral occurring  on the product space $(({\mathbb{Z}}^d)^k,f^{\otimes k}(n,\cdot))$,
where $f^{\otimes k}(n,\cdot)$ is the product measure whose every marginal has $f(n,\cdot)$ as its probability mass function.)
Indeed, if $f, g \in S$ are such that $d(f,g) = 0$, then by Lemma \ref{better_def_cor} there exists a bijection $\sigma$ between their ${\mathbb{N}}$-supports (denoted $H_f$ and $H_g$, respectively) and a collection $(z_n)_{n \in H_f}$ in ${\mathbb{Z}}^d$ such that
{\begin{align*} {
f(n,x) = g(\sigma(n),x-z_n), \quad x \in {\mathbb{Z}}^d.
} \end{align*}}
In this case, \eqref{trans_invariant} gives
{\begin{align*} {
\sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
&= \sum_{n \in H_f} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\sigma(n),x_i-z_n) \\
&= \sum_{n \in H_f} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i)\\
&= \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(n,x_i).
} \end{align*}}
So $I(W,\cdot)$ is well-defined on ${\mathcal{S}}$.
We can thus define the following metric on ${\mathcal{S}}$:
{\begin{align*} {
D(f,g) := \sum_{r = 1}^\infty \frac{1}{2^r}\frac{1}{1+\|W_r\|_\infty} |I(W_r,f) - I(W_r,g)|.
} \end{align*}}
Since the family $\{I(\cdot,f): f \in {\mathcal{S}}\}$ is uniformly equicontinuous,
{\begin{align*} {
|I(W_1,f) - I(W_2,f)| &= \bigg| \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} (W_1({{\boldsymbol {x}}}) - W_2({{\boldsymbol {x}}})) \prod_{i = 1}^k f(n,x_i)\bigg| \\
&\leq \|W_1-W_2\|_\infty \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} \sum_{i = 1}^k f(n,x_i) \\
&= \|W_1-W_2\|_\infty \sum_{n \in {\mathbb{N}}} \bigg(\sum_{x \in {\mathbb{Z}}^d} f(n,x)\bigg)^k \\
&\leq \|W_1-W_2\|_\infty \sum_{n \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d} f(n,x) 
\leq \|W_1-W_2\|_\infty,
} \end{align*}}
and $(W_r)_{r \in {\mathbb{N}}}$ is dense in ${\mathcal{I}}$, convergence in this metric implies convergence for \textit{all} test functions:
{\begin{align} \begin{split} { \label{convergence_criterion}
\lim_{j \to \infty} D(f_j,f) = 0 \quad \Leftrightarrow \quad
\lim_{j \to \infty} I(W,f_j) = I(W,f) \quad \text{for all $W \in {\mathcal{I}}$.}
} \end{split} \end{align}}
It is clear that $D$ is reflexive and satisfies the triangle inequality.
It is nontrivial, however, that $D$ separates points.

\begin{prop} \label{not_pseudo}
For $f,g \in S$, $D(f,g) = 0$ if and only if $d(f,g) = 0$.
\end{prop}

Therefore, $D$ is indeed a metric on ${\mathcal{S}}$. The topology induced by $D$ on ${\mathcal{S}}$ is the lattice analog of the topology defined in Mukherjee and Varadhan~\cite{mukherjee-varadhan14}.

To prove Proposition \ref{not_pseudo}, we will use the lemma below.
Recall that a sequence of real numbers $(a_j)$ \textit{lexicographically dominates} $(a_j')$ 
(which we denote by $(a_j) \succeq (a_j')$) 
if $a_j > a_j'$ for the smallest $j$ for which $a_j \neq a_j'$. 
Of course, if there is no such $j$, then the two sequences are equal. If the two sequences are not equal, then we will say that $(a_j)$ strictly dominates $(a_j')$, and write $(a_j) \succ (a_j')$.

We will say a collection of sequences $\{(a_{i,\, j})\}$ is lexicographically descending ``in $i$" if
{\begin{align*} {
i \leq i' \quad \Rightarrow \quad (a_{i,\, j}) \succeq (a_{i',\, j}).
} \end{align*}}
Given an infinite collection of sequences $\{(a_{i,\, j}) : i \in {\mathbb{N}}\}$, it is not always possible to rearrange the $i$-indices so that the collection is lexicographically descending.
One can easily check, however, that rearrangement  is possible for \textit{nonnegative} sequences satisfying the following condition:
{\begin{align} \begin{split} {
|\{i : a_{i,\, j} > {\varepsilon}\}|,\,  |\{i : b_{i,\, j} > {\varepsilon}\}| < \infty \quad \text{for all ${\varepsilon} > 0$, $j \geq 1$.} \label{all_j_zero}
} \end{split} \end{align}}

\begin{lemma} \label{powers_lemma}
Let $\{(a_{i,\, j})_{j = 1}^\infty : 1 \leq i \leq N_1\}$ and $\{(b_{i,\, j})_{j = 1}^\infty : 1 \leq i \leq N_2\}$ be two collections of sequences in $[0,1]$, where $N_1,N_2 \in {\mathbb{N}} \cup \{\infty\}$.
Suppose that \eqref{all_j_zero} holds and
{\begin{align} \begin{split} {
a_{i,\, 1},\, b_{i,\, 1} > 0 \quad \text{for all $i$,} \label{first_terms_pos}
} \end{split} \end{align}}
so that we may assume each collection is lexicographically descending in $i$.
If, for every $\ell \in {\mathbb{N}}$,
{\begin{align} \begin{split} {
\sum_{i = 1}^{N_1} a_{i,\, 1}\prod_{j = 1}^\ell a_{i,\, j}^{p_j} = \sum_{i = 1}^{N_2} b_{i,\, 1}\prod_{j = 1}^\ell b_{i,\, j}^{p_j} < \infty 
\quad \text{for all integers $p_1,\dots,p_\ell \geq 0$ with $\sum_{j=1}^\ell p_j \geq 1$,} \label{tests_agree}
} \end{split} \end{align}}
then $N_1 = N_2$, and $a_{i,\, j} = b_{i,\, j}$ for every $i$ and $j$.
\end{lemma}

\begin{proof}
We will show by induction that for each finite $\ell$, $a_{i,\, \ell} = b_{i,\, \ell}$ for all $i$.
First consider the case when $\ell = 1$.
Since $a_{1,\, 1} = \max_i a_{i,\, 1}$ and $b_1 = \max_i b_{i,\, 1}$, we have
{\begin{align*} {
a_{1,\, 1} = \lim_{p \to \infty} \bigg(\sum_{i = 1}^{N_1} a_{i,\, 1}^p\bigg)^{1/p}
\stackrel{\eqref{tests_agree}}{=} \lim_{p \to \infty} \bigg(\sum_{i = 1}^{N_2} b_{i,\, 1}^p\bigg)^{1/p}
= b_{1,\, 1}.
} \end{align*}}
But then this argument can be repeated with the sequences $\{a_{i,\, 1} : 2 \leq i \leq N_1\}$ and $\{b_{i,\, 1} : 2 \leq i \leq N_2\}$ to obtain $a_{2,\, 1} = b_{2,\, 1}$.
Continuing in this way, one exhaustively determines that $N_1 = N_2 = N$, and $a_{i,\, 1} = b_{i,\, 1}$ for every $i$.

For $\ell \geq 2$, 
assume by induction that for each $j \leq \ell-1$, we have $a_{i,\, j} = b_{i,\, j}$ for every $i$.
By hypothesis \eqref{tests_agree}, for any nonnegative integers $q_1,q_2,\dots,q_{\ell-1}$ with $q_1 \geq 1$,
{\begin{align} \begin{split} { \label{max_agree}
\max_{i} \bigg(a_{i,\, \ell} \prod_{j = 1}^{\ell-1} a_{i,\, j}^{q_j}\bigg)
&= \lim_{p \to \infty} \Bigg[\sum_{i = 1}^N \bigg( a_{i,\, \ell} \prod_{j = 1}^{\ell-1} a_{i,\, j}^{q_j}\bigg)^p\Bigg]^{1/p} \\
&= \lim_{p \to \infty} \Bigg[\sum_{i = 1}^N \bigg( b_{i,\, \ell} \prod_{j = 1}^{\ell-1} b_{i,\, j}^{q_j}\bigg)^p\Bigg]^{1/p}
= \max_{i} \bigg(b_{i,\, \ell} \prod_{j = 1}^{\ell-1} b_{i,\, j}^{q_j}\bigg).
} \end{split} \end{align}}
We now specify $q_1,q_2,\dots,q_{\ell-1}$ via the following backward induction:
\begin{itemize}
\item If $a_{1,\, \ell-1} = 0$, then set $q_{\ell-1} = 0$.
Otherwise, take $q_{\ell-1} = 1$.
\item Given $q_{\ell-1},q_{\ell-2},\dots,q_{k+1}$, choose $q_k$ as follows:
\begin{itemize}
\item If $a_{1,\, k} = 0$, then set $q_{k} = 0$.
\item Otherwise, consider all $i$ such that the sequences $(a_{i,\, j})_{j = 1}^{\ell-1}$ and $(a_{1,\, j})_{j=1}^{\ell-1}$ first differ when $j = k$.
If it exists, the smallest such $i$, call it $i_k$, will maximize $a_{i,\, k} < a_{1,\, k}$  (in particular, $a_{1,\, k} > 0$).
We then take $q_k$ sufficiently large that
{\begin{align} \begin{split} {
\Big(\frac{a_{i_k,\,k}}{a_{1,\, k}}\Big)^{q_k} < \prod_{j = k+1}^{\ell-1} a_{1,\, j}^{q_j}.
\label{prod_construction}
} \end{split} \end{align}}
If no such $i$ exists, then set $q_k = 0$.
Notice that \eqref{first_terms_pos} and \eqref{all_j_zero} force $q_1 \geq 1$.
\end{itemize}
\end{itemize}
Having defined $q_1,q_2,\dots,q_{\ell-1}$ to satisfy \eqref{prod_construction}, we obtain the following implication:
\begin{align}
(a_{1,\, j})_{j = 1}^{\ell-1} \succ (a_{i,\, j})_{j = 1}^{\ell-1} \quad &\Rightarrow \quad
\exists\ k,\text{ $a_{1,\, j}$ and $a_{i,\, j}$ first differ at $j = k$} \label{differ} \\
&\Rightarrow \quad
 \prod_{j = 1}^{\ell-1} \frac{a_{i,\, j}^{q_j}}{a_{1,\, j}^{q_j}}
 =  \prod_{j = k}^{\ell-1} \frac{a_{i,\, j}^{q_j}}{a_{1,\, j}^{q_j}}
\leq \Big(\frac{a_{i_k,\, k}}{a_{1,\, k}}\Big)^{q_k} \prod_{j = k+1}^{\ell-1} \frac{1}{a_{1,\, j}^{q_j}} < 1 \label{k_bound} \\
&\Rightarrow \quad \lim_{p \to \infty} \bigg(\prod_{j = 1}^{\ell-1} \frac{a_{i,\, j}^{q_j}}{a_{1,\, j}^{q_j}}\bigg)^p = 0. \label{ratio_to_0}
\end{align}
Moreover, because the first inequality in \eqref{k_bound} holds for all $i$ for which \eqref{differ} is true, the convergence in \eqref{ratio_to_0} is uniform over such $i$.
It follows that
{\begin{align*} {
\lim_{p \to \infty} \max_{i} \Bigg[a_{i,\, \ell} \bigg(\prod_{j = 1}^{\ell-1} \frac{a_{i,\, j}^{q_j}}{a_{1,\, j}^{q_j}}\bigg)^p\Bigg] = a_{1,\, \ell}.
} \end{align*}}
Since the choice of $q_1,q_2,\dots,q_{\ell-1}$ depended only on the $a_{i,\, j}$ with $j \leq \ell-1$, the induction hypothesis gives the same result for the $b$-collection:
{\begin{align*} {
\lim_{p \to \infty} \max_{i} \Bigg[b_{i,\, \ell}\bigg(\prod_{j = 1}^{\ell-1} \frac{b_{i,\, j}^{q_j}}{b_{1,\, j}^{q_j}}\bigg)^p\Bigg] = b_{1,\, \ell}.
} \end{align*}}
Now \eqref{max_agree}, with the fact that $a_{1,\, j} = b_{1,\, j}$ for $j \leq \ell-1$, allows us to conclude $a_{1,\, \ell} = b_{1,\, \ell}$.
As in the $\ell = 1$ case, we can repeat the above argument with the collections $\{(a_{i,\, j})_{j = 1}^\ell : 2 \leq i \leq N\}$ and $\{(b_{i,\, j})_{j = 1}^\ell : 2 \leq i \leq N\}$ to determine $a_{2,\, \ell} = b_{2,\, \ell}$.
Indeed, induction gives $a_{i,\, \ell} = b_{i,\, \ell}$ for every $i$.
\end{proof}

\begin{proof}[Proof of Proposition \ref{not_pseudo}]
The ``if" direction follows from the fact that $D$ is well-defined.
For the converse, we assume $D(f,g) = 0$ and prove $d(f,g) = 0$.
Let $\{z_1,z_2,\dots\}$ be any enumeration of ${\mathbb{Z}}^d$.
Given integers $\ell \geq 1$ and $p_1,p_2,\dots,p_\ell \geq 0$ with $k:= 1 + \sum_{j=1}^\ell p_j \geq 2$, consider the following member of ${\mathcal{I}}_k$:
{\begin{align*} {
W({{\boldsymbol {x}}}) := \begin{cases}
1 &\text{if }x_{i} - x_1 = z_j \quad \text{for all $1+\sum_{t = 1}^{j-1} p_t < i \leq 1+ \sum_{t = 1}^{j} p_t$, \quad $1 \leq j \leq \ell$}, \\
0 &\text{else}.
\end{cases}
} \end{align*}}
Since $D(f,g) = 0$, we have
{\begin{align} \begin{split} {
I(W,f) = \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u) \prod_{j = 1}^\ell f(u+z_j)^{p_j}
= \sum_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} g(v) \prod_{j = 1}^\ell g(v+z_j)^{p_j}
= I(W,g). \label{tests_agree_2}
} \end{split} \end{align}}
Furthermore, these quantities are finite:
{\begin{align} \begin{split} { \label{finiteness}
\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u) \prod_{j = 1}^\ell f(u+z_j)^{p_j}
\leq \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u) \leq 1.
} \end{split} \end{align}}
Now we lexicographically order (descending in $i$) the sequences given by
{\begin{align*} {
a_{i,\, j} := f(u_i+z_j), \quad f(u_i) > 0, \qquad
b_{i,\, j} := g(v_i+z_j), \quad g(v_i) > 0,
} \end{align*}}
for which \eqref{all_j_zero} is true because $\|f\|, \|g\| \leq 1$, and \eqref{tests_agree} is equivalent to \eqref{tests_agree_2}--\eqref{finiteness}.
Therefore, Lemma \ref{powers_lemma} shows
{\begin{align} \begin{split} {
f(u_i+z_j) = g(v_i+z_j) \quad \text{for all $i$ and $j$}. \label{lemma_consequence}
} \end{split} \end{align}}
Let $H_f$ and $H_g$ denote the ${\mathbb{N}}$-supports of $f$ and $g$, respectively.
Since $z_j$ ranges over all of ${\mathbb{Z}}^d$, \eqref{lemma_consequence} implies that for every $n \in N_f$, there is $m \in H_g$ such that $f(n,\cdot)$ and $g(m,\cdot)$ are translates of each another.
The proof that $d(f,g) = 0$ will be complete if we can show that for each $n \in H_f$, a \textit{distinct} $m \in H_g$ can be chosen, since then we would have an injection $\sigma : H_f \to H_g$ such that $f(n,\cdot)$ and $g(\sigma(n),\cdot)$ are always translates.
By interchanging $f$ and $g$, we would then see that $\sigma$ is necessarily a bijection, and so Lemma \ref{better_def_cor} gives $d(f,g) = 0$.

We now verify the final fact needed from above: $m \in H_g$ can be chosen distinctly for each $n \in H_f$.
Suppose that $f(n_1,\cdot),f(n_2,\cdot),\dots,f(n_K,\cdot)$ are translates of one another, where $n_1,\dots,n_K$ are distinct elements of $H_f$.
Even when $K$ is chosen maximally, $\|f\| \leq 1$ forces $K$ to be finite.
We can choose indices $i_1,i_2,\dots,i_K$ to simultaneously ``align" all these translates:
\begin{align}
u_{i_k} \in \{n_k\} \times {\mathbb{Z}}^d \quad \text{and} \quad
f(u_{i_k}+z_j) &= f(u_{i_1}+z_j) \quad \text{for all $j \geq 1$, $k = 1,2,\dots,K$.} \nonumber
\intertext{By \eqref{lemma_consequence}, we then have}
g(v_{i_k}+z_j) &= g(v_{i_1}+z_j)  \quad \text{for all $j \geq 1$, $k = 1,2,\dots,K$.} \label{duplicate_copies}
\end{align}
Since $n_1,\dots,n_K$ are distinct, so too are $i_1,\dots,i_K$, and therefore $v_{i_1},\dots,v_{i_K}$ are distinct.
Upon writing $v_{i_k} = (m_k,y_k)$, we claim that $m_k \neq m_\ell$ for $k \neq \ell$.
Indeed, if $m_k = m_\ell$, then distinctness forces $y_k \neq y_\ell$.
Therefore, $z:=y_\ell-y_k$ is nonzero and satisfies $v_{i_\ell} = v_{i_k}+z$.
In particular,
{\begin{align*} {
g(v_{i_\ell}) = g(v_{i_k}+z) \stackrel{\eqref{duplicate_copies}}{=} g(v_{i_\ell}+z).
} \end{align*}}
More generally, for any integer $q$,
{\begin{align*} {
g(v_{i_\ell}+qz) = g(v_{i_k} + (q+1)z) \stackrel{\eqref{duplicate_copies}}{=} g(v_{i_\ell}+(q+1)z).
} \end{align*}}
Since $g(v_{i_\ell}) > 0$, it follows that
{\begin{align*} {
\sum_{q = 0}^\infty g(v_{i_\ell}+qz) = \sum_{q = 0}^\infty g(v_{i_\ell}) = \infty,
} \end{align*}}
an obvious contradiction to $\|g\| \leq 1$.
Now each $f(n_k,\cdot)$ is a translate of $g(m_k,\cdot)$, and $m_1,m_2,\dots,m_K$ are all distinct, as desired.
\end{proof}

\subsection{Equivalence of the metrics}
The metric $D$ does, in fact, give rise to a compact topology on ${\mathcal{S}}$.
Rather than prove this directly, though, we first show that $d$ induces a topology at least as fine as the one induced by $D$.
As the continuous image of a compact set is compact, this immediately implies $D$ also induces a compact topology.  
But the result actually implies more: The topologies are necessarily equivalent.

\begin{prop} \label{equal_topologies}
$D(f_j,f) \to 0$ if and only if $d(f_j,f) \to 0$.
\end{prop}

The following topological fact reduces the proof of Proposition \ref{equal_topologies} to showing only one direction of the equivalence.

\begin{lemma}[see \cite{munkres00}, Theorem 26.6] \label{topology_fact}
Suppose ${\mathcal{X}}$ is a compact topological space, and ${\mathcal{Y}}$ is a Hausdorff topological space. 
If $F : {\mathcal{X}} \to {\mathcal{Y}}$ is bijective and continuous, then $F$ must be a homeomorphism.
\end{lemma}

In the present setting, we consider ${\mathcal{X}} = ({\mathcal{S}},d)$ and ${\mathcal{Y}} = ({\mathcal{S}},D)$.
With $F$ equal to the identity map, Lemma \ref{topology_fact} says the following: If $d(f_j,f) \to 0$ implies $D(f_j,f) \to 0$, then the converse is also true.

\begin{proof}[Proof of Proposition \ref{equal_topologies}]
By Lemma \ref{topology_fact}, it suffices to show that if $d(f_j,f) \to 0$, then $D(f_j,f) \to 0$.
And by \eqref{convergence_criterion}, it suffices to check that given any $W \in {\mathcal{I}}$, we have
$I(W,f_j) \to I(W,f)$.
So consider any $W \in {\mathcal{I}}_k$, and let ${\varepsilon} > 0$ be given.
We seek a number $\delta > 0$ such that for any $g \in {\mathcal{S}}$,
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad |I(W,f) - I(W,g)| < {\varepsilon}. 
} \end{align*}}
This is trivial if $W$ is constant zero, and so we will henceforth assume $\|W\|_\infty > 0$.
By \eqref{vanish_inf}, there is $K$ large enough that
{\begin{align} \begin{split} { \label{K_choice}
\max_{i \neq j} \|x_i - x_j\|_1 \geq K \quad \Rightarrow \quad |W({{\boldsymbol {x}}})| < \frac{\varepsilon}{8}.
} \end{split} \end{align}}
Upon fixing a representative $f \in S$, we can take $N \in {\mathbb{N}}$ such that
{\begin{align} \begin{split} { \label{small_after_N}
\sum_{n = N+1}^\infty \sum_{x \in {\mathbb{Z}}^d} f(n,x) < \frac{\varepsilon}{8k\|W\|_\infty}.
} \end{split} \end{align}} 
Next, for each $1 \leq n \leq N$, we choose $A_n \subset {\mathbb{Z}}^d$ finite but large enough that
{\begin{align} \begin{split} { \label{small_before_N}
\sum_{x \notin A_n} f(n,x) < \frac{\varepsilon}{8kN\|W\|_\infty}.
} \end{split} \end{align}}
Now define $A := \bigcup_{n = 1}^N (\{n\} \times A_n)$, so that \eqref{small_after_N} and \eqref{small_before_N} together show
{\begin{align} \begin{split} { \label{small_total}
\sum_{u \notin A} f(u) < \frac{\varepsilon}{4k\|W\|_\infty}.
} \end{split} \end{align}}
By possibly omitting some elements of $A$ and/or taking $N$ smaller, we may assume $f$ is strictly positive on $A$.
We may also assume $K \geq \sup_{1 \leq n \leq N} \operatorname{diam}(A_n)$, since \eqref{K_choice} still holds if $K$ is made larger.
Now we choose $\delta > 0$ satisfying
\begin{align}
\delta &< \inf_{u \in A} f(u)^2, \label{delta_condition_1} \\
\delta &< 2^{-K}, \label{delta_condition_2} \\
\delta &< \frac{\varepsilon}{8k\max\{1,|A|^{k-1}\}\|W\|_\infty}, \label{delta_condition_3} \\
\sqrt{\delta} &< \frac{\varepsilon}{8k(2K)^{(k-1)d}\|W\|_\infty} \label{delta_condition_4}.
\end{align}
If $d(f,g) < \delta$, then there is a representative $g \in S$ and an isometry $\phi : C \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $d_\phi(f,g) < \delta$.
The condition \eqref{delta_condition_1} implies $A \subset C$, while \eqref{delta_condition_2} guarantees that 
{\begin{align} \begin{split} {
\deg(\phi) > K \geq \operatorname{diam}(A_n) \quad \text{for any $n \leq N$.} \label{deg_big_enough}
} \end{split} \end{align}}
Consequently, $\phi$ acts by translation on $A_n$. 
That is, for each $n = 1,2,\dots,N$, there is $\sigma(n) \in {\mathbb{N}}$ and $z_n \in {\mathbb{Z}}^d$ so that
{\begin{align} \begin{split} {
\phi(n,x) = (\sigma(n),x + z_n) \quad \text{for all $x \in A_n$.} \label{An_translate}
} \end{split} \end{align}}
(Here $n \mapsto \sigma(n)$ may not be injective.) We thus have
{\begin{align} \begin{split} { \label{triple_sum}
|I(W,f) - I(W,g)| &=
\bigg|\sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
- \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(n,x_i)\bigg| \\
&\leq D_1 + D_2 + D_3,
} \end{split} \end{align}}
where
\begin{align}
D_1 &:= \bigg|\sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
- \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)\bigg| \label{D1} \\
D_2 &:= \bigg|\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
- \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i+z_n)\bigg| \nonumber \\
D_3 &:= \bigg|\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i+z_n)
- \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(n,x_i)\bigg|. \nonumber
\end{align}
We shall produce an upper bound for each of $D_1$, $D_2$, and $D_3$, and then use \eqref{triple_sum} to yield the desired result.

First, for $D_1$, notice that the summand  $W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)$ will appear in the first sum of \eqref{D1} but not the second if and only if ${{\boldsymbol {x}}} \notin A_{n}^k$ or $n > N$.
Considering the first case, we observe that ${{\boldsymbol {x}}} \notin A_n^k$ if and only if some $x_j$ does not belong to $A_n$. 
Hence
{\begin{align} \begin{split} { \label{bound_1_1}
\bigg| \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \notin A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i) \bigg|
&\leq \|W\|_\infty \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \notin A_n^k} \prod_{i = 1}^k f(n,x_i) \\
&= \|W\|_\infty \sum_{n = 1}^N \sum_{j = 1}^k \sum_{x_j \notin A_n} f(n,x_j) \sum_{{{\boldsymbol {y}}} \in ({\mathbb{Z}}^d)^{k-1}} \prod_{i = 1}^{k-1} f(n,y_i) \\
&= \|W\|_\infty \sum_{n = 1}^N \sum_{j = 1}^k \sum_{x \notin A_n} f(n,x) \bigg(\sum_{y \in {\mathbb{Z}}^d} f(n,y)\bigg)^{k-1} \\
&\leq k\|W\|_\infty \sum_{n = 1}^N \sum_{x \notin A_n} f(n,x)
< \frac{\varepsilon}{8},
} \end{split} \end{align}}
where the final inequality is a consequence of \eqref{small_before_N}. 
Considering the second case, we have
{\begin{align} \begin{split} { \label{bound_1_2}
\bigg| \sum_{n = N+1}^\infty \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i) \bigg|
&\leq \|W\|_\infty \sum_{n = N+1}^\infty \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} \prod_{i = 1}^k f(n,x_i) \\
&= \|W\|_\infty \sum_{n = N+1}^\infty \bigg(\sum_{x \in {\mathbb{Z}}^d} f(n,x)\bigg)^k \\
&\leq \|W\|_\infty \sum_{n = N+1}^\infty \sum_{x \in {\mathbb{Z}}^d} f(n,x) < \frac{\varepsilon}{8k} < \frac{\varepsilon}{8},
} \end{split} \end{align}}
where we have used \eqref{small_after_N} in the penultimate inequality.
Together, \eqref{bound_1_1} and \eqref{bound_1_2} yield
{\begin{align} \begin{split} { \label{bound_1}
D_1 < \frac{\varepsilon}{4}.
} \end{split} \end{align}}
Next we analyze the second difference, $D_2$, from \eqref{triple_sum}.
Here we use the translation invariance of $W$:
Making use of \eqref{An_translate}, we determine that
{\begin{align} \begin{split} { \label{before_telescope}
D_2 &= \bigg|\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
- \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i+z_n)\bigg| \\
&= \bigg|\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k f(n,x_i)
- \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\phi(n,x_i))\bigg| \\
&\leq \|W\|_\infty \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} \bigg|\prod_{i = 1}^k f(n,x_i) - \prod_{i = 1}^k g(\phi(n,x_i))\bigg|.
} \end{split} \end{align}}
For $1 \leq n \leq N$ and ${{\boldsymbol {x}}} \in A_n^k$, we can use a telescoping sum to write
{\begin{align*} {
&\bigg|\prod_{i = 1}^k f(n,x_i) - \prod_{i = 1}^k g(\phi(n,x_i))\bigg| \\
&= \bigg|\sum_{i = 1}^k f(n,x_1)\cdots f(n,x_{i-1})\big(f(n,x_i) - g(\phi(n,x_i))\big) g(\phi(n,x_{i+1}))\cdots g(\phi(n,x_k))\bigg| \\
&\leq\sum_{i = 1}^k |f(n,x_i) - g(\phi(n,x_i))|.
} \end{align*}}
Therefore, \eqref{before_telescope} becomes 
{\begin{align*} {
D_2 \leq \|W\|_\infty \sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} \sum_{i = 1}^k |f(n,x_i) - g(\phi(n,x_i))|.
} \end{align*}}
Now, given any $u = (n,x) \in A$, the summand $|f(u) - g(\phi(u))|$ will appear $k|A_n|^{k-1}$ times in the above sum: There must be some $i$ for which $x_i = x$, and the remaining $k-1$ coordinates of ${{\boldsymbol {x}}}$ can be any elements of $A_n$.
Using this fact and \eqref{delta_condition_3}, we arrive at
{\begin{align} \begin{split} { \label{bound_2}
D_2 &\leq k|A|^{k-1}\|W\|_\infty\sum_{u \in A} |f(u) - g(\phi(u))| < \frac{\varepsilon}{8}.
} \end{split} \end{align}}
Finally, we need to bound the third difference, $D_3$, in \eqref{triple_sum}.
Recall the map $n \mapsto \sigma(n)$ from \eqref{An_translate}.
For each $\ell \in {\mathbb{N}}$, consider the partition of $({\mathbb{Z}}^d)^k = J_1(\ell) \cup J_2(\ell)$, where
{\begin{align*} {
J_1(\ell) &:= \bigcup_{1 \leq n \leq N\, :\, \sigma(n) = \ell} \{{{\boldsymbol {x}}} +z_n : {{\boldsymbol {x}}} \in A_n^k\}, \qquad
J_2(\ell) := ({\mathbb{Z}}^d)^k \setminus J_1(\ell).
} \end{align*}}
In this notation, the product $W({{\boldsymbol {y}}}) \prod_{i = 1}^k g(\ell,y_i)$ appears in the sum
{\begin{align*} {
\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i+z_n)
} \end{align*}}
if and only if ${{\boldsymbol {y}}} \in J_1(\ell)$.
Furthermore, in this case it will appear exactly once, since
{\begin{align*} {
\big\{(\sigma(n),x+z_n):x \in A_n\big\} \cap \big\{(\sigma(m),x+z_m):x \in A_m\big\} &= \phi(\{n\} \times A_n) \cap \phi(\{m\} \times A_m) = \varnothing
} \end{align*}} 
for $n \neq m$.
Therefore,
{\begin{align} \begin{split} { \label{bound_3_rewrite}
D_3 &= \bigg|\sum_{n = 1}^N \sum_{{{\boldsymbol {x}}} \in A_n^k} W({{\boldsymbol {x}}}+z_n) \prod_{i = 1}^k g(\sigma(n),x_i+z_n)
- \sum_{n \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(n,x_i)\bigg| \\
&= \bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_2(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg|.
} \end{split} \end{align}}
To analyze this quantity, we consider the further partition $J_2(\ell) = J_3(\ell) \cup J_4(\ell)$, where
{\begin{align*} {
J_3(\ell) := \Big\{{{\boldsymbol {x}}} \in J_2(\ell) : \max_{i \neq j} \|x_i - x_j\|_1 \geq K\Big\}, \qquad
J_4(\ell) := J_2(\ell) \setminus J_3(\ell).
} \end{align*}}
The sum over the various $J_3(\ell)$ is easy to control because of \eqref{K_choice}:
{\begin{align} \begin{split} { \label{bound_3_1}
\bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_3(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg|
< \frac{\varepsilon}{8} \sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_3(\ell)} \prod_{i = 1}^k g(\ell,x_i)
&\leq \frac{\varepsilon}{8} \sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k} \prod_{i = 1}^k g(\ell,x_i) \\
&= \frac{\varepsilon}{8} \sum_{\ell \in {\mathbb{N}}} \bigg(\sum_{x \in {\mathbb{Z}}^d} g(\ell,x)\bigg)^k \\
&\leq \frac{\varepsilon}{8} \sum_{\ell \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d} g(\ell,x)
\leq \frac{\varepsilon}{8}.
} \end{split} \end{align}}
Next considering $J_4(\ell)$, we make the following observation.
If ${{\boldsymbol {x}}} \in J_4(\ell)$, then there must be some coordinate $x_i$ for which $(\ell,x_i) \notin \phi(A)$.
Indeed, if $(\ell,x_i) = \phi(n,x_i')$ and $(\ell,x_j) = \phi(m,x_j')$ both belong to $\phi(A)$ (that is, $x_i' \in A_n$ and $x_j' \in A_m$), then
{\begin{align*} {
{{\boldsymbol {x}}} \in J_4(\ell) \quad \Rightarrow \quad
\|(\ell,x_i) - (\ell,x_j)\|_1 < K \quad \stackrel{\eqref{deg_big_enough}}{\Rightarrow }\quad \|(n,x_i') - (m,x_j')\|_1 < K < \infty
\quad \Rightarrow \quad n = m.
} \end{align*}}
Therefore, if it were the case that every coordinate $x_i$ satisfied $(\ell,x_i) \in \phi(A)$, then ${{\boldsymbol {x}}}$ would belong to $A_n^k + z_n$ for some $n$ such that $\ell = \sigma(n)$.
This contradicts $J_4(\ell) \cap J_1(\ell) = \varnothing$.

We now consider one final (non-disjoint) partition: $J_4(\ell) = J_5(\ell) \cup J_6(\ell)$, where
{\begin{align*} {
J_5(\ell) &:= \{{{\boldsymbol {x}}} \in J_4(\ell) : (\ell,x_i) \in \phi(C \setminus A) \text{ for some $i$}\},
\intertext{and}
J_6(\ell) &:= \{{{\boldsymbol {x}}} \in J_4(\ell) : (\ell,x_i) \notin \phi(C) \text{ for some $i$}\}.
} \end{align*}}
\textit{A priori}, these definitions only imply $J_5(\ell) \cup J_6(\ell) \subset J_4(\ell)$, but the observation of the previous paragraph ensures that $J_5(\ell) \cup J_6(\ell) = J_4(\ell)$.
For the sum over the $J_5(\ell)$, there is a straightforward upper bound:
{\begin{align} \begin{split} {
\bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_5(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\Bigg|
&\leq \|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{j = 1}^k \sum_{{{\boldsymbol {x}}} \in ({\mathbb{Z}}^d)^k\, :\, (\ell,x_j) \in \phi(C\setminus A)} \prod_{i = 1}^k g(\ell,x_i) \\
&= \|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{j = 1}^k \sum_{x_j \in {\mathbb{Z}}^d\, :\, (\ell,x_j) \in \phi(C \setminus A)} g(\ell,x_j) \sum_{{{\boldsymbol {y}}} \in ({\mathbb{Z}}^d)^{k-1}} \prod_{i = 1}^{k-1} g(\ell,y_i) \\
&= k\|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d\, :\, (\ell,x) \in \phi(C \setminus A)} g(\ell,x)\bigg(\sum_{y \in {\mathbb{Z}}^d} g(\ell,y)\bigg)^{k-1} \\
&\leq k\|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d\, :\, (\ell,x) \in \phi(C \setminus A)} g(\ell,x) \\
&= k\|W\|_\infty \sum_{u \in C \setminus A} g(\phi(u)) \\
&\leq k\|W\|_\infty \bigg(\sum_{u \in C \setminus A} f(u) + \sum_{u \in C \setminus A} |f(u) - g(\phi(u))|\bigg) \\
&\leq k\|W\|_\infty\Big(\frac{\varepsilon}{4k\|W\|_\infty} + d_\phi(f,g)\Big)
< \frac{3{\varepsilon}}{8} \label{bound_3_2},
} \end{split} \end{align}}
where we have used \eqref{small_total} and \eqref{delta_condition_3} to establish the final two inequalities.
Now turning our focus to the sum over the $J_6(\ell)$, we define for each $x_1 \in {\mathbb{Z}}^d$ the set
{\begin{align*} {
{\mathcal{N}}(x_1) := \Big\{(x_2,\dots,x_{k}) \in ({\mathbb{Z}}^d)^{k-1} : \max_{1 \leq i < j \leq k} \|x_i - x_j\|_1 < K \Big\}.
} \end{align*}}
Note that
{\begin{align*} {
|{\mathcal{N}}(x_1)| \leq (2K)^{(k-1)d},
} \end{align*}}
since there are no more than $(2K)^d$ elements of ${\mathbb{Z}}^d$ at distance less than $K$ from $x_1$, and each of $x_2,\dots,x_{k}$ must satisfy this property.
By definition, if ${{\boldsymbol {x}}} \in J_4(\ell)$, then for every $j$ we have 
$(x_1,\dots,x_{j-1},x_{j+1},\dots,x_k) \in {\mathcal{N}}(x_j)$.
Therefore, we obtain the bound
{\begin{align} \begin{split} { \label{first_attack}
\bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_6(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\Bigg|
&\leq \|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{j = 1}^{k} \sum_{{{\boldsymbol {x}}} \in J_6(\ell)\, :\, (\ell,x_{j}) \notin \phi(C)}\prod_{i = 1}^{k} g(\ell,x_i) \\
&\leq \|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{j = 1}^k \sum_{x_j \in {\mathbb{Z}}^d\, :\, (\ell,x_j) \notin \phi(C)} \sum_{{{\boldsymbol {y}}} \in {\mathcal{N}}(x_j)} g(\ell,x_j) \prod_{i=1}^{k-1} g(\ell,y_i) \\
&= k\|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{x \in {\mathbb{Z}}^d\, :\, (\ell,x) \notin \phi(C)} \sum_{{{\boldsymbol {y}}} \in {\mathcal{N}}(x)} g(\ell,x) \prod_{i = 1}^{k-1} g(\ell,y_i).
} \end{split} \end{align}}
Now notice that
{\begin{align*} {
{{\boldsymbol {y}}} = (y_1,y_2,\dots,y_{k-1}) \in {\mathcal{N}}(x) \quad \Leftrightarrow \quad
(x,y_2,\dots,y_{k-1}) \in {\mathcal{N}}(y_1),
} \end{align*}}
and that
{\begin{align*} {
(\ell,x) \notin \phi(C) \quad \Rightarrow \quad g(\ell,x) \leq \sqrt{\sum_{u \notin \phi(C)} g(u)^2} \leq \sqrt{d_\phi(f,g)} < \sqrt{\delta}.
} \end{align*}}
Therefore, we can rewrite \eqref{first_attack} as
{\begin{align} \begin{split} {
\bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_6(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\Bigg|
&\leq k\|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{y \in {\mathbb{Z}}^d} \sum_{{{\boldsymbol {x}}} \in {\mathcal{N}}(y)\, :\, (\ell,x_1) \notin \phi(C)} g(\ell,y) \prod_{i = 1}^{k-1} g(\ell,x_i) \\
&\leq k\|W\|_\infty \sum_{\ell \in {\mathbb{N}}} \sum_{y \in {\mathbb{Z}}^d}g(\ell,y) \sum_{{{\boldsymbol {x}}} \in {\mathcal{N}}(y)} \sqrt{\delta}\prod_{i = 2}^{k-1} g(\ell,x_i) \\
&\leq k\|W\|_\infty (2K)^{(k-1)d}\sqrt{\delta} \sum_{\ell \in {\mathbb{N}}} \sum_{y \in {\mathbb{Z}}^d} g(\ell,y) \\
&\leq k\|W\|_\infty (2K)^{(k-1)d}\sqrt{\delta} < \frac{\varepsilon}{8}, \label{bound_3_3}
} \end{split} \end{align}}
where \eqref{delta_condition_4} yields the last inequality.
In light of \eqref{bound_3_rewrite}, \eqref{bound_3_1}--\eqref{bound_3_3} now yield
\begin{align}
D_3 &=\bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_2(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg| \nonumber \\
&\leq \bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_3(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg| + \bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_5(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg|
+ \bigg|\sum_{\ell \in {\mathbb{N}}} \sum_{{{\boldsymbol {x}}} \in J_6(\ell)} W({{\boldsymbol {x}}}) \prod_{i = 1}^k g(\ell,x_i)\bigg| \nonumber \\
&< \frac{\varepsilon}{8} + \frac{3{\varepsilon}}{8} + \frac{\varepsilon}{8} = \frac{5{\varepsilon}}{8}. \label{bound_3}
\end{align}
Then using \eqref{bound_1}, \eqref{bound_2}, and \eqref{bound_3} in \eqref{triple_sum}, we find
{\begin{align*} {
|I(W,f)-I(W,g)| < \frac{\varepsilon}{4} + \frac{\varepsilon}{8} + \frac{5{\varepsilon}}{8} = {\varepsilon},
} \end{align*}}
as desired.
\end{proof}

\section{The update map} \label{transformation}
Consider the law of $\omega_n$ given the random environment $(X_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$. 
In this section we identify said law with an element $f_n \in {\mathcal{S}}$ and define a transformation $T$ that maps $f_n$ to the law of $\omega_{n+1}$ when conditioned on knowledge of the environment only up to time $n$.
In the following, the notation $u \sim v$ is used when $u,v \in {\mathbb{N}} \times {\mathbb{Z}}^d$ satisfy $\|u-v\|_1 = 1$.
The same notation will be used for adjacent elements $x,y \in {\mathbb{Z}}^d$.

\subsection{Definition of the update map} \label{endpoint_distributions}
To make the setup precise, we recall the polymer measure $\rho_n$ defined by \eqref{rho_def} and the endpoint probability mass function $f_n : {\mathbb{Z}}^d \to {\mathbb{R}}$ given by
{\begin{align} \begin{split} {
f_n(x) := \rho_n(\omega_n = x) = \frac{(2d)^{-n}}{Z_n} \sum_{\substack{|\gamma| = n \\ \gamma(0) = 0,\: \gamma(n) = x}} \exp\bigg(\beta\sum_{i = 1}^n X_{i,\, \gamma(i)}\bigg), \label{fn_def}
} \end{split} \end{align}}
where the sum is over nearest-neighbor paths $\gamma : \{0,1,\dots,n\} \to {\mathbb{Z}}^d$ of length $|\gamma| = n$, starting at the origin and ending at $x$.
Then $f_n$ is a $[0,1]$-valued function on ${\mathbb{Z}}^d$ and random with respect to $(\Omega_e,{\mathcal{F}}_n)$, where the sigma-algebra ${\mathcal{F}}_n$ is defined in \eqref{gndef}.
Its value at $x$ gives the probability that a polymer of length $n$ in the random environment has $x$ as its endpoint.

When the polymer is extended by one monomer, the mass function updates to
{\begin{align*} {
f_{n+1}(x) &= \frac{(2d)^{-n-1}}{Z_{n+1}} \sum_{\substack{|\gamma| = n+1 \\ \gamma(0) = 0,\: \gamma(n+1) = x}} \exp\bigg(\beta\sum_{i = 1}^{n+1} X_{i,\, \gamma(i)}\bigg) \\
&= \frac{(2d)^{-n-1}}{Z_{n+1}} \sum_{y\sim x} \sum_{\substack{|\gamma| = n \\ \gamma(0) = 0,\: \gamma(n) = y}} \exp\bigg(\beta\sum_{i = 1}^n X_{i,\, \gamma(i)} + \beta X_{n+1,\, x}\bigg) \\
&= \frac{Z_{n}}{(2d)Z_{n+1}} \sum_{y\sim x} f_{n}(y) e^{\beta X_{n+1,\, x}},
} \end{align*}}
where
{\begin{align} \begin{split} {
2d\, \frac{Z_{n+1}}{Z_{n}} = 2d\, \frac{Z_{n+1}}{Z_n}\sum_{x \in {\mathbb{Z}}^d} f_{n+1}(x) = \sum_{x \in {\mathbb{Z}}^d}\sum_{y\sim x} f_n(y) e^{\beta X_{n+1,\, x}}. \label{Zfrac}
} \end{split} \end{align}}
Recall that $(X_{n+1,\, x})_{x \in {\mathbb{Z}}^d}$ is independent of ${\mathcal{F}}_n$, while $f_n$ is measurable with respect to ${\mathcal{F}}_n$. 
Therefore, the distribution of $f_{n+1}(x)$ given ${\mathcal{F}}_n$ is equal to the distribution of
{\begin{align} \begin{split} {
F_{n+1}(x) := \frac{\sum_{y\sim x} f_{n}(y) e^{\beta Y_x}}{\sum_{z \in {\mathbb{Z}}^d} \sum_{y\sim z} f_{n}(y) e^{\beta Y_z}}, \label{F_predef}
} \end{split} \end{align}}
where $(Y_z)_{z \in {\mathbb{Z}}^d}$ are i.i.d.~random variables distributed according to $\lambda$ and independent of ${\mathcal{F}}_n$.
More generally, for non-random $f \in S$ consider the law of the random variable $F \in {\mathcal{S}}$ whose representative is defined by
{\begin{align} \begin{split} {
F(u) := \frac{\sum_{v \sim u} f(v) e^{\beta Y_u}}{\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} f(v)e^{\beta Y_w} + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y})}, \quad u \in {\mathbb{N}} \times {\mathbb{Z}}^d, \label{F_def}
} \end{split} \end{align}}
where $Y$ and $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ are i.i.d.~random variables, each having law $\lambda$.
Indeed, \eqref{F_def} is a generalization of \eqref{F_predef}, since $\|f_n\| = 1$ for any $n$.
The additional summand in the denominator is needed so that $F$ is defined even when $f \equiv 0$; its precise value is chosen so that
{\begin{align*} {
{\mathbf{E}}\Bigg[\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} f(v) e^{\beta Y_w} + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y})\Bigg] = 2d\, {\mathbf{E}}(e^{\beta Y}) = 2d\, {\mathbf{E}}\, \frac{Z_{n+1}}{Z_n}.
} \end{align*}}
In this way, the ``map" $f \mapsto F$ will be used to approximate the ratio of successive partition functions.
In turn, the free energy can be calculated from these ratios, as shown in Section \ref{free_energy}.
One caveat is that when $F$ is viewed as an element of ${\mathcal{S}}$, its exact value --- even given the  environment $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ --- depends on the representative $f \in S$ chosen. 
But we claim that its law does not.

\begin{prop} \label{same_law}
Suppose $f,g \in S$ satisfy $d(f,g) = 0$.
Define $F$ as in $\eqref{F_def}$, and similarly define
{\begin{align} \begin{split} {
G(u) := \frac{\sum_{v \sim u} g(v) e^{\beta Z_u}}{\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} g(v)e^{\beta Z_w} + 2d(1 - \|g\|){\mathbf{E}}(e^{\beta Y})},\quad u \in {\mathbb{N}} \times {\mathbb{Z}}^d, \label{G_def}
} \end{split} \end{align}}
where the $Z_u$ are i.i.d., each having law $\lambda$.
Then the law of $F \in {\mathcal{S}}$ is equal to the law of $G \in {\mathcal{S}}$.
\end{prop}

\begin{proof}
To show that $F$ and $G$ have the same law, it suffices to exhibit a coupling of the environments $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ and $(Z_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ such that $F = G$ in ${\mathcal{S}}$.
So we let $H_f$ and $H_g$ denote the ${\mathbb{N}}$-supports of $f$ and $g$, respectively, and take
$\sigma : H_f \to H_g$ and $(x_n)_{n \in H_f}$ as in Corollary \ref{better_def_cor}, so that \eqref{better_def} holds.
Define $\psi : H_f \times {\mathbb{Z}}^d \to H_g \times {\mathbb{Z}}^d$ by $\psi(n,x) := (\sigma(n),x-x_n)$, and
let $Z_u$ be equal to $Y_{\psi^{-1}(u)}$ whenever $u \in H_g \times {\mathbb{Z}}^d$.
Otherwise, we may take $Z_u$ to be an independent copy of $Y_u$.
For $u \in H_f \times {\mathbb{Z}}^d$, we have by \eqref{better_def}:
{\begin{align} \begin{split} {
\sum_{v \sim u} f(v)e^{\beta Y_u} = \sum_{v \sim u} g(\psi(v))e^{\beta Y_u}
= \sum_{v \sim \psi(u)} g(v)e^{\beta Z_{\psi(u)}}. \label{numerator_same}
} \end{split} \end{align}}
Since $f(v) = 0$ for all $v \notin H_f \times {\mathbb{Z}}^d$, and similarly $g(v) = 0$ for all $v \notin H_g \times {\mathbb{Z}}^d$, we can sum over all of ${\mathbb{N}} \times {\mathbb{Z}}^d$, and \eqref{numerator_same} gives
{\begin{align} \begin{split} {
\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} f(v) e^{\beta Y_w}
= \sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} g(v) e^{\beta Z_w}. \label{denominator_same}
} \end{split} \end{align}}
Together, \eqref{numerator_same}, \eqref{denominator_same}, and the fact that $\|f\| = \|g\|$ (cf.~Lemma \ref{norm_equivalence}) show $F(u) = G(\psi(u))$ for all $u \in H_f \times {\mathbb{Z}}^d$.
Hence
{\begin{align*} {
\sum_{u \in H_f \times {\mathbb{Z}}^d} |F(u) - G(\psi(u))| + \sum_{u \notin H_f \times {\mathbb{Z}}^d} F(u)^2 + \sum_{u \notin H_g \times {\mathbb{Z}}^d} G(u)^2
+ 2^{-\deg(\psi)} = 0.
} \end{align*}}
For any ${\varepsilon} > 0$, we can find a finite subset $A \subset H_f \times {\mathbb{Z}}^d$ such that
{\begin{align*} {
\sum_{u \notin A} F(u)^2 + \sum_{u \notin \psi(A)} G(u)^2 < {\varepsilon}.
} \end{align*}}
With $\phi := \psi\rvert_{A}$, we thus have $d(F,G) \leq d_\phi(F,G) < {\varepsilon}$.
Letting ${\varepsilon}$ tend to 0 gives the desired result.
\end{proof}

Given Proposition \ref{same_law}, we may define $Tf$ to be the law of $F$, for $f \in {\mathcal{S}}$.
That is, $Tf \in {\mathcal{P}}^1({\mathcal{S}})$.
To apply this map to the functions of interest, namely endpoint distributions, we must first identify $f_{n}$ with the partitioned subprobability measure having representative
{\begin{align} \begin{split} {
f_n(u) = \begin{cases} 
f_n(x) &\text{if } u = (n,x), \\
0 &\text{else,}
\end{cases} \quad
u \in {\mathbb{N}} \times {\mathbb{Z}}^d. \label{fn_def2}
} \end{split} \end{align}}
In review, $f_n$ defined by \eqref{fn_def} is a function on ${\mathbb{Z}}^d$, random with respect to ${\mathcal{F}}_n$.
It is natural (and measurable) to identify $f_n$ with a function on ${\mathbb{N}} \times {\mathbb{Z}}^d$ that is supported on the $n$-th copy of ${\mathbb{Z}}^d$; this is \eqref{fn_def2}.
Finally, that function --- thus far an element of $S$ --- is identified with its equivalence class $\iota(f_n) \in {\mathcal{S}}$, where $\iota$ is the quotient map from Lemma \ref{S_meas}.
In accordance with our previous notation, the symbol $f_n$ will henceforth denote the equivalence class in ${\mathcal{S}}$ unless stated otherwise, while $f_n(u)$ will denote the representative defined by \eqref{fn_def2}, evaluated at $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$.
By the discussion preceding \eqref{F_def}, the law of $f_{n+1} \in {\mathcal{S}}$ given ${\mathcal{F}}_n$ is equal to $Tf_n$.
For this reason we refer to $T$ as the ``update procedure", whose construction is the goal of the present section.

\subsection{Continuity} \label{transformation-continuity}
We now generalize Proposition \ref{same_law} and its proof to show that $f \mapsto Tf$ is a (uniformly) continuous function from ${\mathcal{S}}$ to ${\mathcal{P}}^1({\mathcal{S}})$.

\begin{prop} \label{continuous1}
For any ${\varepsilon} > 0$, there exists $\delta > 0$ such that for $f,g \in {\mathcal{S}}$,
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad {\mathcal{W}}^1(Tf,Tg) < {\varepsilon}.
} \end{align*}}
\end{prop}

In proving Proposition \ref{continuous1}, we will use the following lemmas.

\begin{lemma} \label{amgm}
Let $Z_1,Z_2,\dots$ be i.i.d.~copies of a positive random variable $Z$.
If $\alpha_1,\alpha_2,\dots$ are nonnegative constants 
 satisfying $C \leq \sum_{i = 1}^\infty \alpha_i < \infty $ for a positive constant $C$, then
{\begin{align*} {
{\mathbf{E}}\Bigg[\bigg(\sum_{i=1}^\infty \alpha_i Z_i\bigg)^{-p}\Bigg] \leq C^{-p}\, {\mathbf{E}}(Z^{-p}) \quad \text{for any $p > 0$.}
} \end{align*}}
\end{lemma}

\begin{proof}
If ${\mathbf{E}}(Z^{-p}) = \infty$, then the claim is trivial.
So assume ${\mathbf{E}}(Z^{-p}) < \infty$.
Define $C_n := \sum_{i=1}^n \alpha_i$.
A comparison of (weighted) arithmetic and geometric means shows
{\begin{align*} {
\sum_{i=1}^n \alpha_i Z_i 
= C_n \sum_{i=1}^n \frac{\alpha_i}{C_n} Z_i 
\geq C_n \prod_{i=1}^n Z_i^{\alpha_i/C_n}.
} \end{align*}}
Since the $Z_i$ are i.i.d., this observation gives
{\begin{align*} {
{\mathbf{E}}\Bigg[\bigg(\sum_{i=1}^n \alpha_i Z_i\bigg)^{-p}\Bigg]
\leq C_n^{-p}\, {\mathbf{E}}\Bigg[\bigg(\prod_{i=1}^n Z_i^{\alpha_i/C_n}\bigg)^{-p}\Bigg]
= C_n^{-p} \prod_{i=1}^n {\mathbf{E}}\big[Z^{-p\alpha_i/C_n}\big]
&\leq C_n^{-p} \prod_{i=1}^n {\mathbf{E}}\big[Z^{-p}\big]^{\alpha_i/C_n} \\
&= C_n^{-p}\, {\mathbf{E}}(Z^{-p}),
} \end{align*}}
where Jensen's inequality (since $0 \leq \alpha_i/C_n \leq 1$) has been used in the final inequality.
An application of dominated convergence completes the proof:
{\begin{align*} {
{\mathbf{E}}\Bigg[\bigg(\sum_{i=1}^\infty \alpha_i Z_i\bigg)^{-p}\Bigg]
= \lim_{n \to \infty} {\mathbf{E}}\Bigg[\bigg(\sum_{i=1}^n \alpha_i Z_i\bigg)^{-p}\Bigg]
\leq \lim_{n \to \infty} C_n^{-p}\, {\mathbf{E}}(Z^{-p}) \leq C^{-p}\, {\mathbf{E}}(Z^{-p}).
} \end{align*}}
\end{proof}

\begin{lemma} \label{LLN}
Let $Z_1,Z_2,\dots,$ be i.i.d.~centered random variables with ${\mathbf{E}}(Z_i^2) < \infty$.
If $\alpha_1,\alpha_2,\dots$ constants satisfying $|\alpha_i| \leq C_1$ for all $i$ and $\sum_{i = 1}^\infty|\alpha_i| \leq C_2$ for constants $C_1$ and $C_2$, then
{\begin{align*} {
{\mathbf{E}}\Bigg[\bigg(\sum_{i = 1}^\infty \alpha_i Z_i\bigg)^2\Bigg] \leq C_1C_2\operatorname{Var}(Z_1).
} \end{align*}}
\end{lemma}

\begin{proof}
First observe that by monotone convergence,
{\begin{align*} {
{\mathbf{E}}\Bigg[ \bigg(\sum_{i = 1}^\infty |\alpha_i Z_i|\bigg)^2\Bigg]
= \lim_{n \to \infty} {\mathbf{E}}\Bigg[ \bigg(\sum_{i = 1}^n |\alpha_i Z_i|\bigg)^2\Bigg]
&= \lim_{n \to \infty} {\mathbf{E}} \Bigg[\sum_{i = 1}^n |\alpha_i Z_i|^2 + 2\sum_{1 \leq i < j \leq n} |\alpha_i \alpha_j Z_iZ_j|\Bigg] \\
&= \lim_{n \to \infty} \Bigg[\sum_{i = 1}^n \alpha_i^2\operatorname{Var}(Z_1) + ({\mathbf{E}}|Z_1|)^2 \sum_{1 \leq i < j \leq n} |\alpha_i \alpha_j|\Bigg] \\
&\leq {\mathbf{E}}(Z_1^2) \bigg(\sum_{i = 1}^\infty |\alpha_i|\bigg)^2 < \infty.
} \end{align*}}
Therefore, we may apply dominated convergence to find
{\begin{align*} {
{\mathbf{E}}\Bigg[\bigg(\sum_{i = 1}^\infty \alpha_i Z_i\bigg)^2\Bigg] 
= \lim_{n \to \infty} {\mathbf{E}}\Bigg[\bigg(\sum_{i = 1}^n \alpha_i Z_i\bigg)^2\Bigg] 
= \sum_{i = 1}^\infty \alpha_i^2 \operatorname{Var}(Z_i) \leq C_1\operatorname{Var}(Z_1) \sum_{i = 1}^\infty |\alpha_i| \leq C_1C_2\operatorname{Var}(Z_1).
} \end{align*}}
\end{proof}

\begin{proof}[Proof of Proposition \ref{continuous1}]
Since $({\mathcal{S}},d)$ is a compact metric space, uniform continuity of $f \mapsto Tf$ will be implied by continuity.
So it suffices to prove continuity at a fixed $f \in {\mathcal{S}}$.
Given ${\varepsilon} > 0$, choose $\kappa_1 > 0$ small enough that
{\begin{align} \begin{split} {
2\big(d^{-1}\kappa_1 + d^{-1/2}\sqrt{8\kappa_1}\big) ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})^{1/2} < \frac{\varepsilon}{4}, \label{kappa1_1}
} \end{split} \end{align}}
where $Y$ is a generic random variable having law $\lambda$.
Next choose $\kappa_2 > 0$ small enough that all of the following statements are true:
{\begin{align} \begin{split} {
4\kappa_2\, ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y}) < \frac{\varepsilon}{4}, \label{kappa2_3}
} \end{split} \end{align}}
{\begin{align} \begin{split} {
(4d+1)\kappa_2 < \kappa_1, \label{kappa2_1}
} \end{split} \end{align}}
{\begin{align} \begin{split} {
2d\sqrt{\kappa_2} < \kappa_1. \label{kappa2_2}
} \end{split} \end{align}}
Given a representative $f \in S$, we may take $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$ to be finite but large enough that
{\begin{align} \begin{split} {
\sum_{u \notin A} f(u) < \kappa_2. \label{notAsum}
} \end{split} \end{align}}
By possibly omitting some elements of $A$, we may assume $f(u) > 0$ for all $u \in A$, and then take $\delta > 0$ small enough that
{\begin{align} \begin{split} {
f(u)^2 > \delta \quad \text{for all $u \in A$} \label{pointwiseA}.
} \end{split} \end{align}}
Considering \eqref{kappa2_3}--\eqref{kappa2_2}, we may choose $\delta$ so that all of the following statements are also true:
{\begin{align} \begin{split} {
4(\delta+\kappa_2) ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y}) < \frac{\varepsilon}{4}, \label{delta6}
} \end{split} \end{align}}
{\begin{align} \begin{split} {
(4d+1)(\delta+\kappa_2) < \kappa_1, \label{delta3}
} \end{split} \end{align}}
{\begin{align} \begin{split} {
2d\sqrt{\delta+\kappa_2} < \kappa_1. \label{delta5}
} \end{split} \end{align}}
In addition, we will assume
{\begin{align} \begin{split} {
(2d)^2 |A| \sqrt{\delta} < \kappa_2, \label{borderA}
} \end{split} \end{align}}
and finally
{\begin{align} \begin{split} {
\delta < \min\Big(\frac{\varepsilon}{16},2^{-3}\Big). \label{delta4}
} \end{split} \end{align}}
Now suppose $g \in {\mathcal{S}}$ satisfies $d(f,g) < \delta$.
We will show ${\mathcal{W}}^1(Tf,Tg) < {\varepsilon}$.

Given a representative $g \in S$, we can choose an isometry $\psi : C \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $d_\psi(f,g) < \delta$.
From \eqref{pointwiseA}, we deduce that $A \subset C$, since otherwise $d_\psi(f,g) \geq f(u)^2 > \delta$ for some $u \in A \setminus C$. 
Let $\phi$ be the restriction of $\psi$ to $A$.
It follows that
{\begin{align} \begin{split} {
\sum_{v \in A} |f(v) - g(\phi(v))| \leq \sum_{v \in C} |f(v) - g(\psi(v))| < \delta. \label{Asum}
} \end{split} \end{align}}
Since $d_\psi(f,g) < \delta < 2^{-3}$, we necessarily have $\deg(\phi) \geq \deg(\psi) \geq 4$.
Let $\Phi: B \to {\mathbb{N}} \times {\mathbb{Z}}^d$ be defined as in Lemma \ref{extension} so that $\deg(\Phi) \geq 2$.
Now take $(Y_u)_{u \in {\mathbb{Z}}^d}$ to be i.i.d.~random variables with shared law $\lambda$.
Define $Z_u := Y_{\Phi^{-1}(u)}$ if $u \in \Phi(B)$; otherwise let $Z_u$ be an independent copy of $Y_u$.
As in previous arguments, we define $F,G \in {\mathcal{S}}$ by \eqref{F_def} and \eqref{G_def}.
Then the laws of $F$ and $G$ are $Tf$ and $Tg$, respectively, and we may consider the coupling $(F,G)$ to determine an upper bound on ${\mathcal{W}}^1(Tf,Tg)$.
That is, ${\mathcal{W}}^1(Tf,Tg) \leq {\mathbf{E}}[d(F,g)] \leq {\mathbf{E}}[d_\Phi(F,G)]$.

To simplify notation, we will write
{\begin{align*} {
{\widetilde{{f}}}(u) &= \sum_{v \sim u} f(v)e^{\beta Y_u} & {\widetilde{{F}}} &= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} {\widetilde{{f}}}(u) + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y}) \\
{\widetilde{{g}}}(u) &= \sum_{v \sim u} g(v)e^{\beta Z_u} & {\widetilde{{G}}} &= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} {\widetilde{{g}}}(u) + 2d(1 - \|g\|){\mathbf{E}}(e^{\beta Y})
} \end{align*}}
so that $F(u) = {\widetilde{{f}}}(u)/{\widetilde{{F}}}$ and $G(u) = {\widetilde{{g}}}(u)/{\widetilde{{G}}}$.
For any $u \in B$,
{\begin{align} \begin{split} {
{\widetilde{{f}}}(u) - {\widetilde{{g}}}(\Phi(u)) &= \sum_{v \sim u} f(v)e^{\beta Y_{u}} - \sum_{v \sim \Phi(u)} g(v)e^{\beta Z_{\Phi(u)}}  \\
&= e^{\beta Y_u} \bigg(\sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v)\bigg). \label{first_observation}
} \end{split} \end{align}}
Since $\deg(\Phi) \geq 2$, the two sets
{\begin{align*} {
R_u := \{v: v \sim u,\ v \in A\} \quad \text{and} \quad R_u' := \{v: v \sim \Phi(u),\ v \in \phi(A)\}
} \end{align*}}
satisfy\footnote{If $v \in R_u$, then $\Phi(u) - \phi(v) = \Phi(u) - \Phi(v) = u - v$, and so $\phi(v)$ belongs to $R_u'$.  
Conversely, if $\phi(v) = \Phi(v) \in R_u'$, then $u - v = \Phi(u) - \Phi(v) = \Phi(u) - \phi(v)$, and so $v$ belongs to $R_u$.}
$\phi(R_u) = R_u'$.
We also define
{\begin{align*} {
S_u := \{v: v \sim u,\ v \notin A\} \quad \text{and} \quad S'_u := \{v : v \sim \Phi(u),\ v \notin \psi(C)\},
} \end{align*}}
and finally
{\begin{align*} {
T_u' := \{v: v \sim \Phi(u),\ v \in \psi(C \setminus A)\}.
} \end{align*}}
We thus have
{\begin{align} \begin{split} {
\bigg| \sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v) \bigg|
\leq \sum_{v \in R_u} |f(v) - g(\phi(v))| + \sum_{v \in S_u} f(v) + \sum_{v \in S_u'} g(v) + \sum_{v \in T_u'} g(v). \label{Bsum0}
} \end{split} \end{align}}
When summed over $u \in B$, each of the four terms on the right-hand side of \eqref{Bsum0} can be separately bounded from above.
We do so in the next paragraph.

Any $v \in {\mathbb{N}} \times {\mathbb{Z}}^d$ is the neighbor of at most $2d$ elements of $B$.
Consequently, the containment $R_u \subset A$ and the assumption \eqref{Asum} together imply
{\begin{align} \begin{split} {
\sum_{u \in B} \sum_{v \in R_u} |f(v) - g(\phi(v))| \leq 2d \cdot \delta. \label{Bsum1}
} \end{split} \end{align}}
Next, $S_u \subset A^c$ and \eqref{notAsum} imply
{\begin{align} \begin{split} {
\sum_{u \in B} \sum_{v \in S_u} f(v) \leq 2d \cdot \kappa_2.
} \end{split} \end{align}}
Since $|B| \leq 2d |A|$, the containment $S_u' \subset \psi(C)^c$ and \eqref{borderA} imply
{\begin{align} \begin{split} {
\sum_{u \in B} \sum_{v \in S_u'} g(v) 
\leq (2d)^2 |A| \sup_{v \notin \psi(C)} g(v) 
&\leq (2d)^2 |A| \sqrt{\sum_{v \notin \psi(C)} g(v)^2}  \\ 
&\leq (2d)^2 |A| \sqrt{d_\psi(f,g)} < (2d)^2 |A| \sqrt{\delta} < \kappa_2.
} \end{split} \end{align}}
Finally, one more application of \eqref{notAsum} gives
{\begin{align} \begin{split} {
\sum_{u \in B} \sum_{v \in T_u'} g(v) &\leq \sum_{u \in B} \sum_{v \in T_u'} \bigl(|f(\psi^{-1}(v)) - g(v)| + f(\psi^{-1}(v))\bigr)  \\
&\leq 2d \sum_{u \in C \setminus A} |f(u) - g(\psi(u))| + 2d \sum_{u \notin A} f(u)  \\
&< 2d(d_\psi(f,g) + \kappa_2) < 2d(\delta + \kappa_2). \label{Bsum4}
} \end{split} \end{align}}
Having established \eqref{Bsum0}--\eqref{Bsum4} and assumed \eqref{delta3}, we have shown
{\begin{align} \begin{split} {
\sum_{u \in B} \bigg| \sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v) \bigg| < (4d + 1)(\delta + \kappa_2) < \kappa_1. \label{sumB}
} \end{split} \end{align}}
This inequality will be pivotal in obtaining an upper bound on ${\mathbf{E}}[d_\Phi(F,G)]$. 

Four other inequalities stated below will also be useful, namely \eqref{nodenominator}, \eqref{nodenominator2}, \eqref{Fbound}, and \eqref{Gbound}.
Let us now prove each of them.
First, for any $u \notin B$, an application of Cauchy--Schwarz gives
{\begin{align*} {
\sum_{u \notin B} {\widetilde{{f}}}(u)^{2} &=  
\sum_{u \notin B}\bigg(\sum_{v \sim u} f(v)e^{\beta Y_{u}}\bigg)^{2}
\leq \sum_{u \notin B} 2d \cdot e^{2\beta Y_u} \sum_{v \sim u} f(v)^2.
} \end{align*}}
Notice that if $u \notin B$, and $v \sim u$, then $v \notin A$.
It follows that
{\begin{align} \begin{split} {
{\mathbf{E}}\bigg[\sum_{u \notin B} {\widetilde{{f}}}(u)^{2}\bigg]
&\leq 2d \cdot {\mathbf{E}}(e^{2\beta Y}) \sum_{u \notin B} \sum_{v \sim u} f(v)^2  \\
&\leq (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}) \sum_{v \notin A} f(v)^2   \\
&\leq (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}) \sum_{v \notin A} f(v)
< \kappa_2 \cdot (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y})  \label{nodenominator},
} \end{split} \end{align}}
where we have used \eqref{notAsum} in the last inequality.
The same reasoning as above\footnote{By definition of $\Phi : B \to {\mathbb{N}} \times {\mathbb{Z}}^d$, if $v = \phi(a)$ for $a \in A$ and $u \sim v$, then $a + (u - v) \in B$.  Moreover, $\Phi(a + (u - v)) = \phi(a) + (u - v) = u$, meaning $u \in \Phi(B)$.
Consequently, if $u \notin \Phi(B)$ and $v \sim u$, then $v \notin \phi(A)$.}
gives
{\begin{align} \begin{split} {
{\mathbf{E}}\bigg[\sum_{u \notin \Phi(B)} {\widetilde{{g}}}(u)^{2}\bigg]
\leq (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}) \sum_{v \notin \phi(A)} g(v)^2, \label{notPhiB}
} \end{split} \end{align}}
and we can bound the last sum by again applying \eqref{notAsum}:
{\begin{align} \begin{split} {
\sum_{v \notin \phi(A)} g(v)^2 \leq \sum_{v \in \psi(C \setminus A)} g(v) + \sum_{v \notin \psi(C)} g(v)^2
&\leq \sum_{v \in C \setminus A} \bigl(|f(v) - g(\psi(v))| + f(v)\bigr) + \sum_{v \notin \psi(C)} g(v)^2  \\
&\leq \sum_{v \in C} |f(v) - g(\psi(v))| + \sum_{v \notin \psi(C)} g(v)^2 + \sum_{v \notin A} f(v)  \\
&< d_\psi(f,g) + \kappa_2 < \delta + \kappa_2. \label{notphiA}
} \end{split} \end{align}}
In light of \eqref{notphiA}, now \eqref{notPhiB} yields
{\begin{align} \begin{split} {
{\mathbf{E}}\bigg[\sum_{u \notin \Phi(B)} {\widetilde{{g}}}(u)^{2}\bigg] < (\delta + \kappa_2) \cdot (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}). \label{nodenominator2}
} \end{split} \end{align}}
The third and fourth inequalities to show consider the quantities ${\widetilde{{F}}}$ and ${\widetilde{{G}}}$.
Suppose $p$ is a positive constant.
If $\|f\| \leq \frac{1}{2}$, then
{\begin{align} \begin{split} {
{\mathbf{E}}\, {\widetilde{{F}}}^{-p}
\leq {\mathbf{E}}\big[(d \cdot {\mathbf{E}}(e^{\beta Y}))^{-p}\big]
= (d \cdot {\mathbf{E}}(e^{\beta Y}))^{-p}
\leq d^{-p} \cdot {\mathbf{E}}(e^{-p\beta Y}). \label{Fcase_1}
} \end{split} \end{align}}
Otherwise, if $\|f\| \geq \frac{1}{2}$, then
{\begin{align*} {
\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) \geq d,
} \end{align*}}
and so Lemma \ref{amgm} gives
{\begin{align} \begin{split} {
{\mathbf{E}}\, {\widetilde{{F}}}^{-p}
\leq {\mathbf{E}}\Bigg[\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u}\bigg)^{-p}\Bigg]
\leq d^{-p} \cdot {\mathbf{E}}(e^{-p\beta Y}). \label{Fcase_2}
} \end{split} \end{align}}
Viewing \eqref{Fcase_1} and \eqref{Fcase_2} together, we have unconditionally that
{\begin{align} \begin{split} {
{\mathbf{E}}\, {\widetilde{{F}}}^{-p} \leq d^{-p} \cdot {\mathbf{E}}(e^{-p\beta Y}). \label{Fbound}
} \end{split} \end{align}}
By the same argument with $g$ in place of $f$, we obtain the fourth desired inequality:
{\begin{align} \begin{split} {
{\mathbf{E}}\, {\widetilde{{G}}}^{-p} \leq d^{-p} \cdot {\mathbf{E}}(e^{-p\beta Y}). \label{Gbound}
} \end{split} \end{align}}
We now turn in earnest to bounding ${\mathbf{E}}[d_\Phi(F,G)]$, which is the sum of four expectations corresponding to the four summands in definition \eqref{d_def}.
Seeking an estimate on the first summand, we observe that for $u \in B$,
{\begin{align} \begin{split} {
|F(u) - G(\Phi(u))| = \bigg|\frac{{\widetilde{{f}}}(u)}{{\widetilde{{F}}}} - \frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{G}}}}\bigg| 
&\leq \bigg|\frac{{\widetilde{{f}}}(u)}{{\widetilde{{F}}}} - \frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{F}}}}\bigg| + \bigg|\frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{F}}}}-\frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{G}}}}\bigg|  \\
&= \frac{|{\widetilde{{f}}}(u) - {\widetilde{{g}}}(\Phi(u))|}{{\widetilde{{F}}}} + \frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{G}}}}\cdot\bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}} - 1\bigg|. \label{two_parts}
} \end{split} \end{align}}
Summing over $B$ and taking expectation, we have for the first term in \eqref{two_parts}:
{\begin{align} \begin{split} {
{\mathbf{E}}\Bigg[\sum_{u \in B} \frac{|{\widetilde{{f}}}(u) - {\widetilde{{g}}}(\Phi(u))|}{{\widetilde{{F}}}}\Bigg]
&= \sum_{u \in B} {\mathbf{E}}\bigg[\frac{|{\widetilde{{f}}}(u) - {\widetilde{{g}}}(\Phi(u))|}{{\widetilde{{F}}}}\bigg]  \\
&\leq \sum_{u \in B} ({\mathbf{E}}\, |{\widetilde{{f}}}(u) - {\widetilde{{g}}}(\Phi(u))|^2)^{1/2} ({\mathbf{E}}\, {\widetilde{{F}}}^{-2})^{1/2}  \\
&\leq d^{-1}\sum_{u \in B} ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y} )^{1/2}\bigg| \sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v) \bigg|  \\
&< d^{-1}\kappa_1 ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})^{1/2},  \label{first_part}
} \end{split} \end{align}}
where have applied Cauchy--Schwarz to obtain the first inequality, \eqref{first_observation} and \eqref{Fbound} to obtain the second, and \eqref{sumB} for the third.
Meanwhile, the second term in \eqref{two_parts} satisfies
{\begin{align} \begin{split} {
{\mathbf{E}}\Bigg[\sum_{u \in B} \frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{G}}}}\cdot\bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|\Bigg]
\leq {\mathbf{E}}\, \bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|
= {\mathbf{E}}\, \bigg|\frac{{\widetilde{{G}}}-{\widetilde{{F}}}}{{\widetilde{{F}}}}\bigg|
&\leq ({\mathbf{E}}\, {\widetilde{{F}}}^{-2})^{1/2} ({\mathbf{E}} |{\widetilde{{G}}} - {\widetilde{{F}}}|^2)^{1/2}  \\
&\leq d^{-1}({\mathbf{E}}\, e^{-2\beta Y})^{1/2}({\mathbf{E}} |{\widetilde{{G}}} - {\widetilde{{F}}}|^2)^{1/2}, \label{second_part}
} \end{split} \end{align}}
where we have again applied \eqref{Fbound}.
Notice that
{\begin{align*} {
{\widetilde{{F}}} - 2d \cdot {\mathbf{E}}(e^{\beta Y}) &= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} {\widetilde{{f}}}(u) - 2d \cdot {\mathbf{E}}(e^{\beta Y}) \|f\| \\
&= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u} - \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) {\mathbf{E}}(e^{\beta Y_u}) \\
&= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big] \\
&= \sum_{u \in B} \sum_{v \sim u} f(v)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big]
+ \sum_{u \notin B} \sum_{v \sim u} f(v)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big],
} \end{align*}}
and similarly
{\begin{align*} {
{\widetilde{{G}}} - 2d \cdot {\mathbf{E}}(e^{\beta Y}) 
&= \sum_{u \in \Phi(B)} \sum_{v \sim u} g(v)\big[e^{\beta Z_u} - {\mathbf{E}}(e^{\beta Z_u})\big] + \sum_{u \notin \Phi(B)} \sum_{v \sim u} g(v)\big[e^{\beta Z_u} - {\mathbf{E}}(e^{\beta Z_u})\big] \\
&= \sum_{u \in B} \sum_{v \sim \Phi(u)} g(v)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big] + \sum_{u \notin \Phi(B)} \sum_{v \sim u} g(v)\big[e^{\beta Z_u} - {\mathbf{E}}(e^{\beta Z_u})\big].
} \end{align*}}
Hence
{\begin{align*} {
{\widetilde{{F}}} - {\widetilde{{G}}}
&= \sum_{u \in B}\bigg( \sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v)\bigg)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big] \\
&\phantom{=|} + \sum_{u \notin B} \sum_{v \sim u} f(v)\big[e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u})\big]
+ \sum_{u \notin \Phi(B)} \sum_{v \sim u} g(v)\big[e^{\beta Z_u} - {\mathbf{E}}(e^{\beta Z_u})\big].
} \end{align*}}
In the notation of Lemma \ref{LLN}, the following centered random variables are i.i.d.: 
{\begin{align*} {
Z_{u} &:= e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u}), \quad u \in B \\
Z'_{u} &:= e^{\beta Y_u} - {\mathbf{E}}(e^{\beta Y_u}), \quad u \notin B \\
Z''_{u} &:= e^{\beta Z_u} - {\mathbf{E}}(e^{\beta Z_u}), \quad u \notin \Phi(B).
} \end{align*}} 
Furthermore, their coefficients
{\begin{align*} {
\alpha_{u} &:= \sum_{v \sim u} f(v) - \sum_{v \sim \Phi(u)} g(v), \quad u \in B\\
\alpha'_{u} &:= \sum_{v \sim u} f(v), \quad u \notin B \\
\alpha''_{u} &:= \sum_{v \sim u} g(v), \quad u \notin \Phi(B)
} \end{align*}}
satisfy
{\begin{align*} {
|\alpha_u| &< \kappa_1 \quad &&\text{by \eqref{sumB},} \\
|\alpha_u'| &= \alpha'_u \leq \sum_{v \notin A} f(v) < \kappa_2 < \kappa_1 \quad &&\text{by \eqref{notAsum} and \eqref{kappa2_1},} \\
|\alpha_u''| &= \alpha''_u \leq 2d \sup_{v \notin \phi(A)} g(v) \leq 2d\sqrt{\delta + \kappa_2} < \kappa_1 \quad &&\text{by \eqref{notphiA} and \eqref{delta5}}.
} \end{align*}}
Since $\|f\|,\|g\| \leq 1$, one also has 
{\begin{align*} {
\sum_{u \in B} |\alpha_u| + \sum_{u \notin B} \alpha'_u + \sum_{u \notin \Phi(B)} \alpha''_u \leq (1+1)\cdot 2d + 2d + 2d = 8d.
} \end{align*}}
Therefore, Lemma \ref{LLN} guarantees
{\begin{align} \begin{split} {
{\mathbf{E}}\big[|{\widetilde{{F}}}-{\widetilde{{G}}}|^2\big] \leq 8d \cdot \kappa_1 \cdot \operatorname{Var}(e^{\beta Y}) \leq 8d\cdot \kappa_1 \cdot {\mathbf{E}}(e^{2\beta Y}). \label{top_second_part}
} \end{split} \end{align}}
Upon using \eqref{top_second_part} in \eqref{second_part}, we obtain
{\begin{align} \begin{split} {
{\mathbf{E}}\Bigg[\sum_{u \in B} \frac{{\widetilde{{g}}}(\Phi(u))}{{\widetilde{{G}}}}\cdot\bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|\Bigg]
\leq {\mathbf{E}}\, \bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|
\leq d^{-1/2}\sqrt{8\kappa_1} ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})^{1/2} \label{second_part2}.
} \end{split} \end{align}}
Then applying \eqref{first_part} and \eqref{second_part2} with \eqref{two_parts}, we find
{\begin{align*} {
{\mathbf{E}}\Bigg[\sum_{u \in B} |F(u) - G(\Phi(u))| \Bigg] \leq 
\big(d^{-1}\kappa_1 + d^{-1/2}\sqrt{8\kappa_1}\big)({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})^{1/2}.
} \end{align*}}
Now \eqref{kappa1_1} gives us control on the first summand in ${\mathbf{E}}[d_\Phi(F,G)]$:
{\begin{align} \begin{split} {
{\mathbf{E}}\Bigg[2\sum_{u \in B} |F(u) - G(\Phi(u))| \Bigg] < \frac{\varepsilon}{4} \label{final_bound_1}.
} \end{split} \end{align}}
From what we have argued, the second and third summands in ${\mathbf{E}}[d_\Phi(F,G)]$ are not difficult to bound.
Notice that for any $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$, the random variables ${\widetilde{{f}}}(u)$, ${\widetilde{{g}}}(u)$, ${\widetilde{{F}}}$, and ${\widetilde{{G}}}$ are non-decreasing functions of $Y_u$ or $Z_u$. 
Therefore, we can use the FKG inequality.
For the second summand, we apply FKG, then \eqref{nodenominator} and \eqref{Fbound}, and finally \eqref{kappa2_3}:
{\begin{align} \begin{split} {
{\mathbf{E}}\bigg[\sum_{u \notin B} F(u)^{2}\bigg] \leq 
{\mathbf{E}}\bigg[\sum_{u \notin B} {\widetilde{{f}}}(u)^{2}\bigg] ({\mathbf{E}}\, {\widetilde{{F}}}^{-2}) 
< 4 \kappa_2\, ({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})
< \frac{\varepsilon}{4}. \label{final_bound_2}
} \end{split} \end{align}}
For the third summand, we appeal to FKG, then to \eqref{nodenominator2} and \eqref{Gbound}, and finally to \eqref{delta6}:
{\begin{align} \begin{split} {
{\mathbf{E}}\bigg[\sum_{u \notin \Phi(B)} G(u)^{2}\bigg] 
\leq {\mathbf{E}}\bigg[\sum_{u \notin \Phi(B)} {\widetilde{{g}}}(u)^{2}\bigg] ({\mathbf{E}}\, {\widetilde{{G}}}^{-2})
< 4(\delta + \kappa_2)({\mathbf{E}}\, e^{2\beta Y}\, {\mathbf{E}}\, e^{-2\beta Y})
< \frac{\varepsilon}{4}. \label{final_bound_3}
} \end{split} \end{align}}
The fourth and final summand in ${\mathbf{E}}[d_\Phi(F,G)]$ depends on the maximum degree of the isometry $\Phi$.
From \eqref{delta4} we know $d_\psi(f,g) < \delta < {\varepsilon}/16$, and so
$2^{-\deg(\psi)} < {\varepsilon}/16$.
Since $\phi$ is a restriction of $\psi$, we have $\deg(\phi) \geq \deg(\psi)$.
And by Lemma \ref{extension}, $\deg(\Phi) \geq \deg(\phi) - 2$.
As a result,
{\begin{align} \begin{split} {
2^{-\deg(\Phi)} \leq 2^{-\deg(\phi)+2} \leq 4 \cdot 2^{-\deg(\psi)} < \frac{\varepsilon}{4} \label{final_bound_4}.
} \end{split} \end{align}}
Summing the left-hand sides of \eqref{final_bound_1}--\eqref{final_bound_4}, we reach the desired conclusion:
{\begin{align*} {
{\mathcal{W}}^1(Tf,Tg) \leq {\mathbf{E}}[d_\Phi(F,G)] < {\varepsilon}.
} \end{align*}}
\end{proof}

\subsection{Lifting the update map} \label{extension_to_measures}
Thus far we have only considered the random variable $F$ defined by \eqref{F_def} for \textit{fixed} $f \in {\mathcal{S}}$.
If $f$ is itself a random element of ${\mathcal{S}}$ drawn from the probability measure $\mu \in {\mathcal{P}}^1({\mathcal{S}})$, the resulting total law of $F$ will be written ${\mathcal{T}}\mu$.
That is,
{\begin{align} \begin{split} {
{\mathcal{T}}\mu({\mathrm{d}} g) = \int Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f), \label{t_on_measures}
} \end{split} \end{align}}
which means
{\begin{align*} {
{\mathcal{T}}\mu({\mathcal{A}}) = \int Tf({\mathcal{A}})\ \mu({\mathrm{d}} f), \quad \text{Borel ${\mathcal{A}} \subset {\mathcal{S}}$}.
} \end{align*}}
In this notation, we have a map ${\mathcal{P}}^1({\mathcal{S}}) \to {\mathcal{P}}^1({\mathcal{S}})$ given by $\mu \mapsto {\mathcal{T}}\mu$.
We can recover the map $T$ by restricting to Dirac measures;
that is, $Tf = {\mathcal{T}}\delta_f$, where $\delta_f \in {\mathcal{P}}^1({\mathcal{S}})$ is the unit point mass at $f$.

From Proposition \ref{continuous1}, it is not difficult to show $\mu \mapsto {\mathcal{T}}\mu$ is (uniformly) continuous.
But first we check that the measure ${\mathcal{T}}\mu$ is well-defined.
Specifically, we must check that for every Borel set ${\mathcal{A}}\subset {\mathcal{S}}$, $f \mapsto Tf({\mathcal{A}})$ is measurable as a map ${\mathcal{S}} \to {\mathbb{R}}$.
As we have just shown that $f \mapsto Tf$ is continuous and hence measurable, it suffices to prove the following lemma.

\begin{lemma} \label{evaluation_meas}
Let $({\mathcal{X}},\tau)$ be a Polish space, and $A \subset {\mathcal{X}}$ a Borel set.
For any $p \geq 1$, the map ${\mathcal{P}}^p({\mathcal{X}}) \to {\mathbb{R}}$ given by $\mu \mapsto \mu(A)$ is measurable.
\end{lemma}

\begin{proof}
We will make use of Dynkin's $\pi$-$\lambda$ theorem.
First define
{\begin{align*} {
{\mathcal{C}} := \{C \subset {\mathcal{X}} : \text{$C$ is closed, $\mu \mapsto \mu(C)$ is measurable}\}.
} \end{align*}}
It is easy to see that ${\mathcal{C}}$ actually contains all closed sets: If $C \subset {\mathcal{X}}$ is closed, then Lemma \ref{wasserstein_compact}(b) and Lemma \ref{portmanteau}(e) show $\mu \mapsto \mu(C)$ is upper semi-continuous and hence measurable.
Furthermore, since closed sets remain closed under intersection, $C$ is a $\pi$-system.

Next define the larger collection
{\begin{align*} {
{\mathcal{D}} := \{A \subset {\mathcal{X}}: \text{$A$ is Borel, $\mu \mapsto \mu(A)$ is measurable}\}.
} \end{align*}}
We check that ${\mathcal{D}}$ is a $\lambda$-system:
\begin{itemize}
\item Clearly the empty set is in ${\mathcal{D}}$, since $\mu \mapsto \mu(\varnothing) = 0$ is constant.
\item If $A \in {\mathcal{C}}$, then $\mu \mapsto \mu({\mathcal{X}} \setminus A) = 1 - \mu(A)$ is the difference of measurable functions, and so itself measurable.
That is, ${\mathcal{X}} \setminus A \in {\mathcal{D}}$.
\item If $A_1,A_2,\dots$ is a sequence in ${\mathcal{D}}$ consisting of disjoint sets, then
{\begin{align*} {
\mu\bigg(\bigcup_{i = 1}^\infty A_i\bigg) = \sum_{i = 1}^\infty \mu(A_i) = \lim_{n \to \infty} \sum_{i = 1}^n \mu(A_i),
} \end{align*}}
meaning $\mu \mapsto \mu(\cup_{i = 1}^\infty A_i)$ is the limit of measurable functions, and so again measurable.
Therefore $\cup_{i = 1}^\infty A_i \in {\mathcal{D}}$.
\end{itemize}
Now the $\pi$-$\lambda$ theorem implies that the $\sigma$-algebra generated by ${\mathcal{C}}$ is contained in ${\mathcal{D}}$.
As ${\mathcal{C}}$ generates the Borel $\sigma$-algebra, the claim holds.
\end{proof}

For clarity, we review the measure theoretic definitions we have made.
Lemma \ref{evaluation_meas} verifies that $K(f,{\mathrm{d}} g) := Tf({\mathrm{d}} g) : {\mathcal{S}} \times {\mathcal{B}}({\mathcal{S}}) \to [0,1]$ is a kernel, where ${\mathcal{B}}({\mathcal{S}})$ denotes the Borel $\sigma$-algebra on ${\mathcal{S}}$.
Then for any probability $\mu \in {\mathcal{P}}^1({\mathcal{S}})$, we can define ${\mathcal{T}}\mu$ to be the product measure associated to $\mu$ and $K$:
{\begin{align*} {
{\mathcal{T}}\mu({\mathcal{A}}_1 \times {\mathcal{A}}_2) 
:= \mu K({\mathcal{A}}_1 \times {\mathcal{A}}_2)
= \int_{{\mathcal{A}}_1} K(f,{\mathcal{A}}_2)\ \mu({\mathrm{d}} f)
= \int_{{\mathcal{A}}_1} Tf({\mathcal{A}}_2)\ \mu({\mathrm{d}} f), \quad
\text{Borel ${\mathcal{A}}_1,{\mathcal{A}}_2 \subset {\mathcal{S}}$.}
} \end{align*}}
We then know how to integrate functions $\Phi : {\mathcal{S}} \times {\mathcal{S}} \to {\mathbb{R}}$ with respect to ${\mathcal{T}}\mu$, since
Fubini's theorem for kernels allows us to write
{\begin{align*} {
\int_{{\mathcal{S}} \times {\mathcal{S}}} \Phi(f,g)\ {\mathcal{T}}\mu({\mathrm{d}} f,{\mathrm{d}} g) 
= \int_{{\mathcal{S}} \times {\mathcal{S}}} \Phi(f,g)\ \mu K({\mathrm{d}} f,{\mathrm{d}} g)
&= \int_{\mathcal{S}} \int_{\mathcal{S}} \Phi(f,g)\ K(f,{\mathrm{d}} g)\, \mu({\mathrm{d}} f) \\
&=  \int_{\mathcal{S}} \int_{\mathcal{S}} \Phi(f,g)\ Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f).
} \end{align*}}
The above calculation holds when either $\Phi$ is nonnegative, or
{\begin{align*} {
\int_{\mathcal{S}} \int_{\mathcal{S}} |\Phi(f,g)|\ Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f) < \infty.
} \end{align*}}
Throughout the remainder of the manuscript, we will only be interested in functions depending on a single coordinate: $\Phi(f,g) = {\varphi}(g)$, in which case
{\begin{align*} {
 \int_{\mathcal{S}} \int_{\mathcal{S}} \Phi(f,g)\ Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f) = \int_{\mathcal{S}} \int_{\mathcal{S}} {\varphi}(g)\ Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f).
} \end{align*}}
Therefore, it will not be a problem to henceforth write ${\mathcal{T}}\mu$ for restriction of the product measure to the second coordinate:
${\mathcal{T}}\mu({\mathcal{A}}_2) = {\mathcal{T}}\mu({\mathcal{S}} \times {\mathcal{A}}_2)$.
That is, by ${\mathcal{T}}\mu$ we mean a measure on ${\mathcal{S}}$ rather than ${\mathcal{S}} \times {\mathcal{S}}$.
With this convention, the integral
{\begin{align} \begin{split} {
\int {\varphi}(g)\ {\mathcal{T}}\mu({\mathrm{d}} g) = \int_{\mathcal{S}} \int_{\mathcal{S}} {\varphi}(g)\ Tf({\mathrm{d}} g)\, \mu({\mathrm{d}} f) \label{T_fubini}
} \end{split} \end{align}}
is well-defined when either ${\varphi}$ is nonnegative, or
{\begin{align*} {
\int_{\mathcal{S}} \int_{\mathcal{S}} |{\varphi}(g)|\ Tf({\mathrm{d}} g)\,\mu({\mathrm{d}} f) < \infty. 
} \end{align*}}
Finally, we prove that $\mu \mapsto {\mathcal{T}}\mu$ is a continuous map ${\mathcal{P}}^1({\mathcal{S}}) \to {\mathcal{P}}^1({\mathcal{S}})$.

\begin{prop} \label{continuous2}
For any ${\varepsilon} > 0$, there exists $\delta > 0$ such that for $\mu,\nu \in {\mathcal{P}}^1({\mathcal{S}})$,
{\begin{align*} {
{\mathcal{W}}^1(\mu,\nu) < \delta \quad \Rightarrow \quad {\mathcal{W}}^1({\mathcal{T}}\mu,{\mathcal{T}}\nu) < {\varepsilon}.
} \end{align*}}
\end{prop}

\begin{proof}
Given ${\varepsilon} > 0$, we may choose by Proposition \ref{continuous1} some $\delta > 0$ such that
{\begin{align} \begin{split} {
d(f,g) < \delta \quad \Rightarrow \quad {\mathcal{W}}^1(Tf,Tg) < \frac{\varepsilon}{2}. \label{from_before}
} \end{split} \end{align}}
Now suppose $\mu,\nu \in {\mathcal{P}}^1({\mathcal{S}})$ satisfy ${\mathcal{W}}^1(\mu,\nu) < \delta \cdot {\varepsilon}/4$.
Then there exists a coupling $(f,g) \in {\mathcal{S}} \times {\mathcal{S}}$ of the distributions $\mu$ and $\nu$ such that
${\mathbf{E}}[d(f,g)] < \delta \cdot {\varepsilon}/4$.
Denote the joint distribution of $(f,g)$ by $\pi \in \Pi(\mu,\nu)$.
Markov's inequality gives
{\begin{align} \begin{split} {
{\mathbf{P}}(d(f,g) \geq \delta) \leq \delta^{-1} {\mathbf{E}}[d(f,g)] < \frac{\varepsilon}{4}. \label{measure_bound}
} \end{split} \end{align}}
To bound ${\mathcal{W}}^1({\mathcal{T}}\mu,{\mathcal{T}}\nu)$, we will use definition \eqref{kantorovich}. For any Lipschitz function ${\varphi} : {\mathcal{S}} \to {\mathbb{R}}$, we have
$\sup_{f \in {\mathcal{S}}} |{\varphi}(f)| < \infty$ by compactness of ${\mathcal{S}}$.
Considering such ${\varphi}$ with minimal Lipschitz constant at most 1, we can invoke \eqref{T_fubini} to write
{\begin{align} \begin{split} {
\int {\varphi}(h)\ {\mathcal{T}}\mu({\mathrm{d}} h) - \int {\varphi}(h)\ {\mathcal{T}}\nu({\mathrm{d}} h) 
&= \iint {\varphi}(h)\ Tf({\mathrm{d}} h)\, \mu({\mathrm{d}} f) - \iint {\varphi}(h)\ Tg({\mathrm{d}} h)\, \nu({\mathrm{d}} g) \\
&= \int\bigg(\int {\varphi}(h)\ Tf({\mathrm{d}} h) - \int {\varphi}(h)\ Tg({\mathrm{d}} h)\bigg)\, \pi({\mathrm{d}} f,{\mathrm{d}} g)  \\
&\leq \int {\mathcal{W}}^1(Tf,Tg)\ \pi({\mathrm{d}} f,{\mathrm{d}} g) 
= {\mathbf{E}}[{\mathcal{W}}^1(Tf,Tg)]. \label{each_vphi}
} \end{split} \end{align}}
We now consider the expectation of ${\mathcal{W}}^1(Tf,Tg)$ over the set $\{d(f,g) < \delta\}$, where we can apply \eqref{from_before}, and separately over the complement $\{d(f,g) \geq \delta\}$, which has $\pi$-measure less than ${\varepsilon}/4$ by \eqref{measure_bound}.
As the bound \eqref{each_vphi} holds for every ${\varphi}$, we have
{\begin{align*} {
{\mathcal{W}}^1({\mathcal{T}}\mu,{\mathcal{T}}\nu) 
\leq {\mathbf{E}}\big[{\mathcal{W}}^1(Tf,Tg)\big] 
&\leq {\mathbf{E}}{\big[ {{\mathcal{W}}^1(Tf,Tg)} \: \big| \: {d(f,g) < \delta} \big]} + \frac{\varepsilon}{4}\, {\mathbf{E}}{\big[ {{\mathcal{W}}^1(Tf,Tg)} \: \big| \: {d(f,g) \geq \delta} \big]} \\
&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = {\varepsilon},
} \end{align*}}
where we have applied Lemma \ref{trivial_bound} to bound the latter conditional expectation.
\end{proof}

To emphasize that ${\mathcal{T}}\mu$ is a mixture of probability measures on ${\mathcal{S}}$, we could write
{\begin{align*} {
{\mathcal{T}}\mu({\mathrm{d}} g) = \int {\mathcal{T}}\delta_f({\mathrm{d}} g)\, \mu({\mathrm{d}} f).
} \end{align*}}
Also notice that ${\mathcal{T}}$ is linear, and so
{\begin{align} \begin{split} {
{\mathcal{T}}^n\mu({\mathrm{d}} g) = \int {\mathcal{T}}^n\delta_f({\mathrm{d}} g)\, \mu({\mathrm{d}} f) \label{T_linear}
} \end{split} \end{align}}
for any $n \geq 0$.

\section{The empirical measure of the endpoint distribution} \label{free_energy}

\subsection{Definition and properties} \label{empirical_measures}
As discussed in Section \ref{endpoint_distributions}, the law of the endpoint distribution $f_{n+1}$ given ${\mathcal{F}}_n$ is equal to $Tf_n$.
Now define the empirical probability measure on ${\mathcal{S}}$ generated by the $f_i$,
{\begin{align} \begin{split} {
\mu_n := \frac{1}{n+1} \sum_{i = 0}^n \delta_{f_i}, \label{mu_n_def}
} \end{split} \end{align}}
where $\delta_f$ denotes the unit point mass at $f \in {\mathcal{S}}$.
Then $\mu_n$ is a random element of ${\mathcal{P}}^1({\mathcal{S}})$, measurable with respect to ${\mathcal{F}}_n$.\footnote{By the discussion following Proposition \ref{same_law}, $f_i \in {\mathcal{S}}$ is ${\mathcal{F}}_n$-measurable for $0 \leq i \leq n$.
It is clear that $f \mapsto \delta_f$ is a continuous (in particular, measurable) map ${\mathcal{S}} \to {\mathcal{P}}^1({\mathcal{S}})$.
It follows that $\mu_n \in {\mathcal{P}}^1({\mathcal{S}})$ is also ${\mathcal{F}}_n$-measurable.}
Lemma \ref{regT} below compares this measure to the shifted empirical measure,
which is measurable with respect to ${\mathcal{F}}_{n+1}$:
{\begin{align*} {
\mu_n' := \frac{1}{n+1}\sum_{i = 1}^{n+1} \delta_{f_i}.
} \end{align*}}

\begin{lemma} \label{regT}
In $\mathcal{P}^1({\mathcal{S}})$, the following bound holds in any environment $(X_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ (i.e.~almost surely) for all $n \geq 0$:
{\begin{align*} {
\mathcal{W}^1(\mu_n,\mu_n') \leq \frac{2}{n+1}.
} \end{align*}}
\end{lemma}

\begin{proof}
Fixing the environment, consider the following coupling $(f,f')$ of $\mu_n$ and $\mu_{n}'$:
{\begin{align*} {
{\mathbf{P}}{( {f = f_i,\, f' = f_j} \: | \: {{\mathcal{F}}_{n+1}} )} = \begin{cases}
1/(n+1) &\text{if } i = j \in \{1,2,\dots,n\}, \\
1/(n+1) &\text{if } i = 0,\, j = n+1, \\
0 &\text{else.}
\end{cases}
} \end{align*}}
Indeed, $f$ is distributed according to $\mu_n$, $f'$ according to $\mu_n'$, and
{\begin{align*} {
\mathcal{W}^1(\mu_n,\mu_n') \leq {\mathbf{E}}{[ {d(f,f')} \: | \: {{\mathcal{F}}_{n+1}} ]} = \frac{d(f_0,f_{n+1})}{n+1}.
} \end{align*}}
The proof is completed by Lemma \ref{trivial_bound}.
\end{proof}

While we will be interested in the quantity ${\mathcal{W}}^1(\mu_n,{\mathcal{T}}\mu_n)$, it is easier to compare $\mu_n'$ and ${\mathcal{T}}\mu_n$, since $Tf_{i}$ is the distribution of $f_{i+1}$ given ${\mathcal{F}}_{i}$.
Making use of the dual formulation \eqref{kantorovich} of Wasserstein distance, one has
{\begin{align*} {
\mathcal{W}^1(\mu_n',{\mathcal{T}}\mu_n) &= \sup_{\varphi} \biggl(\int {\varphi}(f)\ \mu_n'({\mathrm{d}} f) - \int {\varphi}(f)\ {\mathcal{T}}\mu_n(\mathrm{d}f) \biggr)\\
&= \sup_{\varphi} \frac{1}{n+1}\sum_{i = 0}^{n} \bigl({\varphi}(f_{i+1}) - {\mathbf{E}}{[ {{\varphi}(f_{i+1})} \: | \: {{\mathcal{F}}_{i}} ]}\bigr),
} \end{align*}}
where the supremum is taken over all functions ${\varphi} : {\mathcal{S}} \to {\mathbb{R}}$ satisfying 
{\begin{align} \begin{split} {
|{\varphi}(f) - {\varphi}(g)| \leq d(f,g) \quad \text{for all $f,g \in {\mathcal{S}}$.} \label{lip1}
} \end{split} \end{align}}
Notice, however, that adding a constant to ${\varphi}$ does not change the value of 
$\int {\varphi}(f)\, \mu'_n({\mathrm{d}} f) - \int {\varphi}(f)\, {\mathcal{T}}\mu_n({\mathrm{d}} f)$, and so the supremum can equivalently be taken over ${\varphi}$ satisfying ${\varphi}({{\boldsymbol {0}}}) = 0$, where $\mathbf{0}$ denotes (the equivalence class of) the constant zero function.
Let us denote the set of such functions by
{\begin{align*} {
{\mathcal{L}} = \{{\varphi} : {\mathcal{S}} \to {\mathbb{R}} : |{\varphi}(f) - {\varphi}(g)| \leq d(f,g) \text{ for all $f,g \in {\mathcal{S}}$},\ {\varphi}({{\boldsymbol {0}}}) = 0\}.
} \end{align*}}
Recall that the space of real-valued continuous functions on a compact metric space is equipped with the uniform norm,
{\begin{align*} {
\|{\varphi}\|_\infty := \sup_{f \in {\mathcal{S}}} |{\varphi}(f)| < \infty.
} \end{align*}}
For ${\varphi} \in {\mathcal{L}}$, the Lipschitz condition \eqref{lip1} and Lemma \ref{trivial_bound} imply
{\begin{align*} {
\|{\varphi}\|_\infty \leq 2.
} \end{align*}}
In particular, ${\mathcal{L}}$ is a uniformly bounded family of continuous functions.
Furthermore, since ${\mathcal{L}}$ consists of Lipschitz functions whose minimal Lipschitz constants are uniformly bounded, it is both equicontinuous and closed under the topology induced by the uniform norm.
By the Arzel\`a-Ascoli Theorem (see Munkres \cite{munkres00}, Theorem 47.1), ${\mathcal{L}}$ is compact in this topology.
Having made this observation, we are now ready to prove the following convergence result.

\begin{prop} \label{primeT_as}
As $n \to \infty$, $\mathcal{W}^1(\mu_n',{\mathcal{T}}\mu_n) \to 0$ almost surely.
\end{prop}

We will use the following well-known fact.

\begin{lemma}[Burkholder--Davis--Gundy Inequality, see \cite{burkholder-davis-gundy72}, Theorem 1.1]
\label{bdg}
Let $(M_n)_{n \geq 0}$ be a martingale, and write
{\begin{align*} {
M_n = \sum_{i = 0}^n d_i, \qquad \text{$d_0 = M_0$, $d_i = M_{i}-M_{i-1}$ for $i \geq 1$}.
} \end{align*}}
Let $M_n^* := \sup_{0 \leq i \leq n} M_n$.
Then for any $p \geq 1$, there are positive constants $c_p$ and $C_p$ such that
{\begin{align*} {
c_p\, {\mathbf{E}}\Bigg[\bigg(\sum_{i = 0}^n d_i^2\bigg)^{p/2}\Bigg]
\leq {\mathbf{E}}\big[(M_n^*)^p\big]
\leq C_p\, {\mathbf{E}}\Bigg[\bigg(\sum_{i = 0}^n d_i^2\bigg)^{p/2}\Bigg] \quad \text{for all $n \geq 0$.}
} \end{align*}}
\end{lemma}

\begin{proof}[Proof of Proposition \ref{primeT_as}]
We have
{\begin{align*} {
{\mathcal{W}}^1(\mu_n',{\mathcal{T}}\mu_n) = \sup_{{\varphi} \in {\mathcal{L}}} \frac{1}{n+1} \sum_{i = 0}^n \bigl({\varphi}(f_{i+1}) - {\mathbf{E}}{[ {{\varphi}(f_{i+1})} \: | \: {{\mathcal{F}}_i} ]}\bigr).
} \end{align*}}
Notice that for any fixed ${\varphi} \in {\mathcal{L}}$,
{\begin{align*} {
M_{n}({\varphi}) := \sum_{i = 0}^n \bigl({\varphi}(f_{i+1}) - {\mathbf{E}}{[ {{\varphi}(f_{i+1})} \: | \: {{\mathcal{F}}_i} ]}\bigr)
} \end{align*}}
defines a martingale $(M_n({\varphi}))_{n \geq 0}$ adapted to the filtration $({\mathcal{F}}_{n+1})_{n \geq 0}$.
By Lemma \ref{bdg}, there is a constant $C = C({\varphi})$ such that
{\begin{align*} {
{\mathbf{E}}\big[M_n({\varphi})^4\big] \leq {\mathbf{E}}\big[M_n^*({\varphi})^4\big]
 \leq C\, {\mathbf{E}}\Bigg[\bigg(\sum_{i = 0}^n \big({\varphi}(f_{i+1}) - {\mathbf{E}}{[ {{\varphi}(f_{i+1})} \: | \: {{\mathcal{F}}_i} ]}\big)^2\bigg)^2\Bigg]
\leq 16(n+1)^2,  
} \end{align*}}
where the final inequality follows from \eqref{lip1} and Lemma \ref{trivial_bound}.
A Markov bound now gives
{\begin{align*} {
\sum_{n = 0}^\infty {\mathbf{P}}\big((n+1)^{-1}|M_n({\varphi})| > (n+1)^{-1/5})
&= \sum_{n = 0}^\infty {\mathbf{P}}\big((n+1)^{-4}M_n({\varphi})^4 > (n+1)^{-4/5}) \\
&= \sum_{n = 0}^\infty {\mathbf{P}}\big(M_n({\varphi})^4 > (n+1)^{16/5}) \\
&\leq \sum_{n = 0}^\infty (n+1)^{-16/5}{\mathbf{E}}\big[M_n({\varphi})^4\big] \\
&\leq \sum_{n = 0}^\infty 16(n+1)^{-6/5}
< \infty.
} \end{align*}}
By Borel--Cantelli, we may conclude
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{|M_n({\varphi})|}{n+1} = 0 \quad \mathrm{a.s.} \label{phi_MG}
} \end{split} \end{align}}
As discussed above, ${\mathcal{L}}$ is compact in the uniform norm topology.
In particular, it is separable.
Let ${\varphi}_1,{\varphi}_2,\dots$ be a countable, dense subset of ${\mathcal{L}}$.
Because of \eqref{phi_MG}, we can say that with probability one,
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{M_n({\varphi}_j)}{n+1} = 0 \quad \text{for all $j \geq 1$.} \label{dense_to_0}
} \end{split} \end{align}}
Assume that this almost sure event occurs.
It is a simple observation that
{\begin{align*} {
\|{\varphi} - {\varphi}'\|_\infty < {\varepsilon} \quad \Rightarrow \quad \bigg|\frac{M_n({\varphi})}{n+1} - \frac{M_n({\varphi}')}{n+1}\bigg| < 2{\varepsilon},
} \end{align*}}
meaning $(M_n(\cdot)/(n+1))_{n \geq 0}$ is an equicontinuous sequence of functions on the compact metric space ${\mathcal{L}}$.
The assumption \eqref{dense_to_0} says that this family converges pointwise to $0$ on a dense subset.
The Arzel\`a-Ascoli Theorem forces this convergence to be uniform.
That is, for any ${\varepsilon} > 0$, there is $N$ large enough that
{\begin{align*} {
n \geq N \quad \Rightarrow \quad \sup_{{\varphi} \in {\mathcal{L}}} \frac{M_n({\varphi})}{n+1} = {\mathcal{W}}^1(\mu_n',{\mathcal{T}}\mu_n) < {\varepsilon}.
} \end{align*}}
We conclude that ${\mathcal{W}}^1(\mu_n',{\mathcal{T}}\mu_n)$ tends to 0 as $n \to \infty$.
As this holds given the almost sure event \eqref{dense_to_0}, we are done.
\end{proof}

\subsection{Convergence to fixed points of the update map} \label{convergence_fixed}
Proposition \ref{primeT_as} suggests that for large $n$, the empirical measure will nearly be a fixed point of ${\mathcal{T}}$.
In other words, the empirical measure should be close to the set
{\begin{align} \begin{split} {
{\mathcal{K}} := \{\nu \in {\mathcal{P}}^1({\mathcal{S}}) : {\mathcal{T}}\nu = \nu\}. \label{K_def}
} \end{split} \end{align}}
Indeed, the following corollary makes this intuition precise.
For ${\mathcal{U}} \subset {\mathcal{P}}^1({\mathcal{S}})$, we will denote distance to ${\mathcal{U}}$ by
{\begin{align*} {
{\mathcal{W}}^1(\mu,{\mathcal{U}}) := \inf_{\nu \in {\mathcal{U}}} {\mathcal{W}}^1(\mu,\nu), \quad \mu \in {\mathcal{P}}^1({\mathcal{S}}).
} \end{align*}}

\begin{cor} \label{close_probability}
As $n \to \infty$, ${\mathcal{W}}^1(\mu_n,{\mathcal{K}}) \to 0$ almost surely.
\end{cor}

\begin{proof}
First notice that ${\mathcal{T}}\delta_{{\boldsymbol {0}}} = \delta_{{\boldsymbol {0}}}$ so that ${\mathcal{K}}$ is nonempty.
Next we claim that for every ${\varepsilon} > 0$, there is $\delta > 0$ such that
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\nu,{\mathcal{T}}\nu) < \delta \quad \Rightarrow \quad {\mathcal{W}}^1(\nu,{\mathcal{K}}) < {\varepsilon}. \label{closetoK}
} \end{split} \end{align}}
If this were not true, then there would exist ${\varepsilon} > 0$ and a sequence $(\nu_k)_{k \geq 1}$ such that ${\mathcal{W}}^1(\nu_k,{\mathcal{T}}\nu_k) \to 0$ as $k \to \infty$, but ${\mathcal{W}}^1(\nu_k,{\mathcal{K}}) \geq {\varepsilon}$ for all $k$.
By (sequential) compactness of ${\mathcal{P}}^1({\mathcal{S}})$, we may pass to a subsequence and assume ${\mathcal{W}}^1(\nu_k,\nu) \to 0$ for some $\nu \in {\mathcal{P}}^1({\mathcal{S}})$.  This forces ${\mathcal{W}}^1(\nu,{\mathcal{K}}) \geq {\varepsilon}$, since
{\begin{align*} {
{\mathcal{W}}^1(\nu,{\mathcal{K}}) \geq {\mathcal{W}}^1(\nu_k,{\mathcal{K}}) - {\mathcal{W}}^1(\nu_k,\nu) \geq {\varepsilon} - {\mathcal{W}}^1(\nu_k,\nu) \quad \text{for all $k \geq 1$,}
} \end{align*}}
with ${\mathcal{W}}^1(\nu_k,\nu) \to 0$ as $k \to \infty$.
On the other hand, continuity of ${\mathcal{T}}$ guarantees ${\mathcal{W}}^1({\mathcal{T}}\nu_k,{\mathcal{T}}\nu) \to 0$.
The inequality
{\begin{align*} {
{\mathcal{W}}^1(\nu,{\mathcal{T}}\nu) \leq {\mathcal{W}}^1(\nu,\nu_k) + {\mathcal{W}}^1(\nu_k,{\mathcal{T}}\nu_k) + {\mathcal{W}}^1({\mathcal{T}}\nu_k,{\mathcal{T}}\nu)
} \end{align*}}
thus implies ${\mathcal{W}}^1(\nu,{\mathcal{T}}\nu) = 0$.
That is, $\nu \in {\mathcal{K}}$, a contradiction to ${\mathcal{W}}^1(\nu,{\mathcal{K}}) \geq {\varepsilon}$.

Lemma \ref{regT} and Proposition \ref{primeT_as} together imply ${\mathcal{W}}^1(\mu_n,{\mathcal{T}}\mu_n)$ converges to 0 almost surely as $n \to \infty$, and then
\eqref{closetoK} implies ${\mathcal{W}}^1(\mu_n,{\mathcal{K}}) \to 0$ almost surely as well.
\end{proof}

Now that the set ${\mathcal{K}}$ is seen to contain all possible limits of the empirical measure, we should like to have some description of its elements.
One suggestive fact proved below is that any measure in ${\mathcal{K}}$ places all its mass on those elements of ${\mathcal{S}}$ with norm 0 or 1.
This observation will be crucial in proving our characterization of the low temperature phase in Section \ref{empirical_limits}.

\begin{prop} \label{no_middle}
If $\nu \in {\mathcal{K}}$, then
{\begin{align*} {
\nu(\{f \in {\mathcal{S}} : 0 < \|f\| < 1\}) = 0.
} \end{align*}}
\end{prop}

\begin{proof}
First take $f \in {\mathcal{S}}$ to be non-random.
Then $Tf$ is the law of the random function
{\begin{align*} {
F(u) = \frac{\sum_{v \sim u} f(v) e^{\beta Y_u}}{\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} f(v) e^{\beta Y_w} + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y})}.
} \end{align*}}
If $\|f\| = 0$ or $\|f\| = 1$, then $\|F\| = \|f\|$.
If instead $0 < \|f\| < 1$, then $\|F\|$ is random and still satisfies $0 < \|F\| < 1$.
By summing over $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$, we have
{\begin{align*} {
{\mathbf{E}}\, \|F\| = {\mathbf{E}}\Bigg[\frac{\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u}}{\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})}\Bigg].
} \end{align*}}
Upon observing that for any constant $C > 0$, the mapping
{\begin{align*} {
t \mapsto \frac{t}{t+C} = 1 - \frac{C}{t+C}, \quad t > 0
} \end{align*}}
is strictly concave, we deduce from Jensen's inequality
{\begin{align*} {
{\mathbf{E}}\, \|F\| \leq \frac{{\mathbf{E}} \Big[\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u}\Big]}{{\mathbf{E}}\Big[\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u}\Big] + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})} = \frac{2d\, \|f\|\, {\mathbf{E}}\, e^{\beta Y}}{2d\, {\mathbf{E}}\, e^{\beta Y}} = \|f\|,
} \end{align*}}
where equality holds if and only if $\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u}$ is an almost sure constant.
Since the disorder distribution $\lambda$ is not a point mass, this is not the case.

Now let $\nu \in {\mathcal{P}}^1({\mathcal{S}})$, and take $f \in {\mathcal{S}}$ to be random and have law $\nu$.
If $\nu$ assigns positive measure to the set $\{f \in {\mathcal{S}} : 0 < \|f\| < 1\}$, then the above argument gives
{\begin{align*} {
\int \|F\|\ {\mathcal{T}}\nu({\mathrm{d}} F) = \iint \|F\|\ Tf({\mathrm{d}} F)\, \nu({\mathrm{d}} f) < \iint \|f\|\ Tf({\mathrm{d}} F)\, \nu({\mathrm{d}} f) = \int \|f\|\ \nu({\mathrm{d}} f).
} \end{align*}}
But when $\nu \in {\mathcal{K}}$, we clearly have
{\begin{align*} {
\int \|F\|\ {\mathcal{T}}\nu({\mathrm{d}} F) = \int \|F\|\ \nu({\mathrm{d}} F) =  \int \|f\|\ \nu({\mathrm{d}} f),
} \end{align*}}
contradicting the previous inequality.
We conclude that any $\nu \in {\mathcal{K}}$ must assign mass 0 to the set $\{f \in {\mathcal{S}} : 0 < \|f\| < 1\}$.
\end{proof}

\subsection{Variational formula for the free energy} \label{calculations}
Thus far we have established that for large $n$, $\mu_n$ is close to the set of fixed points of the update transformation ${\mathcal{T}}$.
This fact is useful in estimating free energy because $\mu_n$ is the empirical measure of the endpoint distributions $f_i$, and in turn the $f_i$ can be used to compute the ratio of successive partition functions.
Observe from \eqref{Zfrac} that
{\begin{align} \begin{split} {
F_n = \frac{1}{n}\sum_{i = 0}^{n-1} \bigg[\log \frac{Z_{i+1}}{Z_{i}}\bigg] 
= \frac{1}{n} \sum_{i = 0}^{n-1} \log\bigg((2d)^{-1}\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg). \label{Fn_cesaro}
} \end{split} \end{align}}
Conditioning the $i$-th summand on ${\mathcal{F}}_i$, we have
{\begin{align} \begin{split} {
\frac{1}{n} \sum_{i = 0}^{n-1} {\mathbf{E}}{\Bigg[ {\log\bigg((2d)^{-1}\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg)} \: \Bigg| \: {{\mathcal{F}}_{i}} \Bigg]} \label{Fn_cesaro_conditioned}
= \frac{1}{n} \sum_{i = 0}^{n-1} R(f_i),
} \end{split} \end{align}}
where for fixed $f \in {\mathcal{S}}$, the function
{\begin{align} \begin{split} {
R(f) := {\mathbf{E}} \log\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})\bigg) - \log 2d \label{R_def}
} \end{split} \end{align}}
averages over the randomness of the $Y_u$.
As before, $Y$ and  $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ denote i.i.d.~random variables with law $\lambda$. 
Taking expectation, we obtain from \eqref{Fn_cesaro_conditioned} that
{\begin{align} \begin{split} {
{\mathbf{E}}\, F_n = \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbf{E}}\, R(f_i), \label{full_expectation}
} \end{split} \end{align}}
where the expectations on the right-hand side are well-defined provided
{\begin{align} \begin{split} {
{\mathbf{E}}\, |R(f_i)| < \infty \quad \text{for all $i \geq 0$.} \label{finite_R_expectation}
} \end{split} \end{align}}
Before verifying \eqref{finite_R_expectation}, we pause to interpret the quantities under consideration.
The free energy formula \eqref{Fn_cesaro} has been previously used by many authors, where the quantity
{\begin{align*} {
\log\bigg((2d)^{-1} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg)
= \log\bigg(\sum_{y \in {\mathbb{Z}}^d} \frac{f_i(y)}{2d} \sum_{x \sim y} e^{\beta X_{i+1,\, x}}\bigg)
} \end{align*}}
is the log expected value under $\rho_i$ of the weight $\exp(\beta X_{i+1,\, \omega_{i+1}})$ at time $i+1$ and site $\omega_{i+1}$.
But this still requires knowledge of the environment after time $i$, and so perhaps a more natural quantity is the same log expected value but averaged over all possible realizations of the environment at time $i+1$.
That is, we take a conditional expectation given ${\mathcal{F}}_i$ to define the quantity $R(f_i)$, and \eqref{R_def} extends this definition to all of ${\mathcal{S}}$.
Doing so allows us to use the translation invariance of the law of the environment, since Lemma \ref{regularity}(a) below tells us $R(f)=R(g)$ whenever $f$ and $g$ are translates of each other in the sense of Corollary \ref{better_def_cor}.

\begin{lemma} \label{regularity}
For $f \in {\mathcal{S}}$, let $R(f)$ be defined by \eqref{R_def}.
Then the following statements hold:
\begin{itemize}
\item[(a)] $R : {\mathcal{S}} \to {\mathbb{R}}$ is a well-defined map.
\item[(b)] $R : {\mathcal{S}} \to {\mathbb{R}}$ is continuous:
For any $f \in {\mathcal{S}}$ and $\delta_2 > 0$, there exists $\delta_1 > 0$ such that
{\begin{align} \begin{split} {
d(f,g) < \delta_1 \quad \Rightarrow \quad |R(f) - R(g)| < \delta_2. \label{R_lemma_want}
} \end{split} \end{align}}
\end{itemize}
\end{lemma}

\begin{proof}
Fix $f \in {\mathcal{S}}$.
For (a), we must verify that the right-hand side of \eqref{R_def} is finite and does not depend on the representative $f \in S$.
For any positive random variable $X$, Jensen's inequality gives
$s := {\mathbf{E}} \log X \leq \log {\mathbf{E}}\, X$.
So if $s \ge 0$, then
$|s| = {\mathbf{E}} \log X \leq \log {\mathbf{E}}\, X$.
If $s < 0$, then
$|s| = -{\mathbf{E}} \log X = {\mathbf{E}} \log X^{-1} \leq \log {\mathbf{E}}\, X^{-1}$.
It follows that
{\begin{align} \begin{split} {
|s| = |{\mathbf{E}} \log X| \leq \max\{\log {\mathbf{E}}\, X,\log {\mathbf{E}}\, X^{-1}\} 
= \log\big(\max\{{\mathbf{E}}\, X,{\mathbf{E}}\, X^{-1}\}\big). \label{log_ineq}
} \end{split} \end{align}}
Let us apply \eqref{log_ineq} to the case when $X$ is the argument of the logarithm in \eqref{R_def}.
Taking expectation over the environment $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$, we have by linearity
{\begin{align} \begin{split} {
 {\mathbf{E}} \Bigg[\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y})\Bigg]
 = 2d\cdot {\mathbf{E}}(e^{\beta Y}) < \infty, \label{log1}
} \end{split} \end{align}}
and by \eqref{Fbound} we also have
{\begin{align} \begin{split} {
 {\mathbf{E}} \Bigg[\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f_i(v)e^{\beta Y_u} + 2d(1 - \|f\|){\mathbf{E}}(e^{\beta Y})\bigg)^{-1} \Bigg] \leq d^{-1} \cdot {\mathbf{E}}(e^{-\beta Y}) < \infty. \label{log2}
 } \end{split} \end{align}}
In light of \eqref{log_ineq}, the two inequalities \eqref{log1} and \eqref{log2} prove $|R(f)| < \infty$.
   
To show well-definedness, we suppose $d(f,g) = 0$.
From Lemma \ref{norm_equivalence} and \eqref{denominator_same} in the proof of Proposition \ref{same_law}, we know there is a coupling of the identically distributed environments $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ and $(Z_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ such that
{\begin{align*} {
\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})
= \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} g(v)e^{\beta Z_u} + 2d(1-\|g\|){\mathbf{E}}(e^{\beta Y}).
} \end{align*}}
This equality immediately shows $R(f) = R(g)$.

For (b), we fix $f \in {\mathcal{S}}$ and $\delta_2 > 0$.
Choose ${\varepsilon} \in (0,1)$ such that
{\begin{align*} {
-\frac{\delta_2}{2} < \log(1 - {\varepsilon}) < \log(1 + {\varepsilon}) < \frac{\delta_2}{2}.
} \end{align*}} 
Given ${\varepsilon}$, take $\delta > 0$ as in the proof of Proposition \ref{continuous1} (so that \eqref{kappa1_1}--\eqref{delta4} hold), and
suppose $d(f,g) < \delta$.
Upon choosing representatives $f,g \in S$, define ${\widetilde{{F}}}$ and ${\widetilde{{G}}}$ in the same way as follows \eqref{Asum}.
In this notation,
{\begin{align*} {
R(f) - R(g) = {\mathbf{E}} \log \frac{{\widetilde{{F}}}}{{\widetilde{{G}}}}.
} \end{align*}}
From \eqref{second_part2} and \eqref{kappa1_1}, we have
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad {\mathbf{E}}\, \bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg| < \frac{\varepsilon}{8}.
} \end{align*}}
A parallel analysis\footnote{Simply exchange ${\mathbf{E}}\, {\widetilde{{F}}}^{-2}$ and ${\mathbf{E}}\, {\widetilde{{G}}}^{-2}$ in the second expression of \eqref{second_part}, and then apply \eqref{Gbound} instead of \eqref{Fbound}.}
would also give
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad {\mathbf{E}}\, \bigg|\frac{{\widetilde{{F}}}}{{\widetilde{{G}}}}-1\bigg| < \frac{\varepsilon}{8}.
} \end{align*}}
Now, \eqref{log_ineq} implies
{\begin{align*} {
|R(f) - R(g)| = \bigg| {\mathbf{E}} \log \frac{{\widetilde{{F}}}}{{\widetilde{{G}}}} \bigg|
&\leq \bigg|\log {\mathbf{E}}\, \frac{{\widetilde{{F}}}}{{\widetilde{{G}}}} \bigg| + \bigg|\log {\mathbf{E}}\, \frac{{\widetilde{{G}}}}{{\widetilde{{F}}}} \bigg|.
} \end{align*}}
Since
{\begin{align*} {
\bigg|{\mathbf{E}}\, \frac{{\widetilde{{F}}}}{{\widetilde{{G}}}}-1\bigg|
\leq {\mathbf{E}}\, \bigg|\frac{{\widetilde{{F}}}}{{\widetilde{{G}}}}-1\bigg| < {\varepsilon}
\qquad \text{and} \qquad
\bigg|{\mathbf{E}}\, \frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|
\leq {\mathbf{E}}\, \bigg|\frac{{\widetilde{{G}}}}{{\widetilde{{F}}}}-1\bigg|< {\varepsilon}
} \end{align*}}
when $d(f,g) < \delta$, our choice of ${\varepsilon}$ implies
{\begin{align*} {
|R(f) - R(g)| < \frac{\delta_2}{2} + \frac{\delta_2}{2} = \delta_2.
} \end{align*}}
So $\delta_1 = \delta$ suffices for \eqref{R_lemma_want}.
\end{proof}

Lemma \ref{regularity}(b) implies that $R : {\mathcal{S}} \to {\mathbb{R}}$ is measurable.
We can thus define, for $\mu \in {\mathcal{P}}^1({\mathcal{S}})$, the integral
{\begin{align} \begin{split} {
{\mathcal{R}}(\mu) := \int R(f)\ \mu({\mathrm{d}} f). \label{R_def2}
} \end{split} \end{align}}
Together, Lemma \ref{regularity}(b), Lemma \ref{wasserstein_compact}(b), and Lemma \ref{portmanteau}(a) show ${\mathcal{R}} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ is continuous.
Furthermore, the continuity of $R$ on the compact space ${\mathcal{S}}$ implies boundedness.
In particular, \eqref{finite_R_expectation} is true, and so \eqref{full_expectation} holds.
We may now write
{\begin{align} \begin{split} {
{\mathbf{E}}\, F_n = \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbf{E}}\, R(f_i) 
= {\mathbf{E}}\bigg[\int R(f)\ \mu_{n-1}({\mathrm{d}} f)\bigg] 
= {\mathbf{E}}\, {\mathcal{R}}(\mu_{n-1}). \label{expectations_equal}
} \end{split} \end{align}}
The next result strengthens this comparison.

\begin{prop} \label{FR_prop}
As $n \to \infty$, $F_n - {\mathcal{R}}(\mu_{n-1}) \to 0$ almost surely.
\end{prop}

The proof will use the Burkholder-Davis-Gundy Inequality (Lemma \ref{bdg}), as well as the lemma below.

\begin{lemma} \label{variance_lemma}
Suppose $f \in {\mathcal{S}}$ has $\|f\| = 1$.
Let $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ be a collection of i.i.d.~random variables whose common law is $\lambda$.
Then there exists a constant $C$, depending only on $\lambda$, $\beta$, and $d$, such that
{\begin{align*} {
{\mathbf{E}}\big[(W - {\mathbf{E}}\, W)^4\big] \leq C,
} \end{align*}}
where
{\begin{align} \begin{split} {
W := \log \bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u}\bigg). \label{W_def}
} \end{split} \end{align}}
\end{lemma}

\begin{proof}
We start by expanding the expectation
{\begin{align*} {
{\mathbf{E}}\big[(W - {\mathbf{E}}\, W)^4\big] &= {\mathbf{E}}(W^4) - 4({\mathbf{E}}\, W){\mathbf{E}}(W^3) + 6({\mathbf{E}}\, W)^2{\mathbf{E}}(W^2) - 4({\mathbf{E}}\, W)^4 + ({\mathbf{E}}\, W)^4 \\
&\leq 2{\mathbf{E}}(W^4) + 6{\mathbf{E}}(W^2)
\leq 2{\mathbf{E}}(W^4) + 6\sqrt{{\mathbf{E}}(W^4)}.
} \end{align*}}
It thus suffices to show that ${\mathbf{E}}(W^4)$ is bounded by a constant not depending on $f$.
To simplify notation, we introduce the variable
{\begin{align*} {
X := \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v) e^{\beta Y_u},
} \end{align*}}
and let $Y$ denote a generic random variable with law $\lambda$.
We note that
{\begin{align} \begin{split} {
{\mathbf{E}}(X^2) = {\mathbf{E}}\Bigg[\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u}\bigg)^2\Bigg]
&= {\mathbf{E}} \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u' \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} \sum_{v' \sim u'} f(v)f(v')e^{\beta(Y_u+Y_{u'})} \\
&\leq {\mathbf{E}}(e^{2\beta Y})\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)\bigg)^2 = (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}). \label{2_norm_bound}
} \end{split} \end{align}}
We have
{\begin{align} \begin{split} {
{\mathbf{E}}(W^{4}) &= {\mathbf{P}}(X < 1){\mathbf{E}}{[ {W^{4}} \: | \: {X < 1} ]} + {\mathbf{P}}(X \geq 1){\mathbf{E}}{[ {W^{4}} \: | \: {X \geq 1} ]} \\
&=  {\mathbf{P}}(X < 1){\mathbf{E}}{[ {\log^{4}(X)} \: | \: {X < 1} ]} + {\mathbf{P}}(X \geq 1){\mathbf{E}}{[ {\log^{4}(X)} \: | \: {X \geq 1} ]}.
} \end{split} \end{align}}
Notice that $\log^{4}(t) \leq t^{-2}$ for all $t \in (0,1)$, while $\log^{4}(t) \leq t^{2}$ for all $t \in [1,\infty)$.
Since $X$ is positive, these observations lead to
{\begin{align*} {
{\mathbf{E}}(W^{4}) &\leq {\mathbf{P}}(X < 1) {\mathbf{E}}{[ {X^{-2}} \: | \: {X < 1} ]} + {\mathbf{P}}(X \geq 1){\mathbf{E}}{[ {X^2} \: | \: {X \geq 1} ]} \\
&\leq {\mathbf{E}}\, X^{-2} + {\mathbf{E}}\, X^{2} 
\leq d^{-2}\cdot {\mathbf{E}}(e^{-2\beta Y}) + (2d)^2 \cdot {\mathbf{E}}(e^{2\beta Y}),
} \end{align*}}
where the final inequality is due to \eqref{Fbound} and \eqref{2_norm_bound}.
\end{proof}

\begin{proof}[Proof of Proposition \ref{FR_prop}]
Referring to \eqref{Fn_cesaro_conditioned}, we consider the random variable
{\begin{align*} {
M_n := n(F_n - {\mathcal{R}}(\mu_{n-1})) &= \sum_{i = 0}^{n-1} \log\bigg((2d)^{-1}\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg) - R(f_i).
} \end{align*}}
Notice that $(M_n)_{n \geq 1}$ is a centered martingale adapted to $({\mathcal{F}}_n)_{n \geq 1}$, having martingale increments
{\begin{align*} {
d_i := W_i - {\mathbf{E}}{[ {W_i} \: | \: {{\mathcal{F}}_i} ]},
} \end{align*}}
where
{\begin{align*} {
W_i := \log\bigg(\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg),
} \end{align*}}
and
{\begin{align*} {
{\mathbf{E}}{[ {W_i} \: | \: {{\mathcal{F}}_i} ]} = {\mathbf{E}}{\Bigg[ {\log\bigg(\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f_i(y)e^{\beta X_{i+1,\, x}}\bigg)} \: \Bigg| \: {{\mathcal{F}}_{i}} \Bigg]} = R(f_i) + \log(2d).
} \end{align*}}
Applying Lemma \ref{variance_lemma}, one obtains
{\begin{align*} {
{\mathbf{E}}(d_i^4) = {\mathbf{E}}\big[(W_i-{\mathbf{E}}{[ {W_i} \: | \: {{\mathcal{F}}_i} ]})^4\big]
= {\mathbf{E}}\big[{\mathbf{E}}{[ {(W_i-{\mathbf{E}}{[ {W_i} \: | \: {{\mathcal{F}}_i} ]})^4} \: | \: {{\mathcal{F}}_i} ]}\big] \leq C_1,
} \end{align*}}
where $C_1$ depends only on $\lambda$, $\beta$, and $d$.
From Lemma \ref{bdg}, we now have
{\begin{align*} {
{\mathbf{E}}(M_n^4) \leq C_2\, {\mathbf{E}}\Bigg[\bigg(\sum_{i = 0}^{n-1} d_i^2 \bigg)^2\Bigg]
\leq C_2\sum_{i = 0}^{n-1} \sum_{j = 0}^{n-1} \sqrt{{\mathbf{E}}(d_i^4){\mathbf{E}}(d_j^4)}
\leq Cn^2,
} \end{align*}}
where the constant $C = C_1C_2$ is independent of $n$.
It follows that
{\begin{align*} {
{\mathbf{E}}\big[(n^{-1}M_n)^4] \leq Cn^{-2}.
} \end{align*}}
As in the proof of Proposition \ref{primeT_as}, an argument using Markov's inequality and Borel--Cantelli shows
{\begin{align*} {
\lim_{n \to \infty} |F_n - {\mathcal{R}}(\mu_{n-1})| = \lim_{n \to \infty} \frac{|M_n|}{n} = 0 \quad \mathrm{a.s.}
} \end{align*}}
\end{proof}

Given the convergence of $\mu_{n}$ to the set ${\mathcal{K}}$, one should expect ${\mathbf{E}}\, F_n = {\mathbf{E}}\,{\mathcal{R}}(\mu_{n-1})$ to become close to ${\mathcal{R}}(\nu)$ for some $\nu \in {\mathcal{K}}$.
One difficulty is that $\mu_{n}$ does not converge to a particular $\nu \in {\mathcal{K}}$, but rather becomes arbitrarily close to the set ${\mathcal{K}}$. 
Nevertheless, we can instead consider the subset
{\begin{align} \begin{split} {
{\mathcal{M}} = \Big\{\nu_0 \in {\mathcal{K}} : {\mathcal{R}}(\nu_0) = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)\Big\}, \label{M_def}
} \end{split} \end{align}}
and show convergence to ${\mathcal{M}}$.
One should predict that this is the case, since the system will seek a lowest energy state.
Note that ${\mathcal{R}}$ is continuous and ${\mathcal{K}}$ is compact (being a closed subset of a compact metric space), and so ${\mathcal{M}}$ is nonempty.
Moreover, ${\mathcal{M}}$ is a closed subset of the compact space ${\mathcal{K}}$, and so ${\mathcal{M}}$ is compact.
The first step in proving the desired convergence is the following consequence of Corollary \ref{close_probability}.

\begin{prop} \label{lower_bound}
Let ${\mathcal{K}}$ be defined by \eqref{K_def}.
Let ${\mathcal{R}} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ be defined by \eqref{R_def2}.
Then
{\begin{align} \begin{split} {
\liminf_{n \to \infty} F_n \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) \quad \mathrm{a.s.} \label{lower_bound_as}
} \end{split} \end{align}}
In particular,
{\begin{align} \begin{split} {
\liminf_{n \to \infty} {\mathbf{E}}\, F_n \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu). \label{lower_bound_eq}
} \end{split} \end{align}}
\end{prop}

\begin{proof}
${\mathcal{R}}$ is continuous on the compact metric space $({\mathcal{P}}^1({\mathcal{S}}),{\mathcal{W}}^1)$, and thus uniformly continuous.
Therefore, for any ${\varepsilon} > 0$, there is $\delta > 0$ such that
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu,\nu) < \delta \quad \Rightarrow \quad |{\mathcal{R}}(\mu) - {\mathcal{R}}(\nu)| < {\varepsilon}. \label{second_choice}
} \end{split} \end{align}}
By Corollary \ref{close_probability}, there is almost surely some $N$ satisfying
{\begin{align*} {
n \geq N \quad \Rightarrow \quad {\mathcal{W}}^1(\mu_{n},{\mathcal{K}}) < \delta.
} \end{align*}}
That is, for any $n \geq N$ there is some $\nu_n \in {\mathcal{K}}$ satisfying ${\mathcal{W}}^1(\mu_{n},\nu_n) < \delta$.
Now \eqref{second_choice} shows
{\begin{align*} {
n \geq N \quad \Rightarrow \quad {\mathcal{R}}(\mu_{n}) > {\mathcal{R}}(\nu_n) - {\varepsilon} \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) - {\varepsilon}.
} \end{align*}}
We have thus shown that with probability one,
{\begin{align} \begin{split} {
\liminf_{n \to \infty} {\mathcal{R}}(\mu_n) \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) - {\varepsilon}. \label{liminf_eps}
} \end{split} \end{align}}
Now we take a countable sequence ${\varepsilon}_k \to 0$ so that with probability one, \eqref{liminf_eps} holds with ${\varepsilon} = {\varepsilon}_k$ for every $k$.
That is,
{\begin{align} \begin{split} {
\liminf_{n \to \infty} {\mathcal{R}}(\mu_{n}) \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) \quad \mathrm{a.s.} \label{lower_bound_asR}
} \end{split} \end{align}}
Since ${\mathcal{R}}$ is bounded, we may apply Fatou's Lemma to obtain
{\begin{align} \begin{split} {
\liminf_{n \to \infty} {\mathbf{E}}\, {\mathcal{R}}(\mu_n) \geq {\mathbf{E}}\Big[\liminf_{n \to \infty} {\mathcal{R}}(\mu_{n})\Big] \geq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu). \label{lower_bound_eqR}
} \end{split} \end{align}}
Now \eqref{lower_bound_as} follows from \eqref{lower_bound_asR} and Proposition \ref{FR_prop}, and \eqref{lower_bound_eq} follows from \eqref{lower_bound_eqR} and \eqref{expectations_equal}.
\end{proof}

Following Proposition \ref{lower_bound}, we naturally ask if there is a matching upper bound.
The next result answers this question in the affirmative.
In light of \eqref{avgFn_lim}, we could have replaced the limit infimum in \eqref{lower_bound_eq} with a limit, but the result below independently proves convergence.
To state the full theorem, we need to denote one element of ${\mathcal{S}}$ in particular.
Notice that for $f \in S$ satisfying $f(u) = 1$ for some $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$, Proposition \ref{superisometry} implies $d(f,g) = 0$ for $g \in S$ if and only if $g(v) = 1$ for some $v \in {\mathbb{N}} \times {\mathbb{Z}}^d$.
We can thus define the element ${{\boldsymbol {1}}} \in {\mathcal{S}}$ whose representatives in $S$ are the norm-1 functions supported on a single point.

\begin{thm} \label{upper_bound}
Let ${\mathcal{K}}$ be defined by \eqref{K_def}.  
Let ${\mathcal{R}} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ be defined by \eqref{R_def2}.
Then
{\begin{align*} {
\limsup_{n \to \infty} {\mathbf{E}}\, F_n \leq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) \quad \mathrm{a.s.},
} \end{align*}}
and so
{\begin{align} \begin{split} {
\lim_{n \to \infty} F_n = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) \quad \mathrm{a.s.} \label{limit}
} \end{split} \end{align}}
The minimum value is equal to
{\begin{align} \begin{split} {
\inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) = \lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathcal{R}}({\mathcal{T}}^i\delta_{{{\boldsymbol {1}}}}). \label{minimal_value}
} \end{split} \end{align}}
\end{thm}
The key ingredient in the proof of Theorem \ref{upper_bound} is the following lemma.

\begin{lemma} \label{upper_bound_lemma}
For any $f_0 \in {\mathcal{S}}$ and $n \geq 1$,
{\begin{align*} {
\sum_{i = 0}^{n-1} {\mathcal{R}}({\mathcal{T}}^i \delta_{f_0}) \geq {\mathbf{E}}\log Z_n,
} \end{align*}}
where $\delta_{f_0} \in {\mathcal{P}}^1({\mathcal{S}})$ is the unit mass at $f_0$.
Equality holds if and only if $f_0 = {{\boldsymbol {1}}}$.
\end{lemma}

\begin{proof}
Fix a representative $f_0 \in S$.
Let $(Y^{(i)}_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$, $1 \leq i \leq n$, be independent collections of i.i.d.~random variables with law $\lambda$.
For $1 \leq i \leq n$, inductively define $f_i \in {\mathcal{S}}$ to have representative
{\begin{align*} {
f_i(u) = \frac{\sum_{v \sim u} f_{i-1}(v) \exp(\beta Y^{(i)}_u)}{ \sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d}\sum_{v \sim w} f_{i-1}(v) \exp(\beta Y^{(i)}_w) + 2d(1-\|f_{i-1}\|){\mathbf{E}}(e^{\beta Y})},
} \end{align*}}
where as usual $Y$ is a generic random variable with law $\lambda$, so that the law of $f_i$ is equal to ${\mathcal{T}}$ applied to the law of $f_{i-1}$.
Upon making the inductive assumption that the law of $f_{i-1}$ is ${\mathcal{T}}^{i-1}\delta_{f_0}$, we see that the law of $f_i$ is ${\mathcal{T}}^i \delta_{f_0}$.
By definitions \eqref{R_def2} and \eqref{R_def},
{\begin{align*} {
{\mathcal{R}}({\mathcal{T}}^i \delta_{f_0}) &= {\mathbf{E}}\Bigg[\log\bigg(\sum_{u_{i+1} \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_i \sim u_{i+1}} f_i(u_i)\exp(\beta Y^{(i+1)}_{u_{i+1}}) + 2d(1-\|f_i\|){\mathbf{E}}(e^{\beta Y})\bigg)\Bigg] - \log 2d \\
&= {\mathbf{E}} \log D_{i+1} - \log 2d,
} \end{align*}}
where
{\begin{align} \begin{split} {
D_i := \sum_{u_i \in {\mathbb{N}} \times {\mathbb{Z}}^d}\sum_{u_{i-1} \sim u_i} f_{i-1}(u_{i-1})\exp(\beta Y^{(i)}_{u_i}) + 2d(1 - \|f_{i-1}\|){\mathbf{E}}(e^{\beta Y}), \quad 1 \leq i \leq n. \label{mess0}
} \end{split} \end{align}}
When $i = n$, observe that the first summand in $D_n$ is equal to
{\begin{align} \begin{split} {
&\sum_{u_{n} \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_{n-1} \sim u_{n}} f_{n-1}(u_{n-1})\exp(\beta Y^{(n)}_{u_{n}})  \\
&= \frac{1}{D_{n-1}}\sum_{u_{n} \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_{n-2} \sim u_{n-1} \sim u_{n}} f_{n-2}(u_{n-2})\exp(\beta Y^{(n-1)}_{u_{n-1}} + \beta Y^{(n)}_{u_{n}})  \\
&= \frac{1}{D_{n-2}D_{n-1}}\sum_{u_{n} \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_{n-3} \sim u_{n-2} \sim u_{n-1} \sim u_{n}} f_{n-3}(u_{n-3})\exp(\beta Y^{(n-2)}_{u_{n-2}} + \beta Y^{(n-1)}_{u_{n-1}} + \beta Y^{(n)}_{u_{n}}) \\
&\hspace{0.08in}\vdots \\
&= \frac{1}{D_1D_2\cdots D_{n-1}}\sum_{u_{n} \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_{0} \sim u_{1} \sim \cdots \sim u_{n}} f_{0}(u_{0})\exp\bigg(\beta \sum_{i = 1}^{n} Y^{(i)}_{u_{i}}\bigg)  \\
&= \frac{1}{D_1D_2\cdots D_{n-1}}\sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0} f_0(u_0)\exp\bigg(\beta \sum_{i = 1}^{n} Y^{(i)}_{u_{i}}\bigg)  \\
&= \frac{1}{D_1D_2\cdots D_{n-1}}\sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|}\, \|f_0\|\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0} \exp\bigg(\beta \sum_{i = 1}^{n} Y^{(i)}_{u_{i}}\bigg) \label{mess1},
} \end{split} \end{align}}
while the second summand is
{\begin{align} \begin{split} {
2d(1 - \|f_{n-1}\|){\mathbf{E}}(e^{\beta Y})
&= 2d\bigg(1 - \frac{1}{D_{n-1}}\sum_{u_{n-1} \in {\mathbb{N}} \times {\mathbb{Z}}^d}\sum_{u_{n-2} \sim u_{n-1}} f_{n-2}(u_{n-2})\exp(\beta Y^{(n-1)}_{u_{n-1}})\bigg){\mathbf{E}}(e^{\beta Y})  \\
&= 2d\, \frac{2d(1 - \|f_{n-2}\|){\mathbf{E}}(e^{\beta Y})}{D_{n-1}}\, {\mathbf{E}}(e^{\beta Y})  \\
&= \frac{(2d)^3(1 - \|f_{n-3}\|)({\mathbf{E}}\, e^{\beta Y})^3}{D_{n-2}D_{n-1}} \\
&\hspace{0.08in}\vdots \\
&= \frac{(2d)^{n}(1 - \|f_{0}\|)({\mathbf{E}}\, e^{\beta Y})^{n}}{D_1D_2 \cdots D_{n-1}}
= \frac{(2d)^n(1-\|f_0\|)\, {\mathbf{E}}(Z_{n})}{D_1D_2\cdots D_{n-1}}.
 \label{mess2} \raisetag{5\baselineskip}
} \end{split} \end{align}}
By summing the final expressions in \eqref{mess1} and \eqref{mess2} to obtain the right-hand side of \eqref{mess0}, and then clearing the fraction, we see
{\begin{align*} {
&D_1D_2\cdots D_{n-1}D_{n} \\
&= \sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|}\Bigg[\|f_0\|\Bigg(\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0}\exp\bigg(\beta \sum_{i = 1}^n Y^{(i)}_{u_i}\bigg)\Bigg) + (2d)^n(1 - \|f_0\|){\mathbf{E}}(Z_{n})\Bigg].
} \end{align*}}
Using the concavity of the $\log$ function, we further deduce
{\begin{align*} {
&\log D_1D_2\cdots D_n \\
&\geq 
\sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|} \log \Bigg[\|f_0\|\Bigg(\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0}\exp\bigg(\beta \sum_{i = 1}^n Y^{(i)}_{u_i}\bigg)\Bigg) + (2d)^n(1 - \|f_0\|){\mathbf{E}}(Z_{n})\Bigg] \\
&\geq \sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|}
\Bigg[\|f_0\|\log \Bigg(\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0}\exp\bigg(\beta \sum_{i = 1}^n Y^{(i)}_{u_i}\bigg)\Bigg) + (1 - \|f_0\|)\log\big((2d)^n {\mathbf{E}}\, Z_{n}\big)\Bigg] \\
&\geq \sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|}
\Bigg[\|f_0\|\log \Bigg(\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0}\exp\bigg(\beta \sum_{i = 1}^n Y^{(i)}_{u_i}\bigg)\Bigg) + (1 - \|f_0\|){\mathbf{E}}\log\big((2d)^nZ_{n}\big)\Bigg],
} \end{align*}}
where equality holds throughout if and only if $f_0(u_0) = 1$ for some $u_0 \in {\mathbb{Z}}^d$.
Since the random variable $\sum_{u_{n} \sim u_{n-1} \sim \cdots \sim u_0}\exp\big(\beta \sum_{i = 1}^n Y^{(i)}_{u_i}\big)$ is equal in law to $(2d)^n Z_n$ for any fixed $u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d$, taking expectation yields
{\begin{align*} {
{\mathbf{E}} \log D_1D_2\cdots D_n &\geq \sum_{u_0 \in {\mathbb{N}} \times {\mathbb{Z}}^d} \frac{f_0(u_0)}{\|f_0\|} \Big(\|f_0\|\, {\mathbf{E}} \log\big((2d)^n Z_n) + (1 - \|f_0\|)\, {\mathbf{E}}\log\big((2d)^nZ_n\big)\Big) \\
&= {\mathbf{E}}\, \log\big((2d)^nZ_n\big).
} \end{align*}}
It follows that
{\begin{align*} {
\sum_{i = 0}^{n-1}  {\mathcal{R}}({\mathcal{T}}^i \delta_{f_0})
= \sum_{i = 0}^{n-1}\big({\mathbf{E}}\log D_{i+1} - \log 2d\big)
&= {\mathbf{E}} \log(D_1D_2\cdots D_n) - \log (2d)^n \\
&\geq {\mathbf{E}}\log (2d)^n Z_n - \log(2d)^n
= {\mathbf{E}}\log Z_n,
} \end{align*}}
with equality if and only if $f_0 = {{\boldsymbol {1}}}$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{upper_bound}]
Consider any $\nu \in {\mathcal{K}}$.
Using the fact that ${\mathcal{T}}\nu = \nu$ and Lemma \ref{upper_bound_lemma}, we have
{\begin{align*} {
{\mathcal{R}}(\nu) = \frac{1}{n} \sum_{i = 0}^{n-1} {\mathcal{R}}({\mathcal{T}}^i \nu)
= \int \frac{1}{n} \sum_{i = 0}^{n-1} {\mathcal{R}}({\mathcal{T}}^i\delta_f)\ \nu({\mathrm{d}} f)
\geq \int \frac{1}{n}\, {\mathbf{E}} \log Z_n\ \nu({\mathrm{d}} f)
= {\mathbf{E}}\, F_n.
} \end{align*}}
We note that linearity \eqref{T_linear} and Fubini's theorem \eqref{T_fubini} have been applied, which is permissible since Lemma \ref{regularity}(b) shows that $R$ is continuous on ${\mathcal{S}}$ and hence bounded.
As this estimate holds for every $\nu \in {\mathcal{K}}$ and every $n$, we have
{\begin{align} \begin{split} {
\limsup_{n \to \infty} {\mathbf{E}}\, F_n \leq \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu). \label{upper_bound_eq}
} \end{split} \end{align}}
It now follows from \eqref{lower_bound_eq} and \eqref{upper_bound_eq} that
{\begin{align} \begin{split} {
\lim_{n \to \infty} {\mathbf{E}}\, F_n = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu). \label{expectations_converge}
} \end{split} \end{align}}
Theorem \ref{Fn_converges} improves \eqref{expectations_converge} to \eqref{limit}.
Finally, equation \eqref{minimal_value} follows from the final statement in Lemma~\ref{upper_bound_lemma}.
\end{proof}

We now turn to strengthening Corollary \ref{close_probability} by proving convergence not only to ${\mathcal{K}}$, but to the smaller set ${\mathcal{M}}$, defined in \eqref{M_def}.

\begin{thm} \label{close_M}
As $n \to \infty$, ${\mathcal{W}}^1(\mu_n,{\mathcal{M}}) \to 0$ almost surely.
\end{thm}

\begin{proof}
Let ${\varepsilon} > 0$ be given.
We claim there exists $\delta_2 > 0$ such that
{\begin{align} \begin{split} {
\nu_0 \in {\mathcal{K}},\ {\mathcal{R}}(\nu_0) < \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) + \delta_2 \quad \Rightarrow \quad \label{setup1}
{\mathcal{W}}^1(\nu_0,{\mathcal{M}}) < \frac{\varepsilon}{2}.
} \end{split} \end{align}}
If this were not the case, then one could find a sequence $(\nu_k)_{k \geq 1}$ in ${\mathcal{K}}$ such that
${\mathcal{R}}(\nu_k) \searrow \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)$ as $k \to \infty$, but ${\mathcal{W}}^1(\nu_k,{\mathcal{M}}) \geq {\varepsilon}/2$ for all $k$.
Since ${\mathcal{K}}$ is compact, we may pass to a subsequence and assume $\nu_k$ converges to some $\nu_0 \in {\mathcal{K}}$.
It follows that ${\mathcal{W}}^1(\nu_0,{\mathcal{M}}) \geq {\varepsilon}/2$.
But the continuity of ${\mathcal{R}}$ implies ${\mathcal{R}}(\nu_0) = \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)$, meaning $\nu \in {\mathcal{M}}$, a contradiction.

Again by (uniform) continuity of ${\mathcal{R}}$, we may choose $\delta_1 < {\varepsilon}/2$ such that
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu,\nu) < \delta_1 \quad \Rightarrow \quad |{\mathcal{R}}(\mu) - {\mathcal{R}}(\nu)| < \frac{\delta_2}{2}. \label{setup2}
} \end{split} \end{align}}
We know from Corollary \ref{close_probability} that ${\mathcal{W}}^1(\mu_n,{\mathcal{K}}) \to 0$ almost surely as $n \to \infty$.
Therefore, there is almost surely some $N_1$ such that
{\begin{align*} {
n \geq N_1 \quad \Rightarrow \quad {\mathcal{W}}^1(\mu_n,{\mathcal{K}}) < \delta_1.
} \end{align*}}
Furthermore, Proposition \ref{FR_prop} and Theorem \ref{upper_bound} tell us ${\mathcal{R}}(\mu_{n-1}) \to \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)$ almost surely.
We can thus choose $N_2$ so that
{\begin{align*} {
n \geq N_2 \quad \Rightarrow \quad \Big|{\mathcal{R}}(\mu_n) - \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)\Big| < \frac{\delta_2}{2}.
} \end{align*}}
So for $n \geq \max\{N_1,N_2\}$, we have both
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu_n,{\mathcal{K}}) < \delta_1 < \frac{\varepsilon}{2} \label{event1}
} \end{split} \end{align}}
and
{\begin{align} \begin{split} {
\Big|{\mathcal{R}}(\mu_n) - \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu)\Big| < \frac{\delta_2}{2}. \label{event2}
} \end{split} \end{align}}
The first inequality in \eqref{event1} implies the existence of $\nu_n \in {\mathcal{K}}$ satisfying ${\mathcal{W}}^1(\mu_n,\nu_n) < \delta_1$.
Then \eqref{setup2} and \eqref{event2} give
{\begin{align*} {
{\mathcal{R}}(\nu_n) < {\mathcal{R}}(\mu_n) + \frac{\delta_2}{2} < \inf_{\nu \in {\mathcal{K}}} {\mathcal{R}}(\nu) + \delta_2.
} \end{align*}}
Now the second inequality in \eqref{event1} and \eqref{setup1} yield
{\begin{align*} {
{\mathcal{W}}^1(\mu_n,{\mathcal{M}}) \leq {\mathcal{W}}^1(\mu_n,\nu_n) + {\mathcal{W}}^1(\nu_n,{\mathcal{M}}) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = {\varepsilon}.
} \end{align*}}
That is, for all $n \geq \max\{N_1,N_2\}$,
{\begin{align*} {
{\mathcal{W}}^1(\mu_n,{\mathcal{M}}) < {\varepsilon}.
} \end{align*}}
We have thus shown that almost surely,
{\begin{align*} {
\limsup_{n \to \infty} {\mathcal{W}}^1(\mu_n,{\mathcal{M}}) \leq {\varepsilon}.
} \end{align*}}
The claim now follows by taking a countable sequence ${\varepsilon}_k \to 0$ and using the above argument to conclude that almost surely,
{\begin{align*} {
\limsup_{n \to \infty} {\mathcal{W}}^1(\mu_n,{\mathcal{M}}) \leq {\varepsilon}_k \quad \text{for all $k$}
\quad \Rightarrow \quad
\lim_{n \to \infty} {\mathcal{W}}^1(\mu_n,{\mathcal{M}}) = 0.
} \end{align*}}
\end{proof}

\section{Limits of empirical measures} \label{empirical_limits}
In the first part of this section, we give a characterization of the low temperature regime in terms of the fixed points of the update map ${\mathcal{T}}$.
This is stated as Theorem \ref{characterization}(b) and completes the construction of our so-called abstract machine.
We know from Theorem \ref{close_M} that those fixed points minimizing the energy functional ${\mathcal{R}}$ constitute the possible limits of the empirical measure $\mu_n$, and so this characterization will allow us to study the Ces\`aro asymptotics of endpoint distributions.

In Section \ref{example_application}, we make this connection more concrete by outlining the general steps with which the abstract machine can be used to prove statements about directed polymers.
Indeed, the strategy described will be employed in Sections \ref{main_thm} and \ref{main_thm2}.
But to provide a more brief, instructional example in the present section, we give a new proof of Theorem \ref{characterization0},
which offered a first characterization of the low temperature phase.
Once appropriate definitions are made, only a short argument is needed to prove the statement.

\subsection{Characterizing high and low temperature phases} \label{norm_monotonicity}
We first set forth some notation: For $f \in {\mathcal{S}}$ and $\alpha \in [0,1]$, let $\alpha f \in {\mathcal{S}}$ have representative in $S$ defined by $(\alpha f)(u) := \alpha f(u)$.
It is immediate from Corollary \ref{better_def_cor} that $\alpha f = \alpha g$ in ${\mathcal{S}}$ whenever $f = g$ in $S$, and so $\alpha f$ is a well-defined element of ${\mathcal{S}}$.
In particular, $0f = {{\boldsymbol {0}}}$ is the element of ${\mathcal{S}}$ whose unique representative in $S$ is the constant zero function.
Also recall from Section \ref{calculations} that ${{\boldsymbol {1}}}$ is the element of ${\mathcal{S}}$ whose representatives in $S$ are the norm-1 functions supported on a single point.

\begin{lemma} \label{unique_max}
The map $R : {\mathcal{S}} \to {\mathbb{R}}$ defined by \eqref{R_def} has the following monotonicity property:
{\begin{align*} {
R(\alpha f) > R(f) \quad \text{for all $\alpha \in [0,1)$, $f \neq {{\boldsymbol {0}}}$.}
} \end{align*}}
Furthermore, $R$ achieves a unique maximum 
{\begin{align*} {
R({{\boldsymbol {0}}}) = \log({\mathbf{E}}\, e^{\beta X_u}) = c(\beta),
} \end{align*}}
and a unique minimum 
{\begin{align*} {
R({{\boldsymbol {1}}}) = {\mathbf{E}}\log Z_1.
} \end{align*}}
\end{lemma}

\begin{proof}
As before, let $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ and $Y$ be i.i.d.~random variables with law $\lambda$. By the concavity of the $\log$ function,
{\begin{align} \begin{split} {
R(f) &= {\mathbf{E}} \log \bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})\bigg) - \log 2d \\
&\leq \log {\mathbf{E}}\Bigg[\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u}+ 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})\Bigg] - \log 2d \\
&= \log({\mathbf{E}}\, e^{\beta Y}) = R({{\boldsymbol {0}}}). \label{firstcase}
} \end{split} \end{align}}
Now let $f \in {\mathcal{S}}$ be arbitrary and $\alpha \in [0,1)$.
Once again we use the concavity of the $\log$ function, but now before taking expectation:
{\begin{align*} {
R(\alpha f) &= {\mathbf{E}}\log\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} \alpha f(v)e^{\beta Y_u} + 2d(1-\|\alpha f\|){\mathbf{E}}(e^{\beta Y})\bigg) - \log 2d \\
&= {\mathbf{E}}\log\Bigg[\alpha\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})\Bigg) + (1 - \alpha)(2d\, {\mathbf{E}}\, e^{\beta Y}) \bigg] - \log 2d \\
&\geq  \alpha\, {\mathbf{E}}\log\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})\bigg) + (1-\alpha) \log(2d\, {\mathbf{E}}\, e^{\beta Y}) - \log 2d \\
&= \alpha\, R(f) + (1-\alpha) \log({\mathbf{E}}\, e^{\beta Y})
\geq R(f),
} \end{align*}}
where the final inequality is due to \eqref{firstcase}.
Furthermore, equality occurs in \eqref{firstcase} if and only if the integrand
{\begin{align*} {
{\widetilde{{F}}} = \sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u} f(v)e^{\beta Y_u} + 2d(1-\|f\|){\mathbf{E}}(e^{\beta Y})
} \end{align*}}
is an almost sure constant.
Since the $Y_u$ are non-degenerate, it is evident that whenever $f \neq {{\boldsymbol {0}}}$, ${\widetilde{{F}}}$ is not an almost sure constant.
Consequently, $R$ has a unique maximum at ${{\boldsymbol {0}}}$,
and the inequality $R(\alpha f) > R(f)$ is indeed strict when $f \neq {{\boldsymbol {0}}}$.

On the other hand, $R({{\boldsymbol {1}}}) = {\mathbf{E}} \log Z_1$ by Lemma \ref{upper_bound_lemma}.
Furthermore, for any $f \in {\mathcal{S}}$ with $\|f\| = 1$, we have
{\begin{align*} {
R(f) =  {\mathbf{E}}\log\bigg(\sum_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim u}  f(v)e^{\beta Y_u} \bigg) - \log 2d
&=  {\mathbf{E}}\log\bigg(\sum_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(v) \sum_{u \sim v}  e^{\beta Y_u} \bigg) - \log 2d \\
&\geq \sum_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(v)\, {\mathbf{E}} \log \bigg(\sum_{u \sim v}  e^{\beta Y_u} \bigg) - \log 2d \\
&= {\mathbf{E}} \log\bigg((2d)^{-1} \sum_{u \sim (0,0)} e^{\beta Y_u}\bigg) = {\mathbf{E}} \log Z_1 = R({{\boldsymbol {1}}}),
} \end{align*}}
where the inequality is strict if and only if there are $v_1,v_2 \in {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $f(v_1)$ and $f(v_2)$ are both positive.
That is, the inequality is strict whenever $f \neq {{\boldsymbol {1}}}$.
We have thus shown
{\begin{align*} {
R({{\boldsymbol {1}}}) \leq R\bigg(\frac{f}{\|f\|}\bigg) \leq R(f),
} \end{align*}}
where at least one of the inequalities is strict if $f \neq {{\boldsymbol {1}}}$.
Specifically, the first is strict if $f$ is not a multiple of ${{\boldsymbol {1}}}$, and the second is strict if $\|f\| < 1$.
Indeed, $R$ has a unique minimum at ${{\boldsymbol {1}}}$.
\end{proof}

\begin{thm} \label{characterization}
Assume \eqref{mgf}. 
Consider ${\mathcal{K}}$ and ${\mathcal{M}}$ defined by \eqref{K_def} and \eqref{M_def}, respectively.
The following statements hold:
\begin{itemize}
\item[(a)] If $0 \leq \beta \leq \beta_c$, then ${\mathcal{K}} = {\mathcal{M}} = \{\delta_{{{\boldsymbol {0}}}}\}$.
\item[(b)] If $\beta > \beta_c$, then $\nu(\{f \in {\mathcal{S}} : \|f\| = 1\}) = 1$ for every $\nu \in {\mathcal{M}}$, and so ${\mathcal{T}}$ has more than one fixed point.
\end{itemize}
\end{thm}

\begin{proof}
Since the hypotheses of (a) and (b) are complementary, it suffices to prove their converses.
We always have ${\mathcal{T}}\delta_{{{\boldsymbol {0}}}} = \delta_{{{\boldsymbol {0}}}}$.
That is, $\delta_{{{\boldsymbol {0}}}}$ is an element of ${\mathcal{K}}$, the set of fixed points of ${\mathcal{T}}$.
If ${\mathcal{K}}$ contains no other elements of ${\mathcal{P}}^1({\mathcal{S}})$, then ${\mathcal{M}} = \{\delta_{{{\boldsymbol {0}}}}\}$ and Theorem \ref{upper_bound} shows
{\begin{align*} {
p(\beta) := \lim_{n \to \infty} {\mathbf{E}}\, F_n = {\mathcal{R}}(\delta_{{\boldsymbol {0}}}) = R({{\boldsymbol {0}}}) = c(\beta).
} \end{align*}}
Hence $\Lambda(\beta) = 0$, where $\Lambda(\beta)$ is the Lyapunov exponent defined in Section \ref{disorder_background}.
That is, $0 \leq \beta \leq \beta_c$ when ${\mathcal{K}} = {\mathcal{M}} = \{\delta_{{{\boldsymbol {0}}}}\}$.

If instead there exists $\nu \in {\mathcal{K}}$ distinct from $\delta_{{\boldsymbol {0}}}$, then Proposition \ref{no_middle} implies that $\nu$ assigns positive mass to the set
{\begin{align*} {
{\mathcal{U}} := \{f \in {\mathcal{S}} : \|f\| = 1\}, 
} \end{align*}}
which is measurable by Lemma \ref{norm_equivalence}.
Moreover, Proposition \ref{no_middle} guarantees $\nu(\{{{\boldsymbol {0}}}\}) = 1 - \nu({\mathcal{U}})$,
and Lemma \ref{unique_max} shows $R(f) < R({{\boldsymbol {0}}})$ for all $f \in {\mathcal{U}}$.
It follows that
{\begin{align*} {
\lim_{n \to \infty} {\mathbf{E}}\, F_n \leq {\mathcal{R}}(\nu) = \int R(f)\ \nu({\mathrm{d}} f) < R({{\boldsymbol {0}}}).
} \end{align*}}
In this case we have $\Lambda(\beta) > 0$, meaning $\beta > \beta_c$.
Furthermore, we can consider the probability measure
{\begin{align*} {
\nu_{\mathcal{U}}({\mathcal{A}}) := \frac{\nu({\mathcal{A}} \cap {\mathcal{U}})}{\nu({\mathcal{U}})}, \quad \text{Borel } {\mathcal{A}} \subset {\mathcal{S}}.
} \end{align*}}
Notice that ${\mathcal{U}}$ is invariant under ${\mathcal{T}}$ in the following sense:
\begin{align}
f \in {\mathcal{U}} \quad \Rightarrow \quad Tf({\mathcal{U}}) = 1, \label{invariant1} \\
f \notin {\mathcal{U}} \quad \Rightarrow \quad Tf({\mathcal{U}})  = 0. \label{invariant2}
\end{align}
Indeed, this observation was made in the proof of Proposition \ref{no_middle}.
As ${\mathcal{T}}\nu = \nu$ and ${\mathcal{U}}$ is invariant under ${\mathcal{T}}$, we claim that $\nu_{\mathcal{U}}$ is again an element of ${\mathcal{K}}$.
For every Borel set ${\mathcal{A}} \subset {\mathcal{S}}$,
{\begin{align*} {
{\mathcal{T}}\nu_{\mathcal{U}}({\mathcal{A}})
= \int_{\mathcal{S}} Tf({\mathcal{A}})\ \nu_{\mathcal{U}}({\mathrm{d}} f)
= \int_{\mathcal{U}}  Tf({\mathcal{A}})\ \nu_{\mathcal{U}}({\mathrm{d}} f)
= \int_{\mathcal{U}}  Tf({\mathcal{A}}\cap {\mathcal{U}})\ \nu_{\mathcal{U}}({\mathrm{d}} f),
} \end{align*}}
where the second equality is justified by $\nu_{\mathcal{U}}({\mathcal{U}}) = 1$, and the third equality by \eqref{invariant1}.
Then using the fact that $\nu_{\mathcal{U}} = \nu/\nu({\mathcal{U}})$ on ${\mathcal{U}}$, followed by \eqref{invariant2}, we in turn get
{\begin{align*} {
{\mathcal{T}}\nu_{\mathcal{U}}({\mathcal{A}}) = \int_{\mathcal{U}}  Tf({\mathcal{A}}\cap {\mathcal{U}})\ \nu_{\mathcal{U}}({\mathrm{d}} f)
= \frac{1}{\nu({\mathcal{U}})} \int_{\mathcal{U}}  Tf({\mathcal{A}}\cap {\mathcal{U}})\ \nu({\mathrm{d}} f)
&= \frac{1}{\nu({\mathcal{U}})}  \int_{\mathcal{S}}  Tf({\mathcal{A}}\cap {\mathcal{U}})\ \nu({\mathrm{d}} f) \\
&= \frac{\nu({\mathcal{A}} \cap {\mathcal{U}})}{\nu({\mathcal{U}})} 
= \nu_{\mathcal{U}}({\mathcal{A}}).
} \end{align*}}
We have thus shown ${\mathcal{T}}\nu_{\mathcal{U}}({\mathcal{A}}) = \nu_{\mathcal{U}}({\mathcal{A}})$ for every Borel subset ${\mathcal{A}}$, and so $\nu_{\mathcal{U}} \in {\mathcal{K}}$ as claimed.
If $\nu({\mathcal{U}}) < 1$, then $\nu_{\mathcal{U}}$ satisfies
{\begin{align*} {
{\mathcal{R}}(\nu_{\mathcal{U}}) = \frac{1}{\nu({\mathcal{U}})} \int_{\mathcal{U}} R(f)\ \nu({\mathrm{d}} f)
&= \int_{\mathcal{U}} R(f)\ \nu({\mathrm{d}} f) + \frac{1 - \nu({\mathcal{U}})}{\nu({\mathcal{U}})} \int_{\mathcal{U}} R(f)\ \nu({\mathrm{d}} f) \\
&< \int_{\mathcal{U}} R(f)\ \nu({\mathrm{d}} f) + \big(1 - \nu({\mathcal{U}})\big)R({{\boldsymbol {0}}}) = {\mathcal{R}}(\nu).
} \end{align*}}
It follows that $\nu \in {\mathcal{M}}$ only if $\nu({\mathcal{U}}) = 1$.
\end{proof}

\subsection{An illustrative application} \label{example_application}
To demonstrate how the abstract machine can be employed to prove results on directed polymers, we will use it to prove Theorem \ref{characterization0}, which is restated here.
 
\begin{thm} \label{reprove_equivalence}
Assume \eqref{mgf}.
Then the following statements hold:
\begin{itemize}
\item[(a)] If $0 \leq \beta \leq \beta_c$, then
{\begin{align*} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \max_{x \in {\mathbb{Z}}^d} f_i(x) = 0 
 \quad \mathrm{a.s.}
 } \end{align*}}
\item[(b)] If $\beta > \beta_c$, then there exists $c > 0$ such that
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1}  \max_{x \in {\mathbb{Z}}^d} f_i(x) \geq c \quad \mathrm{a.s.} \label{liminf_max}
} \end{split} \end{align}}
 \end{itemize}
\end{thm}
The first step is to define the functional(s) relevant to the problem.
At first, definitions are made on the space $S$.
For instance, from \eqref{liminf_max} we are motivated to define $\max : S \to [0,1]$ by
{\begin{align*} {
\max(f) := \max_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u).
} \end{align*}}
By appealing to Corollary \ref{defined_pspm}, we see that the functional is well-defined on the quotient space ${\mathcal{S}}$.
This should be true in order for the abstract machinery to be applicable.
Notice that when $f_i$ is seen as an element of ${\mathcal{S}}$,
{\begin{align} \begin{split} {
\frac{1}{n} \sum_{i = 0}^{n-1} \max_{x \in {\mathbb{Z}}^d} f_i(x) = \int \max(f)\ \mu_{n-1}({\mathrm{d}} f). \label{max_abstract}
} \end{split} \end{align}}
The second step is to prove a continuity condition so that Portmanteau (Lemma \ref{portmanteau}) may be applied.
Therefore, it is important to understand the topology induced by the metric $d$.
Both here and in later sections, this step is the most technical part of the process, but is generally straightforward and follows a similar proof strategy.

\begin{lemma} \label{max_lemma}
The function $\max : {\mathcal{S}} \to [0,1]$ given by
{\begin{align*} {
\max(f) := \max_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u),
} \end{align*}}
is continuous and thus measurable.
\end{lemma}

\begin{proof}
This maximum function is well-defined by Corollary \ref{defined_pspm}.
Let $f \in {\mathcal{S}}$ be given, and fix a representative in $S$.
Choose $u \in {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $f(u) = \max_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(v)$.
If $f(u) = 0$, then $f$ is identically zero, and
{\begin{align*} {
d(f,g) < \delta \quad &\Rightarrow \quad \exists\ \phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d,\quad
\sum_{v \in A} g(\phi(v)) + \sum_{v \notin \phi(A)} g(v)^2 < \delta \\
&\Rightarrow \quad \max_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} g(v) < \max\{\delta,\sqrt{\delta}\}.
} \end{align*}}
Otherwise, for any $\delta \in (0,f(u)^2)$, we have
{\begin{align*} {
d(f,g) < \delta \quad &\Rightarrow \quad \exists\ \phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d,\quad
d_\phi(f,g) < \delta < f(u)^2 \\
&\Rightarrow \quad u \in A \text{ and } |f(v) - g(\phi(v))| < \delta \text{ for all $v \in A$}, \text{ and }
g(v)^2 < \delta \text{ for all $v \notin \phi(A)$}  \\
&\Rightarrow \quad \Big|\max_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} g(v) - \max_{v \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(v)\Big| < \max\{\delta,\sqrt{\delta}\}.
} \end{align*}}
From these two cases, we conclude that $f \mapsto \max f$ is continuous on ${\mathcal{S}}$.
\end{proof}

After the appropriate functionals have been defined, their limiting behavior (in a Ces\`aro sense) can be determined by applying Theorem \ref{characterization}.
Consequently, the next step in our abstract program is to study how the functionals of interest behave according to the elements of ${\mathcal{M}}$.
This will depend on the value of $\beta$.
For instance, in the high temperature phase, ${\mathcal{M}}$ consists of only the point mass $\delta_{{\boldsymbol {0}}}$ at the zero function ${{\boldsymbol {0}}}$, and so trivially we have
{\begin{align} \begin{split} {
0 \leq \beta \leq \beta_c \quad \Rightarrow \quad \int \max(f)\ \nu({\mathrm{d}} f) = \max({{\boldsymbol {0}}}) = 0 \quad \text{for all $\nu \in {\mathcal{M}}$.} \label{max_high}
} \end{split} \end{align}}
On the other hand, in the low temperature phase, every $\nu \in {\mathcal{M}}$ is supported on those $f \in {\mathcal{S}}$ with $\|f\| = 1$.
So clearly
{\begin{align*} {
\beta > \beta_c \quad \Rightarrow \quad \int \max(f)\ \nu({\mathrm{d}} f) > 0 \quad \text{for all $\nu \in {\mathcal{M}}$.}
} \end{align*}}
Furthermore, because of Lemmas \ref{wasserstein_compact}(b) and \ref{portmanteau}(a), Lemma \ref{max_lemma} shows $\nu \mapsto \int \max(f)\, \nu({\mathrm{d}} f)$ is continuous on the compact set ${\mathcal{M}}$, and so we actually have
{\begin{align} \begin{split} {
\beta > \beta_c \quad \Rightarrow \quad \int \max(f)\ \nu({\mathrm{d}} f) \geq c \quad \text{for all $\nu \in {\mathcal{M}}$,} \label{max_low}
} \end{split} \end{align}}
for some $c > 0$.
Now we are poised to prove the desired result.
The final step is to interpret the above observations in terms of the directed polymer model, via the almost sure convergence to ${\mathcal{M}}$ that is stated in Theorem \ref{close_M}.
It is useful to remember that ${\mathcal{S}}$ and ${\mathcal{P}}^1({\mathcal{S}})$ are compact spaces.

\begin{proof}[Proof of Theorem \ref{reprove_equivalence}]
By the (uniform) continuity of the map $\mu \mapsto \int\max(f)\, \mu({\mathrm{d}} f)$ on the compact space ${\mathcal{P}}^1({\mathcal{S}})$, we can find for any ${\varepsilon} > 0$ some $\delta > 0$ such that
{\begin{align*} {
{\mathcal{W}}^1(\mu,{\mathcal{M}}) < \delta \quad \Rightarrow \quad 
\inf_{\nu \in {\mathcal{M}}} \int \max(f)\ \nu({\mathrm{d}} f) - {\varepsilon}
\leq \int \max(f)\ \mu({\mathrm{d}} f)
\leq \sup_{\nu \in {\mathcal{M}}} \int \max(f)\ \nu({\mathrm{d}} f) + {\varepsilon}. 
} \end{align*}}
Thus Theorem \ref{close_M} implies that almost surely,
{\begin{align*} {
\inf_{\nu \in {\mathcal{M}}} \int \max(f)\ \nu({\mathrm{d}} f)
&\leq \liminf_{n \to \infty} \int \max(f)\ \mu_{n-1}({\mathrm{d}} f) \\
&\leq \limsup_{n \to \infty} \int \max(f)\ \mu_{n-1}({\mathrm{d}} f)
\leq \sup_{\nu \in {\mathcal{M}}} \int \max(f)\ \nu({\mathrm{d}} f).
} \end{align*}}
When $0 \leq \beta \leq \beta_c$, \eqref{max_high} says that the infimum and supremum appearing above are both equal to 0, and so
{\begin{align} \begin{split} {
\lim_{n \to \infty} \int \max(f)\ \mu_{n-1}({\mathrm{d}} f) = 0 \quad \mathrm{a.s.} \label{max_claim2}
} \end{split} \end{align}}
When $\beta > \beta_c$, \eqref{max_low} shows that the infimum is bounded below by $c > 0$, in which case we have
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \int \max(f)\ \mu_{n-1}({\mathrm{d}} f) \geq c \quad \mathrm{a.s.} \label{max_claim1}
} \end{split} \end{align}}
Recalling \eqref{max_abstract}, we see that \eqref{max_claim2} and \eqref{max_claim1} are exactly what we wanted to show.
\end{proof} 

\section{Asymptotic pure atomicity} \label{main_thm}

Following the approach outlined in Section \ref{example_application}, we will prove that directed polymers are asymptotically purely atomic if and only if the parameter $\beta$ falls in the low temperature regime.
We say that the sequence of endpoint probability mass functions $(f_i)_{i \geq 0}$ is \textit{asymptotically purely atomic}\footnote{In \cite{vargas07}, Vargas defines asymptotic pure atomicity by the same convergence condition, but in probability rather than almost surely.  \textit{A priori}, the definition considered here is a stronger one, but since \eqref{noVSD_claim} implies the negation of the ``in probability" definition, Theorem \ref{total_mass} shows the two notions are equivalent given \eqref{mgf}.}  if for every sequence $({\varepsilon}_i)_{i \geq 0}$ tending to $0$ as $i \to \infty$, we have
{\begin{align} \begin{split} { \label{apa_def}
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) = 1 \quad \mathrm{a.s.},
} \end{split} \end{align}}
where
{\begin{align*} {
{\mathcal{B}}_i^{\varepsilon} := \{x \in {\mathbb{Z}}^d : f_i(x) > {\varepsilon}\}, \quad i \geq 0,\ {\varepsilon} > 0.
} \end{align*}}
We begin by making the definitions necessary to put the abstract machine to work.

\subsection{Definitions of relevant functionals}
Observe that the quantity of interest, $\rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon})$, is a function of the mass function $f_i$.
Specifically,
{\begin{align*} {
\rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) = \sum_{x \in {\mathbb{Z}}^d\, : f_i(x) > {\varepsilon}} f_i(x).
} \end{align*}}
We are thus motivated to define $\|\cdot\|_{\varepsilon} : S \to [0,1]$ by
{\begin{align*} {
\|f\|_{\varepsilon} := \sum_{u\in{\mathbb{N}}\times{\mathbb{Z}}^d\, :\, f(u) > {\varepsilon}} f(u).
} \end{align*}}
For any ${\varepsilon} \in (0,1)$, the map $f \mapsto \|f\|_{\varepsilon}$ satisfies (i)--(iii) of Corollary \ref{defined_pspm} and thus induces a well-defined function on ${\mathcal{S}}$.
To establish asymptotic pure atomicity, it will be important that this function is lower semi-continuous, a fact we prove in the lemma below.
Another useful functional will be ${{\boldsymbol {I}}}_{\varepsilon} : S \to \{0,1\}$ given by
{\begin{align*} {
{{\boldsymbol {I}}}_{\varepsilon}(f) = \begin{cases}
1 &\textup{if } \max_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u) \geq {\varepsilon}, \\
0 &\textup{else}.
\end{cases}
} \end{align*}}
Once more, it is clear that ${{\boldsymbol {I}}}_{\varepsilon}$ satisfies the hypotheses of Corollary \ref{defined_pspm}, and so it is well-defined on~${\mathcal{S}}$.

\begin{lemma} \label{eps_norm_equivalence}
For any ${\varepsilon} \in (0,1)$, the following statements hold:
\begin{itemize}
\item[(a)] The map $\|\cdot\|_{\varepsilon} : {\mathcal{S}} \to [0,1]$ is lower semi-continuous and thus measurable.
\item[(b)] The map ${{\boldsymbol {I}}}_{\varepsilon} : {\mathcal{S}} \to \{0,1\}$ is upper semi-continuous and thus measurable.
\end{itemize}
\end{lemma}

\begin{proof}
For (a) it suffices to fix $f \in {\mathcal{S}}$, let $\delta_2 > 0$ be arbitrary, and find $\delta_1 > 0$ sufficiently small that
{\begin{align*} {
d(f,g) < \delta_1 \quad \Rightarrow \quad \|g\|_{\varepsilon} > \|f\|_{\varepsilon} - \delta_2.
} \end{align*}}
This task is trivial if $\|f\|_{\varepsilon} = 0$.
Otherwise, we pick a representative $f \in S$, and consider the nonempty set
{\begin{align*} {
A := \{u \in {\mathbb{N}} \times {\mathbb{Z}}^d : f(u) > {\varepsilon}\}.
} \end{align*}}
Since $A$ is finite,
{\begin{align*} {
t := \inf_{u \in A} f(u) > {\varepsilon}.
} \end{align*}}
Now choose $\delta_1 > 0$ such that
{\begin{align*} {
\delta_1 < \min\{\delta_2,{\varepsilon}^2,t-{\varepsilon}\}.
} \end{align*}}
If $d(f,g) < \delta_1$, then there is a representative $g \in S$ and an isometry $\phi : C \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $d_\phi(f,g) < \delta_1$.
It follows that $A \subset C$, since otherwise $d_\phi(f,g) \geq f(u)^2 > {\varepsilon}^2 > \delta_1$ for some $u \in A \setminus C$.
Consequently,
{\begin{align*} {
\sum_{u \in A} |f(u) - g(\phi(u))| \leq d_\phi(f,g) < \delta_1 < t - {\varepsilon}.
} \end{align*}}
In particular, for every $u \in A$,
{\begin{align*} {
g(\phi(u)) \geq f(u) - |f(u) - g(\phi(u))| > t - (t - {\varepsilon}) = {\varepsilon}.
} \end{align*}}
Hence
{\begin{align*} {
\|g\|_{\varepsilon} \geq \sum_{u \in A} g(\phi(u)) &\geq \sum_{u \in A} f(u) - \sum_{u \in A} |f(u) - g(\phi(u))| \\
&\geq \|f\|_{\varepsilon} - d_\phi(f,g) > \|f\|_{\varepsilon} - \delta_1 > \|f\|_{\varepsilon} - \delta_2,
} \end{align*}}
which completes the proof of (a).

Next, for claim (b) we need only to consider the case when $(f_n)_{n \geq 1}$ is a sequence in ${\mathcal{S}}$ satisfying ${{\boldsymbol {I}}}_{\varepsilon}(f_n) = 1$ for infinitely many $n$, and $d(f_n,f) \to 0$ as $n \to \infty$.
We must show ${{\boldsymbol {I}}}_{\varepsilon}(f) = 1$.
By passing to a subsequence, we may assume ${{\boldsymbol {I}}}_{\varepsilon}(f_n) = 1$ for all $n$.
That is, $\max(f_n) \geq {\varepsilon}$ for all $n$, and so Lemma \ref{max_lemma} forces $\max(f) \geq {\varepsilon}$, meaning ${{\boldsymbol {I}}}_{\varepsilon}(f) = 1$.
\end{proof}

\subsection{Proof of asymptotic pure atomicity at low temperature} \label{characterize_proof}
Before proving the main theorem of this section, we give a sufficient condition for asymptotic pure atomicity.

\begin{lemma} \label{equiv_apa}
If for every $c < 1$, there is ${\varepsilon} > 0$ such that
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) > c \quad \mathrm{a.s.}, \label{equiv_apa_eq}
} \end{split} \end{align}}
then $(f_i)_{i \geq 0}$ is asymptotically purely atomic.
\end{lemma}

\begin{proof}
Let $({\varepsilon}_i)_{i \geq 0}$ be a sequence tending to 0 as $i \to \infty$.
We will show that under the given hypothesis, for any $k \in {\mathbb{N}}$ there is almost surely some $N$ such that
{\begin{align} \begin{split} {
n \geq N \quad \Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \geq 1 - \frac{1}{k}. \label{apa_lemma_want}
} \end{split} \end{align}}
This is sufficient to verify asymptotic pure atomicity, since then with probability one,
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \geq 1 - \frac{1}{k} \quad \text{for every $k$},
} \end{align*}}
which of course implies \eqref{apa_def}.
So set $t := 1 - 1/k$, and choose any $c \in (t,1)$.
Then let ${\varepsilon} > 0$ be such that \eqref{equiv_apa_eq} is satisfied.
In particular, there almost surely exists $N_1$ such that
{\begin{align*} {
n \geq N_1 \quad \Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) > c.
} \end{align*}}
Next take $N_2$ large enough that
{\begin{align*} {
i \geq N_2 \quad \Rightarrow \quad {\varepsilon}_i < {\varepsilon} \quad \Rightarrow \quad \rho_i(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \geq \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}),
} \end{align*}}
and then finally choose $N_3 > N_2$ so large that
{\begin{align*} {
c - \frac{N_2}{N_3} > t.
} \end{align*}}
It follows that for all $n \geq \max\{N_1,N_3\}$,
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i})
&\geq \frac{1}{n} \sum_{i = N_2}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \\
&\geq \frac{N_2}{n} + \frac{1}{n} \sum_{i = N_2}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) - \frac{N_2}{N_3} \\
&\geq \frac{1}{n}\sum_{i = 0}^{N_2-1} \rho_{i}(\omega_{i} \in {\mathcal{B}}_i^{\varepsilon}) + \frac{1}{n} \sum_{i = N_2}^{n-1} \rho_{i}(\omega_{i} \in {\mathcal{B}}_i^{{\varepsilon}_i}) - \frac{N_2}{N_3} \\
&\geq \frac{1}{n}\sum_{i = 0}^{n-1} \rho_{i}(\omega_{i} \in {\mathcal{B}}_i^{\varepsilon}) - \frac{N_2}{N_3}
> c - \frac{N_2}{N_3} > t.
} \end{align*}}
So $N = \max\{N_1,N_3\}$ suffices for \eqref{apa_lemma_want}.
\end{proof}

\begin{thm} \label{total_mass}
Assume \eqref{mgf}.
Then the following statements hold:
\begin{itemize}
\item[(a)] If $\beta > \beta_c$, then $(f_i)_{i \geq 0}$ is asymptotically purely atomic.
\item[(b)] If $0 \leq \beta \leq \beta_c$, then there is a sequence $({\varepsilon}_i)_{i \geq 0}$ tending to 0 as $i \to \infty$, such that
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_{i}(\omega_i \in {\mathcal{B}}_i^{{\varepsilon}_i}) \to 0 \quad \mathrm{a.s.}\label{noVSD_claim}
} \end{split} \end{align}}
\end{itemize}
\end{thm}

While the functionals $\|\cdot\|_{\varepsilon}$ and ${{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\cdot)$ are not continuous, their semi-continuity will be sufficient to make a limiting argument, thanks to the following generalization of Dini's theorem.

\begin{lemma} \label{dini}
Let $({\mathcal{X}},\tau)$ be a compact metric space.
If $(F_n)_{n \geq 1}$ is a non-decreasing sequence of lower semi-continuous functions ${\mathcal{X}} \to {\mathbb{R}}$ converging pointwise to an upper semi-continuous function $F$, then the convergence is uniform.
\end{lemma}

\begin{proof}
Let $G_n := F - F_n$ so that $G_n \searrow 0$ pointwise on ${\mathcal{X}}$, and $G_n$ is upper semi-continuous.
For given ${\varepsilon} > 0$, consider the set
{\begin{align*} {
U_n := \{x \in {\mathcal{X}} : G_n(x) < {\varepsilon}\},
} \end{align*}}
which is open because $G_n$ is upper semi-continuous.
Since $G_n(x) \to 0$ for every $x \in {\mathcal{X}}$, we have $\bigcup_n U_n = {\mathcal{X}}$.
By compactness of ${\mathcal{X}}$, there is a finite list $n_1 < n_2 < \dots < n_k$ such that
$\bigcup_{j = 1}^k U_{n_j} = {\mathcal{X}}$.
But the monotonicity assumption guarantees $U_n$ is an ascending sequence.
Hence ${\mathcal{X}} = \bigcup_{j = 1}^k U_{n_j} = U_{n_k} = U_n$ for all $n \geq n_k$, meaning
{\begin{align*} {
n \geq n_k \quad \Rightarrow \quad
U_n = {\mathcal{X}} \quad \Rightarrow \quad
|F(x) - F_n(x)| = F(x) - F_n(x) < {\varepsilon} \quad \text{for all $x \in {\mathcal{X}}$.}
} \end{align*}}
That is, $F_n \nearrow F$ uniformly on ${\mathcal{X}}$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{total_mass}]
We first prove (a).
Suppose that $\beta > \beta_c$.
For ${\varepsilon} > 0$, define $\|\cdot\|_{\varepsilon}$ on ${\mathcal{P}}^1({\mathcal{S}})$ by
{\begin{align*} {
\|\nu\|_{\varepsilon} := \int \|f\|_{\varepsilon}\ \nu({\mathrm{d}} f)
= \int \sum_{u\, : f(u) > {\varepsilon}} f(u)\ \nu({\mathrm{d}} f).
} \end{align*}}
Then note that
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) 
= \frac{1}{n}\sum_{i = 0}^{n-1} \|f_i\|_{\varepsilon}
= \|\mu_{n-1}\|_{\varepsilon}.
} \end{align*}}
In light of Lemma \ref{wasserstein_compact}(b) and Lemma \ref{portmanteau}(b), Lemma \ref{eps_norm_equivalence}(a) implies $\|\cdot\|_{\varepsilon} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ is lower semi-continuous.
For any $f \in {\mathcal{S}}$ such that $\|f\| = 1$, we have $\|f\|_{\varepsilon} \nearrow 1$ as ${\varepsilon} \to 0$.
In particular, under the assumption that $\beta > \beta_c$,
Theorem \ref{characterization} implies that for any $\nu \in {\mathcal{M}}$,
{\begin{align*} {
 \|f\|_{\varepsilon} \nearrow 1 \quad \text{as ${\varepsilon} \to 0$,} \quad \nu\text{-a.s.}
} \end{align*}}
It follows by monotone convergence that
{\begin{align} \begin{split} {
\|\nu\|_{\varepsilon} \nearrow 1 \quad \text{as ${\varepsilon} \to 0$,} \label{pointwise_on_M}
} \end{split} \end{align}}
for every $\nu \in {\mathcal{M}}$.
Recall that ${\mathcal{M}}$ is compact.
Therefore, Lemma \ref{dini} strengthens the pointwise convergence \eqref{pointwise_on_M} to uniform convergence.
That is, for any $c < 1$, there is ${\varepsilon} > 0$ such that
{\begin{align} \begin{split} {
\|\nu\|_{\varepsilon} > c \quad \text{for all $\nu \in {\mathcal{M}}$.} \label{uniform_min}
} \end{split} \end{align}}
Given $c$ and ${\varepsilon}$ satisfying \eqref{uniform_min}, we claim there is $\delta > 0$ such that for $\mu \in {\mathcal{P}}^1({\mathcal{S}})$,
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu,{\mathcal{M}}) < \delta \quad \Rightarrow \quad \|\mu\|_{\varepsilon} > c. \label{uniform_close}
} \end{split} \end{align}}
If this were not true, then one could construct a sequence $(\nu_k)_{k \geq 1}$ such that ${\mathcal{W}}^1(\nu_k,{\mathcal{M}}) \to 0$ as $k \to \infty$, but $\|\nu_k\|_{\varepsilon} \leq c$ for all $k$.
By compactness, we could pass to a subsequence and assume $\nu_k \to \nu$ for some $\nu \in {\mathcal{P}}^1({\mathcal{S}})$.
But then ${\mathcal{W}}^1(\nu,{\mathcal{M}}) = 0$, implying $\nu \in {\mathcal{M}}$ since ${\mathcal{M}}$ is closed.
Hence $\|\nu\|_{\varepsilon} > c$ by \eqref{uniform_min}, a contradiction to the inequality given by lower semi-continuity:
{\begin{align*} {
\|\nu\|_{\varepsilon} \leq \liminf_{k \to \infty} \|\nu_k\|_{\varepsilon} \leq c.
} \end{align*}}
We can now verify the hypothesis of Lemma \ref{equiv_apa}.
Given $c < 1$, choose ${\varepsilon} > 0$ and $\delta > 0$ such that \eqref{uniform_min} and \eqref{uniform_close} hold.
By Theorem \ref{close_M}, there is almost surely $N$ large enough that
{\begin{align*} {
n \geq N \quad \Rightarrow \quad {\mathcal{W}}^1(\mu_{n-1},{\mathcal{M}}) < \delta
\quad \Rightarrow \quad \|\mu_{n-1}\|_{\varepsilon} > c,
} \end{align*}}
where
{\begin{align*} {
\|\mu_{n-1}\|_{\varepsilon} = \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}).
} \end{align*}}
Indeed, the hypothesis of Lemma \ref{equiv_apa} holds, and so $(f_i)_{i \geq 0}$ is asymptotically purely atomic.

For (b), we assume $0 \leq \beta \leq \beta_c$.
For ${\varepsilon} > 0$, define ${{\boldsymbol {\mathcal{I}}}}_{\varepsilon} : {\mathcal{P}}^1({\mathcal{S}}) \to {\mathbb{R}}$ by
{\begin{align*} {
{{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\nu) := \int {{\boldsymbol {I}}}_{\varepsilon}(f)\ \nu({\mathrm{d}} f)
= \int {\mathbb{I}}\Big\{\max_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d} f(u) \geq {\varepsilon}\Big\}\ \nu({\mathrm{d}} f),
} \end{align*}}
where ${\mathbb{I}}$ denotes the indicator function.
Considering Lemma \ref{wasserstein_compact}(b) and Lemma \ref{portmanteau}(c), we see from Lemma \ref{eps_norm_equivalence}(b) that ${{\boldsymbol {\mathcal{I}}}}_{\varepsilon}$ is an upper semi-continuous map.
By Theorem \ref{characterization} and Theorem \ref{close_M}, ${\mathcal{W}}^1(\mu_n,\delta_{{\boldsymbol {0}}}) \to 0$ almost surely as $n \to \infty$, and so
{\begin{align*} {
\limsup_{n \to \infty} {{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\mu_n) \leq {{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\delta_{{\boldsymbol {0}}}) = {{\boldsymbol {I}}}_{\varepsilon}({{\boldsymbol {0}}}) = 0 \quad \mathrm{a.s.}
} \end{align*}}
In particular, for any $j \in {\mathbb{N}}$,
{\begin{align} \begin{split} {
1 =  {\mathbf{P}}\bigg(\bigcup_{N = 1}^\infty \bigcap_{n = N}^\infty {{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\mu_{n-1}) < 2^{-j}\bigg)
&= \lim_{M \to \infty} {\mathbf{P}}\bigg(\bigcup_{N = 1}^M \bigcap_{n = N}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\mu_{n-1}) < 2^{-j}\}\bigg) \\
&=\lim_{M \to \infty} {\mathbf{P}}\bigg(\bigcap_{n = M}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\varepsilon}(\mu_{n-1}) < 2^{-j}\}\bigg). \label{limit_bad_probability}
} \end{split} \end{align}}
Now let $(\kappa_j)_{j \geq 1}$ be any decreasing sequence tending to $0$ as $j \to \infty$.
Set $M_0 := 1$.
By \eqref{limit_bad_probability}, we can inductively choose $M_j > M_{j-1}$ such that
\begin{align}
{\mathbf{P}}\bigg(\bigcap_{n = M_j}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}) < 2^{-j}\}\bigg) &> 1 - 2^{-j}, \nonumber
\intertext{which means}
{\mathbf{P}}\bigg(\bigcup_{n = M_j}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}) \geq 2^{-j}\} \bigg) &< 2^{-j}.
\label{bad_probability}
\end{align}
Define ${\varepsilon}_i := \kappa_j$ when $M_{j-1} \leq i \leq M_j - 1$.
For $n \geq M_k$ that satisfies $M_{\ell-1} \leq n < M_\ell$, the monotonicity of $\kappa_j$ gives
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{{\varepsilon}_i}(f_i)
&= \frac{1}{n} \Bigg[\sum_{i = 0}^{M_k-1} {{\boldsymbol {I}}}_{\kappa_j}(f_i) + \sum_{j = k+1}^{\ell-1} \sum_{i = M_{j-1}}^{M_j-1} {{\boldsymbol {I}}}_{\kappa_j}(f_i) + \sum_{i = M_{\ell-1}}^{n-1} {{\boldsymbol {I}}}_{\kappa_j}(f_i)\Bigg]  \\
&\leq \frac{1}{n}\Bigg[\sum_{i = 0}^{M_k-1} {{\boldsymbol {I}}}_{\kappa_k}(f_i) + \sum_{j = k+1}^{\ell} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{\kappa_j}(f_i)\Bigg]\\
&\leq \sum_{j = k}^{\ell} \frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{\kappa_j}(f_i)
= \sum_{j = k}^\ell {{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}),
} \end{align*}}
where we have identified $f_i$ with an element of ${\mathcal{S}}$ (this identification is measurable by Lemma~\ref{S_meas}, cf.~the discussion following Proposition \ref{same_law}).
Writing more generally, we can say that for all $n \geq M_k$,
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{{\varepsilon}_i}(f_i)
\leq \sum_{j \geq k :\ n \geq M_{j-1}} {{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}).
} \end{align*}}
It follows from a union bound and \eqref{bad_probability}  that
{\begin{align*} {
{\mathbf{P}}\Bigg(\bigcup_{n = M_k}^\infty \bigg\{\frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{{\varepsilon}_i}(f_i) \geq \sum_{j \geq k} 2^{-j}\bigg\}\Bigg)
&\leq {\mathbf{P}}\bigg(\bigcup_{n = M_k}^\infty \bigcup_{j \geq k\, :\, n \geq M_{j-1}} \{{{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}) \geq 2^{-j}\}\bigg) \\
&= {\mathbf{P}}\bigg(\bigcup_{j \geq k} \bigcup_{n = M_{j}}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}) \geq 2^{-j}\}\bigg) \\
&\leq \sum_{j \geq k} {\mathbf{P}}\bigg(\bigcup_{n = M_j}^\infty \{{{\boldsymbol {\mathcal{I}}}}_{\kappa_j}(\mu_{n-1}) \geq 2^{-j}\}\bigg) \\ 
&< \sum_{j \geq k} 2^{-j} = 2^{-k+1}.
} \end{align*}}
By Borel--Cantelli, the following is true with probability one: For only finitely many $k$ is
{\begin{align*} {
\frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{{\varepsilon}_i}(f_i) \geq 2^{-k+1} \quad \text{for some $n \geq M_k$.}
} \end{align*}}
This implies
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {{\boldsymbol {I}}}_{{\varepsilon}_i}(f_i) = 0 \quad \mathrm{a.s.} \label{bigger_to_0}
} \end{split} \end{align}}
Finally, note that $\rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon})$ is nonzero only when ${\mathcal{B}}_i^{\varepsilon}$ is nonempty, in which case ${{\boldsymbol {I}}}_{\varepsilon}(f_i) = 1$.
Hence
{\begin{align*} {
\rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) \leq {{\boldsymbol {I}}}_{\varepsilon} (f_i) \quad \text{for any $i \geq 0$, ${\varepsilon} > 0$,}
} \end{align*}}
and so \eqref{noVSD_claim} follows from \eqref{bigger_to_0}.
\end{proof}

\begin{cor} \label{apa_either_way}
Assume \eqref{mgf}. Let $g_i(x) := \rho_{i-1}(\omega_i =x)$. 
Then the sequence $(g_i)_{i \geq 1}$ is asymptotically purely atomic if and only if $(f_i)_{i \geq 0}$ is asymptotically purely atomic.
\end{cor}

\begin{proof}
Matching the notation used in \cite{vargas07}, we write
{\begin{align*} {
{\mathcal{A}}_i^{\varepsilon} := \{x \in {\mathbb{Z}}^d : \rho_{i-1}(\omega_i = x) > {\varepsilon}\}, \quad i \geq 1,\ {\varepsilon} > 0.
} \end{align*}}
Suppose $(f_i)_{i \geq 0}$ is asymptotically purely atomic.
Let $({\varepsilon}_i)_{i \geq 0}$ be any sequence tending to 0 as $i \to \infty$.
Then $2d \cdot {\varepsilon}_i$ also tends to 0 as $i \to \infty$, and so
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{2d \cdot {\varepsilon}_i}) = 1 \quad \mathrm{a.s.} \label{b_apa}
} \end{split} \end{align}}
The observation
{\begin{align} \begin{split} {
\rho_{i-1}(\omega_i = x) = \frac{1}{2d} \sum_{y \sim x} \rho_{i-1}(\omega_{i-1} = y) \label{next_step_repeat}
} \end{split} \end{align}}
shows
{\begin{align*} {
y \in {\mathcal{B}}_{i-1}^{2d\cdot{\varepsilon}} \quad \Rightarrow \quad x \in {\mathcal{A}}_i^{\varepsilon} \text{ for all $x$ such that $\|x-y\|_1 = 1$.}
} \end{align*}}
Consequently, for any ${\varepsilon} > 0$ we have the bound
{\begin{align*} {
\rho_{i-1}(\omega_i \in {\mathcal{A}}_i^{\varepsilon}) 
\geq \sum_{y \in {\mathcal{B}}_{i-1}^{2d\cdot {\varepsilon}}} \sum_{x \sim y} \rho_{i-1}(\omega_i = x)
&= \sum_{y \in {\mathcal{B}}_{i-1}^{2d\cdot {\varepsilon}}} \sum_{x \sim y} \frac{1}{2d} \sum_{z \sim x} \rho_{i-1}(\omega_{i-1} = z) \\
&\geq \sum_{y \in {\mathcal{B}}_{i-1}^{2d\cdot {\varepsilon}}} \rho_{i-1}(\omega_{i-1} = y) = \rho_{i-1}(\omega_{i-1} \in {\mathcal{B}}_{i-1}^{2d\cdot{\varepsilon}}).
} \end{align*}}
It now immediately follows from \eqref{b_apa} that
{\begin{align*} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^{n} \rho_{i-1}(\omega_i \in {\mathcal{A}}_i^{{\varepsilon}_i}) = 1 \quad \mathrm{a.s.}
} \end{align*}}
So $(g_i)_{i \geq 1}$ is asymptotically purely atomic.

Conversely, assume $(f_i)_{i \geq 0}$ is not asymptotically purely atomic.
By Theorem \ref{total_mass}, we must have $0 \leq \beta \leq \beta_c$, and so \eqref{bigger_to_0} holds for some sequence $({\varepsilon}_i)_{i \geq 0}$ tending to 0 as $i \to \infty$.
From \eqref{next_step_repeat}, we see that ${\mathcal{A}}_i^{\varepsilon}$ is nonempty only when ${\mathcal{B}}_{i-1}^{\varepsilon}$ is nonempty, in which case ${{\boldsymbol {I}}}_{\varepsilon}(f_{i-1}) = 1$.
As a result,
{\begin{align*} {
\rho_{i-1}(\omega_i \in {\mathcal{A}}_i^{\varepsilon}) \leq {{\boldsymbol {I}}}_{\varepsilon}(f_{i-1}) \quad \text{for any $i \geq 1$, ${\varepsilon} > 0$,}
} \end{align*}}
and so \eqref{bigger_to_0} forces
{\begin{align*} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^{n} \rho_{i-1}(\omega_i \in {\mathcal{A}}_i^{{\varepsilon}_{i-1}}) = 0 \quad \mathrm{a.s.}
} \end{align*}}
Indeed, $(g_i)_{i \geq 1}$ is not asymptotically purely atomic.
\end{proof}

\section{Geometric localization} \label{main_thm2}
Recall the following definition from Section \ref{results}.
We say that the sequence $(f_i)_{i \geq 0}$ exhibits \textit{subsequential geometric localization} if for every $\delta > 0$, there is $K < \infty$ and $\theta  > 0$ such that
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{\delta,K}\} \geq \theta \quad \mathrm{a.s.},
} \end{align*}}
where
${\mathcal{G}}_{\delta,K}$ is the set of probability mass functions on ${\mathbb{Z}}^d$ that assign measure greater than $1-\delta$ to some subset of ${\mathbb{Z}}^d$ having diameter at most $K$.
If $\theta$ can always be taken equal to 1, then the sequence is ``geometrically localized".
The main goal of this section is to establish that under \eqref{mgf}, there is subsequential geometric localization if and only if $\beta > \beta_c$.
But first, we have to deal with some measurability issues.

\subsection{Measurability of support number}
For $f \in S$, let $H_f$ denote ${\mathbb{N}}$-support of $f$:
{\begin{align*} {
H_f := \{n \in {\mathbb{N}} : f(n,x) > 0 \text{ for some $x \in {\mathbb{Z}}^d$}\}.
} \end{align*}}
In this section we consider the size of the ${\mathbb{N}}$-support of $f$,
{\begin{align*} {
N(f) := |H_f|,
} \end{align*}}
which we call the \textit{support number} of $f$.

\begin{lemma} \label{N_meas1}
$N(\cdot) : S \to {\mathbb{N}} \cup \{0,\infty\}$ is Borel measurable.
\end{lemma}

\begin{proof}
Fix $N \in \{0\} \cup {\mathbb{N}}$.
Let $U_N := \{f \in S : N(f) = N\}$ so that
{\begin{align*} {
U_N = \bigcup_{A \subset {\mathbb{N}}\, :\, |A| = N} \Bigg[\bigg(\bigcap_{n \in A} \bigcup_{x \in {\mathbb{Z}}^d} \{f \in S : f(n,x) > 0\} \bigg) \cap \bigg(\bigcap_{n \in {\mathbb{N}} \setminus A} \bigcap_{x \in {\mathbb{Z}}^d} \{f \in S : f(n,x) = 0\}\bigg)\Bigg].
} \end{align*}}
For any $(n,x) \in {\mathbb{N}} \times {\mathbb{Z}}^d$, the map $f \mapsto f(n,x)$ is continuous and thus measurable.
Furthermore, all intersections and unions in the above display are taken over countable sets, and so $U_N$ is measurable.
It follows that $U_\infty := \{f \in S : N(f) = \infty\} = S \setminus \big(\bigcup_{N \geq 0} U_N\big)$ is also measurable.
We conclude that $N(\cdot) : S \to {\mathbb{N}} \cup \{0,\infty\}$ is a measurable function.
\end{proof}

By Corollary \ref{defined_pspm}, $N(\cdot)$ is well-defined on ${\mathcal{S}}$.
That is, if we define for $f \in S$ the set
{\begin{align*} {
A_f = \{g \in S : d(f,g) = 0\},
} \end{align*}}
then $N(g) = N(f)$ for all $g \in A_f$.
It remains to show, however, that $N(\cdot) : {\mathcal{S}} \to {\mathbb{N}} \cup \{0,\infty\}$ is a measurable map.

\begin{lemma} \label{at_most_countable}
If $N(f) < \infty$, then the cardinality of $A_f$ is at most countable.
\end{lemma}

\begin{proof}
Fix $f \in S$ such that $N(f) < \infty$.
By Corollary \ref{better_def_cor}, for any $g \in A_f$ there is $\sigma : H_f \to H_g$ and a set of vectors $(x_n)_{n \in H_f}$ such that \eqref{better_def} holds.
Notice that because $|H_f| < \infty$, there are only countably many choices for the set $\sigma(H_f) = H_g$, and only countably many possibilities for the $x_n$.
As $g$ is determined from $f$ by $H_g$ and $(x_n)_{n \in H_f}$, we conclude that $A_f$ is at most countable.
\end{proof}

We will use the following fact about countable-to-one Borel maps.

\begin{lemma}[see Srivastava \cite{srivastava98}, Theorem 4.12.4] \label{countable_to_one}
Suppose ${\mathcal{X}},{\mathcal{Y}}$ are Polish spaces and $F : {\mathcal{X}} \to {\mathcal{Y}}$ is a countable-to-one Borel map. 
Then $F(B)$ is Borel for every Borel set $B$ in ${\mathcal{X}}$. 
\end{lemma}

Recall the quotient map $\iota : S \to {\mathcal{S}}$ defined in Lemma \ref{S_meas}(b), sending $f \in S$ to its equivalence class in ${\mathcal{S}}$.

\begin{prop} \label{N_meas2}
The map $N(\cdot) : {\mathcal{S}} \to {\mathbb{N}} \cup \{0,\infty\}$ is measurable.
\end{prop}

\begin{proof}
It suffices to show that for every $N \in {\mathbb{N}} \cup \{0,\infty\}$, the set
{\begin{align*} {
{\mathcal{U}}_N := \{f \in {\mathcal{S}} : N(f) = N\}
} \end{align*}}
is measurable.
We first restrict to the finite case.
For any nonnegative integer $N$, observe that ${\mathcal{X}} := \bigcup_{i = 0}^N U_i$ is a Polish space (under the $L^1$ metric) and, by Lemma \ref{N_meas1}, a measurable subset of $S$.
Lemma \ref{at_most_countable} says that $\iota |_{\mathcal{X}} : {\mathcal{X}} \to {\mathcal{S}}$ is a countable-to-one map.
Furthermore, this map is Borel measurable by Lemma \ref{S_meas}(b).
Now Lemma \ref{countable_to_one} shows $\iota(U_N) = {\mathcal{U}}_N$ is a measurable subset of ${\mathcal{S}}$.

Given that ${\mathcal{U}}_N$ is measurable for every finite $N$, the set ${\mathcal{U}}_\infty = {\mathcal{S}} \setminus \big(\bigcup_{N \geq 0} {\mathcal{U}}_N\big)$ is also measurable.
We have thus shown $N(\cdot) : {\mathcal{S}} \to {\mathbb{N}} \cup \{0,\infty\}$ is measurable.
\end{proof}

\subsection{Definitions of relevant functionals}
To employ the abstract machine, we make a few definitions.
First, for $f\in {\mathcal{S}}$ and $\delta \in (0,1)$, it is natural in studying geometric localization to consider the quantity
{\begin{align*} {
W_\delta(f) := \inf\bigg\{\operatorname{diam}(D) : D \subset {\mathbb{Z}}^d \text{ satisfies } \sum_{x \in D} f(n,x) > 1 - \delta \text{ for some $n \in {\mathbb{N}}$}\bigg\},
} \end{align*}}
where
{\begin{align*} {
\operatorname{diam}(D) := \sup\{\|x - y\|_1 : x,y \in D\}.
} \end{align*}}
In words, $W_\delta(f)$ is the smallest $K$ such that a region of diameter $K$ in ${\mathbb{N}} \times {\mathbb{Z}}^d$ has $f$-mass strictly greater than $1-\delta$.
Here we follow the convention that the infimum of an empty set is infinity, meaning $W_\delta(f) = \infty$ whenever there is no one copy of ${\mathbb{Z}}^d$ on which $f$ has mass greater than $1-\delta$.
We will write
{\begin{align*} {
{\mathcal{V}}_{\delta,K} := \{f \in {\mathcal{S}} : W_\delta(f) \leq K\},
} \end{align*}}
so that we can identify
{\begin{align} \begin{split} {
{\mathcal{G}}_{\delta,K} = {\mathcal{V}}_{\delta,K} \cap \{f \in {\mathcal{S}} : N(f) = 1,\, \|f\| = 1\}. \label{g_intersection}
} \end{split} \end{align}}
Next define $q_n : S \to [0,1]$ by
{\begin{align*} {
q_n(f) := \sum_{x\in {\mathbb{Z}}^d} f(n,x),
} \end{align*}}
and let
{\begin{align*} {
m(f) := \max_{n\in {\mathbb{N}}} q_n(f).
} \end{align*}}
It will also be useful to analyze the random variable
{\begin{align*} {
Q(f) := \sum_{n \in {\mathbb{N}}} \frac{q_n}{1 - q_n},
} \end{align*}}
where $1/0 = \infty$.
Observe that $Q(f) = \infty$ if and only if $N(f) = 1$ and $\|f\| = 1$.

It is easy to see from Corollary \ref{defined_pspm} that $W_\delta$, $m$, and $Q$ are well-defined on ${\mathcal{S}}$.

\begin{lemma} \label{more_equivalences}
The following statements hold:
\begin{itemize}
\item[(a)] For any $\delta \in (0,1)$, $W_\delta: {\mathcal{S}} \to {\mathbb{N}} \cup \{0,\infty\}$ is upper semi-continuous and thus measurable.
\item[(b)] $m : {\mathcal{S}} \to [0,1]$ is lower semi-continuous and thus measurable.
\item[(c)] $Q : {\mathcal{S}} \to [0,\infty]$ is lower semi-continuous and thus measurable.
\end{itemize}
\end{lemma}

\begin{proof}
For (a), we wish to show that for any $f \in {\mathcal{S}}$, there is ${\varepsilon} > 0$ such that
{\begin{align*} {
d(f,g) < {\varepsilon} \quad \Rightarrow \quad W_\delta(g) \leq W_\delta(f).
} \end{align*}}
It is only necessary to consider the case when $K := W_\delta(f)$ is finite (in particular, $f$ is nonzero).
Then $m(f) > 1 - \delta$, and we can select a representative $f \in S$ and $D \subset {\mathbb{Z}}^d$ such that
{\begin{align*} {
\sum_{x \in D} f(1,x) > 1 - \delta, \qquad \operatorname{diam}(D) \leq K.
} \end{align*}}
By possibly omitting some elements of $D$, we may assume $f$ is strictly positive on $\{1\} \times D$.
Choose ${\varepsilon} > 0$ to satisfy the following three inequalities:
\begin{align}
{\varepsilon} &< \inf_{x \in D} f(1,x)^2, \label{eps_choice_1} \\
{\varepsilon} &< 2^{-K}, \label{eps_choice_2} \\
\sum_{x \in D} f(1,x) &> 1 - \delta + {\varepsilon}. \label{eps_choice_3}
\end{align}
Suppose $g \in {\mathcal{S}}$ has $d(f,g) < {\varepsilon}$.
Then there is some representative $g \in S$ and some isometry $\phi : A \to {\mathbb{N}} \times {\mathbb{Z}}^d$ so that $d_\phi(f,g) < {\varepsilon}$.
It follows from \eqref{eps_choice_1} that $A \supset \{1\} \times D$.
Furthermore, \eqref{eps_choice_2} guarantees $\deg(\phi) > K$, implying
{\begin{align} \begin{split} {
x,y \in D \quad \Rightarrow \quad \phi(1,x) - \phi(1,y) = x - y. \label{perfect_on_D}
} \end{split} \end{align}}
By \eqref{eps_choice_3}, we now have
{\begin{align*} {
\sum_{x \in D} g(\phi(1,x)) &\geq \sum_{x \in D} f(1,x) - \sum_{x \in D} |f(1,x) - g(\phi(1,x))| \\
&> 1 - \delta + {\varepsilon} - \sum_{u \in A} |f(u) - g(\phi(u))| \\
&\geq 1 - \delta + {\varepsilon} - d_\phi(f,g) > 1 - \delta.
} \end{align*}}
Because of \eqref{perfect_on_D}, the set $\{\phi(1,x) : x \in D\}$ has diameter equal to $\operatorname{diam}(D) \leq K$, and so the above inequality shows $W_\delta(g) \leq K = W_\delta(f)$.

Next we prove (b) and (c) together, for which
we consider two cases.
First, if $m(f) = 1$, then $Q(f) = \infty$, and we must show that for any ${\varepsilon} > 0$ and any $L > 0$, there is $\delta > 0$ such that
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad m(g) > 1 - {\varepsilon}, \quad Q(g) > L.
} \end{align*}}
In this case, it suffices to prove $\delta$ may be chosen so that $m(g) > 1 - {\varepsilon}$, since any ${\varepsilon} \leq 1/(L+1)$ gives
{\begin{align*} {
m(g) > 1 - {\varepsilon} \quad \Rightarrow \quad Q(g) > \frac{1-{\varepsilon}}{1-(1-{\varepsilon})} \geq \frac{\frac{L}{L+1}}{\frac{1}{L+1}} = L.
} \end{align*}}
The first step in doing so is to choose $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$ finite but sufficiently large that
{\begin{align} \begin{split} {
\sum_{u \notin A} f(u) < \frac{\varepsilon}{2}. \label{really_small_sum}
} \end{split} \end{align}}
On the other hand, if $m(f) < 1$, so that $Q(f) < \infty$, then we shall find $\delta > 0$ satisfying
{\begin{align*} {
d(f,g) < \delta \quad \Rightarrow \quad m(g) > m(f) - {\varepsilon}, \quad Q(g) > Q(f) - {\varepsilon}.
} \end{align*}}
This is trivial if $f = {{\boldsymbol {0}}}$.
Otherwise, fix a representative $f \in S$, and again choose a finite $A \subset {\mathbb{N}} \times {\mathbb{Z}}^d$, but now satisfying a slightly different condition:
{\begin{align} \begin{split} {
\frac{\sum_{u \notin A} f(u)}{(1-m(f))^2} < \frac{\varepsilon}{2}. \label{really_small_sum2}
} \end{split} \end{align}}
In either case --- \eqref{really_small_sum} or \eqref{really_small_sum2} --- we may assume $f$ is strictly positive on $A$ by possibly omitting some elements.
Next consider the integer
{\begin{align*} {
K := \sup\{\|x-y\|_1 : (n,x),(n,y) \in A \text{ for some $n \in {\mathbb{N}}$}\},
} \end{align*}}
and take $\delta > 0$ satisfying the following conditions:
\begin{align}
\delta &< \inf_{u \in A} f(u)^2, \label{delta_condition1} \\
\delta &< 2^{-K}, \label{delta_condition2} \\
\delta &< \frac{\varepsilon}{2}. \label{delta_condition3}
\end{align}
If $m(f) < 1$, we will further assume
{\begin{align} \begin{split} {
0 < \frac{\delta}{(1-m(f))(1-m(f)-\delta)} < \frac{\varepsilon}{2}.
\label{delta_condition4}
} \end{split} \end{align}}
Now, if $d(f,g) < \delta$, then there is a representative $g \in S$ and an isometry $\phi : C \to {\mathbb{N}} \times {\mathbb{Z}}^d$ such that $d_\phi(f,g) < \delta$.
By \eqref{delta_condition1}, we must have $A \subset C$.
Furthermore, \eqref{delta_condition2} ensures $\deg(\phi) > K$, meaning the following implication holds:
{\begin{align*} {
(n,x),(n,y) \in A \quad \Rightarrow \quad \|(n,x)-(n,y)\|_1 \leq K \quad \Rightarrow \quad
\|\phi(n,x) - \phi(n,y)\|_1 = \|x-y\|_1 < \infty.
} \end{align*}}
Consequently, for any $n$ such that $A \cap (\{n\} \times {\mathbb{Z}}^d)$ is nonempty, we can define $\sigma(n)$ to be the unique integer such that 
{\begin{align*} {
\phi(n,x) \in \{\sigma(n)\} \times {\mathbb{Z}}^d \quad \text{whenever $(n,x) \in A$}.
} \end{align*}}
Note that $n \mapsto \sigma(n)$ may not be injective.
We consider the quantities
{\begin{align*} {
r_n := \sum_{x\, :\, (n,x) \in A} f(n,x), \qquad 
u_n := \sum_{x\, :\, (n,x) \in A} g(\phi(n,x)), \qquad
U_n := \sum_{m\, :\, \sigma(m) = \sigma(n)} u_m,
} \end{align*}}
the first two of which satisfy
{\begin{align} \begin{split} {
\sum_{n \in {\mathbb{N}}}|r_n-u_n|
= \sum_{n \in {\mathbb{N}}}\bigg|\sum_{x\, :\, (n,x) \in A} f(n,x) - g(\phi(n,x))\bigg|
&\leq \sum_{u \in A} |f(u) - g(\phi(u))| < \delta. \label{first_two}
} \end{split} \end{align}}
Also notice that \eqref{really_small_sum} or \eqref{really_small_sum2} implies
{\begin{align*} {
m(f) - \max_{n \in {\mathbb{N}}} r_n < \frac{\varepsilon}{2},
} \end{align*}}
and so \eqref{first_two} and \eqref{delta_condition3} now give
{\begin{align*} {
m(g) \geq \max_{n \in {\mathbb{N}}} U_n \geq \max_{n \in {\mathbb{N}}} u_n \geq \max_{n \in {\mathbb{N}}} r_n - \delta
> m(f) - {\varepsilon}.
} \end{align*}}
This completes the proof in the case $m(f) = 1$.
Otherwise, \eqref{delta_condition4} yields
{\begin{align} \begin{split} { \label{u_to_r}
\sum_{n \in {\mathbb{N}}} \frac{r_n}{1-r_n} - \sum_{n \in {\mathbb{N}}} \frac{u_n}{1-u_n}
= \sum_{n \in {\mathbb{N}}} \frac{r_n - u_n}{(1-r_n)(1-u_n)}
&< \frac{1}{(1-m(f))(1-m(f)-\delta)}\sum_{n \in {\mathbb{N}}}|r_n-u_n| \\
&< \frac{\delta}{(1-m(f))(1-m(f)-\delta)}
< \frac{\varepsilon}{2}.
} \end{split} \end{align}}
Also observe that
{\begin{align} \begin{split} { \label{U_to_u}
\frac{U_n}{1-U_n} = \frac{\sum_{m\, :\, \sigma(m) = \sigma(n)} u_m}{1-\sum_{m\, :\, \sigma(m) = \sigma(n)} u_m}
\geq \sum_{m\, :\, \sigma(m) = \sigma(n)} \frac{u_m}{1-u_m}.
} \end{split} \end{align}}
Together, \eqref{u_to_r} and \eqref{U_to_u} show
{\begin{align} \begin{split} { \label{t_to_r}
\sum_{n \in {\mathbb{N}}} \frac{q_n(g)}{1-q_n(g)} \geq \sum_{n \in {\mathbb{N}}} \frac{U_n}{1-U_n} \geq \sum_{n \in {\mathbb{N}}} \frac{u_n}{1-u_n} > \sum_{n \in {\mathbb{N}}} \frac{r_n}{1-r_n} - \frac{\varepsilon}{2}.
} \end{split} \end{align}}
Finally, using \eqref{really_small_sum2} we find
{\begin{align} \begin{split} { \label{s_to_r}
\sum_{n \in {\mathbb{N}}} \frac{q_n(f)}{1-q_n(f)} - \sum_{n \in {\mathbb{N}}} \frac{r_n}{1-r_n} 
= \sum_{n \in {\mathbb{N}}} \frac{q_n(f) - r_n}{(1 - q_n(f))(1-r_n)}
&\leq \frac{1}{(1-m(f))^2} \sum_{n \in {\mathbb{N}}} (q_n(f) - r_n )\\
&= \frac{\sum_{u \notin A} f(u)}{(1-m(f))^2}
< \frac{\varepsilon}{2}.
} \end{split} \end{align}}
Applying \eqref{s_to_r} in \eqref{t_to_r}, we obtain the desired result:
{\begin{align*} {
Q(g) = \sum_{n \in {\mathbb{N}}} \frac{q_n(g)}{1-q_n(g)} > \sum_{n \in {\mathbb{N}}} \frac{q_n(f)}{1-q_n(f)} - {\varepsilon} = Q(f) - {\varepsilon}.
} \end{align*}}
\end{proof}

An observation that will be useful is that at low temperature, the functional $Q$ must have infinite expectation according to any element of ${\mathcal{M}}$.

\begin{lemma} \label{Q_lemma}
Assume $\beta > \beta_c$.
Then for any $\nu \in {\mathcal{M}}$,
{\begin{align*} {
\int Q(f)\ \nu({\mathrm{d}} f) = \infty.
} \end{align*}}
\end{lemma}

\begin{proof}
Consider any $\nu \in {\mathcal{M}}$.
By Theorem \ref{characterization}, the assumption of $\beta > \beta_c$ implies 
$\nu(\{f \in {\mathcal{S}}: \|f\| = 1\}) = 1$.
Suppose toward a contradiction that $\nu \in {\mathcal{M}}$ satisfies
{\begin{align} \begin{split} {
\int Q(f)\ \nu({\mathrm{d}} f) < \infty. \label{finite_Q_integral}
} \end{split} \end{align}}
It must then be the case that $\nu(\{f \in {\mathcal{S}}: N(f) = 1\}) = 0$, since $Q(f) = \infty$ whenever $N(f) = 1$ and $\|f\| = 1$.
Let $f \in {\mathcal{S}}$ be random with law $\nu$.
By the previous observation, $1-q_n(f) > 0$ for all $n \in {\mathbb{N}}$ with $\nu$-probability 1.
Consider an environment $(Y_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ of i.i.d.~random variables with law $\lambda$, that is independent of $f$.
As usual, $Y$ will denote a generic random variable distributed according to $\lambda$.
When viewed as an element of ${\mathcal{S}}$, the law of the function
{\begin{align*} {
F(u) := \frac{\sum_{v \sim u} f(v)e^{\beta Y_u}}{\sum_{w \in {\mathbb{N}} \times {\mathbb{Z}}^d} \sum_{v \sim w} f(v)e^{\beta Y_w}}, \quad u \in {\mathbb{N}} \times {\mathbb{Z}}^d,
} \end{align*}}
is ${\mathcal{T}}\nu = \nu$.
Observe that because $\|F\| = 1$,
{\begin{align*} {
Q(F) = \sum_{n \in {\mathbb{N}}} \frac{q_n(F)}{1-q_n(F)}
= \sum_{n \in {\mathbb{N}}} \frac{\sum_{x \in {\mathbb{Z}}^d} F(n,x)}{\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} F(k,x)}
= \sum_{n \in {\mathbb{N}}} \frac{\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(n,y)e^{\beta Y_{n,\, x}}}{\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(k,y)e^{\beta Y_{k,\, x}}}.
} \end{align*}}
Conditioned on $f$, the numerator and denominator of
{\begin{align*} {
\frac{\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(n,y)e^{\beta Y_{n,\, x}}}{\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(k,y)e^{\beta Y_{k,\, x}}}
} \end{align*}}
are independent.
Hence
{\begin{align*} {
{\mathbf{E}}{[ {Q(F)} \: | \: {f} ]} &= \sum_{n \in {\mathbb{N}}} {\mathbf{E}}{\bigg[ {\frac{\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(n,y)e^{\beta Y_{n,\, x}}}{\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(k,y)e^{\beta Y_{k,\, x}}}} \: \bigg| \: {f} \bigg]} \\
&= \sum_{n \in {\mathbb{N}}} {\mathbf{E}}{\bigg[ {\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(n,y)e^{\beta Y_{n,\, x}}} \: \bigg| \: {f} \bigg]}
{\mathbf{E}}{\bigg[ {\frac{1}{\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(k,y)e^{\beta Y_{k,\, x}}}} \: \bigg| \: {f} \bigg]} \\
&> \sum_{n \in {\mathbb{N}}} \frac{{\mathbf{E}}{\big[ {\sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(n,y)e^{\beta Y_{n,\, x}}} \: \big| \: {f} \big]}}{{\mathbf{E}}{\big[ {\sum_{k \neq n} \sum_{x \in {\mathbb{Z}}^d} \sum_{y \sim x} f(k,y)e^{\beta Y_{k,\, x}}} \: \big| \: {f} \big]}}
= \sum_{n \in {\mathbb{N}}} \frac{(2d\, {\mathbf{E}}\, e^{\beta Y})q_n(f)}{(2d\, {\mathbf{E}}\, e^{\beta Y})(1-q_n(f))}
= Q(f),
} \end{align*}}
where the strict inequality is due to the strict convexity of $t \mapsto 1/t$ on $(0,\infty)$ and the non-degeneracy of $\lambda$.
But now \eqref{finite_Q_integral} allows us to write
{\begin{align*} {
\int Q(f)\ \nu({\mathrm{d}} f) = \iint Q(F)\ Tf({\mathrm{d}} F)\, \nu({\mathrm{d}} f) 
= \int {\mathbf{E}}{[ {Q(F)} \: | \: {f} ]}\, \nu({\mathrm{d}} f)
> \int Q(f)\, \nu({\mathrm{d}} f),
} \end{align*}}
yielding the desired contradiction.
\end{proof}

\subsection{Proof of subsequential geometric localization}
We can now prove the main theorem of Section \ref{main_thm2}. This theorem shows that subsequential geometric localization is equivalent to $\beta > \beta_c$. It also proves that the single copy condition (stated as \eqref{single_copy_assumption} below) implies full geometric localization.

\begin{thm} \label{localized_subsequence}
Assume \eqref{mgf}.
Let $f_i(\cdot) := \rho_i(\omega_i = \cdot)$.
Then the following statements hold:
\begin{itemize}
\item[(a)] If $\beta > \beta_c$, then $(f_i)_{i \geq 0}$ is subsequentially geometrically localized. Moreover, the quantities $K$ and $\theta$ in the definition of subsequential geometric localization are deterministic, and depend only on the choice of $\delta$, as well as $\lambda$, $\beta$, and $d$. 
\item[(b)] If $\beta > \beta_c$ and 
{\begin{align} \begin{split} {
\nu({\mathcal{U}}_1) = \nu(\{f \in {\mathcal{S}}: N(f) = 1\}) = 1 \quad \text{for all $\nu \in {\mathcal{M}}$}, \label{single_copy_assumption}
} \end{split} \end{align}}
then $(f_i)_{i \geq 0}$ is geometrically localized
\item[(c)] If $0 \leq \beta \leq \beta_c$, then for any $\delta \in (0,1)$ and any $K$,
{\begin{align} \begin{split} {
\lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{\delta,K}\} = 0 \quad \mathrm{a.s.} \label{no_localization}
} \end{split} \end{align}}.
\end{itemize}
\end{thm}

\begin{proof}
Fix $\delta\in (0,1)$ throughout. 
Recall that
{\begin{align*} {
{\mathcal{V}}_{\delta,K} = \{f \in {\mathcal{S}} : W_\delta(f) \leq K\}.
} \end{align*}}
For (a) and (c), we assume $\beta > \beta_c$.
Given $\delta > 0$, define the set
{\begin{align*} {
{\mathcal{U}}_\delta := \{f\in {\mathcal{S}}: m(f) > 1-\delta\} = \bigcup_{K = 0}^\infty {\mathcal{V}}_{\delta,K}.
} \end{align*}}
If \eqref{single_copy_assumption} is true, then it follows from Theorem \ref{characterization} that
{\begin{align} \begin{split} {
\nu({\mathcal{U}}_\delta) = 1 \quad \text{for all $\nu \in {\mathcal{M}}$.} \label{theta1}
} \end{split} \end{align}} 
Otherwise, we refer to Lemma \ref{Q_lemma} which tells us that for any $\nu\in  {\mathcal{M}}$,
{\begin{align*} {
\int Q(f)\, \nu({\mathrm{d}} f) =\infty.
} \end{align*}}
It follows that
{\begin{align} \begin{split} {
\nu({\mathcal{U}}_\delta) > 0,\label{nueps}
} \end{split} \end{align}}
since otherwise the $\nu$-essential supremum of $m(f)$ would be strictly less than 1, forcing the $\nu$-essential supremum of $Q(f)$ to be finite.
By Lemma \ref{more_equivalences}(b), ${\mathcal{U}}_\delta$ is an open set. Consequently, the map $\mu \mapsto \mu({\mathcal{U}}_\delta)$ is lower semi-continuous on ${\mathcal{P}}^1({\mathcal{S}})$, because of Lemmas \ref{wasserstein_compact}(b) and \ref{portmanteau}(d).
Since a lower semi-continuous function on a compact set attains its minimum value, \eqref{nueps} shows that
{\begin{align} \begin{split} {
\theta:= \inf_{\nu\in{\mathcal{M}}} \nu({\mathcal{U}}_\delta) > 0.\label{nueps2}
} \end{split} \end{align}}
For any $f \in {\mathcal{U}}_\delta$, the quantity $W_\delta(f)$ is finite.
Therefore, for any $\nu \in {\mathcal{M}}$,
{\begin{align*} {
\theta \leq \nu({\mathcal{U}}_\delta) = \lim_{K \to \infty} \nu({\mathcal{V}}_{\delta,K}).
} \end{align*}}
We can thus choose, for any ${\varepsilon} > 0$, a number $K_\nu$ so large that
{\begin{align} \begin{split} {
\nu({\mathcal{V}}_{\delta, K_\nu}) > (1-{\varepsilon})\theta. \label{nupos}
} \end{split} \end{align}}
Since $W_\delta$ is upper semi-continuous by Lemma \ref{more_equivalences}(a), the set ${\mathcal{V}}_{\delta,K_\nu}$ is open. 
Hence $\mu \mapsto \mu({\mathcal{V}}_{\delta,K_\nu})$ is lower semi-continuous as a map ${\mathcal{P}}^1({\mathcal{S}}) \to [0,1]$, by Lemma \ref{portmanteau}(d).
So there is $\xi_\nu > 0$ such that
{\begin{align} \begin{split} {
\inf_{\mu\in {\mathcal{B}}(\nu,\xi_\nu)} \mu({\mathcal{V}}_{\delta, K_\nu}) > (1-{\varepsilon})\theta,\label{mupos}
} \end{split} \end{align}}
where ${\mathcal{B}}(\nu,\xi_\nu)$ is the open ball in ${\mathcal{P}}^1({\mathcal{S}})$ with center $\nu$ and radius $\xi_\nu$.
 Since ${\mathcal{M}}$ is compact, there exists a finite subset ${\mathcal{M}}'$ of ${\mathcal{M}}$ such that
 {\begin{align} \begin{split} {
 {\mathcal{M}} = \bigcup_{\nu\in {\mathcal{M}}'} ({\mathcal{B}}(\nu, \xi_\nu/2) \cap {\mathcal{M}}). \label{cover_M}
 } \end{split} \end{align}}
Thus, if $K := \max\{K_\nu: \nu\in {\mathcal{M}}'\}$ and $\xi := \min\{\xi_\nu/2 : \nu \in {\mathcal{M}}'\}$, then $K$ is finite and $\xi > 0$, and together they satisfy
{\begin{align} \begin{split} {
{\mathcal{W}}^1(\mu,{\mathcal{M}}) < \xi \quad &\stackrel{\phantom{\eqref{cover_M}}}{\Rightarrow} \quad {\mathcal{W}}^1(\mu,\nu_0) < \xi \quad \text{for some $\nu_0 \in {\mathcal{M}}$} \\
&\stackrel{\eqref{cover_M}}{\Rightarrow} \quad {\mathcal{W}}^1(\mu,\nu) < \xi + \xi_\nu/2 < \xi_\nu \quad \text{for some $\nu \in {\mathcal{M}}'$} \\
&\stackrel{\eqref{mupos}}{\Rightarrow} \quad \mu({\mathcal{V}}_{\delta,K}) \geq \mu({\mathcal{V}}_{\delta,K_{\nu}}) > (1-{\varepsilon})\theta. \label{close_alpha}
} \end{split} \end{align}}
Recall from Theorem \ref{close_M} that  ${\mathcal{W}}^1(\mu_n, {\mathcal{M}})\to 0$ almost surely. 
Therefore, by \eqref{close_alpha} there is almost surely some $N$ satisfying
{\begin{align*} {
n \geq N \quad &\Rightarrow \quad \mu_{n-1}({\mathcal{V}}_{\delta,K}) > (1-{\varepsilon})\theta \\
\quad &\Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{V}}_{\delta,K}\} > (1-{\varepsilon})\theta.
} \end{align*}}
In particular,
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{V}}_{\delta,K}\} \geq (1-{\varepsilon})\theta \quad \mathrm{a.s.}
} \end{align*}}
As ${\varepsilon}$ is arbitrary, we conclude (by taking a countable sequence ${\varepsilon}_k \to 0$) that
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{V}}_{\delta,K}\} \geq \theta \quad \mathrm{a.s.},
} \end{align*}}
where \eqref{theta1} says $\theta = 1$ if \eqref{single_copy_assumption} is true.
Upon observing from \eqref{g_intersection} that $f_i \in {\mathcal{G}}_{\delta,K}$ if and only if $f_i \in {\mathcal{V}}_{\delta,K}$, we have proved (a) and (c).

For claim (b), suppose $0 \leq \beta \leq \beta_c$, so that Theorem \ref{characterization} gives ${\mathcal{M}} = \{\delta_{{\boldsymbol {0}}}\}$.
Fix $K > 0$.
Recall from the proof of Theorem \ref{reprove_equivalence} that in this high temperature case,
{\begin{align} \begin{split} {
\lim_{n \to \infty} \int \max(f)\ \mu_{n-1}({\mathrm{d}} f) = \lim_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \max_{x \in {\mathbb{Z}}^d} f_i(x) = 0 \quad \mathrm{a.s.} \label{max0}
} \end{split} \end{align}}
Notice that if ${\varepsilon} > 0$ is sufficiently small that
{\begin{align} \begin{split} {
D \subset {\mathbb{Z}}^d,\, \operatorname{diam}(D) \leq K \quad \Rightarrow \quad {\varepsilon}|D| < 1 - \delta, \label{diam_constraint}
} \end{split} \end{align}}
then
{\begin{align} \begin{split} {
\frac{1}{n} \sum_{i = 0}^{n-1} \max_{x \in {\mathbb{Z}}^d} f_i(x) < {\varepsilon}^2 \quad \Rightarrow \quad
\frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{\delta,K}\} < {\varepsilon}. \label{max_diam}
} \end{split} \end{align}}
Indeed, the hypothesis in \eqref{max_diam} implies there are fewer than ${\varepsilon} n$ numbers $i$ between 0 and $n-1$ such that $\max_{x \in {\mathbb{Z}}^d} f_i(x) \geq {\varepsilon}$, and \eqref{diam_constraint} implies all the remaining $i$ must satisfy $f_i \notin {\mathcal{G}}_{\delta,K}$.
Therefore, \eqref{max_diam} is true, and so \eqref{max0} implies \eqref{no_localization}.
\end{proof}

\subsection{Localization in a favorite region}
In this final section we prove that single copy condition~\eqref{single_copy_assumption} implies localization in a ``favorite region" of size ${\mathcal{O}}(1)$.
Recall that the mode of a probability mass function is a location where the function attains its maximum. For any $n\ge 0$ and $K\ge 0$, let ${\mathcal{C}}_{n}^K$ be the set of all $x\in {\mathbb{Z}}^d$ that are within $\ell^1$ distance $K$ from {\it every} mode of the random probability mass function $f_n$. Note that ${\mathcal{C}}_n^K$ is a set with diameter at most $2K$. The following theorem establishes localization of the endpoint in ${\mathcal{C}}_n^K$, if the single copy condition holds.

\begin{prop}\label{localization_thm}
Assume  \eqref{mgf} and \eqref{single_copy_assumption}.
Then
{\begin{align} \begin{split} {
\lim_{K \to \infty} \liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{C}}_{i}^K) = 1\quad \mathrm{a.s.} \label{favorite_region}
} \end{split} \end{align}}
\end{prop}

\begin{proof}
For the sake of completeness, we first verify that $\rho_i(\omega_i \in {\mathcal{C}}_i^K)$ is a measurable function (with respect to ${\mathcal{F}}_i$).
For any Borel set $B \subset {\mathbb{R}}$, the event $\{\rho_i(\omega_i \in {\mathcal{C}}_i^K) \in B\}$ can be expressed as
{\begin{align*} {
\bigcup_{\substack{A \subset {\mathbb{Z}}^d \\ |A| \leq (2d)^i}} \Bigg[\bigg(\bigcap_{x \in A} \{f_i(x) = \max_{y\in{\mathbb{Z}}^d} f_i(y)\}\bigg) \cap
\bigg\{\sum_{\substack{y \in {\mathbb{Z}}^d\, :\, \|x - y\|_1 \leq K\, \forall\, x \in A }} f_i(y) \in B\bigg\}\Bigg].
} \end{align*}}
It is clear that the above display is a measurable event, and so $\rho_i(\omega_i \in {\mathcal{C}}_i^K)$ is measurable.

Assume \eqref{single_copy_assumption}, so that $(f_i)_{i \geq 0}$ is geometrically localized.
By Theorem \ref{localized_subsequence}(c), we must have $\beta > \beta_c$.
Now fix $c < 1$, and choose $\theta < 1$ such that $\theta(2\theta-1) \geq c$.
As in the proof of Theorem \ref{total_mass}(a), the assumption $\beta > \beta_c$ ensures that the conclusion of Lemma \ref{equiv_apa} holds.
So there is ${\varepsilon} > 0$ such that
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) > \theta \quad \mathrm{a.s.}, \label{for_N1}
} \end{split} \end{align}}
where
{\begin{align*} {
{\mathcal{B}}_i^{\varepsilon} = \{x \in {\mathbb{Z}}^d : f_i(x) > {\varepsilon}\}.
} \end{align*}}
Since \eqref{for_N1} will still hold if ${\varepsilon}$ is made smaller, we may assume ${\varepsilon} < 1 - \theta$.
By geometric localization, there is $K$ such that
{\begin{align} \begin{split} {
\liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{{\varepsilon},K}\} = 1 \quad \mathrm{a.s.} \label{for_N2}
} \end{split} \end{align}}
By \eqref{for_N1} and \eqref{for_N2}, there almost surely exists some $N$ satisfying both
{\begin{align*} {
n \geq N \quad &\Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{B}}_i^{\varepsilon}) > \theta
\quad \Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} {\mathbb{I}}\{{\mathcal{B}}_i^{\varepsilon} \text{ is nonempty}\} > \theta,
\intertext{and}
n \geq N \quad &\Rightarrow \quad \frac{1}{n}\sum_{i = 0}^{n-1} {\mathbb{I}}\{f_i \in {\mathcal{G}}_{{\varepsilon},K}\} > \theta.
} \end{align*}}
So for $n \geq N$, there are at least $(2\theta - 1)n$ numbers $i$ between 0 and $n-1$ such that
$f_i(x) > {\varepsilon}$ for some (and thus any) mode $x \in {\mathbb{Z}}^d$ of $f_i$, and $\rho_i(\omega_i \in {\mathcal{D}}_i) > 1 - {\varepsilon}$ for some ${\mathcal{D}}_i \subset {\mathbb{Z}}^d$ with $\operatorname{diam}({\mathcal{D}}_i) \leq K$.
For such $i$, \textit{all} modes must belong to ${\mathcal{D}}_i$, which implies ${\mathcal{D}}_i \subset {\mathcal{C}}_i^K$.
In particular, 
{\begin{align*} {
\rho_i(\omega_i \in {\mathcal{C}}_i^K) \geq \rho_i(\omega_i \in {\mathcal{D}}_i) > 1 - {\varepsilon} > \theta.
} \end{align*}}
Therefore,
{\begin{align*} {
n \geq N \quad \Rightarrow \quad \frac{1}{n} \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{C}}_i^K) > \theta(2\theta - 1) \geq c.
} \end{align*}}
We have thus chosen $K$ sufficiently large that
{\begin{align*} {
\liminf_{n \to \infty} \frac{1}{n}  \sum_{i = 0}^{n-1} \rho_i(\omega_i \in {\mathcal{C}}_i^K) \geq c \quad \mathrm{a.s.}
} \end{align*}}
As $\rho_i(\omega_i \in {\mathcal{C}}_i^L) \geq \rho_i(\omega_i \in {\mathcal{C}}_i^{K})$ for $L \geq K$, we in fact have
{\begin{align*} {
\lim_{K \to \infty} \liminf_{n \to \infty} \frac{1}{n} \sum_{i = 0}^{n-1}\rho_i(\omega_i \in {\mathcal{C}}_i^K) \geq c \quad \mathrm{a.s.}
} \end{align*}}
As $c < 1$ is arbitrary, we can take a countable sequence $c_k \to 1$ to conclude \eqref{favorite_region}.
\end{proof}

\section{Open problems}
Below we mention some remaining questions concerning the methods of this manuscript.
Given the subsequent applications \cite{koenig-mukherjee15,bolthausen-koenig-mukherjee15} of Mukherjee and Varadhan's construction in \cite{mukherjee-varadhan14} and of a variant construction in \cite{mukherjee15}, as well as the success of the concentration compactness phenomenon in general, we are optimistic that the ideas presented in this study can fuel future work.
For a broader discussion of  open problems on directed polymers, we refer the reader to Section 12.9 of~\cite{denHollander09}.
\begin{enumerate}
\item For the (1+1)-dimensional log-gamma model, localization occurs around a single favorite region \cite{comets-nguyen15}. Does this hold more generally, specifically in the low temperature regime? 
We showed via Theorem \ref{localized_subsequence}(b) and Proposition \ref{localization_thm} that the answer is yes if the single copy condition holds.
Nevertheless, it appears challenging to even rule out the possibility that the endpoint distribution maintains an unbounded number of favorite sites, so that some $\nu \in {\mathcal{M}}$ may put mass on partitioned subprobability measures supported on infinitely many copies of ${\mathbb{Z}}^d$. Such behavior has been observed for other types of Gibbs measures \cite{comets-dembo01}.
\item The set ${\mathcal{M}}$ defined by \eqref{M_def} is closed and convex in ${\mathcal{P}}^1({\mathcal{S}})$.
By Theorem \ref{characterization}, ${\mathcal{M}}$ is a singleton at high temperature.  
Is the same true in the low temperature phase?  If true, this would imply that the empirical measure converges to a deterministic limit, instead of the subsequential convergence that is established in this paper.
If false, can one describe the extreme points of ${\mathcal{M}}$?
\item Does the endpoint distribution converge in law? In other words, can we go beyond the Ces\`aro averages and prove limiting results for the actual endpoint distribution? Note that if this is true, then the limiting law must be an element of ${\mathcal{M}}$.
\item Can the methods of this paper be extended to understand the distinction between $(\operatorname{SD})$ and $(\operatorname{VSD})$? Such an extension may lead to the resolution of some longstanding questions in this area, such as the following. (a) For $d \geq 3$, do the critical values $\beta_c$ and ${\widetilde{{\beta_c}}}$ coincide?
This is known to be the case for $d = 1$ \cite{comets-vargas06} and for $d = 2$ \cite{lacoin10}.
If the answer is no, then there would exist \textit{critical strong disorder}, in which $(\operatorname{SD})$ holds but not $(\operatorname{VSD})$. (b) For $d \geq 3$, is there strong disorder at inverse temperature ${\widetilde{{\beta}}}_c$?
For analogous models on self-similar trees, the answer is yes \cite{kahane-peyriere76}.
\end{enumerate}

\section*{Acknowledgements}
We thank David Aldous, Alexei Borodin, Persi Diaconis, Jafar Jafarov, Chiranjib Mukherjee, and Clifford Weil for fruitful discussions.
We are also grateful to  Erwin Bolthausen, Francis Comets, Tom Spencer, Srinivasa Varadhan, Vincent Vargas, and Zhengqing Zhou for useful comments on our preliminary draft.

\bibliography{directed_polymers.bib}

\end{document}

\afterpage{\clearpage}

\begin{table}[p]
\centering
\caption{Notations and definitions}
\begin{tabular}{ccl}
Notation & Defined in Section & Description \\ \hlineB{3}
$(\Omega_p,{\mathcal{F}})$ & \ref{model} & probability space of nearest-neighbor paths in ${\mathbb{Z}}^d$\\
$P$ &  & law of simple random walk on ${\mathbb{Z}}^d$, a probability measure on $(\Omega_p,{\mathcal{F}})$ \\
$E$ & & expectation according to $P$ \\
$(\Omega_e,{\mathcal{G}})$ & & probability space supporting the random environment $(X_u)_{u \in {\mathbb{N}} \times {\mathbb{Z}}^d}$ \\
${\mathcal{F}}_n$ & & $\sigma$-algebra generated by random environment up to time $n$ \\
${\mathbf{P}}$ & & probability measure on $(\Omega_e,{\mathcal{G}})$ \\
${\mathbf{E}}$ & & expectation according to ${\mathbf{P}}$ \\
$\beta$  & & inverse temperature \\
$\lambda$ & & disorder distribution, law of $X_u$ \\
$c(\alpha)$ & & logarithmic moment generating function for $\lambda$ \\
$\rho_n$ & & polymer measure of length $n$, a probability on $(\Omega_p,{\mathcal{F}})$ \\
$Z_n$ & & quenched partition function \\
${\widetilde{{Z}}}_n$ &\ref{disorder_background}& normalized partition function \\
$F_n$ & & quenched free energy \\
$p(\beta)$ & & limit of quenched free energy \\
$\Lambda(\beta)$ & & Lyapunov exponent, difference between annealed energy and $p(\beta)$ \\
$\beta_c$ & & critical inverse temperature separating weak and strong disorder \\
${\widetilde{{\beta}}}_c$ & & critical inverse temperature bordering very strong disorder \\
$S$ & \ref{topology-definitions} &set of representatives for partitioned subprobability measures \\
$\|\cdot\|$ & &norm on partitioned subprobability measures or $L^1$ functions \\
${\mathcal{S}}$ & \ref{metric} &set of partitioned subprobability measures \\
$d$ & &metric on ${\mathcal{S}}$ \\
$\|\cdot\|_{\varepsilon}$ & & sum of function's values above ${\varepsilon} > 0$ \\
${{\boldsymbol {I}}}_{\varepsilon}$ & & indicator function on ${\mathcal{S}}$ for achieving a maximum of at least ${\varepsilon} > 0$ \\
${\mathcal{P}}^1({\mathcal{S}})$ & \ref{compactness} & Borel probability measures on ${\mathcal{S}}$ \\
${\mathcal{W}}^1({\mathcal{S}})$ & & Wasserstein distance on ${\mathcal{P}}^1({\mathcal{S}})$ \\
$f_n$ & \ref{endpoint_distributions} & endpoint distribution for polymer of length $n$ \\
$T$ & & update map sending $f_n$ to law of $f_{n+1}$ given ${\mathcal{F}}_n$ \\
${\mathcal{T}}$ &\ref{extension_to_measures} &update map sending $\mu$ to the mixture of $Tf$, $f$ drawn from $\mu$ \\
$\mu_n$ &\ref{empirical_measures} & empirical probability measure on ${\mathcal{S}}$ generated by $f_0,f_1,\dots,f_n$ \\
$\mu_n'$ & & empirical probability measure on ${\mathcal{S}}$ generated by $f_1,f_2,\dots,f_{n+1}$ \\
${\mathcal{L}}$ & &1-Lipschitz functions on ${\mathcal{S}}$ evaluating to 0 at zero function \\
${\mathcal{K}}$ &\ref{convergence_fixed} & set of fixed points of ${\mathcal{T}}$ \\
$R(\cdot)$ &\ref{calculations} & energy functional on ${\mathcal{S}}$ \\
${\mathcal{R}}(\cdot)$ & & expectation of $R$ according to given measure \\
${\mathcal{M}}$ & &set of fixed points of ${\mathcal{T}}$ minimizing ${\mathcal{R}}$ \\
${{\boldsymbol {0}}}$ & \ref{norm_monotonicity} &zero functions in ${\mathcal{S}}$ \\
${{\boldsymbol {1}}}$ & &unit point in ${\mathcal{S}}$ \\
${\mathcal{A}}_i^{\varepsilon}$ &\ref{characterize_proof} & values of $\omega_i$ of probability greater than ${\varepsilon} > 0$ according to $\rho_{i-1}$ \\
${\mathcal{B}}_i^{\varepsilon}$ & & values of $\omega_i$ of probability greater than ${\varepsilon} > 0$ according to $\rho_{i}$ \\
${\mathcal{I}}_{\varepsilon}(\cdot)$ & & expectation of ${{\boldsymbol {I}}}_{\varepsilon}$ according to given measure
\end{tabular}
\end{table}

