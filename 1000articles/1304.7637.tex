\documentclass{aptpubarxiv}
 \usepackage{amsmath,natbib,amsfonts,amsbsy,url,enumerate,bm}
 \RequirePackage{hypernat}
 
\usepackage{color}
\usepackage[normalem]{ulem}

\hyphenation{Mar-kov}

\usepackage{etoolbox}
\reversemarginpar

\numberwithin{equation}{section}
\allowdisplaybreaks
\authornames{Jan\ss en, A. and Segers, J.}
\shorttitle{Markov tail chains}
\begin{document}

\title{Markov tail chains}

\authorone[University of Hamburg]{A. Janssen}
\addressone{University of Hamburg, Department of Mathematics, Bundesstr.\ 55, 20146 Hamburg, Germany}
\emailone{anja.janssen@math.uni-hamburg.de}
\authortwo[Universit\'e catholique de Louvain]{J. Segers}
\addresstwo{Universit\'e catholique de Louvain, Institut de statistique, Voie du Roman Pays 20, B-1348 Louvain-la-Neuve, Belgium}
\emailtwo{johan.segers@uclouvain.be}

\begin{abstract}
The extremes of a univariate Markov chain with regulary varying stationary marginal distribution and asymptotically linear behavior are known to exhibit a multiplicative random walk structure called the tail chain. In this paper, we extend this fact to Markov chains with multivariate regularly varying marginal distribution in $\mathbb{R}^d$. We analyze both the forward and the backward tail process and show that they mutually determine each other through a kind of adjoint relation. In a broader setting, it will be seen that even for non-Markovian underlying processes a Markovian forward tail chain always implies that the backward tail chain is Markovian as well. We analyze the resulting class of limiting processes in detail. Applications of the theory yield the asymptotic distribution of both the past and the future of univariate and multivariate stochastic difference equations conditioned on an extreme event.
\end{abstract}

\ams{60G70; 60J05}{60G10; 60H25; 62P05}

\keywords{autoregressive conditional heteroskedasticity; extreme value distribution;
(multivariate) Markov chain; multivariate regular variation; random walk; stochastic difference equation; tail chain; tail-switching potential}

\section{Introduction}
\label{S:intro}

Consider a discrete-time, $\mathbb{R}^d$-valued random process $\{ X_t : t = 0, 1, 2, \ldots \}$ defined by the recursive equation
\begin{equation}
\label{E:MC:1}
	X_t = \Phi ( X_{t-1}, {\varepsilon}_t ), \qquad t = 1, 2, \ldots,
\end{equation}
where
\vspace{0.2cm}
\begin{equation}
\label{E:MC:2}
\mbox{\begin{minipage}[h]{0.90\textwidth}
\begin{itemize}
\item[(i)] ${\varepsilon}_1, {\varepsilon}_2, \ldots$ are independent and identically distributed random elements of a measurable space $(\mathbb{E}, \mathcal{E})$ and independent of $X_0$;
\item[(ii)] $\Phi$ is a measurable function from ${\mathbb{R}}^d \times {\mathbb{E}}$ to ${\mathbb{R}}^d$.
\end{itemize}
\end{minipage}}
\end{equation}

If the process $\{ X_t \}$ happens to be stationary, it will be assumed to be defined for all integer $t$. The distribution of $X_0$ is assumed to be multivariate regularly varying.

The aim of the paper is to analyze the special structure of weak limits of the finite-dimensional distributions of the process conditionally on $\|X_0\|$ being large, where $\|\cdot\|$ denotes the Euclidean norm. More precisely, we will investigate the weak limits, called the forward tail chain, of vectors of the form $(X_0, \ldots, X_t)$ given that $\|X_0\|$ exceeds a high threshold. If in addition the process is stationary we will extend this to find the so-called back-and-forth tail chain, which corresponds to the weak limits of vectors of the form $(X_{-s}, \ldots, X_t)$ given that $\|X_0\|$ is large. A close relation of these processes to multivariate regular variation of the whole process has been analyzed in \citet{BS09}. In this article, we are interested in the special form of the processes, in particular the Markovian structure of both the forward and the backward process and how they necessarily determine each other.

The process $\{ X_t \}$ is obviously a discrete-time homogeneous Markov chain. On the other hand, every homogeneous discrete-time Markov chain $\{ X_t \}$ on a complete separable metric space can be represented as in \eqref{E:MC:1}--\eqref{E:MC:2} \citep{Ki86}. Of course, for a given Markov chain $\{ X_t \}$ the above representation is not unique. Still, in examples, the way in which Markov chains are defined is often through a recursive equation; all examples in \citet[][pp.~126--127]{Goldie91}, for instance, are of this type. The chain is stationary if and only if the random vectors $X_1 = \Phi(X_0, {\varepsilon}_1)$ and $X_0$ are equal in law.

In \citet{Smith92} and \citet{Perfekt94}, excursions of a univariate Markov chain over a high threshold following an extreme event are shown to behave asymptotically and under quite general conditions as a (multiplicative) random walk. The theory has been extended to multivariate Markov chains in \citet{Perfekt97} and to higher-order Markov chains in \citet{Yun98, Yun00}. More recently, \citet{ReZe13} have analyzed the topic with a special view towards the convergence of Markov kernels and added a criterion to distinguish between extreme and non-extreme states of a Markov chain as the threshold rises. The random-walk representation is useful from a statistical perspective because it gives a handle on how to model the extremes of certain time series (\citet{BC00, CST97, STC97}). A useful, well-investigated class of processes for which the random walk structure is quite revealing are the stationary solutions to certain stochastic difference equations, including squared (generalized) autoregressive 
conditionally heteroskedastic (ARCH/GARCH) processes as a special case (\citet{BDM02b, GdHP04, dHRRdV89}). 

A limitation of the theory of \citet{Smith92}, \citet{Perfekt94} and \citet{ReZe13} is that it is specialized to univariate, nonnegative Markov chains. Similarly, \citet{Perfekt97} only considers the upper extremes of a multivariate Markov chain. When extending the theory to real-valued and higher dimensional chains, one has to keep in mind that extremes may be both positive or negative and that extreme values of $X_t$ may depend not only on $\|X_{t-1}\|$ but also on $X_{t-1}/ \|X_{t-1}\|$. The simplest case of the extension on which we will focus deals with real-valued univariate Markov chains, where an extreme value of $X_t$ may depend on the sign of $X_{t-1}$ as can be observed for instance in time series of logreturns of prices of financial securities in periods of high volatility. The observation of this so-called leverage effect has lead to the formulation of asymmetric extensions of GARCH models (cf., for example, \cite{Zi09}). For such Markov chains with tail switching potential, the 
random walk representation of excursions over high thresholds breaks down in the sense that the distribution of the multiplicative increment now depends in general 
on the sign of the chain on the previous step. In \citet{BC03}, a more general representation is postulated, involving in fact four transition mechanisms rather than one, corresponding to the four cases of transitions from and to upper or lower extreme states.

The novelty of this paper is two-fold: first, to explicitly state the random walk representation in the general $\mathbb{R}^d$-valued case; second, in the stationary case, to study the joint distribution of the forward and backward tail chain, coined the {\em back-and-forth tail chain}. Throughout, some remarkable simplifications in the (univariate) real-valued case will be studied in more detail. In particular, in the univariate case the backward tail chain is again a random walk which is in some sense dual the forward tail chain. Besides the assumption that the distribution of $X_0$ is regularly varying, the only condition is a relatively easy-to-check statement on the asymptotic behaviour of $\Phi(x, \, \cdot \,)$ for large $\|x\|$.

The outline of the paper is as follows. The forward tail chain of a possibly non-stationary $\mathbb{R}^d$-valued Markov chain is studied in section~\ref{S:forward}. For stationary Markov chains, the tail chain can be extended to the past of the process, the backward tail chain, see section~\ref{S:backforth}. Section~\ref{S:adjoint} describes a kind of adjoint relation between distributions which is motivated by a general property of tail processes of stationary processes. In section~\ref{S:BFTC}, we show that a certain class of processes, coined back-and-forth tail chains, which are derived from this adjoint distribution, form exactly the class of tail processes which arise in our Markovian setting. Finally, section~\ref{S:examples} provides some examples to the theory, including an application to stationary solutions of (multivariate) stochastic difference equations.

To conclude this section, let us fix some notations. We write $(x)_+=\max(x,0)$ for the positive part of $x \in\mathbb{R}$ and $(x)_-=\min(x,0)$ for the negative part. The transpose of a matrix $A$ is denoted by $A'$. The law of a random vector $X$ is denoted by ${\mathcal{L}}(X)$; weak convergence of probability measures is denoted by $\Rightarrow$. The probability measure degenerate at a point $x$ is denoted by $\delta_x$, and $\mbox{Unif}(E)$ denotes the uniform distribution on a compact set $E$. The indicator of an event $A$ is denoted by ${\boldsymbol{1}}_A(\cdot)$. We write $\overline{\mathbb{R}}$ for $\mathbb{R}\cup\{-\infty,\infty\}$, $\mathbb{S}^{d-1}$ for $\{x \in \mathbb{R}^d: \|x\|=1\}$ and $0$ for a vector (of suitable dimension) which consists of all zeros. Let ${\mathbb{Z}}$ be the set of integers and ${\mathbb{N}}_0$ be the set of nonnegative integers.

\section{Forward tail chains}
\label{S:forward}

Let $X_0, X_1, X_2, \ldots$ be a homogeneous Markov chain as in \eqref{E:MC:1} and \eqref{E:MC:2}, not necessarily stationary. The focus of this section is on the weak limits of the finite-dimensional distributions of the process conditionally on $\|X_0\|$ being large (Theorem~\ref{T:forward}). Two conditions are required: Condition~\ref{C:RV} on the tails of $X_0$, and Condition~\ref{C:phi} on the asymptotics of $x \mapsto \Phi(x, e)$ for large $\|x\|$. See for instance \citet{Re07} for details on multivariate regular variation.

\begin{cond}
\label{C:RV}
The distribution of $X_0$ is multivariate regularly varying on $\overline{\mathbb{R}}^d \setminus \{0\}$, that is, there exists a non-degenerate probability measure $\Upsilon$ on $\mathbb{S}^{d-1}$ (called the spectral measure) and an $\alpha>0$ such that
\begin{equation}
\label{E:RV}
      \lim_{x \to \infty} {\mathrm{P}}\left(\|X_0\|>ux, \frac{X_0}{\|X_0\|} \in S\, \vrule \, \|X_0\| > x\right) = u^{-\alpha}\Upsilon(S) 
\end{equation}
for all Borel sets $S \subset \mathbb{S}^{d-1}$ which satisfy $\Upsilon(\partial S)=0$ and $u \geq 1$. \end{cond}

The second condition states that the function $\Phi$ in \eqref{E:MC:1} is asymptotically homogeneous in $x$ for large values of $\|x\|$.
\begin{cond}
\label{C:phi} 
There exists a measurable map $\phi: \mathbb{S}^{d-1} \times \mathbb{E} \mapsto \mathbb{R}^d$ such that, for all $e \in \mathbb{E}$,
\begin{equation}
\label{E:phi:V}
	\lim_{x \to \infty} x^{-1} \Phi(x s(x), e) = \phi(s,e)
\end{equation}
whenever $s(x) \to s$ in $\mathbb{S}^{d-1}$.

Moreover, if ${\mathrm{P}}(\phi(s, {\varepsilon}_1) = 0) > 0$ for some $s \in \mathbb{S}^{d-1}$, then also ${\mathrm{P}}({\varepsilon}_1 \in {\mathbb{W}}) = 1$, where ${\mathbb{W}}$ is a measurable subset of ${\mathbb{E}}$ such that for all $e \in {\mathbb{W}}$,
\begin{equation}
\label{E:phi:W}
	\sup_{\|y\| \leq x} |\Phi(y, e)| = O(x), \qquad x \to \infty.
\end{equation}
\end{cond}

We extend the domain of the limit function $\phi$ in \eqref{E:phi:V} to $\mathbb{R}^d \times \mathbb{E}$ by setting
\begin{equation}
\label{eq:phi}
  \phi(v, e) = 
  \begin{cases}
    \| v \| \, \phi( v / \| v \|, e ) & \text{if $v \ne 0$,} \\
    0 & \text{if $v = 0$.}
  \end{cases}
\end{equation}

\begin{lem}\label{L:generalphi}
If Condition \ref{C:phi} holds, then 
\begin{equation}
\label{E:generalphi}
  \lim_{x \to \infty} x^{-1} \Phi(x v(x), e) 
  = \phi(v,e)
\end{equation}
whenever $v(x) \to v \in \mathbb{R}^d \setminus{0}$ and $e \in \mathbb{E}$. If ${\mathrm{P}}(\phi(s, {\varepsilon}_1) = 0) > 0$ for some $s \in \mathbb{S}^{d-1}$, then \eqref{E:generalphi} also holds for $v(x) \to v = 0$ and $e \in {\mathbb{W}}$.
\end{lem}
\begin{proof} If $v(x) \to v \in \mathbb{R}^d \setminus{0}$, then both $\|v(x)\|\to \|v\|$ and $v(x)/\|v(x)\| \to v/\|v\|$.  Thus
\[
  \lim_{x \to \infty} \frac{\Phi(x v(x), e)}{x} 
  = \lim_{x \to \infty} \|v(x)\|\frac{\Phi(x \|v(x)\|(v(x)/\|v(x)\|), e)}{x\|v(x)\|}
  = \| v \| \, \phi( v / \| v \|, e )
\]
which, by \eqref{eq:phi}, gives \eqref{E:generalphi}. The case $v(x) \to 0$  follows from \eqref{E:phi:W}.
\end{proof}

\begin{thm}
\label{T:forward}
Let $\{ X_t : t \in \mathbb{N}_0 \}$ be given by \eqref{E:MC:1}--\eqref{E:MC:2}. If Conditions~\ref{C:RV} and \ref{C:phi} hold, then for every integer $t \geq 0$, as $x \to \infty$,
\begin{equation}
\label{E:forward:1}
	{\mathcal{L}} \biggl( \frac{\|X_0\|}{x}, \frac{X_0}{\|X_0\|}, \frac{X_1}{\|X_0\|}, \ldots, \frac{X_t}{\|X_0\|} \bigg| \|X_0\| > x \biggr)
	\Rightarrow {\mathcal{L}}(Y, M_0, M_1, \ldots, M_t)
\end{equation}
with 
\begin{equation}
\label{E:Mj}
	M_j =  \phi(M_{j-1},{\varepsilon}_j), \qquad j = 1, 2, \ldots,
\end{equation}
and
\begin{equation}
\label{E:forward:2}
\mbox{\begin{minipage}[h]{0.90\textwidth}
\begin{itemize}
\item[(i)] $Y, M_0, {\varepsilon}_1, {\varepsilon}_2, \ldots$ are independent with ${\varepsilon}_t$ as in \eqref{E:MC:2}(i);
\item[(ii)] ${\mathrm{P}}(Y > y) = y^{-\alpha}$ for $y \geq 1$;
\item[(iii)] $\mathcal{L}(M_0)=\Upsilon$.
\end{itemize}
\end{minipage}}
\end{equation}
We call $\{M_t: t \in \mathbb{N}_0\}$ the \emph{forward tail chain of} $\{X_t: t \in \mathbb{N}_0\}$.
\end{thm}

\begin{proof}
The argument is by induction on $t$. The case $t = 0$ is a straightforward consequence of Condition~\ref{C:RV}. So let $t$ be a positive integer and let $f : {\mathbb{R}}\times({\mathbb{R}}^d)^{t+1} \to {\mathbb{R}}$ be bounded and continuous. We have to show that
\begin{equation}
\label{E:forward:10}
	\lim_{x \to \infty}
	{\mathrm{E}} \biggl[
	f \biggl( \frac{\|X_0\|}{x}, \frac{X_0}{\|X_0\|}, \ldots, \frac{X_t}{\|X_0\|} \biggr)
	\, \biggl| \, \|X_0\| > x
	\biggr]
	= {\mathrm{E}} [ f(Y, M_0, \ldots, M_t) ].
\end{equation}
By \eqref{E:MC:1}, if $X_0 \neq 0$,
\[
	\frac{X_t}{\|X_0\|}
	= \frac{\Phi(X_{t-1}, {\varepsilon}_t)}{\|X_0\|}
	= \frac{\Phi(x \frac{\|X_0\|}{x} \frac{X_{t-1}}{\|X_0\|}, {\varepsilon}_t)}
	{x \frac{\|X_0\|}{x}}.
\]
Hence,
\begin{eqnarray}
\label{E:forward:20}
	\lefteqn{
	{\mathrm{E}} \biggl[
	f \biggl( \frac{\|X_0\|}{x}, \frac{X_0}{\|X_0\|}, \ldots, \frac{X_t}{\|X_0\|} \biggr)
	\, \biggl| \, \|X_0\| > x
	\biggr]
	} \\
	&=&
	{\mathrm{E}} \biggl[
	g_x \biggl( \frac{\|X_0\|}{x}, \frac{X_0}{\|X_0\|}, \ldots, \frac{X_{t-1}}{\|X_0\|} \biggr)
	\, \biggl| \, \|X_0\| > x
	\biggr] 
	\nonumber
\end{eqnarray}
where
\begin{equation}
\label{E:forward:gx}
	g_x(y, x_0, \ldots, x_{t-1})
	= {\mathrm{E}} \biggl[ f \biggl( y, x_0, \ldots, x_{t-1}, \frac{\Phi(xyx_{t-1}, {\varepsilon}_t)}{xy} \biggr) 
	\biggr]
\end{equation}
(note that the expectation is taken with respect to the distribution of $\epsilon_t$).
Define
\begin{equation}
\label{E:forward:g}
	g(y, x_0, \ldots, x_{t-1})
	= {\mathrm{E}} [ f (y, x_0, \ldots, x_{t-1}, \phi(x_{t-1},{\varepsilon}_t) ) ].
\end{equation}
By \eqref{E:Mj},
\begin{equation}
\label{E:forward:40}
	{\mathrm{E}} [ f(Y, M_0, \ldots, M_t) ] = {\mathrm{E}} [ g(Y, M_0, \ldots, M_{t-1}) ].
\end{equation}
In view of the identities \eqref{E:forward:20} and \eqref{E:forward:40}, the limit relation in \eqref{E:forward:10} will follow if we can show that
\begin{equation}
\label{E:forward:50}
	{\mathrm{E}} \biggl[
	g_x \biggl( \frac{\|X_0\|}{x}, \frac{X_0}{\|X_0\|}, \ldots, \frac{X_{t-1}}{\|X_0\|} \biggr)
	\, \biggl| \, \|X_0\| > x
	\biggr]
	\to {\mathrm{E}} [ g(Y, M_0, \ldots, M_{t-1}) ]
\end{equation}
as $x \to \infty$. In turn, \eqref{E:forward:50} will follow from the induction hypothesis and an extension of the continuous mapping theorem \citep[][Theorem~18.11]{vdV98} provided
\begin{equation}
\label{E:forward:55}
	\lim_{x \to \infty} g_x( y(x), x_0(x), \ldots, x_{t-1}(x) ) = g( y, x_0, \ldots, x_{t-1} )
\end{equation}
whenever $y(x) \to y$ and $x_i(x) \to x_i$ as $x \to \infty$ with $(y, x_0, \ldots, x_{t-1})$ ranging over a set $E \subset {\mathbb{R}} \times ({\mathbb{R}}^d)^t$ with ${\mathrm{P}}((Y, M_0, \ldots, M_{t-1}) \in E) = 1$. From the definitions of $g_x$ and $g$ in \eqref{E:forward:gx} and \eqref{E:forward:g}, respectively, equation~\eqref{E:forward:55} is implied by
\begin{equation}
\label{E:forward:60}
	\lim_{x \to \infty} \frac{\Phi(x w(x), v)}{x} = \phi(w,v)
\end{equation}
whenever $\lim_{x \to \infty} w(x) = w$ and where $w$ and $v$ range over sets that receive probability one by the distributions of $M_{t-1}$ and ${\varepsilon}_1$, respectively. Since \eqref{E:forward:60} is ensured by Condition~\ref{C:phi} and Lemma \ref{L:generalphi}, the statement follows. 
\end{proof}

\section{Backward tail processes}
\label{S:backforth}

From now on, the process $\{ X_t \}$ in \eqref{E:MC:1} and \eqref{E:MC:2} is assumed to be strictly stationary. A necessary and sufficient condition for stationarity is that 
\begin{equation}
\label{E:MC:3}
	{\mathcal{L}}(\Phi(X_0, {\varepsilon}_1)) = {\mathcal{L}}(X_0).
\end{equation}
It may be highly non-trivial to find the law for $X_0$ that solves \eqref{E:MC:3}. But even when the stationary distribution does not admit an explicit expression, its tails may in many cases be found by the theory developed originally in \citet{Kesten73}, \citet{Letac86} and \citet{Goldie91}. For recent results on specific models, see for instance \citet{KluPer03, KluPer04}, \citet{DeSa04}, \citet{Mirek11}, \citet{Bura12}, and \citet{CV13}.

If the process $\{ X_t \}$ is stationary, then by Kolmogorov's extension theorem and changing the probability space if necessary, the range of $t$ can without loss of generality be assumed to be the set of all integers, ${\mathbb{Z}}$; recall that we are interested in distributional properties only, not in almost sure properties, for instance.

Our aim is to extend Theorem~\ref{T:forward} and find the asymptotic distribution of the random vector $(X_{-s}, \ldots, X_t)$ conditionally on $\|X_0\| > x$ as $\| x \| \to \infty$, for all integer $s$ and $t$ (Corollary~\ref{Cor:spectralisBFTC}). 
According to \citet[][Theorem~2.1]{BS09}, if the underlying process is stationary, the existence of a forward tail process $(t \in {\mathbb{N}}_0)$ is enough to guarantee the existence of the tail process as a whole ($t \in \mathbb{Z}$).

\begin{prop}
\label{P:BS09:2.1}
Let $\{ X_t : t \in {\mathbb{Z}}\}$ be a stationary Markov chain with distribution determined by \eqref{E:MC:1}, \eqref{E:MC:2} and \eqref{E:MC:3}. If Conditions~\ref{C:RV} and \ref{C:phi} hold, then there exists a process $\{M_t: t \in \mathbb{Z}\}$ such that
\begin{equation}
\label{E:forwardandbackward:existence}
	{\mathcal{L}} \biggl(\frac{X_{-s}}{\|X_0\|}, \dots, \frac{X_0}{\|X_0\|}, \ldots, \frac{X_t}{\|X_0\|} \bigg| \|X_0\| > x \biggr)
	\Rightarrow {\mathcal{L}}(M_{-s}, \ldots, M_0, \ldots, M_t)
\end{equation}
for all integer $s,t \geq 0$.
\end{prop}

\begin{proof}
This follows from our Theorem \ref{T:forward} and Theorem 2.1 in \citet{BS09}, combined with a continuous mapping argument.
\end{proof}

We call the process $\{M_t: t \in \mathbb{Z}\}$ the \emph{spectral (tail) process} of $\{X_t: t \in \mathbb{Z}\}$, in accordance with the definition of the process $\{\Theta_t: t \in \mathbb{Z}\}$ in \citet{BS09}. 

\citet{BS09} also state an important property of the limiting process.
\begin{prop}
\label{P:BS09:3.1} 
Let $\{ X_t : t \in {\mathbb{Z}}\}$ be a stationary Markov chain with distribution determined by \eqref{E:MC:1}, \eqref{E:MC:2} and \eqref{E:MC:3} and spectral process $\{M_t: t \in \mathbb{Z}\}$. Then for all $s,t \geq 0$ and for all bounded and measurable $f:(\mathbb{R}^d)^{s+t+1} \to \mathbb{R}$ satisfying $f(y_{-s}, \ldots, y_t)=0$ whenever $y_{-s}=0$,
\begin{equation}
\label{E:timechange}
 E\left[f(M_{-s}, \ldots, M_{t}) \right]= E\left[f\left(\frac{M_0}{\|M_s\|}, \ldots, \frac{M_{s+t}}{\|M_s\|} \right) \|M_s\|^\alpha{\boldsymbol{1}}_{\{M_s \neq 0\}} \right].
\end{equation}
\end{prop}
\begin{proof}
It follows directly from our Proposition \ref{P:BS09:2.1} and Theorem 3.1 in \cite{BS09} that 
\begin{equation}\label{TC1}
 E\left[f(M_{-s-i}, \ldots, M_{t-i}) \right]= E\left[f\left(\frac{M_{-s}}{\|M_i\|}, \ldots, \frac{M_{t}}{\|M_i\|} \right) \|M_i\|^\alpha{\boldsymbol{1}}_{\{M_i \neq 0\}} \right].
\end{equation}
holds for all bounded and continuous $f:(\mathbb{R}^d)^{t+s+1} \to \mathbb{R}$ satisfying $f(y_{-s}, \ldots, y_t)=0$ whenever $y_0=0$ (instead of $y_{-s}=0$) and all $i \in \mathbb{Z}$. We have added the indicator function on the right-hand side for greater clarity. Let $s, t$ and $f$ be as in the statement of the Proposition. Apply \eqref{TC1} to the indices $(\underline{s}, \underline{t}, \underline{i}) = (0, t+s, s)$ to arrive at \eqref{E:timechange}; note that $\underline{s} + 1 + \underline{t} = s + 1 + t$ and that $f(x_{-\underline{s}}, \ldots, x_{\underline{t}}) = 0$ as soon as $x_0 = 0$. Thus, for functions $f$ which are additionally assumed to be continuous, the statement follows directly. 

For the general case, set for abbreviation $\mathbb{A}^\ast:=(\mathbb{R}^d)^{s+t+1}\setminus(\{0\} \times (\mathbb{R}^d)^{s+t})$. Furthermore, let $\mu$ denote the restriction of the law of $(M_{-s}, \ldots, M_t)$ to $\mathbb{A}^\ast$ and let $\nu$ denote the measure on $\mathbb{A}^\ast$ defined by 
$$ \nu(f)=E\left[f\left(\frac{M_{-s}}{\|M_i\|}, \ldots, \frac{M_{t}}{\|M_i\|} \right) \|M_i\|^\alpha{\boldsymbol{1}}_{\{M_i \neq 0\}} \right] $$
for all bounded and continuous $f$ on $\mathbb{A}^\ast$. In order to show \eqref{E:timechange} for general bounded and measurable $f$ with $f(y_{-s}, \ldots, y_t)=0$ if $y_{-s}=0$ it suffices to show that $\mu$ and $\nu$ coincide. The closed sets of $(\mathbb{R}^d)^{s+t+1}$ which are bounded away from $\{0\} \times (\mathbb{R}^d)^{s+t}$ are a $\pi$-system generating $\mathbb{B}(\mathbb{A}^\ast)$. Indicator functions of closed sets $A$ can be written as pointwise limits of continuous functions with values in $[0,1]$. If $A$ is bounded away from $\{0\} \times (\mathbb{R}^d)^{s+t}$ we can choose these approximating continuous functions in such a way that they vanish on $\{0\} \times (\mathbb{R}^d)^{s+t}$. Thus, by dominated convergence $\mu(A)=\nu(A)$ for all sets $A$ of a generating $\pi$-system and therefore $\mu=\nu$ on the Borel sets of $\mathbb{A}^\ast$ \citep[][Theorem~2.2]{Bi68}, which finishes the proof.
\end{proof}

By Lemma~2.2 in \citet{BS09} it follows that the distribution of $\{M_t: t \in {\mathbb{Z}}\}$ is uniquely determined by the distribution of $\{M_t: t \in {\mathbb{N}}_0\}$ (and $\alpha>0$). We will use \eqref{E:timechange} to analyze the structure of the spectral process with a special focus on the backward process $\{M_ {-t}: t \in {\mathbb{N}}_0\}$. At the heart of the connection between the forward and backward processes is an adjoint relation between the laws of $(M_0,M_1)$ and $(M_0,M_{-1})$, studied next.

\section{An adjoint relation between distributions}
\label{S:adjoint}

A special case of the equality \eqref{E:timechange} is
\begin{equation}\label{timechange:onestep}
{\mathrm{E}}\left[f(M_{-1}, M_0)\right]= {\mathrm{E}}\left[f\left(\frac{M_0}{\|M_1\|}, \frac{M_1}{\|M_1\|} \right) \|M_1\|^\alpha{\boldsymbol{1}}_{\{M_1 \neq 0\}} \right]
\end{equation}
for all $f:(\mathbb{R}^d)^2 \to \mathbb{R}$ satisfying $f(y_0, y_1)=0$ whenever $y_0=0$. Starting from a given distribution of $(M_0, M_1)$ we will in the following characterize the distributions of $(M_{-1}, M_0)$ which satisfy \eqref{timechange:onestep}. 
For such an adjoint distribution to exist,
the distribution $(M_0, M_1)$ cannot be chosen arbitrarily from the distributions on $\mathbb{S}^{d-1}\times \mathbb{R}^d$. We therefore introduce the following set of ``admissible'' distributions.

\begin{defn}
\label{def:admissible}
For $\alpha \in (0, \infty)$, let $\mathcal{M}_\alpha = \mathcal{M}_{\alpha,d}$ be the set of all probability measures $P$ on $\mathbb{S}^{d-1} \times {\mathbb{R}}^d$ such that
\begin{equation}
\label{eq:admissible}
  \int_{\mathbb{S}^{d-1}\times ({\mathbb{R}}^d \setminus \{ 0 \})} {\boldsymbol{1}}_S( m / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \le P( S \times {\mathbb{R}}^d )
\end{equation}
for every Borel set $S \subset \mathbb{S}^{d-1}$. We call $\mathcal{M}_\alpha$ the set of \emph{admissible distributions} for $\alpha>0$. 
\end{defn}

Note that for $P \in \mathcal{M}_\alpha$ we have
\[
  \int_{\mathbb{S}^{d-1} \times {\mathbb{R}}^d} {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \le 1.
\]
We now make the already mentioned notion of an ``adjoint'' distribution more concise. 

\begin{defn}
\label{def:adjoint}
For $P \in \mathcal{M}_\alpha$, define a signed Borel measure $P^*$ on $\mathbb{S}^{d-1} \times {\mathbb{R}}^d$ by
\begin{align}
\label{eq:adjoint:S0}
  P^*(S \times \{ 0 \}) &= P(S \times {\mathbb{R}}^d) - \int_{\mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})} {\boldsymbol{1}}_S( m / {\|{m}\|} ) \, \|m\|^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}), \\
\label{eq:adjoint:E}
  P^*(E) &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } {\boldsymbol{1}}_E( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}),
\end{align}
for Borel sets $S \subset \mathbb{S}^{d-1}$ and $E \subset \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})$.
We call $P^*$ the \emph{adjoint measure of $P$ in $\mathcal{M}_\alpha$}.
\end{defn}

\begin{lem}
\label{lem:adjoint}
Let $P \in \mathcal{M}_\alpha$ and let $P^*$ be as in Definition~\ref{def:adjoint}.
 \begin{enumerate}[(i)]
  \item $P^*$ is a probability measure and the marginal distributions induced by $P$ and $P^*$ on $\mathbb{S}^{d-1}$ are the same.
\item For every measurable function $f : \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) \to {\mathbb{R}}$,
\begin{multline}
\label{eq:adjoint:f}
  \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f(s^*, m^*) \, P^*({\mathrm{d}s}^*, {\mathrm{d}m}^*) \\*
  = \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m})
\end{multline}
in the sense that if one integral exists, then so does the other, and they are the same.
\item $P^* \in \mathcal{M}_\alpha$.
\item $(P^*)^* = P$.
  \end{enumerate}
\end{lem}

\begin{proof}
(i) By \eqref{eq:admissible}, $P^*$ is a nonnegative Borel measure. Let $S$ be a Borel subset of $\mathbb{S}^{d-1}$. We have
\[
  P^*( S \times {\mathbb{R}}^d ) = P^*( S \times \{ 0 \} ) + P^*\bigl( S \times ({\mathbb{R}}^d \setminus \{ 0 \}) \bigr).
\]
Applying \eqref{eq:adjoint:S0} to the first term on the right-hand side and applying \eqref{eq:adjoint:E} with $E = S \times ({\mathbb{R}}^d \setminus \{ 0 \})$ to the second term on the right-hand side yields
\[
  P^*( S \times {\mathbb{R}}^d ) = P( S \times {\mathbb{R}}^d ).
\]
It follows that $P^*$ is a probability measure (take $S = \mathbb{S}^{d-1}$) on $\mathbb{S}^{d-1} \times {\mathbb{R}}^d$ inducing the same marginal distribution on $\mathbb{S}^{d-1}$ as $P$.

(ii) By \eqref{eq:adjoint:E}, equation~\eqref{eq:adjoint:f} holds for indicator functions ${\boldsymbol{1}}_E$ of Borel subsets $E$ of $\mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})$. The extension to general bounded, measurable functions follows from the definition of the integral.

(iii) Let $S$ be a Borel subset of $\mathbb{S}^{d-1}$. We will apply \eqref{eq:adjoint:f} to the function
\[
  f(s, m) = {\boldsymbol{1}}_S( m / {\|{m}\|} ) \, {\|{m}\|}^\alpha \qquad \text{for $(s, m) \in \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})$}.
\]
We find
\begin{align*}
  \lefteqn{
  \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } {\boldsymbol{1}}_S( m^* / {\|{m^*}\|} ) \, {\|{m^*}\|}^\alpha \, P^*({\mathrm{d}s}^*, {\mathrm{d}m}^*)
  } \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f( s^*, m^* ) \, P^*({\mathrm{d}s}^*, {\mathrm{d}m}^*) \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } {\boldsymbol{1}}_S \biggl( \frac{s / {\|{m}\|}}{{\|{(s / {\|{m}\|})}\|}} \biggr) \, {\|{(s / {\|{m}\|})}\|}^\alpha \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \\
  &= P \bigl( S \times ( {\mathbb{R}}^d \setminus \{ 0 \} ) \bigr) \\
  &\le P ( S \times {\mathbb{R}}^d ) = P^* ( S \times {\mathbb{R}}^d ),
\end{align*}
where we applied (i) in the last step.

(iv) Let $Q = (P^*)^*$. We already know that $Q$ is a probability measure on $\mathbb{S}^{d-1} \times {\mathbb{R}}^d$, that $Q \in \mathcal{M}_\alpha$, and that the marginal induced by $Q$ on $\mathbb{S}^{d-1}$ coincides with the one of $P^*$ and thus with the one of $P$. Let $f$ be a nonnegative, measurable function on $\mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus\{ 0 \})$. Define the nonnegative, measurable function $g$ on $\mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})$ by
\[
  g( s, m ) = f( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha, \qquad \text{for $(s, m) \in \mathbb{S}^{d-1} \times ({\mathbb{R}}^d\setminus\{0\})$}.
\]
We have
\begin{multline}
\label{eq:adjoint:g2f}
  g( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha \\
  = f\biggl( \frac{s / {\|{m}\|}}{{\|{(s / {\|{m}\|})}\|}}, \frac{m / {\|{m}\|}}{{\|{(s / {\|{m}\|})}\|}} \biggr) \, {\|{(s / {\|{m}\|})}\|}^\alpha \, {\|{m}\|}^\alpha
  = f(s, m).
\end{multline}
By \eqref{eq:adjoint:f} applied first to $Q$ and $f$ and then to $P^*$ and $g$, we have
\begin{align*}
  \lefteqn{
  \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f( s, m ) \, Q({\mathrm{d}s}, {\mathrm{d}m})
  } \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f( m^* / {\|{m^*}\|}, s^* / {\|{m^*}\|} ) \, {\|{m^*}\|}^\alpha \, P^*({\mathrm{d}s}^*, {\mathrm{d}m}^*) \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } g( s^*, m^* ) \, P^*({\mathrm{d}s}^*, {\mathrm{d}m}^*) \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } g( m / {\|{m}\|}, s / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \\
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \}) } f(s, m) \, P({\mathrm{d}s}, {\mathrm{d}m}),
\end{align*}
where we used \eqref{eq:adjoint:g2f} in the last step. It follows that $Q$ and $P$ coincide on $\mathbb{S}^{d-1} \times ({\mathbb{R}}^d \setminus \{ 0 \})$. As $Q$ and $P$ also induce the same marginal distributions on $\mathbb{S}^{d-1}$, it follows that they must also coincide on $\mathbb{S}^{d-1} \times \{ 0 \}$. As a consequence, $Q$ is equal to $P$.
\end{proof} 

The next lemma shows that the class $\mathcal{M}_\alpha$ and the adjoint relation on it arise naturally in the context of regularly varying Markov chains.

\begin{lem}
\label{L:M1}
Let $\{ X_t : t \in {\mathbb{Z}}\}$ be a stationary Markov chain with distribution determined by \eqref{E:MC:1}, \eqref{E:MC:2} and \eqref{E:MC:3}. If Conditions~\ref{C:RV} and \ref{C:phi} hold, then ${\mathcal{L}}(M_0, M_1)$ belongs to $\mathcal{M}_\alpha$ and its adjoint is equal to ${\mathcal{L}}(M_0, M_{-1})$.
\end{lem}

\begin{proof}
To prove admissibility, we have to show that
\begin{equation}
\label{eq:markov-admissible:S}
  {\mathrm{E}}[ {\boldsymbol{1}}_S( M_1 / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha ] \le {\mathrm{P}}( M_0 \in S )
\end{equation}
for every Borel set $S \subset \mathbb{S}^{d-1}$. Let $f$ be a bounded, nonnegative and continuous function on $\mathbb{S}^{d-1}$. We will show that
\begin{equation}
\label{eq:markov-admissible:f}
  {\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha ] \le {\mathrm{E}}[ f(M_0) ].
\end{equation}
Equation~\eqref{eq:markov-admissible:f} implies \eqref{eq:markov-admissible:S} for closed sets $S$ because the indicator function of a closed set $S$ can be written as the pointwise limit of a decreasing sequence of continuous functions taking values in the interval $[0, 1]$. From this we arrive at \eqref{eq:markov-admissible:S} for an arbitrary Borel set $S$ by invoking an increasing sequence of closed sets $S_n$ contained in $S$ such that ${\mathrm{E}}[ {\boldsymbol{1}}_{S_n}( M_1 / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha ]$ and ${\mathrm{P}}( M_0 \in S_n )$ converge to ${\mathrm{E}}[ {\boldsymbol{1}}_S( M_1 / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha ]$ and ${\mathrm{P}}( M_0 \in S )$ respectively; see for instance Theorem~1.1 on p.~7 in \citet{Bi68}.

Let $\delta > 0$. By stationarity of $\{X_t:t \in {\mathbb{Z}}\}$ and by definition of the spectral process $\{M_t:t \in {\mathbb{Z}}\}$, we have
\begin{align*}
  {\mathrm{E}}[ f(M_0) ]
  &= \lim_{x \to \infty} {\mathrm{E}}[ f( X_1 / {\|{X_1}\|} ) \mid {\|{X_1}\|} > x ] \\
  &\ge \limsup_{x \to \infty} 
  {\mathrm{E}}[ {\boldsymbol{1}}_{ \{ {\|{X_0}\|} > \delta x \} } \, f( X_1 / {\|{X_1}\|} ) \mid {\|{X_1}\|} > x ] \\
  &= \limsup_{x \to \infty} 
  \frac{{\mathrm{P}}[ {\|{X_0}\|} > \delta x ]}{{\mathrm{P}}[ {\|{X_1}\|} > x]} \, {\mathrm{E}}[ f( X_1 / {\|{X_1}\|} ) \, {\boldsymbol{1}}_{ \{ {\|{X_1}\|} > x \} } \mid {\|{X_0}\|} > \delta x ] \\
  &= \delta^{-\alpha} {\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, {\boldsymbol{1}}_{ \{ Y {\|{M_1}\|} > \delta^{-1} \} } ].
\end{align*}
In the last line, $Y$ is a Pareto($\alpha$) random variable, independent of $M_1$. As ${\mathrm{P}}( Y {\|{M_1}\|} = \delta^{-1} ) = 0$ by continuity of the law of $Y$, the last equality in the above display follows from the continuous mapping theorem.

Since the distribution of $Y^{-\alpha}$ is uniform on the interval $(0, 1)$, we have
\begin{eqnarray*}
 \delta^{-\alpha} {\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, {\boldsymbol{1}}_{ \{ Y {\|{M_1}\|} > \delta^{-1} \} } ] &=& \delta^{-\alpha} {\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, {\boldsymbol{1}}_{ \{ \delta^\alpha {\|{M_1}\|}^\alpha > Y^{-\alpha} \} } ] \\
  &=& \delta^{-\alpha} {\mathrm{E}}[{\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, {\boldsymbol{1}}_{ \{ \delta^\alpha {\|{M_1}\|}^\alpha > Y^{-\alpha} \} }|M_1] ] \\
  &=& \delta^{-\alpha} {\mathrm{E}}[ f( M_1 / {\|{M_1}\|} ) \, \min( \delta^\alpha {\|{M_1}\|}^\alpha, 1 ) ] \\
  &=& {\mathrm{E}}[ f(M_1 / {\|{M_1}\|} ) \, \min( {\|{M_1}\|}^\alpha, \delta^{-\alpha} ) ].
\end{eqnarray*}
We obtain that for every $\delta > 0$,
\[
  {\mathrm{E}}[ f(M_0) ] \ge {\mathrm{E}}[ f(M_1 / {\|{M_1}\|} ) \, \min( {\|{M_1}\|}^\alpha, \delta^{-\alpha} ) ].
\]
Take the limit as $\delta \to 0$ and apply the monotone convergence theorem to obtain \eqref{eq:markov-admissible:f}. 

Next we show that the adjoint of ${\mathcal{L}}(M_0, M_1)$ is equal to ${\mathcal{L}}(M_0, M_{-1})$. We have to check the two equations
\begin{align}
\nonumber
  {\mathrm{P}}((M_0,M_{-1}) \in S \times \{0\})
  &={\mathrm{P}}(M_0 \in S)-{\mathrm{E}}[{\boldsymbol{1}}_{\mathbb{R}^d \setminus\{0\}}(M_1){\boldsymbol{1}}_S(M_1/\|M_1\|)\|M_1\|^\alpha], \\
\label{Eq:lemM2:2}
  {\mathrm{P}}((M_0,M_{-1}) \in E)
  &={\mathrm{E}}[{\boldsymbol{1}}_{\mathbb{R}^d \setminus\{0\}}(M_1){\boldsymbol{1}}_E(M_1/\|M_1\|,M_0/\|M_1\|)\|M_1\|^\alpha],
\end{align}
for all Borel sets $S \subset \mathbb{S}^{d-1}$ and $E \subset \mathbb{S}^{d-1}\times (\mathbb{R}^d\setminus\{0\})$. Since the first component $M_0$ is common to both laws, it is sufficient to check only the second equation, \eqref{Eq:lemM2:2}.

Set $f(m_{-1},m_0)={\boldsymbol{1}}_E(m_0,m_{-1})$ on $\mathbb{R} \times \mathbb{S}^{d-1}$. Note that $f(0,m_0)=0$. Apply equation \eqref{timechange:onestep} to $f$:
\begin{align*} 
  {\mathrm{P}}((M_0,M_{-1}) \in E)
  &= {\mathrm{E}}[f(M_{-1},M_0)] \\
  &= {\mathrm{E}}[f(M_0/\|M_1\|,M_1/\|M_1\|)\|M_1\|^\alpha{\boldsymbol{1}}_{\{M_1 \neq 0\}}] \\
  &= {\mathrm{E}}[{\boldsymbol{1}}_{\mathbb{R}^d \setminus\{0\}}(M_1){\boldsymbol{1}}_E(M_1/\|M_1\|,M_0/\|M_1\|)\|M_1\|^\alpha],
\end{align*}
which gives \eqref{Eq:lemM2:2}, as required. 
\end{proof}

\begin{rem}\label{partsimple} The determination of the adjoint measure is particularly simple for probability measures $P$ such that 
\begin{equation}
\label{eq:partsimple}
  \int_{\mathbb{S}^{d-1} \times \mathbb{R}^d}\|m\|^\alpha P({\mathrm{d}s}, {\mathrm{d}m})=1,  
\end{equation}
since in this case $P^*(\mathbb{S}^{d-1} \times \{ 0 \})=0$ by \eqref{eq:adjoint:S0} and $P^*$ is completely described by \eqref{eq:adjoint:E}.
\end{rem}

\begin{rem}\label{selfadjoint}
We call a measure $P \in \mathcal{M}_\alpha$ \emph{self-adjoint} if $P^\ast=P$. An example for such a distribution in the case of $d=1$ and $\alpha=1$ is given by $P=\mathcal{L}(1,Y)$, where $Y=\exp(X-1/2)$ for standard normally distributed $X$ (cf.\ Example~3.2 in \cite{S07}).
 \end{rem}

Definition~\ref{def:adjoint} and Lemma~\ref{lem:adjoint} generalize Proposition~3.1 in \cite{S07} to the multivariate case. Examples~3.2--3.4 in the latter reference illustrate the adjoint relation for laws on $\{-1, +1\} \times \mathbb{R}$. We conclude the section with a multivariate example.

\iffalse
\subsection*{Examples} At the end of this section, we give some examples which shall help to illustrate the connection between a probability measure and its adjoint measure. We will start with one-dimensional examples. The case $d=1$ is special since $\mathbb{S}^0=\{-1,1\}$ and the first marginal distribution of measures $P \in \mathcal{M}_\alpha$ is necessarily discrete, which simplifies definitions and analysis. Before we start with more specific examples, we will state a general principle, which often helps to determine the adjoint distribution in the one-dimensional case.
\begin{cor}
Let $\alpha>0$ and $P\in \mathcal{M}_\alpha$ be a probability measure on $\{-1,1\} \times \mathbb{R}$. Then for $P^*$ the relation
\begin{eqnarray}\nonumber && \int_{\{-1,1\} \times \mathbb{R}}{\boldsymbol{1}}_{\{\sigma\}}(s^*)f(m^*)P^*({\mathrm{d}s}^*,{\mathrm{d}m}^*) \\
\nonumber &=&\int_{\{-1,1\} \times \mathbb{R} \setminus\{0\}}f(s/|m|)(\sigma m)_+^\alpha P({\mathrm{d}s},{\mathrm{d}m}) \\
\label{E:adjoint:univ} && + f(0)\left\{P(\{\sigma\} \times \mathbb{R})- \int_{\{-1,1\} \times \mathbb{R}}(\sigma m)_+^\alpha P({\mathrm{d}s}, {\mathrm{d}m})\right\}
\end{eqnarray}
holds for all $\sigma \in \{-1,1\}$ and measurable functions $f$. 
\end{cor}
\begin{proof}
Let $\sigma \in \{-1,1\}$ and $f$ be a measurable function. Then
\begin{eqnarray*}
 && \int_{\{-1,1\} \times \mathbb{R}}{\boldsymbol{1}}_{\{\sigma\}}(s^*)f(m^*)P^*({\mathrm{d}s}^*,{\mathrm{d}m}^*)\\
 &=& \int_{\{-1,1\} \times (\mathbb{R}\setminus \{0\})}{\boldsymbol{1}}_{\{\sigma\}}(s^*)f(m^*)P^*({\mathrm{d}s}^*,{\mathrm{d}m}^*)\\
 && +f(0)\int_{\{-1,1\} \times \{0\}}{\boldsymbol{1}}_{\{\sigma\}}(s^*)P^*({\mathrm{d}s}^*,{\mathrm{d}m}^*)\\
 &=& \int_{\{-1,1\} \times (\mathbb{R}\setminus \{0\})}{\boldsymbol{1}}_{\{\sigma\}}(m/|m|)f(s/|m|)|m|^\alpha P({\mathrm{d}s},{\mathrm{d}m})\\
 && +f(0)\left\{P(\{\sigma\} \times {\mathbb{R}}) - \int_{\{-1,1\} \times ({\mathbb{R}} \setminus \{ 0 \})} {\boldsymbol{1}}_{\{\sigma\}}(m/|m|)|m|^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m})\right\}
\end{eqnarray*}
 where, in the last equation, we have applied \eqref{eq:adjoint:f} and \eqref{eq:adjoint:S0} to the first and second summand respectively. Noting that 
 ${\boldsymbol{1}}_{\{\sigma\}}(m/|m|)|m|^\alpha=(\sigma m)_+^\alpha$
 gives \eqref{E:adjoint:univ}.
\end{proof}
\begin{rem}
 Equation \eqref{E:adjoint:univ} may sometimes be more convenient if written with the help of random variables. If $P=\mathcal{L}(X,Y) \in \mathcal{M}_\alpha$ and $P^*=\mathcal{L}(X^*,Y^*)$ then \eqref{E:adjoint:univ} reduces to
 \begin{equation}\label{E:adjoint:univ:RV}{\mathrm{E}} [{\boldsymbol{1}}_{\sigma}(X^*)f(Y^*)]={\mathrm{E}}[f(X/|Y|)(\sigma Y)_+^\alpha]+f(0)\left\{{\mathrm{P}}(X=\sigma)-{\mathrm{E}}[(\sigma Y)_+^\alpha]\right\}
 \end{equation}
 for $\sigma \in \{-1,1\}$ and measurable functions $f$. 
\end{rem}

\begin{ex}
\label{Ex:adjoint:ARCH}
Let $P = {\mathcal{L}}(I, IZ)$ where $I$ and $Z$ are independent (univariate) random variables with ${\mathrm{P}}(I = 1) = 1/2 = {\mathrm{P}}(I = -1)$ and ${\mathrm{E}} [ |Z|^\alpha ] \leq 1$. For $d=1$, equation \eqref{eq:admissible} reduces to
$$\int_{\{-1,1\} \times \mathbb{R}\setminus\{0\}} (\sigma m)_+^\alpha P({\mathrm{d}s}, {\mathrm{d}m}) \leq P(\{\sigma\} \times \mathbb{R}) $$
for $\sigma \in \{-1,1\}$. For our example, this inequality holds since for $\sigma \in \{-1,1\}$:
\begin{eqnarray}
&&\nonumber \int_{\{-1,1\} \times \mathbb{R}\setminus\{0\}} (\sigma m)_+^\alpha P({\mathrm{d}s}, {\mathrm{d}m})\\
&=&\nonumber {\mathrm{E}}[(\sigma IZ)_+^\alpha]=\frac{1}{2}\cdot{\mathrm{E}}[(\sigma Z)_+^\alpha]+\frac{1}{2}\cdot{\mathrm{E}}[(\sigma Z)_-^\alpha] \\
&=&\label{E:Ex3.1:admissible} \frac{1}{2}\cdot{\mathrm{E}}[|Z|^\alpha] \leq \frac{1}{2}=P(\{\sigma\} \times \mathbb{R}).
\end{eqnarray}
Therefore, $P \in \mathcal{M}_\alpha$. We will show that the adjoint measure $P^*$ shares the multiplicative structure of $P$. Let therefore $I^\ast \in \{-1,1\}$ and $Z^\ast \in \mathbb{R}$ be two independent random variables with $\mathcal{L}(I^\ast)=\mathcal{L}(I)$ and 
\begin{equation}\label{E:backward:mult}{\mathrm{E}}[f(Z^*)]= {\mathrm{E}}[f(1/Z)|Z|^\alpha{\boldsymbol{1}}_{\{Z \neq 0\}}]+f(0)(1-{\mathrm{E}}[|Z|^\alpha])
\end{equation}
for all measurable functions $f$. For $\sigma \in \{-1,1\}$ and a measurable function $f$
\begin{eqnarray*}
&& E[{\boldsymbol{1}}_{\{\sigma\}}(I^\ast)f(I^\ast Z^\ast)]=E[{\boldsymbol{1}}_{\{\sigma\}}(I^\ast)f(\sigma Z^\ast)]\\
&=&P(I^\ast = \sigma)\left\{{\mathrm{E}}[f(\sigma/Z)|Z|^\alpha{\boldsymbol{1}}_{\{Z \neq 0\}}]+f(0)(1-{\mathrm{E}}[|Z|^\alpha]) \right\}
\end{eqnarray*}
by \eqref{E:backward:mult}. Since $P(I^\ast = \sigma)=1/2=P(IZ/|IZ|=\sigma)$ this is equal to
\begin{eqnarray*}
&& {\mathrm{E}}[{\boldsymbol{1}}_{\{\sigma\}}(IZ/|IZ|)f(I/|IZ|)|IZ|^\alpha{\boldsymbol{1}}_{\{Z \neq 0\}}]+f(0)\left\{\frac{1}{2}-\frac{1}{2}\cdot {\mathrm{E}}[|Z|^\alpha]\right\} \\
&=& \int_{\{-1,1\} \times \mathbb{R} \setminus\{0\}}{\boldsymbol{1}}_{\{\sigma\}}(m/|m|)f\left(s/|m|\right)|m|^\alpha P({\mathrm{d}s},{\mathrm{d}m}) \\
&& + f(0)\left\{P(\{\sigma\} \times \mathbb{R})- \int_{\{-1,1\} \times \mathbb{R}}(\sigma m)_+^\alpha P({\mathrm{d}s}, {\mathrm{d}m})\right\}\\
&=& \int_{\{-1,1\}\times\mathbb{R}}{\boldsymbol{1}}_{\{\sigma\}}(s^*)f(m^*)P^*({\mathrm{d}s}^*,{\mathrm{d}m}^*),
\end{eqnarray*}
where equations \eqref{E:Ex3.1:admissible} and \eqref{E:adjoint:univ} have been used. Thus, $P^\ast=(\mathcal{L}(I, IZ))^\ast$ $=$ \linebreak $\mathcal{L}(I^\ast, I^\ast Z^\ast)$.
\end{ex}

\begin{ex}
\label{Ex:adjoint:1}
A special case arises for $P \in \mathcal{M}_\alpha$ with $P(\{1\} \times \mathbb{R})=1$. Equation \eqref{eq:admissible} implies that $P(\{1\} \times [0, \infty))=1$, thus $P = \delta_1 \otimes \mathcal{L}(Z)$ for some random variable $Z$ on $[0, \infty)$ with $E[Z^\alpha] \leq 1$. The adjoint of $P$ is $P^\ast = \delta_1 \otimes \mathcal{L}(Z^\ast)$ where $Z^\ast$ is related to $Z$ via \eqref{E:backward:mult} as in the preceeding example.

Some examples of pairs $(\mathcal{L}(Z), \mathcal{L}(Z^\ast))$ are the following:
\begin{itemize}
\item For every $\alpha$, if $\mathcal{L}(Z)$ is concentrated on $\{0, 1\}$, then $\mathcal{L}(Z^\ast)=\mathcal{L}(Z)$.
\item If $\alpha = 1$ and $Z$ is a unit exponential random variable, then $\mathcal{L}(Z^\ast)$ is the distribution of the reciprocal of the sum of two independent unit exponential random variables.
\item If $\alpha = 1$ and $Z$ is a lognormal random variable with unit expectation, then $\mathcal{L}(Z^\ast) =\mathcal{L}(Z)$.
\end{itemize}
All examples may be verified by applying \eqref{E:backward:mult} to the respective specification of $\mathcal{L}(Z)$.
Except for a change of sign, the case $P(\{-1\} \times \mathbb{R})=1$ is similar to the case $P(\{1\} \times \mathbb{R})=1$.
\end{ex}

We close the example section with a multivariate example. Again, a multiplicative structure in the admissible distribution helps to simplify the derivation of the adjoint measure.
\fi

\begin{ex}
\label{Ex:mult:1}
Let $\alpha > 0$ and let $P$ be the law of $(C, R Q C)$ with $C$, $R$ and $Q$ independent, $C$ taking values in $\mathbb{S}^{d-1}$, $R$ a positive random variable with ${\mathrm{E}}[R^\alpha] = 1$, and $Q$ a random orthogonal $d \times d$ matrix, that is $Q' = Q^{-1}$ a.s.; also assume that the laws of $C$ and $QC$ are the same (cf.\ also Example \ref{Ex:KestenRDE}). One verifies easily that $P \in \mathcal{M}_\alpha$ and that \eqref{eq:partsimple} holds, so that the adjoint law $P^*$ is concentrated on $\mathbb{S}^{d-1} \times (\mathbb{R}^d \setminus \{ 0 \})$. It may thus be derived from \eqref{eq:adjoint:E} that for Borel sets $S \subset \mathbb{S}^{d-1}$ and $T \subset \mathbb{R}^d \setminus \{ 0 \}$,
\begin{equation}
\label{E:Ex:mult:1}
  P^*(S \times T)={\mathrm{E}}[{\boldsymbol{1}}_S(QC){\boldsymbol{1}}_{T}(C/R)R^\alpha].
\end{equation}
If we assume in addition that $C$ is uniformly distributed on $\mathbb{S}^{d-1}$ (which readily implies $\mathcal{L}(C)=\mathcal{L}(QC)$ for any law of $Q$), then
\begin{eqnarray*}
{\mathrm{E}}[{\boldsymbol{1}}_S(QC){\boldsymbol{1}}_{T}(C)]&=&{\mathrm{E}}\left[\int_{\mathbb{R}^{d \times d}} {\boldsymbol{1}}_S(qC){\boldsymbol{1}}_{T}(C)P^Q(dq)\right] \\
&=& {\mathrm{E}}\left[\int_{\mathbb{R}^{d \times d}} {\boldsymbol{1}}_S(C){\boldsymbol{1}}_{T}(q'C)P^Q(dq)\right] \\
&=& {\mathrm{E}}[{\boldsymbol{1}}_S(C){\boldsymbol{1}}_{T}(Q'C)]
\end{eqnarray*}
and it follows from \eqref{E:Ex:mult:1} that $P^*$ is the law of $(C^*, R^* Q^* C^*)$, with $C^*$, $R^*$ and $Q^*$ independent, ${\mathcal{L}}(C^*) = {\mathcal{L}}(C)$, ${\mathcal{L}}(Q^*) = {\mathcal{L}}(Q')$, and the law of $R^* > 0$ given by ${\mathrm{E}}[f(R^*)] = {\mathrm{E}}[f(1/R) \, R^\alpha]$ for measurable functions $f$ on $(0, \infty)$.
\end{ex}

\iffalse
\begin{ex}
\label{Ex:mult:1}
Let $\alpha>0$ and $P$ be the law of $(I,AI) \in \mathbb{S}^{d-1} \times \mathbb{R}^d$, where $A \in \mathbb{R}^{d \times d}$ is of the form $A=Y \cdot R$, with random variable $Y>0$ and $R$ is a random rotation matrix, i.e. $\|Rx\|=1$ for all $x \in \mathbb{S}^{d-1}$. Let $I, Y$ and $R$ be mutually independent. Assume furthermore that ${\mathrm{E}}[Y^\alpha]<1$ and ${\mathrm{E}}[Y^\alpha]{\mathrm{P}}(RI \in S)\leq {\mathrm{P}}(I \in S)$ for all Borel sets $S \subset \mathbb{S}^{d-1}$ (cf.\ also Example \ref{Ex:KestenRDE} for the latter assumption). In this case
\begin{eqnarray*}&&\int_{\mathbb{S}^{d-1}\times(\mathbb{R}^d \setminus\{0\})}{\boldsymbol{1}}_S(m/\|m\|)\|m\|^\alpha P(ds,dm)\\
&=&{\mathrm{E}}[{\boldsymbol{1}}_S(RI)Y^\alpha]={\mathrm{E}}[Y^\alpha]P(RI \in S)\leq P(S \times \mathbb{R}^d),
\end{eqnarray*}

so $P \in \mathcal{M}_\alpha$. Now, for Borel sets $S \subset \mathbb{S}^{d-1}$ and $T \subset \mathbb{R}^d$ it follows that
 \begin{eqnarray*}
 P^*(S \times T)&=&\int_{\mathbb{S}^{d-1} \times (\mathbb{R}^d\setminus\{0\})}{\boldsymbol{1}}_S(m/\|m\|) {\boldsymbol{1}}_T(s/\|m\|) \|m\|^\alpha P({\mathrm{d}s},{\mathrm{d}m}) \\
 && + {\boldsymbol{1}}_T(0)\left[P(S \times \mathbb{R}^d)-\int_{\mathbb{S}^{d-1} \times (\mathbb{R}^d\setminus\{0\})}{\boldsymbol{1}}_S(m/\|m\|)\|m\|^\alpha P({\mathrm{d}s},{\mathrm{d}m})\right] \\
 &=& {\mathrm{E}}[{\boldsymbol{1}}_S(RI){\boldsymbol{1}}_{T}(I/Y)Y^\alpha]+ {\boldsymbol{1}}_T(0)({\mathrm{P}}(I \in S)-{\mathrm{E}}[Y^\alpha]{\mathrm{P}}(RI \in S)).  
 \end{eqnarray*}
 If we assume that ${\mathrm{E}}[Y^\alpha]=1$ and ${\mathrm{P}}(RI \in S)={\mathrm{P}}(I \in S)$ for all Borel sets $S \in \mathbb{S}^{d-1}$ (again, cf.\ Example \ref{Ex:KestenRDE} for this assumptions) then 
 $$P^*(S \times T)={\mathrm{E}}[{\boldsymbol{1}}_S(RI){\boldsymbol{1}}_{T}(I/Y)Y^\alpha].$$
Being a rotation matrix, $R$ is almost surely invertible and $\mathcal{L}(RI)=\mathcal{L}(I)$ implies that $P^\ast=\mathcal{L}(I^\ast, Y^\ast R^\ast I^\ast)$ with $\mathcal{L}(I^\ast)=\mathcal{L}(I)$, $\mathcal{L}(R^\ast)=\mathcal{L}(R^{-1})$ and $E[f(Y^\ast)]=E[f(1/Y){\boldsymbol{1}}_{(0, \infty)}(Y)Y^\alpha]$ for all measurable functions $f$ with $I^\ast, R^\ast, Y^\ast$ being pairwise independent.
\end{ex}
\fi

\section{Back-and-forth tail chains  and the spectral process}
\label{S:BFTC}

In this section, we will analyze a certain class of discrete-time processes which are constructed from a pair of adjoint distributions. We will see that this class of processes fulfills equation \eqref{E:timechange} for all $i,s,t \in \mathbb{Z}$ with $s \leq 0 \leq t$. 

\begin{defn}
\label{D:BFTC}
A $d$-dimensional discrete-time process $\{M_t:t \in {\mathbb{Z}}\}$ is called a \emph{back-and-forth tail chain} with index $\alpha>0$, notation ${\mbox{\sc bftc}}(\alpha)$, if the following properties hold:
\begin{enumerate}[(i)]
\item ${\mathcal{L}}(M_0, M_1)$ and ${\mathcal{L}}(M_0, M_{-1})$ belong to $\mathcal{M}_\alpha$ and are adjoint;
\item the forward process $\{M_t:t \in {\mathbb{N}}_0\}$ is a Markov chain with respect to the filtration \linebreak $\sigma( M_s, -\infty < s \le t)$, $t \ge 0$, and the Markov kernel satisfies
\begin{multline*}
  {\mathrm{P}}( M_t \in \,\cdot\, \mid M_{t-1} = x_{t-1}) \\
  = \begin{cases} 
      \delta_0(\,\cdot\,) & \text{if $x_{t-1} = 0$,} \\
      {\mathrm{P}}( {\|{x_{t-1}}\|} M_1 \in \,\cdot\, \mid M_0 = x_{t-1}/{\|{x_{t-1}}\|} ) & \text{if $x_{t-1} \neq 0$;}
    \end{cases}
\end{multline*}
\item the backward process $\{M_{-t}:t \in {\mathbb{N}}_0\}$ is a Markov chain with respect to the filtration $\sigma( M_{-s}, -\infty < s \le t)$, $t \ge 0$, and the Markov kernel satisfies
\begin{multline*}
  {\mathrm{P}}( M_{-t} \in \,\cdot\, \mid M_{-t+1} = x_{-t+1}) \\
  = \begin{cases} 
      \delta_0(\,\cdot\,) & \text{if $x_{-t+1} = 0$,} \\
      {\mathrm{P}}( {\|{x_{-t+1}}\|} M_{-1} \in \,\cdot\, \mid M_0 = x_{-t+1}/{\|{x_{-t+1}}\|} ) & \text{if $x_{-t+1} \neq 0$.}
    \end{cases}
\end{multline*}
\end{enumerate}
\end{defn}
 
Clearly, $\{ M_t : t \in {\mathbb{Z}} \}$ is a ${\mbox{\sc bftc}}(\alpha)$ if and only if $\{ M_{-t} : t \in {\mathbb{Z}} \}$ is a ${\mbox{\sc bftc}}(\alpha)$. The distribution of a BFTC($\alpha$) is completely determined by an admissible law of $(M_0,M_1)$ (and $\alpha>0$). 

The fact that the distributions $P=\mathcal{L}(M_0, M_1)$ and $P^\ast=\mathcal{L}(M_0, M_{-1})$ are adjoint in $\mathcal{M}_\alpha$ implies that for every measurable function $f : {\mathbb{R}}^d \times \mathbb{S}^{d-1} \to {\mathbb{R}}$ such that $f(0, s) = 0$ for all $s \in \mathbb{S}^{d-1}$, we have
 \begin{align}
   {\mathrm{E}} [ f ( M_{-1}, M_0 ) ] \nonumber
  &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \times \{ 0 \}) } f( m, s ) \, P^\ast({\mathrm{d}s}, {\mathrm{d}m}) \nonumber \\
   &= \int_{ \mathbb{S}^{d-1} \times ({\mathbb{R}}^d \times \{ 0 \}) } f( s / {\|{m}\|}, m / {\|{m}\|} ) \, {\|{m}\|}^\alpha \, P({\mathrm{d}s}, {\mathrm{d}m}) \nonumber \\
   &= {\mathrm{E}} \left[ f \left( \frac{M_0}{{\|{M_1}\|}}, \frac{M_1}{{\|{M_1}\|}} \right) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } \right],
 \label{eq:bftc:adjoint}
 \end{align}
in the sense that if one expectation exists, then so does the other, the two expectations being equal. This corresponds to equation \eqref{timechange:onestep} which originally motivated the definition of an adjoint distribution. The above formula is the special case $s = 1$ and $t = 0$ of the following result.

\begin{prop}
\label{P:BFTC}
Let $\{M_t:t \in {\mathbb{Z}}\}$ be a ${\mbox{\sc bftc}}(\alpha)$. For all integer $s, t \ge 0$ and for all measurable functions $f : ({\mathbb{R}}^d)^{s+1+t} \to {\mathbb{R}}$ vanishing on $\{ 0 \} \times ({\mathbb{R}}^d)^{s+t}$, the $s+1$ numbers
\begin{equation}
\label{E:BFTC}
  {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i}}{{\|{M_i}\|}}, \ldots, \frac{M_{t+i}}{{\|{M_i}\|}} \biggr) \, {\|{M_i}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_i \ne 0 \} } \biggr], \qquad
  i = 0, \ldots, s,
\end{equation}
are all the same, in the sense that if one integral exists, then they all exist and they are equal.
\end{prop} 
\begin{proof}
For $s = 0$ there is nothing to prove, so assume that $s \ge 1$. By definition of the integral, it is sufficient to consider the case where $f$ is nonnegative, in which case the expectations in \eqref{E:BFTC} are always well-defined, possibly equal to infinity.

\emph{Reduction to the case $i \in \{ 0, 1 \}$.}
Suppose first that we can show that the numbers corresponding to $i = 0$ and $i = 1$ in \eqref{E:BFTC} are equal, that is (note that $\|M_0\|=1$),
\begin{equation}
\label{E:BFTC:i=1}
  {\mathrm{E}} [ f ( M_{-s}, \ldots, M_t ) ] 
  = {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+1}}{{\|{M_1}\|}}, \ldots, \frac{M_{t+1}}{{\|{M_1}\|}} \biggr) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } \biggr].
\end{equation}
Take arbitrary $i = 0, \ldots, s-1$. Note that
\[
  {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i}}{{\|{M_i}\|}}, \ldots, \frac{M_{t+i}}{{\|{M_i}\|}} \biggr) \, {\|{M_i}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_i \ne 0 \} } \biggr]
  = {\mathrm{E}} [ g( M_{-s+i}, \ldots, M_{t+i} )]
\]
for a measurable function $g : ({\mathbb{R}}^d)^{s+1+t} \to {\mathbb{R}}$ with
that vanishes as soon as its first $d$-tuple of arguments is zero. By \eqref{E:BFTC:i=1} applied to $\tilde{s}=s-i$ and $\tilde{t}=t+i$, we find
\[
  {\mathrm{E}} [ g( M_{-s+i}, \ldots, M_{t+i} )]
  = {\mathrm{E}} \biggl[ g \biggl( \frac{M_{-s+i+1}}{{\|{M_1}\|}}, \ldots, \frac{M_{t+i+1}}{{\|{M_1}\|}} \biggr) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } \biggr].
\]
By definition of $g$, if $M_1 \ne 0$, then
\begin{align*}
  \lefteqn{
  g \biggl( \frac{M_{-s+i+1}}{{\|{M_1}\|}}, \ldots, \frac{M_{t+i+1}}{{\|{M_1}\|}} \biggr)
  } \\
  &= f \biggl( \frac{M_{-s+i+1} / {\|{M_1}\|}}{{\|{(M_{i+1} / {\|{M_1}\|})}\|}}, \ldots, \frac{M_{t+i+1} / {\|{M_1}\|}}{{\|{(M_{i+1} / {\|{M_1}\|})}\|}}  \biggr) \, {\|{(M_{i+1} / {\|{M_1}\|})}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_{i+1} \ne 0 \} } \\
  &= f \biggl( \frac{M_{-s+i+1}}{{\|{M_{i+1}}\|}}, \ldots, \frac{M_{t+i+1}}{{\|{M_{i+1}}\|}} \biggr) \, \frac{{\|{M_{i+1}}\|}^\alpha}{{\|{M_1}\|}^\alpha} \, {\boldsymbol{1}}_{ \{ M_{i+1} \ne 0 \} }.
\end{align*}
Combine the previous three displays to see that
\begin{multline*}
  {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i}}{{\|{M_i}\|}}, \ldots, \frac{M_{t+i}}{{\|{M_i}\|}} \biggr) \, {\|{M_i}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_i \ne 0 \} } \biggr] \\
  = {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i+1}}{{\|{M_{i+1}}\|}}, \ldots, \frac{M_{t+i+1}}{{\|{M_{i+1}}\|}} \biggr) \, {\|{M_{i+1}}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0, M_{i+1} \ne 0 \} } \biggr].
\end{multline*}
By definition of the forward chain $(M_t)_{t \ge 0}$, we have $M_{i+1} = 0$ as soon as $M_1 = 0$. As a consequence, we may the suppress the event $\{ M_1 \ne 0 \}$ in the indicator function on the right-hand side, and thus
\begin{multline*}
  {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i}}{{\|{M_i}\|}}, \ldots, \frac{M_{t+i}}{{\|{M_i}\|}} \biggr) \, {\|{M_i}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_i \ne 0 \} } \biggr] \\
  = {\mathrm{E}} \biggl[ f \biggl( \frac{M_{-s+i+1}}{{\|{M_{i+1}}\|}}, \ldots, \frac{M_{t+i+1}}{{\|{M_{i+1}}\|}} \biggr) \, {\|{M_{i+1}}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_{i+1} \ne 0 \} } \biggr].
\end{multline*}
We conclude that in order to show \eqref{E:BFTC}, it is enough to show \eqref{E:BFTC:i=1}. We will show \eqref{E:BFTC:i=1} by induction on $s \ge 1$.

\emph{Proof of \eqref{E:BFTC:i=1} if $s = 1$.}
We have to show that
\begin{equation}
\label{E:BFTC:i=1,s=1}
  {\mathrm{E}} [ f ( M_{-1}, \ldots, M_t ) ] 
  = {\mathrm{E}} \biggl[ f \biggl( \frac{M_0}{{\|{M_1}\|}}, \ldots, \frac{M_{t+1}}{{\|{M_1}\|}} \biggr) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } \biggr].
\end{equation}
We will proceed by induction on $t \ge 0$. 

The case $t = 0$ is nothing more than the adjoint relation between the laws of $(M_0, M_1)$ and $(M_0, M_{-1})$, see \eqref{eq:bftc:adjoint}. 

Let $t \ge 1$ and let \eqref{E:BFTC:i=1,s=1} be fulfilled for $t-1$.
By the Markov property,
\[
  {\mathrm{E}} [ f ( M_{-1}, \ldots, M_t ) ]
  = {\mathrm{E}} [ g ( M_{-1}, \ldots, M_{t-1} ) ]
\]
with
\[
  g( m_{-1}, \ldots, m_{t-1} ) = {\mathrm{E}} \{ f ( m_{-1}, \ldots, m_{t-1}, M_t ) \mid M_{t-1} = m_{t-1} \}
\]
As $g(0, m_0, \ldots, m_{t-1}) = 0$, we can apply the induction hypothesis, yielding
\[
  {\mathrm{E}} [ g ( M_{-1}, \ldots, M_{t-1} ) ]
  = {\mathrm{E}} [ g ( M_0 / {\|{M_1}\|}, \ldots, M_t / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ].
\]
The defining property of a ${\mbox{\sc bftc}}$ implies that for every $c > 0$, for every integer $r \ge 1$ and for every nonnegative, measurable function $h$ on ${\mathbb{R}}^d$,
\begin{equation}
\label{eq:bftc:scaling:forward}
 {\mathrm{E}} [ h( c M_r ) \mid M_{r-1} = m/c ]
  = \begin{cases} h(0) & \text{if $m = 0$,} \\ {\mathrm{E}} [ h( {\|{m}\|} M_1 ) \mid M_0 = m / {\|{m}\|} ] & \text{if $m \ne 0$,} \end{cases}
\end{equation}
the right-hand side not depending on the scaling constant $c$ nor on the time index $r$. It follows that if $m_1 \ne 0$,
\begin{align*}
  \lefteqn{
  g ( m_0 / {\|{m_1}\|}, \ldots, m_t / {\|{m_1}\|} )
  } \\
  &= {\mathrm{E}} [ f ( m_0 / {\|{m_1}\|}, \ldots, m_t / {\|{m_1}\|}, M_t ) \mid M_{t-1} = m_t / {\|{m_1}\|} ] \\
  &= {\mathrm{E}} [ f ( m_0 / {\|{m_1}\|}, \ldots, m_t / {\|{m_1}\|}, M_{t+1} / {\|{m_1}\|} ) \mid M_t = m_t ].
\end{align*}
We find that, on the event $\{M_1 \ne 0\}$, by the Markov property,
\begin{multline*}
  g ( M_0 / {\|{M_1}\|}, \ldots, M_t / {\|{M_1}\|} ) \\
  = {\mathrm{E}} [ f ( M_0 / {\|{M_1}\|}, \ldots, M_t / {\|{M_1}\|}, M_{t+1} / {\|{M_1}\|} ) \mid M_0, \ldots, M_t ].
\end{multline*}
We can conclude that
\begin{align*}
  \lefteqn{
  {\mathrm{E}} [ f ( M_{-1}, \ldots, M_t ) ]
  } \\
  &= {\mathrm{E}} [ g ( M_{-1}, \ldots, M_{t-1} ) ] \\
  &= {\mathrm{E}} [ g ( M_0 / {\|{M_1}\|}, \ldots, M_t / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ] \\
  &= {\mathrm{E}} [ f ( M_0 / {\|{M_1}\|}, \ldots, M_t / {\|{M_1}\|}, M_{t+1} / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ],
\end{align*}
as required.

\emph{Proof of \eqref{E:BFTC:i=1} for general $s \ge 1$.} 
The case $s = 1$ was treated above. So let $s \ge 2$. By the Markov property, we have
\[
  {\mathrm{E}} [ f ( M_{-s}, \ldots, M_t ) ] = {\mathrm{E}} [ g ( M_{-s+1}, \ldots, M_t ) ]
\]
with $g : ({\mathbb{R}}^d)^{s+t} \to {\mathbb{R}}$ a nonnegative, measurable function defined by
\[
  g ( m_{-s+1}, \ldots, m_t )
  = {\mathrm{E}} \{ f ( M_{-s}, m_{-s+1}, \ldots, m_t ) \mid M_{-s+1} = m_{-s+1} \}.
\]
Conditionally on $M_{-s+1} = 0$, we have $M_{-s} = 0$, and thus $f ( M_{-s}, \ldots ) = 0$ too. It follows that $g ( 0, m_{-s+2}, \ldots, m_t ) = 0$. By the induction hypothesis, we therefore have
\begin{equation*}
  {\mathrm{E}} [ g ( M_{-s+1}, \ldots, M_t ) ]
  = {\mathrm{E}} [ g ( M_{-s+2} / {\|{M_1}\|}, \ldots, M_{t+1} / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ].
\end{equation*}
As for the forward chain in \eqref{eq:bftc:scaling:forward}, we have for every nonnegative, measurable function $h$ on ${\mathbb{R}}^d$ and every $c>0$,
\begin{equation}
\label{eq:bftc:scaling:backward}
 {\mathrm{E}} [ h( c M_{-r} ) \mid M_{-r+1} = m/c ]
  = \begin{cases} h(0) & \text{if $m = 0$,} \\ {\mathrm{E}} [ h( {\|{m}\|} M_{-1} ) \mid M_0 = m / {\|{m}\|} ] & \text{if $m \ne 0$,} \end{cases}
\end{equation}
the right-hand side not depending on the scaling constant $c > 0$ nor on the time index $r = 1, 2, \ldots$. It follows that for $m_1 \ne 0$, we have
\begin{align*}
  \lefteqn{
  g ( m_{-s+2} / {\|{m_1}\|}, \ldots, m_{t+1} / {\|{m_1}\|} )
  } \\
  &= {\mathrm{E}} [ f ( M_{-s}, m_{-s+2} / {\|{m_1}\|}, \ldots, m_{t+1} / {\|{m_1}\|}) \mid M_{-s+1} = m_{-s+2} / {\|{m_1}\|} ] \\
  &= {\mathrm{E}} [ f ( M_{-s+1} / {\|{m_1}\|}, m_{-s+2} / {\|{m_1}\|}, \ldots, m_{t+1} / {\|{m_1}\|}) \mid M_{-s+2} = m_{-s+2} ].
\end{align*}
Invoking the Markov property again, we conclude that
\begin{align*}
    {\mathrm{E}} [ f ( M_{-s}, \ldots, M_t ) ]
  &= {\mathrm{E}} [ g ( M_{-s+1}, \ldots, M_t ) ] \\
  &= {\mathrm{E}} [ g ( M_{-s+2} / {\|{M_1}\|}, \ldots, M_{t+1} / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ] \\
  &= {\mathrm{E}} [ f ( M_{-s+1} / {\|{M_1}\|}, \ldots, M_{t+1} / {\|{M_1}\|} ) \, {\|{M_1}\|}^\alpha \, {\boldsymbol{1}}_{ \{ M_1 \ne 0 \} } ],
\end{align*}
as required. This concludes the proof of Proposition \ref{P:BFTC}.
\end{proof}

The following proposition connects BFTCs and spectral processes. 

\begin{prop}
\label{P:BFTCisspectral}
Let $\{Y_t:t \in \mathbb{Z}\}$ be an $\mathbb{R}^d$-valued process and let $\{M_t:t \in \mathbb{Z}\}$ be an $\mathbb{R}^d$-valued BFTC($\alpha)$. If 
\begin{equation}\label{E:forwardthesame}\mathcal{L}(Y_0, \dots, Y_t)=\mathcal{L}(M_0, \dots, M_t)
\end{equation}
for all $t \geq 0$ and if
\begin{equation}
\label{E:timechange:BFTCspectral}
 {\mathrm{E}}\left[f(Y_{-s}, \ldots, Y_{t}) \right]= {\mathrm{E}}\left[f\left(\frac{Y_0}{\|Y_s\|}, \ldots, \frac{Y_{s+t}}{\|Y_s\|} \right) \|Y_s\|^\alpha{\boldsymbol{1}}_{\{Y_s \neq 0\}} \right]
\end{equation}
for all $s,t \geq 0$ and for all bounded and measurable $f:(\mathbb{R}^d)^{s+t+1} \to \mathbb{R}$ satisfying $f(y_{-s}, \ldots, y_t)=0$ whenever $y_{-s}=0$, then
\begin{equation}\label{E:allthesame} \mathcal{L}(Y_{-s}, \dots, Y_t)=\mathcal{L}(M_{-s}, \dots, M_t)
\end{equation}
for all $s,t \geq 0$. 
\end{prop}
\begin{proof}
The proof relies on the fact that both the process $\{Y_t: t \in \mathbb{Z}\}$ which satisfies \eqref{E:timechange:BFTCspectral} and the BFTC($\alpha$) are uniquely determined by their forward process. Our proof is by induction on $s$. For $s=0$, equation \eqref{E:allthesame} is equal to the assumption \eqref{E:forwardthesame} for all $t \geq 0$. For the induction step, assume that \eqref{E:allthesame} holds for a fixed value of $\tilde{s}=s-1\geq 0$ and all $t \geq 0$. Let $f:(\mathbb{R}^d)^{s+t+1} \to \mathbb{R}$ be a bounded continuous function. Write 
$$f(y_{-s}, \dots, y_t)=f_1(y_{-s}, \dots, y_t)+f_2(y_{-s}, \dots,y_t)$$
with
$$f_1(y_{-s}, \dots, y_t)=f(0,y_{-s+1}, \dots, y_t),$$
$$f_2(y_{-s}, \dots, y_t)=f(y_{-s}, y_{-s+1}, \dots, y_t)-f(0, y_{-s+1}, \dots, y_t).$$
and note that $f_2(0, y_{-s+1}, \dots, y_t)=0$, while the value of $f_1$ does not depend on the first coordinate of the argument.
Then
\begin{eqnarray*}
&& {\mathrm{E}}[f(Y_{-s}, \dots, Y_t)]\\
&=& {\mathrm{E}}[f_1(Y_{-s}, \dots, Y_t)]+{\mathrm{E}}[f_2(Y_{-s}, \dots, Y_t)] \\
&=& {\mathrm{E}}[f_1(Y_{-s}, \dots, Y_{t})]+ {\mathrm{E}}\left[f_2\left(\frac{Y_0}{\|Y_s\|}, \ldots, \frac{Y_{s+t}}{\|Y_s\|}\right) \|Y_s\|^\alpha{\boldsymbol{1}}_{\{Y_s \neq 0\}} \right] \\
&=& {\mathrm{E}}[f_1(M_{-s}, \dots, M_{t})]+ {\mathrm{E}}\left[f_2\left(\frac{M_0}{\|M_s\|}, \ldots, \frac{M_{s+t}}{\|M_s\|}\right) \|M_s\|^\alpha{\boldsymbol{1}}_{\{M_s \neq 0\}} \right] ,
\end{eqnarray*}
where both the induction hypothesis and equations \eqref{E:timechange:BFTCspectral} and \eqref{E:allthesame} have been used. Since $\{M_t: t \in \mathbb{Z}\}$ is a BFTC($\alpha$), we may apply Proposition \ref{P:BFTC} for $i=s$ and $i=0$ (note that $\|M_0\|=1$), so that the above expression is equal to
\begin{equation*} {\mathrm{E}}[f_1(M_{-s}, \dots, M_{t})]+ {\mathrm{E}}\left[f_2\left(M_{-s}, \ldots, M_{t}\right)\right] ={\mathrm{E}}[f(M_{-s}, \dots, M_{t})], 
\end{equation*}
which finishes the induction step and the proof.
\end{proof}
\begin{rem}
Proposition \ref{P:BFTCisspectral} can be read in the following way: Every spectral process $\{M_t:t \in \mathbb{Z}\}$ with a forward process (meaning: $\{M_t:t \in \mathbb{N}_0\}$) which has a BFTC$(\alpha)$ structure, automatically has a BFTC($\alpha$)-backward-distribution as well. This means that a Markovian structure in the forward spectral process (which may also arise in settings where the underlying process is non-Markovian) is enough to secure a Markovian structure of the backward spectral process as well.    
\end{rem}
\begin{cor}
\label{Cor:spectralisBFTC}
Let $\{ X_t : t \in {\mathbb{Z}}\}$ be a stationary Markov chain with distribution determined by \eqref{E:MC:1}, \eqref{E:MC:2} and \eqref{E:MC:3}. Then the corresponding spectral process $\{M_t:t \in \mathbb{Z}\}$ is a BFTC$(\alpha)$. 

We call $\{M_{-t}: t \in \mathbb{N}_0\}$ the \emph{backward tail chain of} $\{X_t:t \in \mathbb{Z}\}$ and $\{M_t: t \in \mathbb{Z}\}$ the \emph{tail chain of} $\{X_t:t \in \mathbb{Z}\}$.
\end{cor}
\begin{proof}
The existence of a corresponding spectral process follows from Proposition \ref{P:BS09:2.1}. Furthermore, it follows from Theorem \ref{T:forward} that the forward process $\{M_t:t \in \mathbb{N}_0\}$ is equal in law to the forward process of a BFTC$(\alpha)$. By Proposition \ref{P:BFTCisspectral} the statement follows. 
\end{proof}

\begin{rem}
Since the forward and backward tail chain of a process $\{X_t:t\in \mathbb{Z}\}$ are uniquely determined by the laws of $(M_0, M_1)$ and $(M_0, M_{-1})$, respectively, it follows that the backward tail chain is equal in distribution to the forward tail chain if and only if the law of $(M_0, M_1)$ is self-adjoint (cf.\ Remark \ref{selfadjoint}). This is for example the case if the process $\{X_t: t \in \mathbb{Z}\}$ fulfills the assumptions of Corollary \ref{Cor:spectralisBFTC} and is in addition a time reversible Markov chain. 

More generally, since the existence of a forward tail process ensures joint regular variation of $(X_0, X_1)$ (cf.\ Corollary 3.2 in \cite{BS09}), the resulting limiting spectral measure of the $2d$-dimensional vector $(X_{0,1}, \ldots, X_{0,d}, X_{1,1}, \ldots, X_{1,d})$ and the law of $(M_0,M_1)$ uniquely determine each other. Therefore, the backward tail chain is equal in distribution to the forward tail chain if and only if the spectral measure of $(X_{0,1}, \ldots, X_{0,d}, X_{1,1}, \ldots, X_{1,d})$ is equal to the spectral measure of $(X_{1,1}, \ldots, X_{1,d}, X_{0,1}, \ldots, X_{0,d})$. For $d=1$ this simply means that the spectral measure of $(X_0,X_1)$ is symmetric.    
\end{rem}

In the univariate case, BFTCs have an additional structure which generalizes a multiplicative  random walk in that the distribution of the increment depends on the sign of the process in its current state \citep{S07}. The random walk structure of the forward tail chain was first observed in \cite{Smith92} for one-sided extremes and extended to allow for both positive and negative extremes in \cite{BC03}.

\iffalse
\subsection*{One-dimensional BFTCs}
As was already seen during the examples of the last section, the one-dimensional case bears some simplifications in the representation of adjoint distributions. This holds true for the whole BFTC as well. In the following, we will show that in one dimension, the BFTC$(\alpha)$ $\{M_t:t \in \mathbb{Z}\}$ has the remarkable property that both the forward process $\{M_t:t \in \mathbb{N}_0\}$ and the backward process $\{M_{-t}:t \in \mathbb{N}_0\}$ have a simple random walk structure. 

Let $P$ on $\{-1,1\}\times \mathbb{R}$ with $P \in {\mathcal{M}}_{\alpha}$ and adjoint $P^* \in {\mathcal{M}}_{\alpha}$, and let $\{M_t:t \in \mathbb{Z}\}$ be a ${\mbox{\sc bftc}}(\alpha)$ with $\mathcal{L}(M_0,M_1)=P$. It then follows from Definition \ref{D:BFTC}(ii)-(iii) that
\begin{eqnarray}
\label{E:BFTC:AB:1}
	M_t &=&
	\left\lbrace \begin{array}{l@{\quad}l}
		|M_{t-1}| A_t & \mbox{if $M_{t-1} > 0$,} \\
		0 & \mbox{if $M_{t-1} = 0$,} \\
		|M_{t-1}| B_t & \mbox{if $M_{t-1} < 0$;}
	\end{array} \right. \\*
	M_{-t} &=&
\label{E:BFTC:AB:2}	\left\lbrace \begin{array}{l@{\quad}l}
		|M_{-t+1}| A_{-t} & \mbox{if $M_{-t+1} > 0$,} \\
		0 & \mbox{if $M_{-t+1} = 0$,} \\
		|M_{-t+1}| B_{-t} & \mbox{if $M_{-t+1} < 0$;}
	\end{array} \right. \end{eqnarray}
where
\begin{equation}
\label{E:BFTC:AB:3}
\mbox{\begin{minipage}[h]{0.9\textwidth}
\begin{itemize}
\item[(i)] $M_0, A_1, A_{-1}, A_2, A_{-2}, \ldots, B_1, B_{-1}, B_2, B_{-2}, \ldots$ are independent;
\item[(ii)] ${\mathrm{P}}(M_0 = 1) = P(\{1\}\times\mathbb{R}) = 1 - {\mathrm{P}}(M_0 = -1)$;
\item[(iii)] ${\mathcal{L}}(A_t) = {\mathcal{L}}(A_1)$=${\mathcal{L}}(M_1|M_0=1)$, ${\mathcal{L}}(A_{-t}) = {\mathcal{L}}(A_{-1})={\mathcal{L}}(M_{-1}|M_0=1)$, ${\mathcal{L}}(B_t) = {\mathcal{L}}(B_1)={\mathcal{L}}(M_1|M_0=-1)$ and ${\mathcal{L}}(B_{-t}) = {\mathcal{L}}(M_{-1}|M_0=-1)$ for $t \geq 1$.
\end{itemize}
\end{minipage}}
\end{equation}

The laws of $A_{-1}$ and $B_{-1}$ in \eqref{E:BFTC:AB:2} can be expressed in terms of $\alpha$ and $P$ through \eqref{E:adjoint:univ:RV}: Let us therefore first assume that $p:={\mathrm{P}}(M_0=1) \in (0,1)$. For $\sigma \in \{-1,1\}$ and integrable $f$,
\begin{equation}
\label{E:BFTC:AB:4}
	\begin{array}{rcl}
	{\mathrm{E}} [ f(A_{-1}) ]
        &=& {\mathrm{E}}[f(M_{-1})|M_0=1] \\
        &=& \displaystyle \frac{1}{p} {\mathrm{E}} \biggl[ f \biggl( \frac{1}{|M_1|} \biggr) (M_1)_+^\alpha \biggr]
	+  f(0)\biggl( 1 - \frac{{\mathrm{E}}[(M_1)_+^\alpha]}{p} \biggr), \\[1ex]
	{\mathrm{E}} [ f(B_{-1}) ]
        &=& {\mathrm{E}}[f(M_{-1})|M_0=-1] \\
	&=& \displaystyle \frac{1}{1-p} {\mathrm{E}} \biggl[ f \biggl(-\frac{1}{|M_1|} \biggr) (M_1)_-^\alpha \biggr]
	+ f(0)\biggl( 1 - \frac{{\mathrm{E}}[(M_1)_-^\alpha]}{1-p} \biggr).
	\end{array}
\end{equation}

On the other hand, if $p = 1$ and $P = \delta_1 \otimes \tilde{P} \in {\mathcal{M}}_{\alpha}$ with adjoint $P^* = \delta_1 \otimes \tilde{P}^*$, a process $\{ M_t: t \in \mathbb{Z} \}$ is a ${\mbox{\sc bftc}}(\alpha)$ if and only if it admits the following distributional representation:
\begin{equation}
\label{E:BFTC:A:1}
	\begin{array}[t]{rcl}
	M_0 &=& 1, \\[1ex]
	M_{\pm t} &=& \prod_{i=1}^t A_{\pm i}, \qquad \mbox{for $t \geq 1$},
	\end{array}
\end{equation}
where
\begin{equation}
\label{E:BFTC:A:2}
\mbox{\begin{minipage}[t]{0.90\textwidth}
\begin{itemize}
\item[(i)] $A_1, A_{-1}, A_2, A_{-2}, \ldots$ are independent;
\item[(ii)] ${\mathcal{L}}(A_t) = \tilde{P}$ and ${\mathcal{L}}(A_{-t}) = \tilde{P}^\ast$ for $t \geq 1$.
\end{itemize}
\end{minipage}}
\end{equation}
The case $p = 0$ is similar to the case $p = 1$.
\fi

\section{Examples for BFTCs}
\label{S:examples}

We conclude the paper with some examples of BFTCs for multivariate Markov processes. For univariate examples, see \citet[][section~7]{S07}.

\iffalse
Again, we start with a univariate example.
\begin{ex}
\label{Ex:ARCH}
Let $0 < \beta < \infty$ and $0 < \lambda < 2 \mathrm{e}^\gamma$ where $\gamma$ is Euler's constant, and consider the ARCH(1) process
\begin{equation}
\label{E:ARCH}
	X_t = (\beta + \lambda X_{t-1}^2)^{1/2} Z_t, \qquad t \in \mathbb{Z},
\end{equation}
where the $Z_t, t \in \mathbb{Z},$ are i.i.d.\ standard normal and $Z_t$ is independent of $X_{t-1}, X_{t-2}, \ldots$ for all $t \in \mathbb{Z}$. By an application of \citet[][Theorem~5]{Kesten73} or \citet[][Theorem~4.1]{Goldie91} to the squared series $X_t^2$, a stationary solution to \eqref{E:ARCH} exists, and this stationary distribution satisfies Condition~\ref{E:RV} with $\Upsilon\{-1\}= \Upsilon\{1\}= 1/2$ and $\alpha$ equal to the unique positive solution to the equation ${\mathrm{E}} [ \lambda^{\alpha / 2} |Z_1|^\alpha] = 1$; see also \citet[][section~8.4.2]{EKM97}. Condition~\ref{C:phi} is easily verified with $\phi(\pm 1,z) = \lambda^{1/2} z$. The BFTC is given by equations \eqref{E:BFTC:AB:1}-\eqref{E:BFTC:AB:3} and can be represented by
\begin{eqnarray*}
	M_t &=& \lambda^{t/2} |Z_1 \cdot \ldots \cdot Z_{t-1}| Z_t, \\
	M_{-t} &=& \lambda^{-t/2} |Z_1^* \cdot \ldots \cdot Z_{t-1}^\ast| Z_t^*,
\end{eqnarray*}
for $t \geq 1$, where $Z_1, Z_1^*, Z_2, Z_2^*, \ldots$ are independent random variables with $Z_t$ standard normal and with the common law of the variables $Z_t^*$ related to the standard normal distribution via
\[
	{\mathrm{E}} [ f(Z_1^*) ] = \lambda^{\alpha/2} {\mathrm{E}} [ f(1 / Z_1) |Z_1|^\alpha ],
\]
for $Z_1^*$-integrable functions $f$. 
\end{ex}
\fi

\begin{ex}
\label{Ex:KestenRDE}
Let $(A_t, B_t), t \in \mathbb{Z},$ be i.i.d.\ with $A_t \in \mathbb{R}^{d \times d}$ and $B_t \in \mathbb{R}^d$. The stationary distribution and asymptotic behavior of the corresponding random difference equation 
\begin{equation}\label{RDE} X_{t}=A_t X_{t-1} + B_t, \;\;\; t \in \mathbb{Z},\end{equation}
have been studied initially in the seminal work by \cite{Kesten73}. Let us assume that the distribution of $(A_t, B_t)$ satisfies the technical, but mild assumptions of Theorems~A and~B or Theorem 6 in \cite{Kesten73} (where the first two theorems deal with the nonnegative case, i.e.\ all components of $A_t, t \in \mathbb{Z},$ are nonnegative almost surely, and the last one treats the general case). Together with results in \cite{BL09} this implies that the stationary distribution of $X_t$ for \eqref{RDE} is multivariate regularly varying in the nonnegative case. In the general case, multivariate regular variation follows if $\kappa_1>0$ in \cite{Kesten73}, Equation~(4.8), is not an integer, cf.\ \cite{BDM02a}. Let $\Upsilon$ denote the spectral measure and $\alpha>0$ the index of regular variation of the stationary distribution of $X_t$. It can be shown that
\begin{equation}\label{RDEequation} {\mathrm{E}}\left[f\left(\frac{AC}{\| AC\|} \right)\|AC\|^\alpha \right]={\mathrm{E}}[f(C)] \end{equation}
for all bounded, continuous funtions $f$ on $\mathbb{S}^{d-1}$, where $C \in \mathbb{S}^{d-1}$ has distribution $\Upsilon$ and $A \in \mathbb{R}^{d \times d}$ is independent of $C$ with $\mathcal{L}(A)=\mathcal{L}(A_1)$, cf.\ \cite{BS09}. 

Due to the linear structure of \eqref{RDE}, Theorem~\ref{T:forward} applies with $P(Y>y)=y^{-\alpha}, y>1, \mathcal{L}(M_0)=\Upsilon$ and $\phi(M_{j-1}, \epsilon_j)=\epsilon_j M_{j-1}$ where the $\epsilon_j \in \mathbb{R}^{d \times d}, j=1,2,\ldots,$ are i.i.d.\ with $\mathcal{L}(\epsilon_j)=\mathcal{L}(A_1)$. In order to find the distribution of the backward tail chain note that Remark~\ref{partsimple} applies to this example by equation~\eqref{RDEequation}. So the law $P^\ast$ of $(M_{0},M_{-1})$ is given by
\begin{equation}\label{simpleRDEadjoint}P^\ast(E)={\mathrm{E}}\left[ {\boldsymbol{1}}_{\mathrm{E}}\left(\frac{AC}{\|AC\|},\frac{C}{\|AC\|} \right)\|AC\|^\alpha \right]\end{equation}
for all Borel sets $E \subset \mathbb{S}^{d-1}\times \mathbb{R}^d$.

Additional assumptions about $\mathcal{L}(A)$ allow us to simplify this characterization: Let us assume that $A$ has a multiplicative form like in Example~\ref{Ex:mult:1}, i.e.\ $A=RQ$ for a positive random variable $R$ with ${\mathrm{E}}[R^\alpha]=1$ and $Q$ is an orthogonal matrix independent of $R$. We may additionally assume that $R$ has a density on $\mathbb{R}_+$ and that the support of the law of $Q$ is equal to the orthogonal group in dimension $d$. In this case, the spectral measure $\Upsilon$ is the uniform distribution on $\mathbb{S}^{d-1}$ (cf.\ \cite{Bura09}, p.\ 390), $\alpha>0$ is the index of regular variation and
 $${\mathrm{E}}\left[f\left(\frac{AC}{\| AC\|} \right)\|AC\|^\alpha \right]={\mathrm{E}}\left[f\left(QC \right)R^\alpha \right]={\mathrm{E}}\left[f\left(QC \right)\right]{\mathrm{E}}[R^\alpha]={\mathrm{E}}[f(C)]$$ 
holds for all bounded, continuous functions $f$ on $\mathbb{S}^{d-1}$ with $C \sim \mbox{Unif}(\mathbb{S}^{d-1})$. Since $\mathcal{L}(C)=\mathcal{L}(QC)$, all assumptions of Example~\ref{Ex:mult:1} are met and the adjoint measure $P^\ast$ is determined by \eqref{E:Ex:mult:1} and equal to the law of $(C^\ast ,R^\ast Q^\ast C^\ast)$ with $R^\ast, Q^\ast, C^\ast$ independent, $\mathcal{L}(C^\ast)=\mbox{Unif}(\mathbb{S}^{d-1})$, $\mathcal{L}(Q^\ast)=\mathcal{L}(Q')$ and $R^\ast$ has density $f_{R^\ast}(y)=f_R(y^{-1})y^{-(2+\alpha)},$ $y>0$, where $f_R$ denotes the density of $R$. Thus, both the forward and the backward tail chain have a simple multiplicative structure:
\[ 
  M_t=M_0A_1 \cdot \ldots \cdot A_t, \qquad 
  M_{-t}=M_0A_{-1}\cdot \ldots \cdot A_{-t}, 
  \qquad t\geq 1, 
\]
with $A_1, A_2, \ldots$ as above and $A_{-1}, A_{-2}, \ldots$ i.i.d. with the same distribution as $R^\ast Q^\ast $, all independent of each other and of $M_0\sim\mbox{Unif}(\mathbb{S}^{d-1})$.
\end{ex}

\begin{ex}
\label{heavy-tailedRDE}
While the preceding example dealt with random difference equations where the random increment $B_t$ has a relatively light tail [\cite{Kesten73} assumes that $E(\|B_1\|^\alpha)<\infty$], the following example deals with AR(1) processes where the innovations themselve are regularly varying. Let 
\begin{equation}
\label{E:StochDiff:2}
	X_t = A X_{t-1} + B_t, \qquad t \in \mathbb{Z},
\end{equation}
where $A$ is a deterministic $\mathbb{R}^{d \times d}$-matrix and $B_t \in \mathbb{R}^d$, $t \in \mathbb{Z}$, are i.i.d.\ and multivariate regularly varying with index $\alpha>0$ and spectral measure $\lambda$ on $\mathbb{S}^{d-1}$. For extensions to random but light-tailed random matrices $A_t$, see for instance \citet{HuSa08}.

If $\sup_{x \in \mathbb{S}^{d-1}}\|A^m x\|<1$ for some positive integer $m$, then \eqref{E:StochDiff:2} has the stationary solution
\[ 
  X_t=\sum_{n=0}^\infty A^n B_{t-n}, \qquad t \in \mathbb{Z}.
\]
It has been shown in \cite{MeSe10} that in this case the stationary distribution of $X_t$ is multivariate regularly varying as well, with the same index $\alpha$ and spectral measure $\Upsilon=\sum_{n=0}^\infty p_n \lambda_n$, where
$$ p_n:= \frac{c_n}{\sum_{k=0}^\infty c_k} \;\;\; \text{with} \;\; c_n:=\int_{\mathbb{S}^{d-1}}\|A^n \theta\|\, \lambda (\mathrm{d} \theta), \;\;\; n \in \mathbb{N}_0,$$
and where $\lambda_n$ is the spectral measure of $A^n B$, provided $c_n > 0$, i.e.
\[
  \lambda_n(f)
  :=
  \frac{1}{c_n}
  \int_{\mathbb{S}^{d-1}}
    f \left( \frac{A^n s}{\|A^n s\|} \right) \, \|A^n s\|^\alpha \, 
  \lambda (\mathrm{d} s),
  \;\;\; n \in \mathbb{N}_0, \;\; \mbox{if }c_n>0,
\]
for all bounded, continuous functions $f$ on $\mathbb{S}^{d-1}$ \citep[][Example~9.3]{MeSe10}. The spectral process $\{ M_t:t \in \mathbb{Z}\}$ in Proposition~\ref{P:BS09:2.1} is of the form
\begin{equation}
\label{E:AR(1)tailproc}
  M_{-N+t}=\begin{cases}
            A^t \Theta, \;\;\; & t=0, 1, 2, \ldots ,\\
            0, & t=-1,-2, \ldots
           \end{cases}
\end{equation}
for a random integer $N$ with ${\mathrm{P}}(N=n)=p_n$, $n \in \mathbb{N}_0$, and a random vector $\Theta$ with distribution
\[ 
  {\mathrm{P}}(\Theta \in E \mid N=n)
  = \frac{1}{c_n}
  \int_{\mathbb{S}^{d-1}}
    {\boldsymbol{1}}_{E} (s / \|A^n s\|) \, \|A^n s\|^\alpha \,
  \lambda (\mathrm{d} s)
\] 
for $n \in \mathbb{N}_0$ and Borel sets $E \in \mathbb{R}^d$. Here, the forward tail chain has a deterministic multiplicative structure with $M_0 \sim \Upsilon$ and $M_n=AM_{n-1}$ for $n\geq 1$. The backward process is Markovian as well, by Corollary~\ref{Cor:spectralisBFTC}. This is also clear if one looks at \eqref{E:AR(1)tailproc} and notices that $M_{-(n+h)}=0$ if $M_{-n}=0$ for all $h \geq 1, n \geq 1$. Furthermore, if $M_{-n} \neq 0$ then $(M_{-n+1}, \ldots, M_0)=(AM_{-n}, \ldots, A^n M_{-n})$ contains no more information about $M_{-(n+1)}$ than $M_{-n}$ does. 

The distribution of $(M_0, M_{-1})$ is adjoint to the one of $(M_0, M_1) = (M_0, AM_0)$. By \eqref{eq:adjoint:E} and since $M_0 \sim \Upsilon$, we find, for every Borel set $E \subset \mathbb{S}^{d-1} \times (\mathbb{R}^d \setminus \{ 0 \})$,
\begin{align*}
  {\mathrm{P}}\bigl( (M_0, M_{-1}) \in E \bigr)
  &= {\mathrm{E}} 
  \biggl[ 
    {\boldsymbol{1}}_E \biggl( \frac{M_1}{\|M_1\|}, \frac{M_0}{\|M_1\|} \biggr) \, \|M_1\|^\alpha 
  \biggr] \\
  &= \frac{1}{\sum_{k = 0}^\infty c_k}
  \sum_{n \ge 0} 
  \int_{\mathbb{S}^{d-1}}
    {\boldsymbol{1}}_E \biggl( \frac{A^{n+1}s}{\|A^{n+1}s\|}, \, \frac{A^n s}{\|A^{n+1} s\|} \biggr) \,
    \| A^{n+1} s \|^\alpha \,
  \lambda( \mathrm{d}s ).
\end{align*}
Choosing $E = S \times (\mathbb{R}^d \setminus \{ 0 \})$ for a Borel set $S \subset \mathbb{S}^{d-1}$ yields, upon taking complements with respect to $\{ M_0 \in S \}$ and noting that $\| s \| = 1$ for $s \in \mathbb{S}^{d-1}$,
\begin{equation}
\label{E:M-1equal0}
  {\mathrm{P}}( M_0 \in S, \, M_{-1} = 0 )
  = \frac{1}{\sum_{k = 0}^\infty c_k} \, \lambda(S).
\end{equation}
In particular, ${\mathrm{P}}(M_{-1} = 0) = p_0 = {\mathrm{P}}(N = 0)$. The backward tail chain now follows from Definition~\ref{D:BFTC}(iii) together with the distribution of $(M_0, M_{-1})$.

In the special case that $A$ is invertible, we find from \eqref{E:AR(1)tailproc} that $M_{-(t+1)}$ is equal to either $A^{-1} M_{-t}$ or to $0$ with conditional probabilities depending on $M_{-t}/\|M_{-t}\|$: if $M_{-t} = 0$, then $M_{-(t+1)} = 0$ too, while if $M_{-t} = x \ne 0$, then
\[
  M_{-(t+1)} =
  \begin{cases}
    A^{-1} x & \text{with probability $1 - {\mathrm{P}}(M_{-1} = 0 \mid M_0 = x / {\|{x}\|})$,} \\
    0 & \text{with probability ${\mathrm{P}}(M_{-1} = 0 \mid M_0 = x / {\|{x}\|})$.}
  \end{cases}
\]
To derive a concrete form of the backward Markov kernel, let us assume that $\lambda$ has a Lebesgue density $f_\lambda$ on $\mathbb{S}^{d-1}$. Then all measures $\lambda_n$ and thus $\Upsilon$ have Lebesgue densities as well and \eqref{E:M-1equal0} gives us
\[
  {\mathrm{P}}(M_{-1} = 0 \mid M_0 = s)
  = \frac{1}{\sum_{k = 0}^\infty c_k} \frac{f_\lambda(s)}{f_\Upsilon(s)}
\]
for all $s \in \mathbb{S}^{d-1}$ such that $f_\Upsilon(s) > 0$.

\iffalse
The explicit form of the Markov kernel for the backward tail chain is not easily found for the general case.  In order to derive an illustrative form in a special case note first that with $\Theta \sim \Upsilon, \Lambda \sim \lambda$
  \begin{eqnarray}
 \nonumber && {\mathrm{E}}\left[f\left(\frac{A \Theta}{\|A \Theta\|}\right)\|A \Theta\|^\alpha\right] \\
 \nonumber&=& \sum_{n=0}^\infty p_n \int_{\mathbb{S}^{d-1}} f\left(\frac{A s}{\|A s\|}\right)\|A s\|^\alpha\, \lambda_n (\mathrm{d} s) \\
 \nonumber&=& (\sum_{k=0}^\infty c_k)^{-1} \sum_{n=0}^\infty \int_{\mathbb{S}^{d-1}} f\left(\frac{A (A^n s/\|A^n s\|)}{\|A (A^n s/\|A^n s\|)\|}\right)\|A (A^n s/\|A^n s\|)\|^\alpha \|A s\|^\alpha\, \lambda (\mathrm{d} s) \\
 \nonumber&=& (\sum_{k=0}^\infty c_k)^{-1} \sum_{n=0}^\infty \int_{\mathbb{S}^{d-1}} f\left(\frac{A^{n+1} s}{\|A^{n+1} s \|}\right)\|A^{n+1} s\|^\alpha\, \lambda (\mathrm{d} s) \\
\nonumber&=& (\sum_{k=1}^\infty c_k)^{-1} \left[\sum_{n=0}^\infty \int_{\mathbb{S}^{d-1}} f\left(\frac{A^{n} s}{\|A^{n} s \|}\right)\|A^{n} s\|^\alpha\, \lambda (\mathrm{d} s)-\int_{\mathbb{S}^{d-1}}f(s) \;\lambda (\mathrm{d} s)\right] \\
 \label{E:losezero}&=&{\mathrm{E}}[f(\Theta)]-(\sum_{k=0}^\infty c_k)^{-1}{\mathrm{E}}[f(\Lambda)],
 \end{eqnarray}
 for all bounded continuous functions $f$ on $\mathbb{S}^{d-1}$. If we assume that $A$ is invertible this leads us to
 \begin{eqnarray*}
&&{\mathrm{P}}(M_{-1} \in T, M_0 \in S)\\
&=&{\mathrm{E}}\left[{\boldsymbol{1}}_T(A^{-1}(A\Theta /\|A \Theta \|)){\boldsymbol{1}}_S(A\Theta /\|A\Theta \|)\|A\Theta \|^\alpha \right] \\
&=& {\mathrm{E}}\left[{\boldsymbol{1}}_T(A^{-1}\Theta){\boldsymbol{1}}_S(\Theta)\right]-(\sum_{k=0}^\infty c_k)^{-1}{\mathrm{E}}\left[{\boldsymbol{1}}_T(A^{-1}\Lambda){\boldsymbol{1}}_S(\Lambda)\right],
\end{eqnarray*}
for Borel sets $T \subset \mathbb{R}^d \setminus \{0\}$  and $S \subset \mathbb{S}^{d-1}$. 
Setting $T=\mathbb{R}^d \setminus \{0\}$ this implies that
\begin{eqnarray} \nonumber {\mathrm{P}}(M_{-1}=0, M_0 \in S)&=&{\mathrm{P}}(M_0 \in S)-{\mathrm{P}}(M_{-1} \in \mathbb{R}^d \setminus \{0\}, M_0 \in S)\\
\label{E:M-1equal0}&=&(\sum_{k=0}^\infty c_k)^{-1}{\mathrm{P}}(\Lambda \in S),
\end{eqnarray}
since $A^{-1}x \neq 0$ for all $x \in \mathbb{S}^{d-1}$.
To derive a concrete form of the backward Markov kernel, let us assume that $\Lambda$ has a density $f_\Lambda$ on $\mathbb{S}^{d-1}$. Then $\Theta$ and $M_0$ have a density $f_\Theta$ as well and \eqref{E:M-1equal0} gives us
$$ {\mathrm{P}}(M_{-1}=0|M_0=s)=\frac{f_\Lambda(s)}{f_\Theta(s)\sum_{k=0}^\infty c_k}, \;\; {\mathrm{P}}(M_{-1}=A^{-1}s|M_0=s)=1-\frac{f_\Lambda(s)}{f_\Theta(s)\sum_{k=0}^\infty c_k}$$
for all $s \in \mathbb{S}^{d-1}$ with $f_\Theta(s)>0$.
An analogous relation holds true if both $\Lambda$ and $\Theta$ have a probability mass function. 
\fi
\end{ex}

\section*{Acknowledgement} 
The authors thank Richard Davis and Holger Drees for helpful discussions. Furthermore, they wish to thank the organisers, especially Paul Doukhan, of the workshop ``Extremes and risk management`` which took place during September 2012 at the university of Cergy-Pontoise. Anja Jan\ss en was supported by DFG (DFG project JA 2160/1-1). Johan Segers was supported by contract ``Projet d'Actions de Recherche Concert\'ees'' No.\ 12/17-045 of the ``Communaut\'e fran\c{c}aise de Belgique'' and by IAP research network grant No.\ P7/06 of the Belgian government (Belgian Science Policy).

The authors would like to thank the anonymous referee for helpful comments and suggestions. 

\begin{thebibliography}{}
\bibitem[Basrak et al.(2002)]{BDM02a}
\textsc{Basrak, B.}, \textsc{Davis, R. A.} and \textsc{Mikosch, T.} (2002). A characterization of multivariate regular variation. \emph{Ann.\ Appl.\ Probab.\/} {\bf 12}, 908--920.

\bibitem[Basrak et al.(2002b)]{BDM02b}
\textsc{Basrak, B.}, \textsc{Davis, R. A.} and \textsc{Mikosch, T.} (2002). Regular variation of GARCH processes. {\em Stoch.\ Proc.\ Appl.\/} {\bf 99}, 95--115.

\bibitem[Basrak and Segers(2009)]{BS09}
\textsc{Basrak, B.} and \textsc{Segers, J.} (2009). Regularly varying multivariate time series. {\em Stoch.\ Proc.\ Appl.\/} {\bf 119}, 1055--1080.

\bibitem[Billingsley(1968)]{Bi68}
\textsc{Billingsley, P.} (1968). \emph{Convergence of Probability Measures}, Wiley, New York.

\bibitem[Boman and Lindskog(2009)]{BL09}
\textsc{Boman, J.} and \textsc{Lindskog, F.} (2009).
Support theorems for the Radon transform and Cram\'{e}r--Wold theorems,
\emph{J. Theoret.\ Probab.\/} \textbf{22}, 683--710.

\bibitem[Bortot and Coles(2000)]{BC00}
\textsc{Bortot, P.} and \textsc{Coles, S. G.} (2000). A sufficiency property arising from the characterization of extremes of Markov chains. {\em Bernoulli} {\bf 6}, 183--190.
 
\bibitem[Bortot and Coles(2003)]{BC03}
\textsc{Bortot, P.} and \textsc{Coles, S. G.} (2003). Extremes of Markov chains with tail switching potential. {\em J. R. Stat. Soc. Ser. B Stat. Methodol.\/} {\bf 65}, 851--867.

\bibitem[Buraczewski et al.(2009)]{Bura09}
\textsc{Buraczewski, D.}, \textsc{Damek, E.}, \textsc{Guivarc'h, Y.}, \textsc{Hulanicki, A.} and \textsc{Urban, R.} (2009). Tail-homogeneity of stationary measures for some
multidimensional stochastic recursions {\em Probab.\ Theory Relat.\ Fields\/} {\bf 145}, 385--420.

\bibitem[Buraczewski et al.(2012)]{Bura12}
\textsc{Buraczewski, D.}, \textsc{Damek, E.} and \textsc{Mirek, M.} (2012). Asymptotics of stationary solutions of multivariate stochastic recursions with heavy tailed inputs and related limit theorems. {\em Stoch.\ Proc.\ Appl.\/} {\bf 122}, 42--67.

\bibitem[Collamore and Vidyashankar(2013)]{CV13}
\textsc{Collamore, J. F.} and \textsc{Vidyashankar, A. N.} (2013). Tail estimates for stochastic fixed point equations via nonlinear renewal theory. {\em Stoch.\ Proc.\ Appl.} {\bf 123}, 3378--3429.

\bibitem[Coles et al.(1997)]{CST97}
\textsc{Coles, S. G.}, \textsc{Smith, R. L.}, and \textsc{Tawn, J. A.} (1997). A seasonal Markov model for extremely low temperatures. {\em Environmetrics\/} {\bf 5}, 221--239.

\bibitem[Embrechts et al.(1997)]{EKM97}
\textsc{Embrechts, P.}, \textsc{Kl\"uppelberg, C.}, and \textsc{Mikosch, T.} (1997). {\em Modelling Extremal Events for Insurance and Finance\/}. Springer-Verlag, Berlin.

\bibitem[Goldie(1991)]{Goldie91}
\textsc{Goldie, C. M.} (1991). Implicit renewal theory and tails of solutions of random equations. {\em Ann.\ Appl.\ Probab.\/} {\bf 1}, 126--166.

\bibitem[Gomes et al.(2004)]{GdHP04}
\textsc{Gomes, I.}, \textsc{de Haan, L.} and \textsc{Pestana, D.} (2004). Joint exceedances of the ARCH process. {\em J.\ Appl.\ Prob.\/} {\bf 41}, 919--926.

\bibitem[de Haan et al.(1989)]{dHRRdV89}
\textsc{de Haan, L.}, \textsc{Resnick, S. I.}, \textsc{Rootz\'en, H.} and de \textsc{Vries, C. G.} (1989). Extremal behaviour of solutions to a stochastic difference equation with applications to ARCH processes. {\em Stoch. Proc. Appl.\/} {\bf 32}, 213--224.

\bibitem[Hult and Samorodnitsky(2008)]{HuSa08}
\textsc{Hult, H.} and \textsc{Samorodnitsky, G.} (2008). Tail probabilities for infinite series of regularly varying random vectors. {\em Bernoulli} {\bf 14}, 838--864.

\bibitem[Kesten(1973)]{Kesten73}
\textsc{Kesten, H.} (1973). Random difference equations and renewal theory for products of random matrices. {\em Acta Math.}\ {\bf 131}, 207--248.

\bibitem[Kifer(1986)]{Ki86}
\textsc{Kifer, Y.} (1986). {\em Ergodic Theory of Random Transformations}, Birkh\"{a}user, Boston.

\bibitem[Kl\"uppelberg and Pergamenchtchikov(2003)]{KluPer03}
\textsc{Kl\"uppelberg, C.} and \textsc{Pergamenchtchikov, S.} (2003). Renewal theory for functionals of a Markov chain with compact state space. {\em Ann.\ Probab.\/} {\bf 31}, 2270--2300.

\bibitem[Kl\"uppelberg and Pergamenchtchikov(2004)]{KluPer04}
\textsc{Kl\"uppelberg, C.} and \textsc{Pergamenchtchikov, S.} (2004). The tail of the stationary distribution of a random coefficient AR($q$) model. {\em Ann.\ Appl.\ Probab.\/} {\bf 14}, 971--1005.

\bibitem[Letac(1986)]{Letac86}
\textsc{Letac, G.} (1986). A contraction principle for certain Markov chains and its applications. \emph{Random matrices and their applications: Proceedings} (Brunswick, Maine, 1984), 263--273. \emph{Contemp.\ Math.\/}, {\bf 50}, Amer.\ Math.\ Soc., Providence, RI.

\bibitem[Meinguet and Segers(2010)]{MeSe10}
\textsc{Meinguet, T.} and \textsc{Segers, J.} (2010). Regularly varying time series in Banach spaces. {\em Arxiv eprint\/}, \url{http://arxiv.org/abs/1001.3262}

\bibitem[Mirek(2011)]{Mirek11}
\textsc{Mirek, M.} (2011). Heavy tail phenomenon and convergence to stable laws for iterated Lipschitz maps. {\em Probab.\ Theory Relat.\ Fields\/} {\bf 151}, 705--734.

\bibitem[Perfekt(1994)]{Perfekt94}
\textsc{Perfekt, R.} (1994). Extremal behaviour of stationary Markov chains with applications. {\em Ann. Appl. Probab.\/} {\bf 4}, 529--548.

\bibitem[Perfekt(1997)]{Perfekt97}
\textsc{Perfekt, R.} (1997). Extreme value theory for a class of Markov chains with values in ${\mathbb{R}}^d$. {\em Adv. Appl. Prob.\/} {\bf 29}, 138--164.

\bibitem[Resnick(2007)]{Re07}
\textsc{Resnick, S. I.} (2007). {\em Heavy-Tail Phenomena. Probabilistic and Statistical Modeling}. Springer, New York.

\bibitem[Resnick and Zeber(2013)]{ReZe13}
\textsc{Resnick, S. I.} and \textsc{Zeber, D.} (2013). Asymptotics of Markov kernels and the tail chain. {\em Adv. Appl. Prob.\/}, {\bf 45}, 186--213.

\bibitem[De Saporta et al.(2004)]{DeSa04}
\textsc{De Saporta, B.}, \textsc{Guivarc'h, Y.}, and \textsc{Le Page, E.} (2004). On the multidimensional stochastic equation $Y(n+1) = A(n)Y(n)+B(n)$. {\em C. R. Acad.\ Sci.\/} {\bf 339}, 499--502.

\bibitem[Segers(2007)]{S07}
\textsc{Segers, J.} (2007). Multivariate regular variation of heavy-tailed {M}arkov chains. Institut de statistique, Universit\'e ca\-tholique de Louvain, Discussion Paper 0703,
arXiv:0701411.

\bibitem[Smith(1992)]{Smith92}
\textsc{Smith, R. L.} (1992). The extremal index for a Markov chain. {\em J. Appl. Prob.\/} {\bf 29}, 37--45.

\bibitem[Smith et al.(1997)]{STC97}
\textsc{Smith, R. L.}, \textsc{Tawn, J. A.}, and \textsc{Coles, S. G.} (1997). Markov chain models for threshold exceedances. {\em Biometrika\/} {\bf 84}, 249--268.

\bibitem[van der Vaart(1998)]{vdV98}
\textsc{van der Vaart, A. W.} (1998). {\em Asymptotic Statistics\/}, Cambridge University Press, Cambridge.

\bibitem[Yun(1998)]{Yun98}
\textsc{Yun, S. (1998)}. The extremal index of a higher-order stationary Markov chain. {\em Ann. Appl. Probab.\/} {\bf 8}, 408--437.

\bibitem[Yun(2000)]{Yun00}
\textsc{Yun, S.} (2000). The distributions of cluster functionals of extreme events in a $d$th-order Markov chain. {\em J. Appl. Prob.\/} {\bf 37}, 29--44.

\bibitem[Zivot(2009)]{Zi09}
{\sc Zivot, E.} (2009). Practical issues in the analysis of univariate GARCH models. In \emph{Handbook of Financial Time Series},
ed. T. G. Andersen, R. A. Davis, J.-P. Krei\ss, T. Mikosch. Springer, Berlin, pp. 113--155.
\end{thebibliography}

\end{document}
