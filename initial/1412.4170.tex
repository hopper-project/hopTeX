\documentclass[11pt,preprint]{imsart}
\usepackage[utf8]{inputenc}
\RequirePackage[OT1]{fontenc}
\usepackage[margin=1.2in]{geometry}
\usepackage[dvips]{graphics}
\DeclareGraphicsExtensions{.eps.gz,.eps,.epsi.gz,.epsi,.ps,.ps.gz}
\DeclareGraphicsRule{*}{ps}{*}{}
\usepackage{epsfig}
\usepackage{color}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{import}
\usepackage{float}
\usepackage[titletoc]{appendix}
\usepackage{hyperref}
\hypersetup{
    
    unicode=false,          
    
    
    pdffitwindow=false,     
    pdfstartview={FitH},    
    pdftitle={My title},    
    pdfauthor={Author},     
    pdfsubject={Subject},   
    pdfcreator={Creator},   
    pdfproducer={Producer}, 
    pdfkeywords={keyword1} {key2} {key3}, 
    pdfnewwindow=true,      
    colorlinks=true,       
    linkcolor=blue,          
    citecolor=blue,        
    
    urlcolor=blue       
}
\usepackage{titlesec}
\titleformat{\section}[runin]
  {\normalfont\bfseries}{\thesection.}{0.4em}{\titleformat.}{\subsection}[runin]
  {\normalfont\bfseries}{\thesubsection.}{0.4em}{\RequirePackage.}[authoryear]{natbib}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheoremstyle{mystyle} 
    {\topsep}                    
    {\topsep}                    
    {}                   
    {}                           
    {\bfseries}                   
    {.}                          
    {.5em}                       
    {}  
\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{mystyle}
\newtheorem{assumption}{Assumption}

 
 
 

 

 

 

 

 

\begin{document}

\begin{frontmatter}
\title{\normalsize{THE BENEFIT OF GROUP SPARSITY IN GROUP\\ INFERENCE 
WITH DE-BIASED SCALED GROUP LASSO}}
\runtitle{Group Inference}
\begin{aug}
  \author{\fnms{Ritwik}  \snm{Mitra}\corref{}\thanksref{t1}\ead[label=e1]{ritwikm@scarletmail.rutgers.edu}}
  \and
  \author{\fnms{Cun-Hui} \snm{Zhang}\thanksref{t2,t3}\ead[label=e2]{czhang@stat.rutgers.edu}}

  \thankstext{t1}{\printead{e1}}
 \thankstext{t2}{\printead{e2}}
\thankstext{t3}{Research was supported in part by the NSF Grants
DMS-11-06753 and DMS-12-09014.}
 
  \runauthor{R. Mitra \& C.-H. Zhang}

  

  \address{Rutgers University}

 
 

\end{aug}

\begin{abstract}
We study confidence regions and approximate chi-square tests for variable groups 
in high-dimensional linear regression. 
When the size of the group is small, low-dimensional projection estimators for individual 
coefficients can be directly used to construct efficient confidence regions and p-values 
for the group. 
However, the existing analyses of low-dimensional projection estimators do not directly carry 
through for chi-square-based inference of a large group of variables. 
We develop and analyze a scaled group Lasso and propose to de-bias the scaled group Lasso 
for efficient chi-square-based statistical inference of variable groups. 
We prove that the proposed methods capture the benefit of group sparsity under proper 
conditions, for statistical inference of the noise level and variable groups, large and small. 
Such benefit is especially strong when the group size is large. 
Oracle inequalities are provided for the scaled group Lasso in prediction and several estimation losses, 
and for the group Lasso as well in a weighted mixed loss. 

\end{abstract}

\begin{keyword}
\kwd{Group Inference, Asymptotic Normality, Relaxed Projection, Chi Square, Scaled Group Lasso}
\end{keyword}
\end{frontmatter}

\section{Introduction}
We consider the linear regression model
\begin{align}\label{eq:mod1}
{{\boldsymbol}{y}} = {{\rm \textbf{X}}}{{\boldsymbol}{\beta}} + {{\boldsymbol}{\varepsilon}},
\end{align}
where ${{\rm \textbf{X}}} = ({{\boldsymbol}{x}}_1,\ldots,{{\boldsymbol}{x}}_p) \in {{\mathbb{R}}}^{n\times p}$ is a design matrix, ${{\boldsymbol}{y}} \in {{\mathbb{R}}}^{n}$ is a response vector, 
${{\boldsymbol}{\varepsilon}} \sim {\mathsf{N}}_{n}({{\boldsymbol}{0}},\sigma^{2}{{\rm \textbf{I}}}_{n})$ with an unknown noise level $\sigma$, 
and ${{\boldsymbol}{\beta}} = (\beta_1,\ldots,\beta_p)^T \in {{\mathbb{R}}}^{p}$ is a vector of unknown regression coefficients. 
We are interested in making statistical inference about a group of coefficients 
${{\boldsymbol}{\beta}}_G=(\beta_j, j\in G)^T$. 
For small $p$, the $F$-distribution, which is approximately chi-square with proper normalization, 
provides classical confidence regions for ${{\boldsymbol}{\beta}}_G$ and p-values for testing ${{\boldsymbol}{\beta}}_G$. 
We want to construct approximate versions of such procedures for potentially large groups 
in high-dimensional models where $p$ is large, possibly much larger than $n$. 

For individual regression coefficients,  
\cite{Zhang2014} proposed a low-dimensional projection estimator (LDPE) 
for regular statistical inference at the parametric $n^{-1/2}$ rate under proper conditions. 
Their results provide 
{\begin{eqnarray}\label}{expansion}
\sqrt{n}\big({{\widehat{{\boldsymbol}{\beta}}}}_G - {{\boldsymbol}{\beta}}_G\big)  = {\mathsf{N}}_{|G|}\big({{\boldsymbol}{0}}, \sigma^2 {{\rm \textbf{V}}}_{G,G}\big) + {\hbox{Rem}}_G
\end{eqnarray}
along with known covariance structure ${{\rm \textbf{V}}}_{G,G}$ and 
sufficient conditions for the asymptotic normality, ${\hbox{Rem}}_G=o(1)$, 
when the group size $|G|$ is bounded. 
For random designs, the above covariance structure matches the Fisher information 
in the least favorable sub-model in a general context as described in \cite{ZhangOber11}, 
and a proof of the asymptotic efficiency of the LDPE was provided in \cite{VandeGeer2013}. 
Earlier, \cite{Sun2012} proved the consistency and efficiency of a scaled Lasso 
estimate of the noise level $\sigma$. 
However, the analysis of the LDPE, which guarantees 
$\|{\hbox{Rem}}_G\|_\infty \lesssim \|{{\boldsymbol}{\beta}}\|_0(\log p)/\sqrt{n}$, does not directly imply sharp error bound for 
the $\ell_2$- or equivalently chi-square-based group inference for large groups. 
As ${\hbox{Var}}(\chi_{|G|}) \approx 1/2$, the trivial bound $\|{\hbox{Rem}}_G\|_2 \lesssim |G|^{1/2}\|{{\boldsymbol}{\beta}}\|_0(\log p)/\sqrt{n}$ 
yields an extra $\sqrt{|G|}$ factor. 
Thus, the group inference problem is unsolved when one is unwilling to impose the condition 
$|G|^{1/2}\|{{\boldsymbol}{\beta}}\|_0(\log p)/\sqrt{n}\to 0$. 
Our goal is to construct ${{\widehat{{\boldsymbol}{\beta}}}}_G$ satisfying $\|{\hbox{Rem}}_G\|_2 = o(1)$ in an 
expansion of form (\ref{expansion}) with moderately large $|G|$. The impact of 
such a result is certainly beyond $F$- or chi-square-type statistical inference. 

Our approach is based on the natural idea that group sparsity can be exploited 
in statistical inference of variable groups. 
To this end, we propose to use an estimated efficient score matrix to correct the bias 
of a scaled group Lasso estimator.  This combines and extends the ideas of the group 
Lasso \citep{Yuan2006}, scaled Lasso and LDPE, and will be shown to captures the benefit of group 
sparsity in both high-dimensional estimation as in \cite{HZ10} and in bias correction. 

The type of statistical inference under consideration here is regular in the sense that 
it does not require model selection consistency, and that it attains asymptotic efficiency 
in the sense of Fisher information without being super-efficient. 
A characterization of such inference is that it does not require a uniform signal strength 
condition on informative features, e.g. a lower bound on the non-zero $|\beta_j|$ 
above an inflated noise level due to model uncertainly adjustment, known as the ``beta-min'' condition. 
Many attempts have been made to assess the model selected by high dimensional regularizers; 
For example, some early work was done in \cite{KnightFu00}, 
sample splitting was considered in \cite{WasRoe09} and \cite{mein09}, 
and subsampling was considered in  \cite{mein10} and \cite{shah13}. 
\cite{LeebP06} proved that the sampling distribution of statistics based on selected models 
is not estimable. 
\cite{BerkBZ10} and \cite{LaberM11} proposed conservative approaches. 
Alternative approaches were proposed in \cite{Lockhart14} and \cite{Mein13}. 

The basic idea of \cite{Zhang2014} and \cite{ZhangOber11} 
is to correct the bias of high-dimensional regularized estimators by 
projecting its residual to a direction close to that of the efficient score. 
Such bias correction, which has been called de-biasing, is parallel to 
correcting the bias of nonparametric estimators in semiparametric inference \citep{BickelKRW98}. 
\cite{Buhl13} adopted a similar approach to correct the bias of ridge regression. 
\cite{VandeGeer2013} considered an extension to generalized linear model. 
\cite{Javanmard2013a} obtained sharper results for Gaussian designs. 
\cite{Belloni14} considered estimation of treatment effects with a large number of controls. 
\cite{SunZ12PartialCorr}, \cite{RSZZ12} and \cite{Jankova14} considered extensions to 
graphical models and precision matrix estimation. 

Since our proposed method relies upon group regularized initial estimator, in the following we provide a 
brief discussion of the literature on the topic. The group Lasso \citep{Yuan2006} can be defined as
\begin{align}\label{eq:grplsoopt-ns}
{{\widehat{{\boldsymbol}{\beta}}}}({{\boldsymbol}{\omega}}) = {\mathop{\rm arg\, min}}_{{\boldsymbol}{\beta}} {{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}}), \quad  {{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}}) = \dfrac{{\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}}\|_{{2}}}^{2}}{2n} + \sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{\beta}}_{G_{j}}}\|_{{2}}}, 
\end{align}
where $\{G_j,, 1\le j\le M\}$ forms a partition of the index set $\{1,\ldots,p\}$ of variables. 
It is worthwhile to note that when the group effects are being regularized, 
the choice of basis ${{\rm \textbf{X}}}_{G_j}=({{\boldsymbol}{x}}_k, k\in G_j)$ within the group may not 
play a prominent role, so that the design is often ``pre-normalized'' to 
satisfy ${{\rm \textbf{X}}}_{G_j}^T{{\rm \textbf{X}}}_{G_j}/n = {{\rm \textbf{I}}}_{G_j\times G_j}$ as in \cite{Yuan2006}. 
The group Lasso and its variants have been studied in \cite{Bach08}, \cite{Kol08}, \cite{obo08}, \cite{NR08}, \cite{LZ09}, \cite{HZ10}, and \cite{Lounici2011} 
among many others. 
\cite{HZ10} characterized the benefit of group Lasso in $\ell_{2}$ estimation, versus the Lasso \citep{Tib96}, 
under the assumption of \emph{strong group sparsity}; see (\ref{def:strnggrpsprse}). 
\cite{HuangMXZ09} and \cite{BrehenyH11} developed methodologies for concave group and bilevel regularization. 
We refer to \cite{buhlsara11} and \cite{HuangBM12} for further discussion and additional references. 

This paper is organized as follows. 
In Section \ref{sec:probsetup}, we describe the main results of the paper on statistical inference 
of variable groups. 
In Section \ref{sec:consisres}, we study a scaled group Lasso needed for the 
construction in Section \ref{sec:probsetup}. 
In Section \ref{sec:simures}, we present some simulation results to demonstrate the feasibility and 
performance of the proposed methods. 

\section{Group Inference}
\label{sec:probsetup} 

We present our results in five subsections. 
In Subsection \ref{sec:AsympInf} describes our working assumption on the availability of 
a certain initial estimates of ${{\boldsymbol}{\beta}}$ and $\sigma$. The working assumption is based on 
the existing literature on group Lasso and will be verified in Section \ref{sec:consisres} 
under proper conditions.  
In Subsection \ref{subsec:biascorrect} develops bias correction formulations as extension 
from statistical inference of real parameters. 
Subsection \ref{subsec:optstrategy} provides optimization strategies 
(see equations (\ref{opt}) and (\ref{general-opt})) 
for construction of inference procedures for groups of variables. 
Subsection \ref{subsec: Feasibility} provides sufficient conditions (Theorem \ref{th:oraclerate}) under which a 
feasible solution to the optimization problem (\ref{opt}) is available. 
Subsection \ref{subsec:findSolution} discusses strategies for finding feasible solutions.

We use the following notation throughout the paper. 
For vectors ${{\boldsymbol}{u}} \in {{\mathbb{R}}}^{d}$, the $\ell_{p}$ norm is denoted by 
${\|{{\boldsymbol}{u}}\|_{{p}}} =(\sum^{d}_{k=1}|u_{k}|^{p})^{1/p}$,  
with $\|{{\boldsymbol}{u}}\|_\infty = \max_{1\le k\le d}|u_k|$ and ${\|{{\boldsymbol}{u}}\|_{{0}}}= \#\{j:u_{j}\neq 0\}$. 
For matrix ${{\rm \textbf{A}}}=(A_{jk})_{d_1\times d_2}\in {{\mathbb{R}}}^{d_1\times d_2}$, 
the spectrum norm is denoted by $\|{{\rm \textbf{A}}}\|_S = \max_{\|{{\boldsymbol}{u}}\|_2=\|{{\boldsymbol}{v}}\|_2=1}{{\boldsymbol}{u}}^T{{\rm \textbf{A}}}{{\boldsymbol}{v}}$, 
the Frobenius norm by $\|{{\rm \textbf{A}}}\|_F =\{{\hbox{trace}}({{\rm \textbf{A}}}^T{{\rm \textbf{A}}})\}^{1/2}$, and 
the nuclear norm by $\|{{\rm \textbf{A}}}\|_N =\max_{\|{{\rm \textbf{B}}}\|_S=1}{\hbox{trace}}({{\rm \textbf{B}}}^T{{\rm \textbf{A}}})$. 
Given $A\subset \{1,\cdots, p\}$, for any vector ${{\boldsymbol}{u}}\in {{\mathbb{R}}}^{p}$, ${{\boldsymbol}{u}}_{A} \in {{\mathbb{R}}}^{|A|}$ denotes a vector with corresponding components from ${{\boldsymbol}{u}}$, ${{\rm \textbf{X}}}_{A} \in {{\mathbb{R}}}^{n\times |A|}$ denotes the sub-matrix of ${{\rm \textbf{X}}}$ with corresponding columns as indicated by the set $A$, ${{\rm \textbf{X}}}_{-A}$ denotes the sub-matrix of ${{\rm \textbf{X}}}$ with column indices belonging to the complement of $A$, and ${{\cal R}}({{\rm \textbf{X}}}_{A})$ denotes 
the column space spanned by columns of ${{\rm \textbf{X}}}_{A}$. 
Additionally, ${\mathbb{E}}$ and ${\mathbb{P}}$, denote the expectation and probability measure and 
${\stackrel{{\rm D}}{\longrightarrow}}$ the convergence in distribution. Finally, ${{\boldsymbol}{\beta}}^*$ denotes the true regression coefficient vector. 

\subsection{Working assumption based on strong group sparsity}\label{sec:AsympInf}

We assume an inherent and pre-specified non overlapping group structure of the feature set. 
Put precisely, assume that $\{1,\cdots, p\} = \cup^{M}_{j=1}{G_{j}}$ such that ${G_{j}}\cap{G_{k}} = \varnothing$. 
Define ${d_{j}}=|{G_{j}}|$ for all $j$ so that $\sum^{M}_{j=1}{d_{j}}=p$. 
For any index set $T \subset \{1,\cdots, M\}$, we define $G_{T} = \cup_{j \in T}{G_{j}}$. 
In the following, we allow the quantities $n, p, M, {d_{j}}$'s etc. to all grow to infinity. 
 
In light of this group structure, further results on consistency of group regularized estimators of 
${{{\boldsymbol}{\beta}}^{*}}$ will be based on a weighted mixed $\ell_{(2,1)}$, defined as 
$\sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}$ for ${{\boldsymbol}{u}} =({{{\boldsymbol}{u}}_{G_{j}}};1\leq j\leq M)\in {{\mathbb{R}}}^{p}$ with ${{{\boldsymbol}{u}}_{G_{j}}} \in {{\mathbb{R}}}^{|{G_{j}}|}$, 
where ${{\boldsymbol}{\omega}}=(\omega_{1},\cdots, \omega_{M})\in {{\mathbb{R}}}^{M}$ with $\omega_{j}>0$ for all $j$.  
This norm will be used both as penalty and as a key loss function. 
Weighted mixture norm of this type provides suitable description of the 
complexity of the unknown ${{\boldsymbol}{\beta}}$ when the following strong group sparsity holds \cite{HZ10}. 

\smallskip{\bf Strong group sparsity:} 
{\it With the given group structure $\{G_j, j=1,\ldots,M\}$ as a partition of $\{1,\ldots,p\}$, 
there exists a group-index set, $S^{*}\subset\{1,\cdots,M\}$, such that
\begin{align}\label{def:strnggrpsprse}
\quad |S^{*}| \leq g, \quad |G_{S^{*}}|\leq s, \quad {\hbox{supp}}({{{\boldsymbol}{\beta}}^{*}}) \subset G_{S^{*}}=\cup_{j\in S^*}G_j.
\end{align}
In this case, we say that the true coefficient vector ${{{\boldsymbol}{\beta}}^{*}}$ is $(g,s)$ strongly group sparse 
with group support $S^*$.}

\smallskip
Under the strong group sparsity assumption, various error bounds for group regularized 
methods have been established in the literature as we reviewed in the introduction. 
With the support of the existing results and our own in Section \ref{sec:consisres}, 
we make the following working assumption. 

\smallskip
{\bf Working assumption:} 
{\it Suppose that we have estimators ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ and ${\widehat{\sigma}}$ satisfying 
\begin{align}\label{eq:prelimbetaconsis}
\left|\frac{\widehat{\sigma}}{\sigma^*}-1\right| + 
\frac{1}{n^{1/2}}\sum_{j=1}^M \omega_j\Big\|{{\rm \textbf{X}}}_{G_j}{{\widehat{{\boldsymbol}{\beta}}}}_{G_j}^{(init)}-{{\rm \textbf{X}}}_{G_j}{{\boldsymbol}{\beta}}^*_{G_j}\Big\|_2
= {{\cal O}}_{\mathbb{P}}\left(\dfrac{s+g\log M}{n}\right),
\end{align}
where $\omega_j \propto \sqrt{|G_j|/n}+\sqrt{(2/n)\log M}$, 
${{\sigma^{*}}} = {\|{{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}}-{{\boldsymbol}{y}}}\|_{{2}}}/\sqrt{n}$ is an oracle estimate of the noise level $\sigma$, 
and $G_j$, $s$ and $g$ 
are as in (\ref{def:strnggrpsprse}). }

\smallskip
As we will prove in Section \ref{sec:consisres}, the error bound for ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ in (\ref{eq:prelimbetaconsis}) 
is attainable under proper conditions on the design matrix if the group Lasso is used with 
a consistent estimate of $\sigma$, and the error bounds for both ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ 
and ${\widehat{\sigma}}$ in (\ref{eq:prelimbetaconsis}) are attainable if a scaled group Lasso is used. 
See Corollaries \ref{cor:grpsparse-ns} and \ref{cor-2}. 
The working assumption exhibits the benefit of strong group sparsity,  
since a reasonable working assumption under the $\ell_0$ sparsity condition $\|{{\widehat{{\boldsymbol}{\beta}}}}\|_0\le s$ 
would be 
{\begin{eqnarray}\label}{lasso-error}
\left|\frac{\widehat{\sigma}}{\sigma^*}-1\right| + \Big(\frac{\log p}{n}\Big)^{1/2}\|{{\widehat{{\boldsymbol}{\beta}}}}^{(init)}-{{\boldsymbol}{\beta}}^*\|_1
= {{\cal O}}_{\mathbb{P}}\left(\dfrac{s\log p}{n}\right). 
\end{eqnarray}
Although error bounds in (\ref{eq:prelimbetaconsis}) and (\ref{lasso-error}) do not dominate each other due to 
different interpretation of $s$ when ${\hbox{supp}}({{\boldsymbol}{\beta}}^*)\neq G_{S^*}$, 
the right-hand side of (\ref{eq:prelimbetaconsis}) is of smaller order when $s$ is of the same order in both 
settings and $g\ll s$. 
  

\subsection{Bias correction via relaxed projection}\label{subsec:biascorrect}
Given a regularized initial estimator ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ of the regression coefficient vector, 
\cite{Zhang2014} proposed to use a relaxed projection to correct the bias of ${\widehat{\beta}}^{(init)}_j$ via  
{\begin{eqnarray}\label}{LDPE-j}
{\widehat{\beta}}_j = {\widehat{\beta}}_j^{(init)} + \frac{{{\boldsymbol}{z}}_j^T({{\boldsymbol}{y}} -{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}^{(init)})}{{{\boldsymbol}{z}}_j^T{{\boldsymbol}{x}}_j}, 
\end{eqnarray}
where ${{\boldsymbol}{z}}_j$ is designed to be nearly orthogonal to all ${{\boldsymbol}{x}}_k, k\neq j$. 
For the estimation of ${{\boldsymbol}{\beta}}_G$, a formal vectorization of (\ref{LDPE-j}) is 
{\begin{eqnarray}\label}{LDPE-G}
{{\widehat{{\boldsymbol}{\beta}}}}_G = {{\widehat{{\boldsymbol}{\beta}}}}_G^{(init)} + ({{\rm \textbf{Z}}}_G^T{{\rm \textbf{X}}}_G)^\dag{{\rm \textbf{Z}}}_G^T({{\boldsymbol}{y}} -{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}^{(init)}), 
\end{eqnarray}
where ${{\rm \textbf{Z}}}_G$ is an $n\times|G|$ matrix and ${{\rm \textbf{A}}}^{\dag}$ denotes Moore-Penrose pseudo inverse of a matrix ${{\rm \textbf{A}}}$. The problem is to choose ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ and ${{\rm \textbf{Z}}}_G$. 

\cite{Zhang2014} proposed two choices of ${{\boldsymbol}{z}}_j$ to match $\ell_1$ regularized initial estimators
${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$, which naturally controls $\|{{\widehat{{\boldsymbol}{\beta}}}}^{(init)}-{{\boldsymbol}{\beta}}^*\|_1$. The first proposal is a point 
${{\boldsymbol}{z}}_j\approx {{\boldsymbol}{z}}_j^o$ in the Lasso path in the regression of ${{\boldsymbol}{x}}_j$ against ${{\rm \textbf{X}}}_{-j}=({{\boldsymbol}{x}}_k,k\neq j)$: 
{\begin{eqnarray}\label}{z_j^o}
{{\boldsymbol}{x}}_j = {{\rm \textbf{X}}}_{-j}{{\boldsymbol}{\gamma}}_{-j}+{{\boldsymbol}{z}}_j^o. 
\end{eqnarray}
The Karush-Kuhn-Tucker (KKT) conditions for ${{\boldsymbol}{z}}_j$ automatically controls 
$\|{{\boldsymbol}{z}}_j^T{{\rm \textbf{X}}}_{-j}\|_\infty$, and thus 
\begin{eqnarray*}
\left|{\widehat{\beta}}_j - \beta^*_j - \frac{{{\boldsymbol}{z}}_j^T{{\boldsymbol}{\varepsilon}}}{{{\boldsymbol}{z}}_j^T{{\boldsymbol}{x}}_j}\right|
= \left|\frac{{{\boldsymbol}{z}}_j^T{{\rm \textbf{X}}}_{-j}({{\widehat{{\boldsymbol}{\beta}}}}^{(init)}_{-j}-{{\boldsymbol}{\beta}}^*_{-j})}{{{\boldsymbol}{z}}_j^T{{\boldsymbol}{x}}_j}\right|
\le \frac{\|{{\boldsymbol}{z}}_j^T{{\rm \textbf{X}}}_{-j}\|_\infty\|{{\widehat{{\boldsymbol}{\beta}}}}^{(init)}_{-j}-{{\boldsymbol}{\beta}}^*_{-j}\|_1}{|{{\boldsymbol}{z}}_j^T{{\boldsymbol}{x}}_j|} 
\end{eqnarray*}
in an $\ell_\infty$-$\ell_1$ split. The second proposal of \cite{Zhang2014}, closely related to the first one and 
given in the discussion section of their paper, is a constrained variance minimization scheme 
{\begin{eqnarray}\label}{opt-z_j}
{{\boldsymbol}{z}}_j = {\mathop{\rm arg\, min}}_{{\boldsymbol}{z}}\Big\{\|{{\boldsymbol}{z}}\|_2^2: |{{\boldsymbol}{z}}^T{{\boldsymbol}{x}}_j/n|=1, \max_{k\neq j}|{{\boldsymbol}{z}}^T{{\boldsymbol}{x}}_k/n|\le {\lambda}_j'\Big\}. 
\end{eqnarray}
While the Lasso with penalty level ${\lambda}_j$ provides a feasible solution 
${{\boldsymbol}{z}}_j/{\lambda}_j' = ({{\boldsymbol}{x}}_j - {{\rm \textbf{X}}}_{-j}{{\widehat{{\boldsymbol}{\gamma}}}}_{-j})/{\lambda}_j$ for (\ref{opt-z_j}), 
an advantage of (\ref{opt-z_j}) is a guaranteed bias bound 
\begin{eqnarray*}
\left|{\widehat{\beta}}_j - \beta^*_j - \frac{{{\boldsymbol}{z}}_j^T{{\boldsymbol}{\varepsilon}}}{n}\right|
\le \frac{{\lambda}_j'\|{{\widehat{{\boldsymbol}{\beta}}}}^{(init)}_{-j}-{{\boldsymbol}{\beta}}^*_{-j}\|_1}{n} 
\end{eqnarray*}
whenever the optimization problem is feasible. 
For Gaussian designs, such feasibility of ${{\boldsymbol}{z}} = n{{\boldsymbol}{z}}_j^o/{{\boldsymbol}{x}}_j^T{{\boldsymbol}{z}}_j^o$ follows 
from an application of the union bound \citep{Javanmard2013a}. 

The algebraic extension of the above proposals is straightforward. Write 
{\begin{eqnarray}\label}{Gamma}
{{\rm \textbf{X}}}_G = {{\rm \textbf{X}}}_{-G}{{\boldsymbol}{\Gamma}}_{-G,G}+{{\rm \textbf{Z}}}_G^o. 
\end{eqnarray}
We may directly approximate ${{\rm \textbf{Z}}}_G^o$ via a regularized multivariate regression in (\ref{Gamma}) 
or mimic properties of ${{\rm \textbf{Z}}}_G^o$ with a regularized optimization scheme. The question is to make 
a right choice of the regularization to match a proper initial estimator of ${{\boldsymbol}{\beta}}$.  
One possibility is to use an $\ell_1$ regularized estimate of ${{\boldsymbol}{\Gamma}}_{-G,j}$ in the univariate 
regression of ${{\boldsymbol}{x}}_j$ against ${{\rm \textbf{X}}}_{-G}$ for all individual $j\in G$. 
This has been considered in \cite{VDG14}. However, the advantage of such a scheme is unclear 
compared with directly using $({\widehat{\beta}}_j,j\in G)^T$ with the ${\widehat{\beta}}_j$ in (\ref{LDPE-j}). 
It is worthwhile to mention that the central limit theorem for (\ref{LDPE-j}) came with large 
deviation bounds to justify Bonferroni adjustments \citep{Zhang2014}, so that 
(\ref{LDPE-j}) and its variations can be used to test $H_0:{{\boldsymbol}{\beta}}^*_G={{\boldsymbol}{0}}$ versus an  
alternative hypothesis on $\|{{\boldsymbol}{\beta}}^*_G\|_\infty$, especially when 
an $\ell_1$ regularized ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ is used \citep{VandeGeer2013}. 
However, we are interested in extensions of traditional $F$- or chi-square-type tests 
for $\ell_2$ alternatives and to take advantage of group sparsity of ${{\boldsymbol}{\beta}}^*$. 

\subsection{An optimization strategy} \label{subsec:optstrategy}
In this subsection we propose a multivariate extension of (\ref{opt-z_j}) 
to match the group structure and weights in our working assumption (\ref{eq:prelimbetaconsis}).

We write (\ref{LDPE-G}) in terms projections so that the 
resulting optimization scheme will be rotation and scale free within the subspaces under consideration. 
As our goal in essence is to construct inferential procedure for ${{\rm \textbf{X}}}_G{{\boldsymbol}{\beta}}_G$, 
we rewrite the regression problem (\ref{eq:mod1}) as follows: 
\begin{align}\label{eq:mod2}
{{\boldsymbol}{y}} ={{\rm \textbf{X}}}_G{{\boldsymbol}{\beta}}_G + \sum_{{G_{k}}\not\subseteq G} {{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{\beta}}_{G_k\setminus G} + {{\boldsymbol}{\varepsilon}} 
= {{\boldsymbol}{\mu}}_G + \sum_{{G_{k}}\not\subseteq G} {{\boldsymbol}{\mu}}_{{G_{k}}\setminus G} + {{\boldsymbol}{\varepsilon}}.
\end{align}
Here and in the sequel, the following notation is used. 
For any $A\subset \{1,\ldots,p\}$, ${{\boldsymbol}{\mu}}_A={{\rm \textbf{X}}}_A{{\boldsymbol}{\beta}}_A$ 
and ${{\rm \textbf{Q}}}_A$ is the orthogonal 
projection to ${{\cal R}}({{\rm \textbf{X}}}_A)$, the column space of ${{\rm \textbf{X}}}_A$, i.e.
\begin{align}
{{\rm \textbf{Q}}}_A = {{\rm \textbf{X}}}_A({{\rm \textbf{X}}}_A^{T}{{\rm \textbf{X}}}_A)^{-1}{{\rm \textbf{X}}}_A^{T}.  
\end{align}
By ${{\rm \textbf{Q}}}_{A}^{\perp}$, we denote orthogonal projection into ${{\cal R}}^{\perp}({{\rm \textbf{X}}}_{A})$. 
In the simplest case where the variable group of interest matches the group sparsity 
in the following sense: 
{\begin{eqnarray}\label}{nested}
{{\rm \textbf{X}}}_G{{\boldsymbol}{\beta}}^*_G = \sum_{G_k\cap G\neq\emptyset}{{\rm \textbf{X}}}_{G_k}{{\boldsymbol}{\beta}}^*_{G_k}, 
\end{eqnarray}
e.g. $G=G_{j_0}$ for some $j_0$, (\ref{eq:mod2}) becomes  
\begin{eqnarray*}
{{\boldsymbol}{y}} = {{\boldsymbol}{\mu}}_G + \sum_{G_k\cap G = \emptyset}{{\boldsymbol}{\mu}}_{G_{k}} + {{\boldsymbol}{\varepsilon}}.
\end{eqnarray*}
Let ${{\rm \textbf{P}}}_G$ be an orthogonal projection matrix close to ${{\rm \textbf{Q}}}_G$ in certain distance and 
approximately orthogonal to ${{\rm \textbf{Q}}}_{G_k\setminus G}$ for all $k$ with ${G_{k}}\not\subseteq G$. 
We write (\ref{LDPE-G}) in terms of projections as 
{\begin{eqnarray}\label}{eq:biascorrect}
{{\widehat{{\boldsymbol}{\beta}}}}_G &=& ({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)^\dag {{\rm \textbf{P}}}_G
\left({{\boldsymbol}{y}} - \sum_{{G_{k}}\not\subseteq G}{{\widehat{{\boldsymbol}{\mu}}}}_{{G_{k}}\setminus G}^{(init)}\right),\ 
\hbox{ when}\ {\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)=|G|,
\\ \label{mu-hat} {{\widehat{{\boldsymbol}{\mu}}}}_G &=& ({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G)^\dag {{\rm \textbf{P}}}_G
\left({{\boldsymbol}{y}} - \sum_{{G_{k}}\not\subseteq G}{{\widehat{{\boldsymbol}{\mu}}}}_{{G_{k}}\setminus G}^{(init)}\right),\ 
\hbox{ when}\ \|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G^\perp\|_S<1, 
\end{eqnarray} 
where ${{\widehat{{\boldsymbol}{\mu}}}}_{A}^{(init)} = {{\rm \textbf{X}}}_A{{\boldsymbol}{\beta}}_A^{(init)}$ with an initial estimator ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$. 
We note that $\|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G^\perp\|_S<1$ iff ${\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)={\hbox{rank}}({{\rm \textbf{X}}}_G)$, 
so that the condition in (\ref{mu-hat}) is slightly weaker than the condition in (\ref{eq:biascorrect}). 
Moreover, ${\|{{{\rm \textbf{P}}}_{G}{{\rm \textbf{Q}}}^{\perp}_{G}}\|_{{S}}} = {\|{{{\rm \textbf{P}}}_{G}- {{\rm \textbf{Q}}}_{G}}\|_{{S}}} =\cos \theta_{\min}$ 
where $\theta_{\min}$ is the minimum principle angle between subspaces ${{\cal R}}({{\rm \textbf{P}}}_{G})$ and 
${{\cal R}}^\perp({{\rm \textbf{X}}}_{G})$. Thus, ${\|{{{\rm \textbf{P}}}_{G}{{\rm \textbf{Q}}}^{\perp}_{G}}\|_{{S}}} =1$ iff 
the two subspaces have a nontrivial intersection. 

Given ${\widehat{\sigma}}$ an estimate of the noise level, we test the hypothesis $H_0:{{\boldsymbol}{\beta}}_G={{\boldsymbol}{0}}$ 
with the following statistic: 
{\begin{eqnarray}\label}{test}
T_G = \frac{1}{\widehat{\sigma}}\left\|{{\rm \textbf{P}}}_G\left({{\boldsymbol}{y}} - 
\sum_{{G_{k}}\not\subseteq G}{{\widehat{{\boldsymbol}{\mu}}}}_{G_k\setminus G}^{(init)}\right)\right\|_2. 
\end{eqnarray}
A test of this form can be easily converted into elliptical confidence regions for linear mappings of ${{\boldsymbol}{\beta}}_G$ 
in usual way. 

Let ${{\rm \textbf{P}}}_G = {{\rm \textbf{Z}}}_G({{\rm \textbf{Z}}}_G^T{{\rm \textbf{Z}}}_G)^\dag{{\rm \textbf{Z}}}_G^T$ and assume 
${\hbox{rank}}({{\rm \textbf{Z}}}_G^T{{\rm \textbf{X}}}_G)=|G|$. 
We show that (\ref{eq:biascorrect}) and (\ref{mu-hat}) are consistent with (\ref{LDPE-G}) as follows. 
Since both ${{\rm \textbf{Z}}}_G$ and ${{\rm \textbf{X}}}_G$ are $n\times |G|$ matrices, 
we have ${\hbox{rank}}({{\rm \textbf{X}}}_{G})={\hbox{rank}}({{\rm \textbf{Z}}}_{G})=|G|\le n$, 
so that ${\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G)={\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)=|G|$. 
It follows that ${{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G ({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)^\dag{{\rm \textbf{P}}}_G = {{\rm \textbf{P}}}_G$.  
As ${{\rm \textbf{Z}}}_G^T{{\rm \textbf{X}}}_G$ is a $|G|\times|G|$ invertible matrix, 
${{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G ({{\rm \textbf{Z}}}_G^T{{\rm \textbf{X}}}_G)^{-1}{{\rm \textbf{Z}}}_G^T =  {{\rm \textbf{P}}}_G$. 
Since ${\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)=|G|$, we are allowed to cancel ${{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G$ to 
obtain $({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)^\dag {{\rm \textbf{P}}}_G =({{\rm \textbf{Z}}}_G^T{{\rm \textbf{X}}}_G)^\dag{{\rm \textbf{Z}}}_G^T$. 
This provides the consistency between (\ref{eq:biascorrect}) and (\ref{LDPE-G}). 
Furthermore, since ${{\rm \textbf{X}}}_G = {{\rm \textbf{Q}}}_G{{\rm \textbf{X}}}_G = ({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G)^\dag{{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G$, 
we also have ${{\rm \textbf{X}}}_G{{\widehat{{\boldsymbol}{\beta}}}}_G = {{\widehat{{\boldsymbol}{\mu}}}}_G$ for the consistency of (\ref{mu-hat}). 

Let ${{\rm \textbf{Q}}}$ be the projection to ${{\cal R}}({{\rm \textbf{X}}})$. 
In the low-dimensional case of ${\hbox{rank}}({{\rm \textbf{X}}}) = p<n$, 
we may set ${{\rm \textbf{P}}}_G = {{\rm \textbf{Q}}}\prod_{{G_{k}}\not\subseteq G}{{\rm \textbf{Q}}}_{G_k\setminus G}^\perp$, 
so that (\ref{eq:biascorrect}) is the least squares estimator of ${{\boldsymbol}{\beta}}_G$ and 
$T_G^2/|G|$ is the $F$-statistic for testing $H_0:{{\boldsymbol}{\beta}}_G=0$ when ${\widehat{\sigma}}$ is 
the degree adjusted estimate of noise level based on the residuals of the least squares estimator. 
Of course, we need to relax the requirement of the orthogonality condition 
${{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G_k\setminus G}=0$ for all ${G_{k}}\not\subseteq G$ in the high-dimensional case. 

To find the proper relaxation, we first inspect the deviation of (\ref{eq:biascorrect}), (\ref{mu-hat})  
and (\ref{test}) from the low-dimensional regression theory. 
Let ${{\boldsymbol}{\beta}}^*$ be the true ${{\boldsymbol}{\beta}}$ and ${{\boldsymbol}{\mu}}_A^*={{\rm \textbf{X}}}_A{{\boldsymbol}{\beta}}_A^*$ 
for all $A\subset\{1,\ldots,p\}$. 
It follows immediately from (\ref{eq:biascorrect}), (\ref{mu-hat}) and (\ref{test}) that 
{\begin{eqnarray}\label}{eq:hmugj-mugj}
{{\widehat{{\boldsymbol}{\beta}}}}_G &=& {{\boldsymbol}{\beta}}^*_G + ({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)^\dag\Big({{\rm \textbf{P}}}_G{{\boldsymbol}{\varepsilon}} - {\hbox{Rem}}_G\Big),\ 
\hbox{ when}\ {\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)=|G|,
\\ \label{mu-hat-remainder} {{\widehat{{\boldsymbol}{\mu}}}}_G &=& 
{{\boldsymbol}{\mu}}_G^*+({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G)^\dag\Big({{\rm \textbf{P}}}_G{{\boldsymbol}{\varepsilon}} - {\hbox{Rem}}_G\Big),\ 
\hbox{ when}\ \|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G^\perp\|_S<1, 
\end{eqnarray}
with a remainder term 
\begin{eqnarray*}
{\hbox{Rem}}_G 
= \sum_{{G_{k}}\not\subseteq G}{{\rm \textbf{P}}}_G\Big({{\widehat{{\boldsymbol}{\mu}}}}_{G_k\setminus G}^{(init)}-{{\boldsymbol}{\mu}}^*_{G_k\setminus G}\Big) 
= \sum_{{G_{k}}\not\subseteq G}\Big({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G_k\setminus G}\Big)
\Big({{\widehat{{\boldsymbol}{\mu}}}}_{G_k\setminus G}^{(init)}-{{\boldsymbol}{\mu}}^*_{G_k\setminus G}\Big). 
\end{eqnarray*} 
Moreover, when ${{\boldsymbol}{\beta}}^*_G={\bf 0}$, 
{\begin{eqnarray}\label}{test-distr}
\left|T_G - \frac{\|{{\boldsymbol}{P}}_G{{\boldsymbol}{\varepsilon}}\|_2}{\sigma}\right| \le \frac{\big\|{\hbox{Rem}}_G\big\|_2}{\widehat{\sigma}}
+ \left|\frac{\sigma}{\widehat{\sigma}}-1\right|\frac{\|{{\boldsymbol}{P}}_G{{\boldsymbol}{\varepsilon}}\|_2}{\sigma}.  
\end{eqnarray}
As ${{\rm \textbf{P}}}_G$ is an orthogonal projection matrix depending on ${{\rm \textbf{X}}}$ only, 
${{\rm \textbf{P}}}_G{{\boldsymbol}{\varepsilon}}/\sigma$ is a standard normal vector living in the image of ${{\rm \textbf{P}}}_G$ and 
${\|{{{\rm \textbf{P}}}_G{{\boldsymbol}{\varepsilon}}/\sigma}\|_{{2}}}^{2}$ has the chi-square distribution with 
${\hbox{rank}}({{\rm \textbf{P}}}_G)$ degrees of freedom. 
Thus, chi-square based inference can be carried out using the projection estimators in  
(\ref{eq:biascorrect}) and (\ref{mu-hat}) and test statistic $T_G$ in (\ref{test}) under proper conditions 
on $\|{\hbox{Rem}}_G\|_2$ and ${\widehat{\sigma}}$. 
For example, 
{\begin{eqnarray}\label}{eq:conv1}
\begin{cases}
\sup_t \Big|{\mathbb{P}}\Big\{ \|({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)( {{\widehat{{\boldsymbol}{\beta}}}}_G- {{\boldsymbol}{\beta}}_G^*)\|_2 \le {\widehat{\sigma}} t\Big\}
 - {\mathbb{P}}\Big\{ \chi^2_{|G|} \le t\Big\}\Big| \to 0, 
 \cr 
\sup_t \Big|{\mathbb{P}}\Big\{ \|({{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_G)( {{\widehat{{\boldsymbol}{\mu}}}}_G- {{\boldsymbol}{\mu}}_G^*)\|_2 \le {\widehat{\sigma}} t\Big\}
 - {\mathbb{P}}\Big\{ \chi^2_{|G|} \le t\Big\}\Big| \to 0, 
 \cr 
{{\boldsymbol}{\mu}}^*_G={\bf 0}\ \Rightarrow\ 
\sup_t \Big|{\mathbb{P}}\Big\{ T_G^2 \le t\Big\} - {\mathbb{P}}\Big\{ \chi^2_{|G|} \le t\Big\}\Big| \to 0, 
\end{cases}
\end{eqnarray}
under the conditions $\|{\hbox{Rem}}_G\|_2 \to 0$, ${\hbox{rank}}({{\rm \textbf{P}}}_G)=|G|$ and $|G|^{1/2}({\widehat{\sigma}}/\sigma-1)\to 0$. 

We still need to find an upper bound for $\|{\hbox{Rem}}_G\|_2$. 
To this end we use (\ref{eq:prelimbetaconsis}) to obtain  
{\begin{eqnarray}\label}{eq:testC1} 
\|{\hbox{Rem}}_G\|_2 
&\le& \left(\max_{{G_{k}}\not\subseteq G}M_k\omega^{-1}_{k}{\|{{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G_k}}\|_{{S}}}\right)
\sum_{{G_{k}}\not\subseteq G}\omega_{k} {\|{{{\widehat{{\boldsymbol}{\mu}}}}_{G_k}^{(init)}-{{\boldsymbol}{\mu}}_{G_k}}\|_{{2}}} 
\cr & = & {{\cal O}}_{\mathbb{P}}\left(\dfrac{s+g\log M}{n^{1/2}}\right)
 \left(\max_{{G_{k}}\not\subseteq G}M_k \omega^{-1}_{k}{\|{{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G_k}}\|_{{S}}}\right), 
\end{eqnarray}
where $M_k=\max_{\|{{\rm \textbf{X}}}_{G_k}{{\boldsymbol}{u}}_{G_k}\|_2=1}\|{{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{u}}_{G_k\setminus G}\|_2$. 
We note that $M_k=1$ when ${{\rm \textbf{X}}}_{G_k}^T{{\rm \textbf{X}}}_{G_k}/n = {{\rm \textbf{I}}}_{d_k\times d_k}$. 
The error bound in (\ref{eq:testC1}) motivates the following extension of (\ref{opt-z_j}): 
\begin{align}\label{opt}
{{\rm \textbf{P}}}_G = {\mathop{\rm arg\, min}}_{{\rm \textbf{P}}}\Big\{\|{{\rm \textbf{P}}}{{\rm \textbf{Q}}}_G^\perp\|_S: {{\rm \textbf{P}}} = {{\rm \textbf{P}}}^{2}= {{\rm \textbf{P}}}^{T},\ 
\|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{{{G_{k}}\setminus G}}\|_S\le \omega_k'\ \forall\ {G_{k}}\not\subseteq G\Big\}. 
\end{align}
We say that ${{\rm \textbf{P}}}_G$ is a feasible solution of (\ref{opt}) if it satisfies all the constraints. 
We summarize the above analysis in the following theorem. 

\begin{theorem}\label{th-opt}
Let ${{\widehat{{\boldsymbol}{\beta}}}}_G$ be given by (\ref{eq:biascorrect}) and $T_G$ by (\ref{test}) 
with a feasible solution ${{\rm \textbf{P}}}_G$ of (\ref{opt}) with ${\hbox{rank}}({{\rm \textbf{P}}}_G)=|G|$. 
Suppose that (\ref{eq:prelimbetaconsis}) holds for ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ and ${\widehat{\sigma}}$, and 
{\begin{eqnarray}\label}{th-1-1}
\frac{|G|}{n} \to 0,\quad 
\dfrac{s+g \log M}{n^{1/2}}\left(\frac{|G|^{1/2}}{n^{1/2}}+ 
\max_{{G_{k}}\not\subseteq G}M_k\frac{\omega_k'}{\omega_k}\right) \rightarrow 0,\quad
\end{eqnarray}
with the $M_k$ in (\ref{eq:testC1}). 
Then, (\ref{eq:conv1}) holds. In particular, with $\|{\hbox{Rem}}_G\|_2 = o_{\mathbb{P}}(1)$, 
{\begin{eqnarray}\label}{th-1-2}
({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)\big({{\widehat{{\boldsymbol}{\beta}}}}_G - {{\boldsymbol}{\beta}}^*_G\big)
= {\mathsf{N}}({{\boldsymbol}{0}},\sigma^2 {{\rm \textbf{P}}}_G) + {\hbox{Rem}}_G. 
\end{eqnarray}
\end{theorem}

\begin{remark} The optimization problem (\ref{opt}) also provides geometric insights. 
As we have mentioned earlier, 
the quantity $\|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G}^\perp\|_S$, which equals $\|{{\rm \textbf{P}}}_G-{{\rm \textbf{Q}}}_{G}\|_S$, 
is the so-called `gap' between the subspaces spanned by ${{\rm \textbf{P}}}_G$ and ${{\rm \textbf{Q}}}_{G}$, 
which we try to minimize. 
This minimization is done subject to upper-bounds on ${\|{{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{{{G_{k}}\setminus G}}}\|_{{S}}}$. 
When $p<n$ and $\omega_k'=0$, ${{\boldsymbol}{P}}_G$ in (\ref{opt}) is the projection to 
the orthogonal complement of $\sum_{{G_{k}}\not\subseteq G}{{\cal R}}({{\rm \textbf{X}}}_{{{G_{k}}\setminus G}})$ in 
${{\cal R}}({{\rm \textbf{X}}})$, or equivalently the linear space 
$\big(\prod_{{G_{k}}\not\subseteq G}{{\rm \textbf{Q}}}_{{{G_{k}}\setminus G}}^\perp\big) {{\cal R}}({{\rm \textbf{X}}})$. 
\end{remark}

\begin{proof}[Proof of Theorem \ref{th-opt}] 
It follows from (\ref{eq:testC1}) and the feasibility of ${{\rm \textbf{P}}}_G$ in (\ref{opt}) that 
 \begin{eqnarray*}
 \|{\hbox{Rem}}_G\|_2 = o_{\mathbb{P}}(1) 
 \end{eqnarray*}
in (\ref{eq:hmugj-mugj}), (\ref{mu-hat-remainder}) under condition (\ref{th-1-1}) and (\ref{test-distr}). 
In addition, (\ref{eq:prelimbetaconsis}) and (\ref{th-1-1}) imply 
\begin{eqnarray*}
\left|\frac{\sigma}{\widehat{\sigma}}-1\right| = o_{\mathbb{P}}(|G|^{-1/2}) + O_{\mathbb{P}}(n^{-1/2})= o_{\mathbb{P}}(|G|^{-1/2}), 
\end{eqnarray*}
so that by (\ref{test-distr})
\begin{eqnarray*}
\left|T_G - \frac{\|{{\boldsymbol}{P}}_G{{\boldsymbol}{\varepsilon}}\|_2}{\sigma}\right| \le o_{\mathbb{P}}(1)
+ o_{\mathbb{P}}(1)\frac{\|{{\boldsymbol}{P}}_G{{\boldsymbol}{\varepsilon}}\|_2}{\sigma|G|^{1/2}} = o_{\mathbb{P}}(1).  
\end{eqnarray*}
The conclusions follow immediately.  
\end{proof}

A modification of (\ref{opt}), which removes the factors $M_k$ in condition (\ref{th-1-1}), is to write 
\begin{eqnarray*}
{{\boldsymbol}{y}} 
= {\widetilde {{\rm \textbf{X}}}}_G{{\boldsymbol}{\beta}}_G + \sum_{{G_{k}}\not\subseteq G} {{\rm \textbf{Q}}}_{G_k\setminus G}{{\boldsymbol}{\mu}}_{G_k} + {{\boldsymbol}{\varepsilon}}, 
\end{eqnarray*}
where ${\widetilde {{\rm \textbf{X}}}}_G$ is a $n\times |G|$ matrix defined by 
${\widetilde {{\rm \textbf{X}}}}_G{{\boldsymbol}{v}}_G = \sum_{k=1}^M \big({{\rm \textbf{Q}}}_{G_k\setminus G}^\perp{{\rm \textbf{X}}}_{G_k\cap G}\big)
{{\boldsymbol}{v}}_{G\cap G_k}$. 
We note that ${\widetilde {{\rm \textbf{X}}}}_G = {{\rm \textbf{X}}}_G$ when 
${{\rm \textbf{X}}}_{G_k}^T{{\rm \textbf{X}}}_{G_k}/n = {{\rm \textbf{I}}}_{G_k\times G_k}$ for all $k$ with $0<|G_k\setminus G|<|G_k|$.  
Let ${\widetilde {{\rm \textbf{Q}}}}_G$ be the projection to the column space of ${\widetilde {{\rm \textbf{X}}}}_G$. 
The optimization scheme and statistical methods are changed accordingly as follows: 
{\begin{eqnarray}\label}{general-opt}
{{\rm \textbf{P}}}_G &=& {\mathop{\rm arg\, min}}_{{\rm \textbf{P}}}\Big\{\|{{\rm \textbf{P}}}{\widetilde {{\rm \textbf{Q}}}}_G^\perp\|_S: {{\rm \textbf{P}}} = {{\rm \textbf{P}}}^{2}= {{\rm \textbf{P}}}^{T},\ 
\|{{\rm \textbf{P}}}_G{{\rm \textbf{Q}}}_{G_k\setminus G}\|_S\le \omega_k'\ \forall\ k\Big\}, 
\cr {{\widehat{{\boldsymbol}{\beta}}}}_G &=& ({{\rm \textbf{P}}}_G{\widetilde {{\rm \textbf{X}}}}_G)^\dag {{\rm \textbf{P}}}_G
\left({{\boldsymbol}{y}} - \sum_{{G_{k}}\not\subseteq G} {{\rm \textbf{Q}}}_{G_k\setminus G}{{\widehat{{\boldsymbol}{\mu}}}}_{G_k}^{(init)}\right), 
\hbox{ when}\ {\hbox{rank}}({{\rm \textbf{P}}}_G{\widetilde {{\rm \textbf{X}}}}_G)=|G|,
\\ \nonumber T_G &=& \frac{1}{\widehat{\sigma}}
\left\|{{\rm \textbf{P}}}_G\left({{\boldsymbol}{y}} - \sum_{{G_{k}}\not\subseteq G} {{\rm \textbf{Q}}}_{G_k\setminus G}{{\widehat{{\boldsymbol}{\mu}}}}_{G_k}^{(init)}\right)\right\|_2. 
\end{eqnarray}
With $\{{{\rm \textbf{X}}}_G,{{\rm \textbf{Q}}}_G\}$ replaced 
by $\{{\widetilde {{\rm \textbf{X}}}}_G,{\widetilde {{\rm \textbf{Q}}}}_G\}$, 
our analysis yields the following theorem. 

\begin{theorem}\label{th-opt-gen} 
Let ${{\rm \textbf{P}}}_G$, ${{\widehat{{\boldsymbol}{\beta}}}}_G$ and $T_G$ be given by (\ref{general-opt}). 
Suppose that (\ref{eq:prelimbetaconsis}) holds and 
{\begin{eqnarray}\label}{th-2-1}
\frac{|G|}{n} \to 0,\quad 
\dfrac{s+g \log M}{n^{1/2}}
\left(\frac{|G|^{1/2}}{n^{1/2}}+ \max_{{G_k\setminus G \neq \emptyset}}\frac{\omega_k'}{\omega_k}\right) 
\rightarrow 0.
\end{eqnarray}
Then, (\ref{eq:conv1}) and (\ref{th-1-2}) hold with $\{{{\rm \textbf{X}}}_G,{{\rm \textbf{Q}}}_G\}$ replaced 
by $\{{\widetilde {{\rm \textbf{X}}}}_G,{\widetilde {{\rm \textbf{Q}}}}_G\}$. 
\end{theorem}

The optimization problems in (\ref{opt}) and (\ref{general-opt}) 
are still somewhat abstract for the moment, 
although our theorems only require feasible solutions. 
In the following we prove the feasibility of ${{\rm \textbf{P}}}_G$ in (\ref{opt})  for Gaussian designs 
and describe penalized regression methods to find feasible solutions of (\ref{opt}) and (\ref{general-opt}).

\subsection{Feasibility of relaxed orthogonal projection for random designs} \label{subsec: Feasibility} 
Let ${{\boldsymbol}{e}}_i$ be the $i$-th canonical unit vector of ${\mathbb{R}}^n$. 
Throughout this subsection, we assume that the matrix ${{\rm \textbf{X}}}$ 
has iid subGaussian rows ${{\boldsymbol}{e}}_i^T{{\rm \textbf{X}}}$ satisfying ${\mathbb{E}}{{\rm \textbf{X}}} = {\bf 0}$, 
${\mathbb{E}}({{\rm \textbf{X}}}^T{{\rm \textbf{X}}}/n) = {{\boldsymbol}{\Sigma}}$ with a positive-definite ${{\boldsymbol}{\Sigma}}$, 
and that for certain constant $v_0>1$
{\begin{eqnarray}\label}{subGaussian-cond}
\sup_{{{\boldsymbol}{b}}\neq {\bf 0}}\ {\mathbb{E}} \exp\left( \frac{({{\boldsymbol}{e}}_i^T{{\rm \textbf{X}}}{{\boldsymbol}{b}})^2}{v_0{{\boldsymbol}{b}}^T{{\boldsymbol}{\Sigma}} {{\boldsymbol}{b}}} +\frac{1}{v_0} \right) \le 2. 
\end{eqnarray}
where 
Let ${{\boldsymbol}{\Gamma}}_{-G,G} = {{\boldsymbol}{\Sigma}}_{-G,-G}^{-1}{{\boldsymbol}{\Sigma}}_{-G,G}$. 
We write the regression model (\ref{Gamma}) as 
{\begin{eqnarray}\label}{eq:mod1ForXj}
{{\rm \textbf{X}}}_G = {{\rm \textbf{X}}}_{-G}{{\boldsymbol}{\Gamma}}_{-G,G}+{{\rm \textbf{Z}}}_G^o  
= \sum_{k=1}^M {{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{\Gamma}}_{G_k\setminus G,G}+{{\rm \textbf{Z}}}_G^o. 
\end{eqnarray}
Let ${{\rm \textbf{P}}}_G^o$ be the orthogonal projection to the column space of ${{\rm \textbf{Z}}}_G^o$,  
{\begin{eqnarray}\label}{P^o}
{{\rm \textbf{P}}}_G^o = {{\rm \textbf{Z}}}_G^o\Big(({{\rm \textbf{Z}}}_G^o)^T{{\rm \textbf{Z}}}_G^o\Big)^\dag({{\rm \textbf{Z}}}_G^o)^T. 
\end{eqnarray}
We use the following lemma to evaluate ${{\rm \textbf{P}}}_G^o$. 
The inequality is well known; See for example \cite{Vers10}, and 
for Gaussian ${{\rm \textbf{X}}}$ the supplementary material for \cite{Ma13}. 

\begin{lemma}\label{lm-subGaussian} Let ${{\rm \textbf{B}}}_k$ be matrices of $p$ rows and rank $r_k$. 
Let ${{\rm \textbf{P}}}_k$ be the projection to the range of ${{\rm \textbf{X}}}{{\rm \textbf{B}}}_k$ and 
${{\boldsymbol}{\Omega}}_{1,2} = (({{\rm \textbf{B}}}_1^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_1)^\dag)^{1/2}{{\rm \textbf{B}}}_1^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_2
(({{\rm \textbf{B}}}_2^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_2)^\dag)^{1/2}$. 
Let $r = {\hbox{rank}}({{\boldsymbol}{\Omega}}_{1,2})$ and $1\ge{\lambda}_1\ge\cdots\ge {\lambda}_r>0$ be the nonzero 
singular values of ${{\boldsymbol}{\Omega}}_{1,2}$. Define ${\lambda}_{\min}={\lambda}_r I\{r=r_1=r_2\}$. 
Then, there exists a numerical constant $C_0>1$ such that 
when $C_0v_0\sqrt{t/n+(r_1+r_2)/n}<{\epsilon}_0<1$, 
{\begin{eqnarray}\label}{lm-1-1}
{\mathbb{P}}\left\{\|(({{\rm \textbf{B}}}_1^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_1)^\dag)^{1/2}{{\rm \textbf{B}}}_1^{T}({{\rm \textbf{X}}}^T{{\rm \textbf{X}}}/n){{\rm \textbf{B}}}_2
(({{\rm \textbf{B}}}_2^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_2)^\dag)^{1/2}-{{\boldsymbol}{\Omega}}_{1,2}\|_S \le {\epsilon}_0\right\} \ge 1- e^{-t},  
\end{eqnarray}
and
{\begin{eqnarray}\label}{lm-1-2}
{\mathbb{P}}\left\{\|{{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2\|_S \le \frac{{\lambda}_1(1+{\epsilon}_0)}{1-{\epsilon}_0}, 
\|{{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2^\perp\|_S^2 \le 1 - \left(\frac{{\lambda}_{\min}(1-{\epsilon}_0)}{1+{\epsilon}_0}\right)^2 \right\} \ge 1- e^{-t}. 
\end{eqnarray}
Moreover, ${\lambda}_1<1$ iff ${\hbox{rank}}({{\rm \textbf{B}}}_1,{{\rm \textbf{B}}}_2)=r_1+r_2$ 
and ${\lambda}_{\min}>0$ iff ${\hbox{rank}}({{\rm \textbf{B}}}_1^T{{\rm \textbf{B}}}_2)=r_1=r_2$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lm-subGaussian}] 
Let ${{\boldsymbol}{u}}_j, 1\le j\le r_k$, be the eigenvectors of ${{\rm \textbf{B}}}_k^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_k$ 
corresponding to positive eigenvalues and ${{\rm \textbf{U}}}_k=({{\boldsymbol}{u}}_1,\ldots,{{\boldsymbol}{u}}_{r_k})$. 
Let ${{\rm \textbf{Z}}}_k = {{\rm \textbf{X}}}{{\rm \textbf{B}}}_k(({{\rm \textbf{B}}}_k^{T}{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_k)^\dag)^{1/2}{{\rm \textbf{U}}}_k\in {\mathbb{R}}^{n\times r_k}$. 
We have ${\mathbb{E}}{{\rm \textbf{Z}}}_k = {\bf 0}$, ${\mathbb{E}}({{\rm \textbf{Z}}}_k^T{{\rm \textbf{Z}}}_k/n) = {{\rm \textbf{I}}}_{r_k\times r_k}$, 
${\mathbb{E}}({{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_2/n) = {{\rm \textbf{U}}}_1^T{{\boldsymbol}{\Omega}}_{1,2}{{\rm \textbf{U}}}_2$, and 
\begin{eqnarray*}
\sup_{\|{{\boldsymbol}{b}}\|_2\le 1}\ {\mathbb{E}} \exp\left( \frac{({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_k{{\boldsymbol}{b}})^2}{v_0} +\frac{1}{v_0} \right) \le 2,\ k=1,2. 
\end{eqnarray*}
Moreover, ${{\rm \textbf{P}}}_k = {{\rm \textbf{Z}}}_k({{\rm \textbf{Z}}}_k^T{{\rm \textbf{Z}}}_k)^{\dag}{{\rm \textbf{Z}}}_k^T$ and 
$\|{{\rm \textbf{U}}}_1^T{{\boldsymbol}{\Omega}}_{1,2}{{\rm \textbf{U}}}_2\|_S=\|{{\boldsymbol}{\Omega}}_{1,2}\|_S \le 1$. 

For $1\le j\le k\le 2$ and any vectors ${{\boldsymbol}{v}}_k\in{\mathbb{R}}^{r_k}$ with $\|{{\boldsymbol}{v}}_k\|_2=1$, 
\begin{eqnarray*}
{{\boldsymbol}{v}}_j^T\Big({{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n - {\mathbb{E}} {{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n\Big){{\boldsymbol}{v}}_k 
= \frac{1}{n}\sum_{i=1}^n \Big\{({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_j{{\boldsymbol}{v}}_j)({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_k{{\boldsymbol}{v}}_k) 
- {{\boldsymbol}{v}}_j^T{\mathbb{E}}({{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n){{\boldsymbol}{v}}_k\Big\}
\end{eqnarray*}
is an average of iid variables with 
\begin{eqnarray*}
&& {\mathbb{E}} \exp\left(\frac{({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_j{{\boldsymbol}{v}}_j)({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_k{{\boldsymbol}{v}}_k) 
- {{\boldsymbol}{v}}_j^T{\mathbb{E}}({{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n){{\boldsymbol}{v}}_k}{v_0}\right) 
\cr &\le& \left\{\prod_{k=1}^2\sqrt{{\mathbb{E}} \exp\left(({{\boldsymbol}{e}}_i^T{{\rm \textbf{Z}}}_k{{\boldsymbol}{v}}_k)^2/v_0\right) }\right\}e^{1/v_0} 
\cr &\le& 2. 
\end{eqnarray*}
Since the size of an $\epsilon$-net of the unit ball in ${{\mathbb{R}}}^{r_k}$ is bounded by $(1+2/{\epsilon})^{r_k}$,
the Bernstein inequality implies that for $r^*=r_1+r_2$ and a certain numerical constant $C_0$, 
\begin{eqnarray*}
{\mathbb{P}}\Big\{ \|{{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n - {\mathbb{E}}({{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n)\|_S > 
C_0v_0\max\Big(\sqrt{t/n+r^*/n},t/n+r^*/n\Big)\Big\}\le e^{-t}/3. 
\end{eqnarray*}
This yields (\ref{lm-1-1}) as $\|{{\rm \textbf{U}}}_1^T{{\boldsymbol}{\Delta}}{{\rm \textbf{U}}}_2\|_S=\|{{\boldsymbol}{\Delta}}\|_S$ 
for all ${{\boldsymbol}{\Delta}}$ of proper dimension. 

Suppose ${\hbox{rank}}({{\rm \textbf{P}}}_k)=r_k$. Let $r_0={\hbox{rank}}({{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2)$ and 
$1\ge {\widehat{\lambda}}_1\ge \cdots \ge{\widehat{\lambda}}_{r_0} > 0$ be the (nonzero) singular values of ${{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2$. 
We have $\|{{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2\|_S={\widehat{\lambda}}_1$ 
and $\|{{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2^\perp\|_S=\|{{\rm \textbf{P}}}_1 - {{\rm \textbf{P}}}_2\|_S = \sqrt{1-{\widehat{\lambda}}_{\min}^2}$ with 
${\widehat{\lambda}}_{\min}={\widehat{\lambda}}_{r_0}I\{r_0=r_1=r_2\}$. By definition, 
\begin{eqnarray*}
{{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2 = {{\rm \textbf{Z}}}_1({{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_1)^{-1}{{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_2({{\rm \textbf{Z}}}_2^T{{\rm \textbf{Z}}}_2)^{-1}{{\rm \textbf{Z}}}_2^T. 
\end{eqnarray*}
Since $({{\rm \textbf{Z}}}_k^T{{\rm \textbf{Z}}}_k)^{-1/2}{{\rm \textbf{Z}}}_k^T$ are unitary maps from the range of ${{\rm \textbf{P}}}_k$ to ${\mathbb{R}}^{r_k}$, 
the singular values of ${{\rm \textbf{P}}}_1{{\rm \textbf{P}}}_2$ is the same as those of 
\begin{eqnarray*}
({{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_1)^{-1/2}{{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_2({{\rm \textbf{Z}}}_2^T{{\rm \textbf{Z}}}_2)^{-1/2}. 
\end{eqnarray*}

Now suppose that $\|{{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n - {\mathbb{E}}({{\rm \textbf{Z}}}_j^T{{\rm \textbf{Z}}}_k/n)\|_S \le C_0v_0\sqrt{t/n+r/n}\le{\epsilon}_0<1$ 
for $1\le j\le k\le 2$. 
Recall that $1\ge{\lambda}_1\ge\cdots\ge {\lambda}_r>0$ are the nonzero singular values of ${{\boldsymbol}{\Omega}}_{1,2}$ 
and ${\lambda}_{\min} = {\lambda}_r I\{r=r_1=r_2\}$.  
As ${\mathbb{E}}({{\rm \textbf{Z}}}_k^T{{\rm \textbf{Z}}}_k/n) = {{\rm \textbf{I}}}_{r_k\times r_k}$, we have ${\hbox{rank}}({{\rm \textbf{P}}}_k)=r_k$. 
Moreover, as ${\mathbb{E}}({{\rm \textbf{Z}}}_1^T{{\rm \textbf{Z}}}_2/n) = {{\rm \textbf{U}}}_1^T{{\boldsymbol}{\Omega}}_{1,2}{{\rm \textbf{U}}}_2$ with unitary 
maps ${{\rm \textbf{U}}}_1$ and ${{\rm \textbf{U}}}_2$, the Weyl inequality implies that 
\begin{eqnarray*}
{\widehat{\lambda}}_1\le \frac{{\lambda}_1(1+{\epsilon}_0)}{1-{\epsilon}_0},\quad {\widehat{\lambda}}_{\min} \ge \frac{{\lambda}_{\min}(1-{\epsilon}_0)}{1+{\epsilon}_0}. 
\end{eqnarray*}
Thus, (\ref{lm-1-2}) holds.  
As the conditions for ${\lambda}_1<1$ and ${\lambda}_{\min}>0$ follow from the positive-definiteness of ${{\boldsymbol}{\Sigma}}$, 
the proof is complete. 
\end{proof}

As we have discussed below (\ref{test}), 
when ${{\rm \textbf{P}}}_G^o$ is used, (\ref{eq:biascorrect}) has the interpretation as 
\begin{eqnarray*}
 {{\widehat{{\boldsymbol}{\beta}}}}_G &=& ({{\rm \textbf{P}}}_G^o{{\rm \textbf{X}}}_G)^\dag {{\rm \textbf{P}}}_G^o
\left({{\boldsymbol}{y}} - {{\widehat{{\boldsymbol}{\mu}}}}_{-G}^{(init)}\right)
= \Big( ({{\rm \textbf{Z}}}_G^o)^T{{\rm \textbf{X}}}_G\Big)^\dag 
({{\rm \textbf{Z}}}_G^o)^T\left({{\boldsymbol}{y}} -{{\rm \textbf{X}}}_{-G}{{\widehat{{\boldsymbol}{\beta}}}}_{-G}^{(init)}\right). 
\end{eqnarray*}

\begin{theorem}\label{th:oraclerate}
Suppose the subGaussian condition (\ref{subGaussian-cond}) holds 
with $0<c_*\le$eigen$({{\boldsymbol}{\Sigma}})\le c^*$ and fixed $\{v_0,c_*,c^*\}$. 
Let $\omega_k' = \xi n^{-1/2}\big(\sqrt{|G|+|G_k\setminus G|}+\sqrt{\log(M/\delta)}\big)$, 
${\lambda}_{\min}$ be the smallest eigenvalue of 
$\{{{\boldsymbol}{\Sigma}}_{G,G}^{-1/2}({{\boldsymbol}{\Sigma}}^{-1})_{G,G}{{\boldsymbol}{\Sigma}}_{G,G}^{-1/2}\}^{1/2}$, 
$\xi n^{-1/2}\big(\sqrt{|G|}+\sqrt{\log(M/\delta)}\big)\le\eta_n$, 
and $a_n = {\lambda}_{\min}(1-\eta_n)/(1+\eta_n)$. 
Then, there exist numerical constants ${\epsilon}_0\in (0,1)$ and $\xi_0<\infty$ such that when 
$\xi\ge \xi_0v_0$ and $\eta_n\le{\epsilon}_0$, 
{\begin{eqnarray}\label}{th-3-1}
{\mathbb{P}}
\begin{cases}\hbox{\ (\ref{opt}) has a feasible solution ${{\boldsymbol}{P}}_G$ with} \cr
\hbox{\ ${\hbox{rank}}({{\rm \textbf{P}}}_G{{\rm \textbf{X}}}_G)=|G|$ and $\|{{\boldsymbol}{P}}_G{{\rm \textbf{Q}}}_G^\perp\|_S\le \sqrt{1-a_n^2}$}\end{cases}\Bigg\}
\ge 1-\delta. 
\end{eqnarray}
Let ${{\widehat{{\boldsymbol}{\beta}}}}_G$ and $T_G$ be as in (\ref{eq:biascorrect}) and (\ref{mu-hat}). 
Suppose that (\ref{eq:prelimbetaconsis}) holds for ${{\widehat{{\boldsymbol}{\beta}}}}^{(init)}$ and ${\widehat{\sigma}}$, and 
{\begin{eqnarray}\label}{th-3-2}
\frac{|G|}{n} \to 0,\  
\max_{{G_k\setminus G \neq \emptyset}}\frac{|G_k|}{n} \to 0,\  
\dfrac{s+g \log M}{n^{1/2}}
\left(\frac{|G|^{1/2}}{n^{1/2}}+ \max_{{G_k\setminus G \neq \emptyset}}\frac{\omega_k'}{\omega_k}\right) 
\rightarrow 0.
\end{eqnarray}
Then, (\ref{eq:conv1}) and (\ref{th-1-2}) hold with $\|{\hbox{Rem}}_G\|_2 = o_{\mathbb{P}}(1)$. 
\end{theorem}

\begin{proof}[Proof of Theorem \ref{th:oraclerate}]
By (\ref{P^o}), ${{\rm \textbf{P}}}_G^o$ is the orthogonal projection to the range of ${{\rm \textbf{Z}}}_G^o = {{\rm \textbf{X}}}{{\rm \textbf{B}}}_G^o$ 
with ${{\rm \textbf{B}}}_G^o=({{\boldsymbol}{\Sigma}}^{-1})_{*,G}({{\boldsymbol}{\Sigma}}^{-1})_{G,G}^{-1}$. 
By definition, ${{\rm \textbf{Q}}}_{G_k\setminus G}$ is the projection to the range of 
${{\rm \textbf{X}}}_{G_k\setminus G} = {{\rm \textbf{X}}}{{\rm \textbf{B}}}_{G_k\setminus G}$ 
and ${{\rm \textbf{Q}}}_{G}$ to the range of ${{\rm \textbf{X}}}_{G}={{\rm \textbf{X}}}{{\rm \textbf{B}}}_G$, 
where ${{\rm \textbf{B}}}_{G_k\setminus G}$ and ${{\rm \textbf{B}}}_G$ are 0-1 diagonal matrices projecting to the indicated spaces. 
Define ${{\boldsymbol}{\Omega}} = {{\boldsymbol}{\Sigma}}_{G,G}^{-1/2}\big\{({{\boldsymbol}{\Sigma}}^{-1})_{G,G}\}^{1/2}$.  
We have 
${{\rm \textbf{B}}}_{G_k\setminus G}^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G^o={{\boldsymbol}{\Sigma}}_{G_k\setminus G,*}{{\rm \textbf{B}}}_G^o=0$,  
${{\rm \textbf{B}}}_G^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G^o={{\boldsymbol}{\Sigma}}_{G,*}{{\rm \textbf{B}}}_G^o = ({{\boldsymbol}{\Sigma}}^{-1})_{G,G} 
= ({{\rm \textbf{B}}}_G^o)^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G^o$ and 
\begin{eqnarray*}
({{\rm \textbf{B}}}_G^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G)^{-1/2}{{\rm \textbf{B}}}_G^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G^o
\big(({{\rm \textbf{B}}}_G^o)^T{{\boldsymbol}{\Sigma}}{{\rm \textbf{B}}}_G^o\big)^{-1/2} 
= {{\boldsymbol}{\Sigma}}_{G,G}^{-1/2}\big\{({{\boldsymbol}{\Sigma}}^{-1})_{G,G}\}^{1/2} = {{\boldsymbol}{\Omega}} \in{\mathbb{R}}^{|G|\times |G|}. 
\end{eqnarray*}
Moreover, ${{\boldsymbol}{\Omega}} = {{\boldsymbol}{\Sigma}}_{G,G}^{-1/2}\big\{({{\boldsymbol}{\Sigma}}^{-1})_{G,G}\}^{1/2}$ is a $|G|\times |G|$ 
matrix of rank $|G|$ and the smallest 
singular value of ${{\boldsymbol}{\Omega}}$ is ${\lambda}_{\min}$. 
Thus, by (\ref{lm-1-2}) of Lemma \ref{lm-subGaussian} and the definition of $\omega_k'$ and $a_n$, 
\begin{eqnarray*}
{\mathbb{P}}\Big\{\|{{\boldsymbol}{P}}_G{{\rm \textbf{Q}}}_{G_k\setminus G}\|_S\le\omega_k'\ \forall k\le M,\ 
\|{{\boldsymbol}{P}}_G{{\rm \textbf{Q}}}_G^\perp\|_S\le \sqrt{1-a_n^2}\Big\}
\ge 1-\delta. 
\end{eqnarray*}
This yields (\ref{th-3-1}). It remains to proof 
$\max_{{G_k\setminus G \neq \emptyset}}M_k=O_{\mathbb{P}}(1)$ 
in view of Theorem \ref{th-opt}. To this end, we notice that due to the condition 
$|G_k| + g\log M \ll n$, (\ref{lm-1-1}) of Lemma \ref{lm-subGaussian} with ${{\rm \textbf{B}}}_1={{\rm \textbf{B}}}_2$
implies $\|{{\rm \textbf{X}}}_{A}^T{{\rm \textbf{X}}}_{A}/n - {{\boldsymbol}{\Sigma}}_{A,A}\|_S=o_{\mathbb{P}}(1)$ for both 
$A=G_k$ and $A=G_k\setminus G$ and all $k$ with $G_k\setminus G \neq \emptyset$,
so that $\max_{{G_k\setminus G \neq \emptyset}}M_k = o_{\mathbb{P}}(1)+O(1)$. 
\end{proof}

\subsection{Finding feasible solutions} \label{subsec:findSolution}
While (\ref{th-3-1}) of Theorems \ref{th:oraclerate} guarantees a feasible solution of (\ref{opt}), 
we discuss here penalized multivariate regression methods for finding feasible solutions of 
(\ref{opt}) and (\ref{general-opt}). 
As the only difference between (\ref{opt}) and (\ref{general-opt}) is the respective use of 
${{\rm \textbf{X}}}_G$ and ${\widetilde {{\rm \textbf{X}}}}_G$. 
We provide formulas here only for (\ref{opt}), with the understanding that 
formulas for (\ref{general-opt}) can be generated in the same way with 
${{\rm \textbf{X}}}_G$ replaced by ${\widetilde {{\rm \textbf{X}}}}_G$. 

In view (\ref{eq:mod1ForXj}), 
a general formulation of the penalized multivariate regression is 
{\begin{eqnarray}\label}{pen-multi-reg}
{{\widehat{{\boldsymbol}{\Gamma}}}}_{-G,G} = {\mathop{\rm arg\, min}}_{{{\boldsymbol}{\Gamma}}_{-G,G}}
\left\{\frac{1}{2n}\left\|{{\rm \textbf{X}}}_G - \sum_{G_k\not\subseteq G} 
{{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{\Gamma}}_{G_k\setminus G,G}\right\|_F^2
+ R({{\boldsymbol}{\Gamma}}_{-G,G})\right\}, 
\end{eqnarray}
where $\|\cdot\|_F$ is the Frobenius norm and $R({{\boldsymbol}{\Gamma}}_{-G,G})$ is a penalty function. Define 
{\begin{eqnarray}\label}{pen-multi-Z}
{{\rm \textbf{Z}}}_G = {{\rm \textbf{X}}}_G - \sum_{G_k\not\subseteq G}  
{{\rm \textbf{X}}}_{G_k\setminus G}{{\widehat{{\boldsymbol}{\Gamma}}}}_{G_k\setminus G,G},\quad 
{{\rm \textbf{P}}}_G = {{\rm \textbf{Z}}}_G({{\rm \textbf{Z}}}_G ^T  {{\rm \textbf{Z}}}_G)^{-1}{{\rm \textbf{Z}}}_G ^T. 
\end{eqnarray}
Our main interest is to find a feasible solutions of (\ref{opt}) and (\ref{general-opt}), 
not to estimate ${{\boldsymbol}{\Gamma}}_{-G,G}$. 

The following weighted group nuclear penalty matches the dual of the constraint 
in (\ref{opt}) and (\ref{general-opt}): 
{\begin{eqnarray}\label}{n-pen}
R({{\boldsymbol}{\Gamma}}_{-G,G}) = \sum_{G_k\not\subseteq G} \frac{\xi \omega_k''}{n^{1/2}}
\Big\|{{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{\Gamma}}_{G_k\setminus G,G}\Big\|_N. 
\end{eqnarray}
It follows from the KKT conditions for (\ref{pen-multi-reg}) and (\ref{n-pen}) that 
{\begin{eqnarray}\label}{dual}
\left\|{{\rm \textbf{Q}}}_{G_k\setminus G}{{\rm \textbf{Z}}}_G/\sqrt{n}\right\|_S \le \xi\omega_k''. 
\end{eqnarray}
If we set $\omega_k''=\omega_k$ in (\ref{n-pen}), then conditions (\ref{th-1-1}) 
and (\ref{th-2-1}) become 
{\begin{eqnarray}\label}{th-2-1a}
\xi \|({{\rm \textbf{Z}}}_G^T{{\rm \textbf{Z}}}_G/n)^{-1/2}\|_S \dfrac{s+g \log M}{n^{1/2}}  \to 0, 
\end{eqnarray}
provided $\max_{G_k\not\subseteq G}M_k=O(1)$ in the case of Theorem \ref{th-opt}.  
When the group sizes are not too large, one may even consider to 
replace the weighted group nuclear penalty with a weighted group Frobenius penalty 
\begin{eqnarray*}
R({{\boldsymbol}{\Gamma}}_{-G,G}) = \sum_{G_k\not\subseteq G} 
\frac{\xi \omega_k''}{n^{1/2}}\Big\|{{\rm \textbf{X}}}_{G_k\setminus G}{{\boldsymbol}{\Gamma}}_{G_k\setminus G,G}\Big\|_F
\end{eqnarray*}
as this can be conveniently computed using the group Lasso software. 

\begin{remark}\label{rem:betterrate}
Compared with existing sample size condition $n^{1/2} \gg \|{{\boldsymbol}{\beta}}\|_0\log p$ 
for statistical inference of a univariate parameter at $n^{-1/2}$ rate, 
the sample size conditions in (\ref{th-1-1}), (\ref{th-2-1})  (\ref{th-3-2}) and  (\ref{th-2-1a}) 
clearly demonstrate the benefit of group sparsity as in \cite{HZ10}. 
Moreover, the extra factor $\sqrt{|G|}$ is removed in a number of scenarios even 
in case of large group sizes. 
For example $|G|\lesssim \min_{G_k\not\subseteq G}\{|G_k|+\log(M/\delta)\}$ in (\ref{th-2-1}) 
and (\ref{th-3-2}), or 
$\xi \|({{\rm \textbf{Z}}}_G^T{{\rm \textbf{Z}}}_G/n)^{-1/2}\|_S\ll |G|^{1/2}$ in (\ref{th-2-1a}). 
\end{remark}

\section{Mixed Norm Consistency Results}\label{sec:consisres}
Using the group sparsity of the regression coefficient vector 
and sparse eigenvalue conditions on the design matrix, 
\cite{HZ10} provided $\ell_{2}$ oracle inequalities to show the benefits of the group Lasso over the Lasso. 
In this section we provide similar results on mixed weighted norms for both the group Lasso and 
the scaled group Lasso under different conditions on the design. 

\subsection{Assumptions for fixed design matrix}\label{subsec:designassump} In the Lasso problem, 
performance bounds of the estimator are derived based on various conditions on the design matrix, 
for example, restricted isometry property \citep{CandesT05}, 
the compatibility condition \citep{VDG07}, the sparse Riesz condition \citep{ZhangH08}, 
the restricted eigenvalue condition \citep{BRT09, Koltchinskii09}, 
and cone invertibility conditions \citep{YeZ10}. 
\cite{VDG09} showed that the compatibility condition is weaker than the restricted eigenvalues condition 
for the prediction and $\ell_1$ loss, 
while \cite{YeZ10} showed that both conditions can be weakened by cone invertibility conditions. 
In the following, we define grouped versions of such conditions. 

Let us first define a group wise mixed norm cone for $T\subset \{1\cdots, M\}$ and $\xi\ge 0$ as 
\begin{align}\label{eq:cone1}
{{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}},T) = \left\{{{\boldsymbol}{u}}:\sum_{j\in T^{c}}\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}} \leq \xi \sum_{j\in T}\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}\neq 0\right\}.
\end{align}
Following \cite{NR08} and \cite{Lounici2011}, the restricted eigenvalue (RE) is defined as 
\begin{align}\label{eq:RE}
{\rm RE}^{(G)}(\xi, {{\boldsymbol}{\omega}}, T) = \inf_{{\boldsymbol}{u}} \left\{\dfrac{{\|{{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}}{\sqrt{n}{\|{{{\boldsymbol}{u}}_{G_T}}\|_{{2}}}}:
{{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}}, T)\right\}.
\end{align}
For the weighted $\ell_{2,1}$ norm, 
the group-wise compatibility constant (CC) can be defined as 
\begin{align}\label{eq:CC}
{\rm CC}^{(G)}(\xi, {{\boldsymbol}{\omega}}, T) = \inf_{{\boldsymbol}{u}} \left\{\dfrac{{\|{{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}\sqrt{\sum_{j\in T}\omega^{2}_{j}}}{\sqrt{n}\sum_{j\in T}{\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}}}:{{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}}, T)\right\}.
\end{align}
We also introduce the notion of group wise cone invertibility factor and extend it to sign-restricted cone invertibility factor. The cone invertibility factor (CIF) is defined as
\begin{align}\label{eq:CIF}
{\rm CIF}^{(G)}_1(\xi,{{\boldsymbol}{\omega}},T) 
= \inf_{{{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}},T)}
\dfrac{\max_{j}\left[\omega^{-1}_{j}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}\right] 
\sum_{j\in T}\omega^{2}_{j}}
{n \sum_{j\in T}{\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}}}. 
\end{align}
Now we define the sign-restricted cone as 
\begin{align}\label{eq:cone2}
{{\mathscr C}}^{(G)}_{-}(\xi,{{\boldsymbol}{\omega}}, T) = \left\{{{\boldsymbol}{u}}: {{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}}, T),\quad  {{{\boldsymbol}{u}}_{G_{j}}}^{T}{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{u}} \leq 0\ \forall j \in T^{c}\right\}, 
\end{align}
and the group-wise sign-restricted cone invertibility factor (SCIF) as
\begin{align}\label{eq:SCIF}
{{\rm SCIF}}^{(G)}_1(\xi,{{\boldsymbol}{\omega}},T) = \inf_{{{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}_{-}(\xi,{{\boldsymbol}{\omega}},T)}
\dfrac{\max_{j}\left[\omega^{-1}_{j}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}\right] 
\sum_{j\in T}\omega^{2}_{j}}
{n \sum_{j\in T}{\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}}}. 
\end{align}
It follows from ${\|{{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}^2\big/
\max_j(\omega_j^{-1}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}) \le \sum_j \omega_j{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}
\le (1+\xi)\sum_{j\in T} \omega_j{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}$ and the Cauchy-Schwarz inequality that 
\begin{align}\label{eq:constineq}
({\rm RE}^{(G)}(\xi, {{\boldsymbol}{\omega}}, T))^{2} \leq ({\rm CC}^{(G)}(\xi, {{\boldsymbol}{\omega}}, T))^{2} 
\leq (\xi+1)\ {\rm CIF}^{(G)}_1(\xi,{{\boldsymbol}{\omega}},T). 
\end{align}
Moreover, the SCIF is always no smaller than the CIF. 
Thus, following (\ref{eq:constineq}), the restricted eigenvalue condition ${\rm RE}^{(G)}(\xi, {{\boldsymbol}{\omega}}, T)>\kappa_0$ 
implies that all the other quantities are bounded from below by $\kappa_0$. 
In the following we derive the mixed norm consistency results for the non-scaled group Lasso problem in 
Theorem \ref{th:betal12-ns} and extend it to the scaled group Lasso in Theorem \ref{th:betagrp-scaled}. 
We establish these results under the weakest assumption on the SCIF. 

The SCIF in (\ref{eq:SCIF}) will be used to derive oracle inequalities for the prediction and weighted $\ell_{2,1}$ 
loss. For the $\ell_2$ loss, we define the SCIF as 
{\begin{eqnarray}\label}{SCIF_2}
{\rm SCIF}^{(G)}_2(\xi,{{\boldsymbol}{\omega}},T) 
= \inf_{{{\boldsymbol}{u}} \in {{\mathscr C}}^{(G)}_-(\xi,{{\boldsymbol}{\omega}},T)}
\dfrac{\max_{j}\left[\omega^{-1}_{j}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{u}}}\|_{{2}}}\right] 
(\sum_{j\in T}\omega^{2}_{j})^{1/2}}
{n {\|{{\boldsymbol}{u}}\|_{{2}}}/(1+\xi)}. 
\end{eqnarray}
We may also use the $\ell_2$ version of the CIF, denoted by ${\rm CIF}^{(G)}_2(\xi,{{\boldsymbol}{\omega}},T)$ and 
defined by replacing the sign-restricted cone 
${{\mathscr C}}^{(G)}_-(\xi,{{\boldsymbol}{\omega}},T)$ with the cone in (\ref{eq:cone1}).  
It follows from a shifting inequality \citep{CaiWX10,YeZ10} that 
\begin{eqnarray*}
 \omega_{\min}\sqrt{s}\|{{\boldsymbol}{u}}\|_2\sum_{j\in {S^{*}}}{\omega_{j}{\|{{{\boldsymbol}{u}}_{G_{j}}}\|_{{2}}}} 
&\le &  3\Big(\sum_{j\in {S^{*}}}\omega^{2}_{j}\Big)
\max_{|T|\le s}{\|{{{\boldsymbol}{u}}_{G_T}}\|_{{2}}}^2
\end{eqnarray*}
for ${{\boldsymbol}{u}}\in {{\mathscr C}}^{(G)}(3,{{\boldsymbol}{\omega}},{S^{*}})$ and $|{S^{*}}|\le s$, 
where $\omega_{\min}=\min_{1\le j\le M} \omega_{j}$. Thus,  
\begin{eqnarray*}
\dfrac{\big(\sum_{j \in {S^{*}}}\omega^{2}_{j}\big)^{1/2}}{{{\rm SCIF}}^{(G)}_2(3, {{\boldsymbol}{\omega}}, {S^{*}})}
\le \dfrac{\big(\sum_{j \in {S^{*}}}\omega^{2}_{j}\big)^{1/2}}{\hbox{CIF}^{(G)}_2(3, {{\boldsymbol}{\omega}}, {S^{*}})}
\le \frac{3\sum_{j\in S^*}\omega_j^2/(\omega_{\min}\sqrt{s})}
{\min_{|T|\le s}({\rm RE}^{(G)}(3, {{\boldsymbol}{\omega}}, T))^{2}}. 
\end{eqnarray*}
Again, the cone invertibility factors provide error bounds of sharper form than (\ref{eq:RE}), 
in view of Theorem \ref{th:betal12-ns} below and Theorem 3.1 of \cite{Lounici2011}. 

\subsection{Mixed norm consistency for group Lasso}

\begin{theorem}\label{th:betal12-ns}
Let ${{\widehat{{\boldsymbol}{\beta}}}}={{\widehat{{\boldsymbol}{\beta}}}}({{\boldsymbol}{\omega}})$ be a solution of (\ref{eq:grplsoopt-ns}) 
with data $({{\rm \textbf{X}}},{{\boldsymbol}{y}})$ and 
${{\boldsymbol}{\beta}}^*$ be a vector with ${\hbox{supp}}({{{\boldsymbol}{\beta}}^{*}}) \subset G_{S^{*}}$ for some 
${S^{*}} \subset \{1,\cdots, M\}$. 
Let $\xi>1$ and define 
\begin{align}
{{\cal E}} = \left\{\max_{1\le j\le M}\frac{{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}})}\|_{{2}}}}
{\omega_{j}n} \leq \dfrac{\xi-1}{\xi+1}\ \right\}.
\end{align}
Then in the event ${{\cal E}}$, we have
{\begin{eqnarray}\label}{gl-pred}
{\|{{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}^{*}}\|_{{2}}}^2/n 
\le \frac{\{2\xi/(\xi+1)\}^2\sum_{j \in {S^{*}}}\omega^{2}_{j}}{{{\rm SCIF}}^{(G)}_1(\xi, {{\boldsymbol}{\omega}}, {S^{*}})},  
\end{eqnarray}
and
\begin{align}\label{eq:betal12-ns}
\bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{\omega_{j}}\bigg)^q\bigg\}^{1/q}
\leq \dfrac{2\xi\big(\sum_{j \in {S^{*}}}\omega^{2}_{j}\big)^{1/q}}{{{\rm SCIF}}^{(G)}_q(\xi, {{\boldsymbol}{\omega}}, {S^{*}})},
\quad q=1,2. 
\end{align}
Moreover, if the regression model in (\ref{eq:mod1}) holds with Gaussian error and 
a design matrix ${{\rm \textbf{X}}}$ satisfying $\max_{j\le M}\|{{\rm \textbf{X}}}_{G_j}/\sqrt{n}\|_{S}\le 1$, then
\begin{align}\label{eq:Eprob-ns}
{\mathbb{P}}({{\cal E}}) > 1-\delta, 
\end{align}
when $\omega_{j} \ge A \sigma \left\{\sqrt{d_{j}/n} + \sqrt{(2/n)\log (M/\delta)}\right\}$ 
for some $0<\delta <1$ and $A\geq (\xi+1)/(\xi-1)$. 
\end{theorem}

\begin{remark}
From Theorem \ref{th:betal12-ns},  
$\max\{\|{{\widehat{{\boldsymbol}{\beta}}}}-{{\boldsymbol}{\beta}}^*\|_2^2,\sum^{M}_{j=1}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}\} 
= {{\cal O}}((s+g\log M)/n)$ when the SCIF can be treated as constant. 
This shows the benefit of the group Lasso compared with the Lasso as in \cite{HZ10}. 
The same convergence rate can be derived from the $\ell_{2}$ consistency result in \cite{HZ10}. 
Their result however, is derived under a sparse eigenvalue condition on the design matrix ${{\rm \textbf{X}}}$. \end{remark}

\begin{proof}[Proof of Theorem \ref{th:betal12-ns}] 
The KKT conditions for the group Lasso asserts that 
\begin{align}\label{eq:kkt-ns}
\begin{array}{lc}
\dfrac{1}{n}{{\rm \textbf{X}}}_{G_{j}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}) = \omega_{j}{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}/{\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}\|_{{2}}}, & {{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} \neq {{\boldsymbol}{0}},\\[0.3cm]
\dfrac{1}{n}{\|{{{\rm \textbf{X}}}_{G_{j}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}})}\|_{{2}}} \leq \omega_{j},& {{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}={{\boldsymbol}{0}}.
\end{array}
\end{align}
It follows that in the event ${{\cal E}}$
\begin{align}\label{prf11}
\omega^{-1}_{j}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}({{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}})}\|_{{2}}}/n 
\leq 1 + {\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}}\|_{{2}}}/(n\omega_{j}) \leq  2\xi/(\xi+1).
\end{align}
Now take any ${{\boldsymbol}{w}} \in {{\mathbb{R}}}^{p}$. Pre-multiplying by $({{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{w}}_{G_{j}})^{T}$ 
on both sides in (\ref{eq:kkt-ns}), we have
\begin{eqnarray*}
({{\widehat{{\boldsymbol}{\beta}}}}-{{\boldsymbol}{w}})^{T}{{\rm \textbf{X}}}^{T}({{\boldsymbol}{\varepsilon}}-{{\rm \textbf{X}}}({{\widehat{{\boldsymbol}{\beta}}}}-{{{\boldsymbol}{\beta}}^{*}}))/n 
\geq \sum^{M}_{j=1}\omega_{j}{\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}\|_{{2}}} 
- \sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}.
\end{eqnarray*}
Rearranging we get, 
\begin{align*}
& \dfrac{1}{n}({{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}})^{T}({{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{\boldsymbol}{w}})
+\sum_{j\not\in S^*}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} - {{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}\\
 \leq & \sum_{j\not\in S^*}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} - {{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}  
 - \sum^{M}_{j=1}\omega_{j}{\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}\|_{{2}}} 
 + \sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} + ({{\widehat{{\boldsymbol}{\beta}}}}-{{\boldsymbol}{w}})^{T}{{\rm \textbf{X}}}^{T}{{\boldsymbol}{\varepsilon}}/n \\
 \leq & \sum_{j\in S^*}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} - {{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} 
 + 2\sum_{j\not\in S^*}\omega_{j}{\|{{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} + \sum^{M}_{j=1} {\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}-{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} {\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}}\|_{{2}}}/n\\
 \leq & \sum_{j\in S^*}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} - {{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} 
 + 2\sum_{j\not\in S^*}\omega_{j}{\|{{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} 
 +\frac{\xi-1}{\xi+1}\sum^{M}_{j=1} \omega_j {\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}-{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}.
\end{align*}
It follows that
\begin{align*}
& \dfrac{1}{n}({{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}})^{T}({{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{\boldsymbol}{w}})
+\frac{2}{\xi+1}\sum_{j\not\in S^*}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}} - {{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}\\
 \leq & \frac{2\xi}{\xi+1}\sum_{j\in {S^{*}}}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}} 
 + 2\sum_{j \notin {S^{*}}}\omega_{j}{\|{{{\boldsymbol}{w}}_{G_{j}}}\|_{{2}}}. 
\end{align*}
Putting ${{\boldsymbol}{w}}={{{\boldsymbol}{\beta}}^{*}}$ and ${{\boldsymbol}{h}}= {{\widehat{{\boldsymbol}{\beta}}}}-{{{\boldsymbol}{\beta}}^{*}}$, we have 
\begin{align*}
(1+\xi){\|{{{\rm \textbf{X}}}{{\boldsymbol}{h}}}\|_{{2}}}^{2}/n + 2\sum_{j \notin {S^{*}}} \omega_{j} {\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}} \leq 2\xi\sum_{j \in {S^{*}}} \omega_{j} {\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}},
\end{align*}
whence it follows that ${{\boldsymbol}{h}} \in {{\mathscr C}}^{(G)}(\xi,{{\boldsymbol}{\omega}}, {S^{*}})$. Moreover, from KKT conditions (\ref{eq:kkt-ns}), pre-multiplying both sides by ${{\boldsymbol}{h}}_{G_{j}}$ for $j \notin {S^{*}}$, we have in the event ${{\cal E}}$,
\begin{align*}
{{\boldsymbol}{h}}_{G_{j}}{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{h}}/n \leq {\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}\left({\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}}\|_{{2}}}/n- \omega_{j}\right) \leq 0.
\end{align*}
Hence ${{\boldsymbol}{h}} \in {{\mathscr C}}^{(G)}_-(\xi, {{\boldsymbol}{\omega}}, {S^{*}})$. 
Consequently, by (\ref{eq:SCIF}) and (\ref{prf11}), 
\begin{eqnarray*}
\frac{(1+\xi){\|{{{\rm \textbf{X}}}{{\boldsymbol}{h}}}\|_{{2}}}^{2}/n}
{2\xi\sum_{j \in {S^{*}}}\omega^{2}_{j}}
\le \frac{\sum_{j\in {S^{*}}}\omega_{j}{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{\sum_{j \in {S^{*}}}\omega^{2}_{j}}
\leq \frac{\max_{j}\omega^{-1}_{j}{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\rm \textbf{X}}}{{\boldsymbol}{h}}}\|_{{2}}}}{n\,{{\rm SCIF}}^{(G)}_1(\xi, {{\boldsymbol}{\omega}}, {S^{*}})}
\le \frac{2\xi/(\xi+1)}{{{\rm SCIF}}^{(G)}_1(\xi, {{\boldsymbol}{\omega}}, {S^{*}})}. 
\end{eqnarray*}
The bound for the weighted $\ell_{2,1}$ loss follows as 
$\sum_{j=1}^M  \omega_{j} {\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}} \leq (1+\xi)
\sum_{j \in {S^{*}}} \omega_{j} {\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}$. 
The proof for the $\ell_2$ loss is nearly identical and thus omitted. 

Finally, we prove (\ref{eq:Eprob-ns}).  As ${{\boldsymbol}{\varepsilon}} \sim {\mathsf{N}}_{n}({{\boldsymbol}{0}}, \sigma^{2}{{\rm \textbf{I}}}_{n})$, 
it follows from 
the Gaussian concentration inequality that for any $0<\delta <1$, with probability at least $1-\delta$,
\[
{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}}\|_{{2}}}/(\sigma {\|{{{\rm \textbf{X}}}_{G_{j}}}\|_{{S}}})
\le {\|{{{\boldsymbol}{\varepsilon}}/\sigma}\|_{{2}}}
\leq \sqrt{n} \left\{\sqrt{d_{j}} + \sqrt{2\log(1/\delta)}\right\}.
\]
The result in (\ref{eq:Eprob-ns}) follows by an application of union bound.
\end{proof}

\subsection{Scaled Group Lasso}\label{subsec:scaledgrplasso}
In the optimization problem (\ref{eq:grplsoopt-ns}), scale-invariance considerations have not been taken into account. Usually the individual penalty level $\omega_{j}$'s could be chosen proportional to the scale $\sigma$ as a remedy. This issue has been discussed and studied, as pertaining to the Lasso problem, in several literature. See  \cite{Huber11}, \cite{SBG10}, \cite{Antoniacomm10}, \cite{SZcomm10}, 
\cite{BelloniCW11}, \cite{Sun2012}, \cite{SunZ13} and many more. 
Following \cite{Antoniacomm10} we define an optimization problem,
\begin{align}\label{eq:grplsoopt-s}
& ({{\widehat{{\boldsymbol}{\beta}}}},{\widehat{\sigma}}) = {\mathop{\rm arg\, min}}_{{{\boldsymbol}{\beta}},\sigma}\ {{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma), \\
& \text{where }\ {{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma)  = \dfrac{{\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}}\|_{{2}}}^{2}}{2n\sigma} + \dfrac{(1-a)\sigma}{2} + \sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{\beta}}_{G_j}}\|_{{2}}}.\label{eq:grplsoopt-s2}
\end{align}
Following \cite{SZcomm10} we define an iterative algorithm for the estimation of 
$\{{{\boldsymbol}{\beta}},\sigma\}$, 
\begin{align}\label{algo:iter}
\begin{array}{ccl}
{\widehat{\sigma}}^{(k+1)} & \leftarrow & {\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}^{(k)}}\|_{{2}}}/ \sqrt{(1-a)n},\\[0.2cm]
{{\boldsymbol}{\omega}}' & \leftarrow & {\widehat{\sigma}}^{(k+1)}{{\boldsymbol}{\omega}},\\[0.2cm]
{{\widehat{{\boldsymbol}{\beta}}}}^{(k+1)} & \leftarrow &  {\mathop{\rm arg\, min}}_{{\boldsymbol}{\beta}} {{\cal L}}_{{{\boldsymbol}{\omega}}'}({{\boldsymbol}{\beta}}),
\end{array}
\end{align}
where ${{\cal L}}_{{{\boldsymbol}{\omega}}'}({{\boldsymbol}{\beta}})$ was as defined in (\ref{eq:grplsoopt-ns}). 
Due to the convexity of the joint loss function ${{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma)$, 
the solution of (\ref{eq:grplsoopt-s}) and the limit of (\ref{algo:iter}) give the same 
estimator, which we call scaled group Lasso. 
The constant $a\geq 0$ provides control over the degrees of freedom adjustments. 
In practice, for scaled group Lasso in the $p>n$ setting, we take $a=0$ for all subsequent discussions. It is clear that that with $a=0$ and ${{\boldsymbol}{\omega}}'={\widehat{\sigma}}{{\boldsymbol}{\omega}}$, one has ${\widehat{\sigma}}{{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},{\widehat{\sigma}}) = {{\cal L}}_{{{\boldsymbol}{\omega}}'}({{\boldsymbol}{\beta}}) + {\widehat{\sigma}}^{2}/2$. The algorithm in (\ref{algo:iter}) suggests a profile optimization approach. The following lemma is similar to Proposition 1 in \cite{Sun2012} and characterizes the solution via partial derivative of the profile objective.

\begin{lemma}\label{lem:partialdelsig}
Let ${{\widehat{{\boldsymbol}{\beta}}}}({{\boldsymbol}{\omega}})$ denote a solution of the optimization problem in (\ref{eq:grplsoopt-ns}). 
Then, ${{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}})$ is a minimizer of ${{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma)$ in 
(\ref{eq:grplsoopt-s2}) for given $\sigma$, and the profile loss function 
${{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}}),\sigma)$ is convex and continuously differentiable in $\sigma$ with 
\begin{align}
\dfrac{\partial}{{\partial}\sigma} {{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}}),\sigma) = \dfrac{1}{2} -\dfrac{ {\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}})}\|_{{2}}}^{2}}{2n\sigma^{2}}. 
\end{align} 
Moreover, the algorithm in (\ref{algo:iter}) converges to a minimizer $({{\widehat{{\boldsymbol}{\beta}}}},{\widehat{\sigma}})$ 
in (\ref{eq:grplsoopt-s}) satisfying ${{\widehat{{\boldsymbol}{\beta}}}} = {{\widehat{{\boldsymbol}{\beta}}}}({\widehat{\sigma}}{{\boldsymbol}{\omega}})$, and 
the estimator ${{\widehat{{\boldsymbol}{\beta}}}}$ and ${\widehat{\sigma}}$ are scale equivariant in ${{\boldsymbol}{y}}$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:partialdelsig}] 
For $\eta \ge 0$ define 
\begin{eqnarray*}
{{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma,\eta) 
= \frac{\|{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}\|_2^2}{2n\sigma} + \frac{\sigma}{2}+\sum_{j=1}^M\omega_j\|{{\boldsymbol}{\beta}}_{G_j}\|_2^{1+\eta}
+ \frac{\eta\sigma^2}{2}
\end{eqnarray*}
and ${{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta) = {\mathop{\rm arg\, min}}_{{\boldsymbol}{\beta}}\ {{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma,\eta)$. 
As ${{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma,\eta)$ is convex in $({{\boldsymbol}{\beta}},\sigma)$, the profile loss 
${{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta),\sigma,\eta)$ is convex in $\sigma$ for all $\eta\ge 0$. 
Note that for $\eta>0$
\begin{eqnarray*}
&& \dfrac{\partial}{{\partial}\sigma}{{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta),\sigma,\eta) 
\cr &=& \left\{\dfrac{\partial}{{\partial}{{\boldsymbol}{\theta}}}{{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\theta}},\sigma,\eta)\Big{|}
_{{{\boldsymbol}{\theta}}={{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta)}\right\}^T
\dfrac{{\partial}{{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta)}{{\partial}\sigma} + \dfrac{\partial}{{\partial} t}{{\cal L}}_{{\boldsymbol}{\omega}}
({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}}),t,\eta)\Big|_{t=\sigma}
\cr &=& 1/2 - \|{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta)\|_2^2/(2n\sigma^2)+\eta\sigma
\end{eqnarray*}
as all derivatives involved are continuous. Moreover, as 
${{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma)={{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma,0)$ is strictly convex in ${{\rm \textbf{X}}}{{\boldsymbol}{\beta}}$, 
\begin{eqnarray*}
\lim_{\eta\to 0+}\dfrac{\partial}{{\partial}\sigma}{{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta),\sigma,\eta) 
\to 1/2 - \|{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}})\|_2^2/(2n\sigma^2). 
\end{eqnarray*}
Consequently, 
\begin{eqnarray*}
{{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma_2{{\boldsymbol}{\omega}}),\sigma_2)
- {{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma_1{{\boldsymbol}{\omega}}),\sigma_1)
&=& \lim_{\eta\to 0+}\int_{\sigma_1}^{\sigma_2}
\Big\{\dfrac{\partial}{{\partial}\sigma}{{\cal L}}_{{\boldsymbol}{\omega}}({{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}},\eta),\sigma,\eta)\Big\} d\sigma
\cr &=& \int_{\sigma_1}^{\sigma_2}\Big\{1/2 - \|{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(\sigma{{\boldsymbol}{\omega}})\|_2^2/(2n\sigma^2)\Big\} d\sigma. 
\end{eqnarray*}
All other claims follow from the joint convexity of ${{\cal L}}_{{\boldsymbol}{\omega}}({{\boldsymbol}{\beta}},\sigma)$ and 
the strict convexity of the loss function in ${{\rm \textbf{X}}}{{\boldsymbol}{\beta}}$.
\end{proof}

We now present the consistency theorem for scaled group Lasso which extends Theorem \ref{th:betal12-ns} by providing convergence results for the estimate of scale.  Define 
\begin{eqnarray*}
\mu({{\boldsymbol}{\omega}}, \xi) =  \frac{2\xi \sum_{j \in {S^{*}}}\omega^{2}_{j}}{{{\rm SCIF}}^{(G)}_1(\xi, {{\boldsymbol}{\omega}},{S^{*}})},\quad  
\tau_- = \frac{2\mu({{\boldsymbol}{\omega}},\xi)(\xi-1)}{\xi+1},\quad 
\tau_+ = \frac{\tau_-}{2} + \mu({{\boldsymbol}{\omega}},\xi). 
\end{eqnarray*}
Let $m_{d,n}$ be the median of the beta$(d/2,n/2-d/2)$ distribution and define 
\begin{eqnarray*}
\omega_{*,j} \ge \sqrt{m_{d_j,n}} + \sqrt{\frac{2\log(M/\delta)}{(n\vee 2)-3/2}},\ 
A_* = \frac{(\xi+1)/(\xi-1)}{\sqrt{1- 2\mu({{\boldsymbol}{\omega}}_*,\xi)(\xi+1)/(\xi-1)}}, 
\end{eqnarray*}
where ${{\boldsymbol}{\omega}}_*$ is the vector with elements $\omega_{*,j}$. We will show that 
$ \sqrt{m_{d_j,n}} \le  (d_j/n)^{1/2}+n^{-1/2}$ in the proof of the following theorem.  

\begin{theorem}\label{th:betagrp-scaled}
Let $\{{{\widehat{{\boldsymbol}{\beta}}}},{\widehat{\sigma}}\}$ be a solution of the optimization problem (\ref{eq:grplsoopt-s2}) 
with data $({{\rm \textbf{X}}},{{\boldsymbol}{y}})$ and  
${{\boldsymbol}{\beta}}^*$ be a vector with ${\hbox{supp}}({{{\boldsymbol}{\beta}}^{*}}) \subset G_{S^{*}}$ for some ${S^{*}} \subset \{1,\cdots, M\}$. 
Let $\xi>1$. \\
(i) Suppose  ${{\rm SCIF}}^{(G)}_1(\xi,{{\boldsymbol}{\omega}},S^{*}) >0$ in (\ref{eq:SCIF}) and $\tau_+<1$. 
Define the following event
\begin{align}
{{\cal E}} = \left\{\max_{1\le j\le M}\frac{{\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}})}\|_{{2}}}}{\omega_{j} n{{\sigma^{*}}}/\sqrt{1+\tau_-}} 
< \dfrac{\xi-1}{\xi+1}\right\},
\end{align}
where ${{\sigma^{*}}}={\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}}}\|_{{2}}}/\sqrt{n}$ is the oracle noise level. 
Then in the event ${{\cal E}}$, we have
{\begin{eqnarray}\label}{sgl-sigma}
\frac{{\sigma^{*}}}{\sqrt{1+\tau_-}}\le {\widehat{\sigma}} \le \frac{{\sigma^{*}}}{\sqrt{1-\tau_+}},
\end{eqnarray}
{\begin{eqnarray}\label}{sgl-pred}
{\|{{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}^{*}}\|_{{2}}}^2/n 
\le \frac{({{\sigma^{*}}})^2\{2\xi/(\xi+1)\}^2\sum_{j \in {S^{*}}}\omega^{2}_{j}}
{(1-\tau_+){{\rm SCIF}}^{(G)}_1(\xi, {{\boldsymbol}{\omega}}, {S^{*}})},  
\end{eqnarray}
and
\begin{align}\label{sgl-est}
\bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{\omega_{j}}\bigg)^q\bigg\}^{1/q}
\leq \dfrac{2{{\sigma^{*}}} \xi\big(\sum_{j \in {S^{*}}}\omega^{2}_{j}\big)^{1/q}}
{\sqrt{1-\tau_+}{{\rm SCIF}}^{(G)}_q(\xi, {{\boldsymbol}{\omega}}, {S^{*}})},
\quad q=1,2. 
\end{align}
(ii) Suppose the regression model in (\ref{eq:mod1}) holds 
with Gaussian error and a design matrix satisfying $\max_{j\le M}\|{{\rm \textbf{X}}}_{G_j}/\sqrt{n}\|_{S}\le 1$. 
If $\sqrt{n}\mu({{\boldsymbol}{\omega}},\xi) \rightarrow 0$, then
\begin{align}\label{eq:teststatsigma}
\sqrt{n}\left({\widehat{\sigma}}/{\sigma} - 1\right) {\stackrel{{\rm D}}{\longrightarrow}} {\mathsf{N}}(0,1/2).
\end{align}
Moreover, if $\omega_{j} = A\omega_{*,j} $ with $A\ge A_*$, then 
\begin{align}
{\mathbb{P}}({{\cal E}}) \geq 1-\delta. 
\end{align}
\end{theorem}

\begin{corollary} \label{cor:grpsparse-ns} Consider the setup of Theorem \ref{th:betagrp-scaled} (ii). 
Assume that the design matrix ${{\rm \textbf{X}}}$ satisfies the following sign restricted cone invertibility condition: 
${{\rm SCIF}}^{(G)}_1(\xi, S^{*})>c>0$ for some fixed $c>0$. Let $0<\delta <1$ be a fixed small constant and take 
\[\omega_{j} = A\left\{\sqrt{d_{j}/n} + \sqrt{(2/n)\log(M/\delta)}\right\} \text{ with a constant } 
A > (\xi+1)/(\xi-1). \] 
Then, for a certain fixed constant $C>0$ and with probability at least $1-\delta$
{\begin{eqnarray}\label}{cor-1-1}
&& \max\left\{\Big|1-\dfrac{\widehat{\sigma}}{{\sigma^{*}}}\Big|,\frac{\|{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}^*\|_2^2}{\sigma^2},
\frac{\|{{\widehat{{\boldsymbol}{\beta}}}}-{{\boldsymbol}{\beta}}^*\|_2^2}{\sigma^2}, 
\sum^{M}_{j=1}\frac{{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}{\sigma/\omega_{j}},
\sum^{M}_{j=1}\frac{{\|{{{\rm \textbf{X}}}_{G_j}({{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*})}\|_{{2}}}}{n^{1/2}\sigma/\omega_{j}}\right\} 
\cr && \leq C\left\{|G_{S^{*}}|+|{S^{*}}|\log(M/\delta)\right\}\big/n. 
\end{eqnarray}
\end{corollary}

Corollary \ref{cor:grpsparse-ns} touches upon the mixed prediction loss 
$\sum^{M}_{j=1}\omega_{j}{\|{{{\rm \textbf{X}}}_{G_j}{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\rm \textbf{X}}}_{G_j}{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}$
the first time in this section. The reason for this omission is two fold. Firstly, 
\begin{eqnarray*}
\bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{\rm \textbf{X}}}_{G_j}{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\rm \textbf{X}}}_{G_j}{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{n^{1/2}\omega_{j}}\bigg)^q\bigg\}^{1/q}
\le \max_{j\le M}\left\|\frac{{{\rm \textbf{X}}}_{G_j}}{\sqrt{n}}\right\|_S 
\bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{\omega_{j}}\bigg)^q\bigg\}^{1/q}
\end{eqnarray*}
so that (\ref{eq:betal12-ns}) and (\ref{sgl-est}) automatically generate the corresponding bounds 
for the mixed prediction error under the respective conditions. Secondly, upper bounds for the 
mixed prediction loss can be obtained by reparametrization within the given group structure. 
The following corollary provides details of such reparametrization in the case of scaled group Lasso. 

\begin{corollary} \label{cor-2} 
Let ${{\rm \textbf{X}}}_{G_j} = {{\rm \textbf{U}}}_{G_j}{{\boldsymbol}{\Lambda}}_{G_j}{{\rm \textbf{V}}}_{G_j}^T$ be the SVD of 
${{\rm \textbf{X}}}_{G_j}$ with ${{\boldsymbol}{\Lambda}}_{G_j}\in{\mathbb{R}}^{|G_j|\times |G_j|}$. 
Define ${{\boldsymbol}{b}}$ by ${{\boldsymbol}{b}}_{G_j}={{\boldsymbol}{\Lambda}}_{G_j}{{\rm \textbf{V}}}_{G_j}^T{{\boldsymbol}{\beta}}_{G_j}$ 
and ${{\rm \textbf{U}}}$ by ${{\rm \textbf{U}}}{{\boldsymbol}{b}} = \sum_{j=1}^M{{\rm \textbf{U}}}_{G_j}{{\boldsymbol}{b}}_{G_j}$. 
Then, 
\begin{eqnarray*}
\bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{\rm \textbf{X}}}_{G_j}{{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}-{{\rm \textbf{X}}}_{G_j}{{\boldsymbol}{\beta}}_{G_{j}}^{*}}\|_{{2}}}}
{\omega_{j}}\bigg)^q\bigg\}^{1/q}
&=& \bigg\{\sum^{M}_{j=1}\omega_{j}^2\bigg(\frac{{\|{{{\widehat{{\boldsymbol}{b}}}}_{G_j}-{{\boldsymbol}{b}}_{G_{j}}^{*}}\|_{{2}}}}
{\omega_{j}}\bigg)^q\bigg\}^{1/q}
\cr  &\leq& \dfrac{2{{\sigma^{*}}} \xi\big(\sum_{j \in {S^{*}}}\omega^{2}_{j}\big)^{1/q}}
{\sqrt{1-\tau_+}{{\rm SCIF}}^{(G)}_q(\xi, {{\boldsymbol}{\omega}}, {S^{*}})},
\quad q=1,2,
\end{eqnarray*}
when the conditions for (\ref{sgl-est}), including the definition of the estimator and the SCIF, 
hold with ${{\rm \textbf{X}}}$, ${{\boldsymbol}{\beta}}$ and ${{\boldsymbol}{\beta}}^*$ replaced by ${{\rm \textbf{U}}}$, 
${{\boldsymbol}{b}}$ and ${{\boldsymbol}{b}}^*$ respectively. 
\end{corollary}

\begin{remark}
Corollary \ref{cor:grpsparse-ns} can be viewed as an extension of the main results of \cite{HZ10} to 
the scaled group Lasso although here the regularity condition of the design is of a weaker form 
and smaller penalty levels are allowed.  
\end{remark}

\begin{proof}[Proof of Theorem \ref{th:betagrp-scaled}]
We follow the proof in \cite{Sun2012}.
Let $t\ge \sigma^*/\sqrt{1+\tau_-}$ and ${{\boldsymbol}{h}}_{G_{j}}={{{\widehat{{\boldsymbol}{\beta}}}}_{G_{j}}}(t{{\boldsymbol}{\omega}})-{{{\boldsymbol}{\beta}}^{*}}_{G_{j}}$. 
As the oracle noise level is defined as 
$({{\sigma^{*}}})^2={\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}}}\|_{{2}}}^{2}/n$, we have 
{\begin{eqnarray}\label}{pf-1a}
({{\sigma^{*}}})^2 - {\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})}\|_{{2}}}^{2}/n 
= ({{\rm \textbf{X}}}{{\boldsymbol}{h}})^{T}(2{{\boldsymbol}{\varepsilon}}-{{\rm \textbf{X}}}{{\boldsymbol}{h}})/n = ({{\rm \textbf{X}}}{{\boldsymbol}{h}})^{T}({{\boldsymbol}{\varepsilon}} + {{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}}))/n.
\end{eqnarray}
Suppose ${{\cal E}}$ happens so that ${\|{{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}}\|_{{2}}}/n\le t\omega_j(\xi-1)/(\xi+1)$. 
It follows that 
\begin{eqnarray*}
\left|({{\rm \textbf{X}}}{{\boldsymbol}{h}})^{T}{{\boldsymbol}{\varepsilon}}/n\right| = \left|\sum^{M}_{j=1}{{\boldsymbol}{h}}^{T}_{G_{j}}{{{\rm \textbf{X}}}_{G_{j}}}^{T}{{\boldsymbol}{\varepsilon}}/n\right| 
\le \frac{\xi-1}{\xi+1}\sum^{M}_{j=1}t\omega_{j}{\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}.
\end{eqnarray*}
Moreover, the KKT condition implies 
\[
\left|{{\boldsymbol}{h}}^{T}{{\rm \textbf{X}}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}}))/n\right|  
=\left|\sum^{M}_{j=1}{{\boldsymbol}{h}}_{G_{j}}^{T}{{{\rm \textbf{X}}}_{G_{j}}}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}}))/n\right|
\leq \sum^{M}_{j=1}t\omega_{j}{\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}.
\]
As $({{\rm \textbf{X}}}{{\boldsymbol}{h}})^{T}(2{{\boldsymbol}{\varepsilon}}-{{\rm \textbf{X}}}{{\boldsymbol}{h}})/n\le 2({{\rm \textbf{X}}}{{\boldsymbol}{h}})^{T}{{\boldsymbol}{\varepsilon}}/n$, 
inserting these inequalities to (\ref{pf-1a}) yields
\begin{align*}
-\left(\frac{\xi-1}{\xi+1} + 1\right) \sum^{M}_{j=1}t\omega_{j}{\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}
\leq {{\sigma^{*}}}^{2} - {\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})}\|_{{2}}}^{2}/n
\leq 2 \frac{\xi-1}{\xi+1}\sum^{M}_{j=1}t\omega_{j}{\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}}.
\end{align*}
A rescaled version ${{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})$ can be written as 
\begin{eqnarray*}
\frac{{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})}{t} = {\mathop{\rm arg\, min}}_{{\boldsymbol}{b}}\left\{\dfrac{{\|{{{\boldsymbol}{y}}/t-{{\rm \textbf{X}}}{{\boldsymbol}{b}}}\|_{{2}}}^{2}}{2n} 
 + \sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{b}}_{G_j}}\|_{{2}}} \right\}
\end{eqnarray*}
as the group Lasso estimator with target ${{{\boldsymbol}{\beta}}^{*}}/t$ and noise vector ${{\boldsymbol}{\varepsilon}}/t$. 
As $t\ge \sigma^*/\sqrt{1+\tau_-}$, the condition of Theorem \ref{th:betal12-ns} is 
satisfied with the rescaled noise ${{\boldsymbol}{\varepsilon}}/t$, so that  
\[
t^{-1}\sum^{M}_{j=1}\omega_{j}{\|{{{\boldsymbol}{h}}_{G_{j}}}\|_{{2}}} = \sum^{M}_{j=1} \omega_{j}{\|{{{\widehat{{\boldsymbol}{\beta}}}}_{G_j}(t{{\boldsymbol}{\omega}})/t-{{{\boldsymbol}{\beta}}^{*}}_{G_j}/t}\|_{{2}}} < \mu({{\boldsymbol}{\omega}},\xi).
\]
As $\tau_- = 2\mu({{\boldsymbol}{\omega}},\xi)(\xi-1)/(\xi+1)$ and $\tau_+ = \mu({{\boldsymbol}{\omega}},\xi)\{(\xi-1)/(\xi+1)+1\}$, we have 
\begin{align*}
-\tau_+t^2 = 
-\left(\frac{\xi-1}{\xi+1} + 1\right)t^2\mu({{\boldsymbol}{\omega}},\xi)
< {{\sigma^{*}}}^{2} - {\|{{{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})}\|_{{2}}}^{2}/n
< 2 \frac{\xi-1}{\xi+1}t^2\mu({{\boldsymbol}{\omega}},\xi) = \tau_-t^2. 
\end{align*}
The upper bound above for $t=\sigma^*/\sqrt{1+\tau_-}$ implies 
\begin{align*}
t^{2} - {\|{y-{{\rm \textbf{X}}}{{\widehat{{\boldsymbol}{\beta}}}}(t{{\boldsymbol}{\omega}})}\|_{{2}}}^{2}/n < t^{2} - {{\sigma^{*}}}^{2} + \tau_- t^2 = 0, 
\end{align*}
so that ${\widehat{\sigma}} > t =\sigma^*/\sqrt{1+\tau_-}$ by Lemma \ref{lem:partialdelsig}. 
Similarly, the lower bound yields ${\widehat{\sigma}} \ge \sigma^*/\sqrt{1-\tau_+}$. 

As ${\widehat{\sigma}} \ge \sigma^*/\sqrt{1+\tau_-}$, the error bounds in Theorem \ref{th:betal12-ns} 
holds for $\{{{\boldsymbol}{y}}/{\widehat{\sigma}},{{\boldsymbol}{\beta}}^*/{\widehat{\sigma}},{{\widehat{{\boldsymbol}{\beta}}}}/{\widehat{\sigma}}\}$, which implies 
(\ref{sgl-pred}) and (\ref{sgl-est}) due to ${\widehat{\sigma}} \le \sigma^*/\sqrt{1-\tau_+}$. 
When (\ref{eq:mod1}) holds with Gaussian error, 
$|{\widehat{\sigma}}/{{\sigma^{*}}}-1|=o_P(\mu({{\boldsymbol}{\omega}},\xi))=o_P(n^{-1/2})$ 
by (\ref{sgl-sigma}) and the condition in $\mu({{\boldsymbol}{\omega}},\xi)$, so that 
(\ref{eq:teststatsigma}) follows from the 
central limit theorem for ${{\sigma^{*}}}/\sigma \sim \chi_n/\sqrt{n}$. 
 

Let ${{\boldsymbol}{u}}^*={{\boldsymbol}{\varepsilon}}/\|{{\boldsymbol}{\varepsilon}}\|_2$, ${{\boldsymbol}{Q}}_{G_j}$ be the orthogonal 
projection to the range of ${{\rm \textbf{X}}}_{G_j}$ and $f({{\boldsymbol}{u}}^*)=\|{{\boldsymbol}{Q}}_{G_j}{{\boldsymbol}{u}}^*\|_2$. 
As $f({{\boldsymbol}{u}}^*)=1$ for $n=1$, we assume $n\ge 2$ without loss of generality. 
The vector ${{\boldsymbol}{u}}^*$ is uniformly distributed in the sphere ${\mathbb{S}}^{n-1}$ 
and $f({{\boldsymbol}{u}}^*)$ is a unit Lipschitz function of ${{\boldsymbol}{u}}^*$ with median $\sqrt{m_{d_j,n}}$. 
As $\sigma^*=\|{{\boldsymbol}{\varepsilon}}\|_2/\sqrt{n}$, $\|{{\rm \textbf{X}}}_{G_j}^{T}({{\boldsymbol}{y}}-{{\rm \textbf{X}}}{{\boldsymbol}{\beta}}^*)/(n\sigma^*)\|_2\le f({{\boldsymbol}{u}}^*)$ 
when $\|{{\rm \textbf{X}}}_{G_j}/\sqrt{n}\|_S\le 1$.  In this case and for $t>0$ and $n\ge 2$, 
\begin{eqnarray*}
{\mathbb{P}}\left\{\|{{\boldsymbol}{Q}}_{G_j}{{\boldsymbol}{u}}^*\|_2\ge \sqrt{m_{d_j,n}}+\frac{t}{\sqrt{n-3/2}}\right\}
\le e^{(4n-6)^{-2}}{\mathbb{P}}\Big\{ {\mathsf{N}}(0,1) > t\Big\}\le e^{-t^2/2}
\end{eqnarray*}
by the L\'evy concentration inequality as in Lemma 17 of \cite{SunZ13}. Thus, 
${\mathbb{P}}({{\cal E}}) \geq 1-\delta$ by the union bound when 
$(\xi-1)\omega_j/\{(\xi+1)\sqrt{1+\tau_-}\} \ge \omega_{*,j}$. 
Now, consider $\omega_j=A\omega_{*,j}$. 
Let $\tau_* = 2\mu({{\boldsymbol}{\omega}}_*,\xi)(\xi-1)/(\xi+1)$. 
It follows from (\ref{eq:cone1}) and (\ref{eq:SCIF}) that $\mu({{\boldsymbol}{\omega}}, \xi) = A^2\mu({{\boldsymbol}{\omega}}^*, \xi)$, 
so that $\tau_-=A^2\tau_*$. Consequently, 
\begin{eqnarray*}
\frac{(\xi-1)\omega_j}{(\xi+1)\sqrt{1+\tau_-}\omega^*_{j}} 
= \frac{(\xi-1)A}{(\xi+1)\sqrt{1+A^2\tau_-^*}} \ge 1
\end{eqnarray*}
if and only if $A \ge \{(\xi+1)/(\xi-1)\}\big/\{1 - \{(\xi+1)/(\xi-1)\}^2 \tau^*_-\}^{1/2}=A_*$. 
Finally, we note that $\sqrt{m_{d_j,n}} \le {\mathbb{E}} f({{\boldsymbol}{u}}^*)+e^{(4n-6)^{-2}}{\mathbb{E}}|{\mathsf{N}}(0,1/(n-3/2))|/2 
\le(d_j/n)^{1/2}+n^{-1/2}$. 
\end{proof}

 
 
\section{Simulation Results}\label{sec:simures}
We provide a few simulation results for our theories developed in Sections \ref{sec:AsympInf} and \ref{sec:consisres}. As a prelude, in the following we first show the performance of scaled group Lasso procedure in a simulation experiment.
We consider a two simulation designs with  $(n=1000,p=200)$ and $(n=1000,p=2000)$ design matrices with the elements of the design matrix generated independently from ${\mathsf{N}}(0,1)$. We assume that the true parameter ${{{\boldsymbol}{\beta}}^{*}}$ has an inherent grouping with total set of $p$ parameters divided into groups of size $d_{j}=4$. In the design  $(n=1000,p=200)$ we have total number of groups $M=50$ and in $(n=1000,p=2000)$, $M=500$. For both scenarios, the true parameter ${{{\boldsymbol}{\beta}}^{*}}$ is assumed to be ($g=2, s=8$) strong group sparse with its non-zero coefficients in $\{-1,1\}$. Both simulation designs have a ${\mathsf{N}}(0,\sigma^{2})$ error added to the true regression model ${{\rm \textbf{X}}}{{{\boldsymbol}{\beta}}^{*}}$ with $\sigma=1$. We also assume that the design matrix is group wise  orthogonalized 
in the sense of ${{\rm \textbf{X}}}_{G_j}^T{{\rm \textbf{X}}}_{G_j}/n = {{\rm \textbf{I}}}_{G_j\times G_j}$, $j=1,\ldots,M$. 

In estimation of $\sigma$ we employ the scaled group Lasso procedure as shown in \ref{algo:iter}. The groupwise penalty factors $\omega_{j}$'s are chosen to equal to $\lambda\sqrt{d_{j}}$ for some fixed $\lambda>0$. The implementation of group Lasso procedure is via the \textsf\textbf{R} package \texttt{gglasso}. 

\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.3\linewidth]{qq200.jpeg}
\end{minipage}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.3\linewidth]{qq2000.jpeg}
\end{minipage}    \caption{Normal QQ plot for the test statistic for ${\widehat{\sigma}}$ in (\ref{eq:teststatsigma}) in Theorem \ref{th:betagrp-scaled} with $n=1000, p=\{200,2000\}, g=2, s=8$. The results are produced with 100 replications of the scaled group Lasso. The red dotted line is fitted through $1^{{\rm st}}$ and $3^{{\rm rd}}$ sample quantile. }
    \label{fig:qqplotsigma}
\end{figure}

In the design setup with $(n=1000,p=200)$, the estimate of ${\widehat{\sigma}}$ averaged over a 100 replications is 0.997 with a standard deviation of 0.02. In the design setup with $(n=1000,p=2000)$, the estimate of ${\widehat{\sigma}}$ averaged over a 100 replications is 1.0002 with a standard deviation of 0.02.
Additionally Figure \ref{fig:qqplotsigma} shows the Gaussian Q-Q plots of the test statistic $\sqrt{2n}\left({\widehat{\sigma}}/{\sigma} - 1\right)$. 

\subsection{Asymptotic  test statistic}
We also seek the empirical validation of the asymptotic convergence of the group ${{{\boldsymbol}{\beta}}_{G_{j}}}$ as described in our theoretical results. For bias correction we take the penalty function in (\ref{pen-multi-reg}) to be the Frobenius norm and apply group Lasso based optimization.
We also consider a new simulation design similar as before with $(n=1000,p=200)$ and $\sigma=1$. We will consider two different schemes for empirical analysis for asymptotic convergence.
\subsubsection{Small group sizes}
The true parameter ${{{\boldsymbol}{\beta}}^{*}}$  is simulated to be $(s=40, g=10)$ strong group sparse with its nonzero values in the interval [2,3]. More specifically, ${{{\boldsymbol}{\beta}}^{*}}$ is grouped into groups of  sizes $d_{j}=4$ for all $j$. We construct the test statistic of ${{{\boldsymbol}{\mu}}_{G_{j}}}$ as in (\ref{test}) for one of the nonzero groups.  Figure \ref{fig:qqplotbeta1} provides $\chi^{2}_{4}$ based Q-Q plot for the sample quantiles of our test statistic. 
\begin{figure}[H]
    \centering
  \includegraphics[width=0.7\linewidth]{n1000p200k4g10.jpeg}
    \caption{Chi square Q-Q plot for the test statistic for ${\widehat{{\boldsymbol}{\mu}}_{G_{j}}}$ with $n=1000, p=200, g=10, s=40$. The theoretical quantiles were drawn from $\chi^{2}_{4}$ random variable. The group being tested has size 4.}
    \label{fig:qqplotbeta1}
\end{figure}
\subsubsection{Large group sizes}
The true parameter ${{{\boldsymbol}{\beta}}^{*}}$  is simulated to be $(s=40, g=2)$ strong group sparse with its nonzero values between [2,3]. More specifically, ${{{\boldsymbol}{\beta}}^{*}}$ is grouped into 20 groups each of sizes $d_{j}=20$ for all $j$. We let the sparsity of the true parameter ${{{\boldsymbol}{\beta}}^{*}}$ to be $s=40$ contained within 2 separate groups. Again, we construct the test statistic of ${{{\boldsymbol}{\mu}}_{G_{j}}}$ as in (\ref{test}) for one of the nonzero groups. Figure \ref{fig:qqplotbeta2} shows the Q-Q plot for this group's test statistic. As the figure suggests, for large group sizes asymptotic normality of the group test statistic is empirically supported.
\begin{figure}[ht]
    \centering
  \includegraphics[width=0.7\linewidth]{n1000p200s40g2-v2.jpeg}
    \caption{Normal QQ plot for the test statistic for ${\widehat{{\boldsymbol}{\mu}}_{G_{j}}}$ with $n=1000, p=200, g=2, s=40$.  Here the group size of the test group is 20.}
    \label{fig:qqplotbeta2}
\end{figure}

\newpage
\bibliographystyle{imsart-nameyear}
\bibliography{GroupInference-V9}
\end{document}
