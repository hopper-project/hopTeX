

\documentclass[reqno]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[foot]{amsaddr}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{dsfont}

\usepackage[cal=cm,
]
{mathalfa}

\usepackage[dvipsnames,svgnames]{xcolor}
\colorlet{MyBlue}{DodgerBlue!75!Black}

\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{acronym}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{paralist}
\usepackage{wasysym}
\usepackage{xspace}

\usepackage[multiple]{footmisc}

\usepackage[sort&compress]{natbib}

\usepackage{hyperref}
\hypersetup{
draft=false,
colorlinks=true,
linktocpage=true,
pdfstartview=FitH,
breaklinks=true,
pdfpagemode=UseNone,
pageanchor=true,
pdfpagemode=UseOutlines,
plainpages=false,
bookmarksnumbered,
bookmarksopen=false,
bookmarksopenlevel=1,
hypertexnames=true,
pdfhighlight=/O,
urlcolor=Maroon,linkcolor=MyBlue!60!black,citecolor=DarkGreen!70!black, 
pdftitle={},
pdfauthor={},
pdfsubject={},
pdfkeywords={},
pdfcreator={pdfLaTeX},
pdfproducer={LaTeX with hyperref}
}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\bra}{\langle}{\rvert}
\DeclarePairedDelimiter{\ket}{\lvert}{\rangle}

\DeclarePairedDelimiterX{\braket}[2]{\langle}{\rangle}{#1\mathopen{}\delimsize\vert\mathopen{}#2}
\DeclarePairedDelimiterX{\product}[2]{\langle}{\rangle}{#1,#2}
\DeclarePairedDelimiterX{\setdef}[2]{\{}{\}}{#1:#2}

\usepackage[textwidth=30mm]{todonotes}
\usepackage{soul}
\setstcolor{red}
\sethlcolor{SkyBlue}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{example*}{Example}

\newenvironment{Proof}[1][Proof]{\begin{proof}[#1]}\end{proof}

\numberwithin{equation}{section}
\numberwithin{theorem}{section}

\begin{document}

\title{Riemannian Game Dynamics}

\author[P.~Mertikopoulos]{Panayotis Mertikopoulos$^{\ast}$}
\address
{$^{\ast}$CNRS (French National Center for Scientific Research), LIG, F-38000 Grenoble, France\\
and
Univ. Grenoble Alpes, LIG, F-38000 Grenoble, France.}
\email{\href{mailto:panayotis.mertikopoulos@imag.fr}{\scriptsize panayotis.mertikopoulos@imag.fr}}

\author[W.~H.~Sandholm]{William H. Sandholm$^{\S}$}
\address
{\quad$^{\S}$\hspace{.5pt}Department of Economics, University of Wisconsin, 1180 Observatory Drive, Madison WI 53706, USA.}
\email{\href{mailto:whs@ssc.wisc.edu}{\scriptsize whs@ssc.wisc.edu}}

\thanks{We thank Josef Hofbauer for helpful discussions.
Part of this work was carried out during the authors' visit to the Hausdorff Research Institute for Mathematics at the University of Bonn in the framework of the Trimester Program ``Stochastic Dynamics in Economics and Finance''.
PM is grateful for financial support from the
the French National Research Agency (ANR) under grant no.~ANR\textendash GAGA\textendash13\textendash JS01\textendash 0004\textendash 01
and
the CNRS under grant no. PEPS\textendash REAL.net\textendash 2016.
WHS is grateful for financial support under NSF Grants SES\textendash1155135 and SES\textendash1458992.}

\subjclass[2010]{Primary:
91A22, 92D25;
secondary
91A26, 37N40.}
\keywords{Evolutionary game dynamics, Riemannian metrics, replicator dynamics, projection dynamics, Riemannian dynamics, Nash equilibrium, \aclp{ESS}, contractive games, potential games, \acl{HR} metrics.}

  
\newacro{1SL}{one-sided Lipschitz}
\newacro{ESS}{evolutionarily stable state}
\newacro{GESS}{globally evolutionarily stable state}
\newacro{ODE}{ordinary differential equation}
\newacro{HR}{Hess\-i\-an Rie\-man\-ni\-an}
\newacro{KKT}{Ka\-rush\textendash Kuhn\textendash Tuc\-ker}
\newacro{RPS}{Rock-Paper-Scissors}
\newacro{MP}{Matching Pennies}
\newacro{KL}{Kull\-back\textendash Le\-ib\-ler}
\newacro{lsc}[l.s.c.]{lower semi-continuous}

\begin{abstract}

We study a class of evolutionary game dynamics under which the population state moves in the direction that agrees most closely with current payoffs.
This agreement is defined by means of a Riemannian metric which imposes a geometric structure on the set of population states.
By supplying microfoundations for our dynamics, we show that the choice of geometry provides a state-dependent but payoff-independent specification of the saliences of and similarities between available strategies.
The replicator dynamics and the (Euclidean) projection dynamics are the archetypal examples of this class.
Similarly to these representative dynamics, all Riemannian game dynamics satisfy certain basic desiderata, including positive correlation and global convergence in potential games.
Moreover, when the underlying Riemannian metric satisfies a Hessian integrability condition, the resulting dynamics preserve many further properties of the replicator and projection dynamics.
We examine the close connections between Hessian game dynamics and reinforcement learning in normal form games, extending and elucidating a well-known link between the replicator dynamics and exponential reinforcement learning.
\end{abstract}

\maketitle

\setcounter{tocdepth}{1}
\vspace{-1.5em}
\tableofcontents

\section{Introduction}
\label{sec:introduction}

Viewed abstractly, evolutionary game dynamics assign to every population game a dynamical system on the game's set of population states.
Under most dynamics, the vector of motion at a given population state depends only on payoffs and behavior at that state, so that changes in aggregate behavior are determined by current strategic conditions.
Such dynamics can thus be construed as state-dependent rules for transforming current payoffs into feasible vectors of motion.
To provide game-theoretic interpretations, one then considers individual choice protocols which, when aggregated over the population, yield the dynamics in question.

This paper introduces \emph{Riemannian game dynamics}, a class of dynamics under which the state of the population tracks the feasible direction of motion that is most closely aligned with the population's current payoffs.
Every member of this class is generated by a \emph{Riemannian metric}: a state-dependent inner product on the positive orthant which is used to evaluate displacements from each point, thereby inducing a geometric structure on the simplex.\footnote{To be clear, a Riemannian metric is not a metric in the sense of measuring distances between points in a metric space, but it induces such a distance function in a canonical way \citep{Lee97}.}
We provide an interpretation of this structure as a state-dependent \textendash\ but payoff-\emph{independent} \textendash\ specification of the \emph{saliences} of and \emph{similarities} between the available strategies, properties that may reflect the nature of the strategies themselves, the context in which the game is played, or the way that agents reevaluate their choices.
This structure is then combined with payoff data to determine how the population evolves over time.

The two archetypal examples of this class are derived from particularly simple structures.
First, the replicator dynamics \citep{TJ78} are derived from the so-called \emph{Shahshahani} metric \citep{Sha79} which sets a strategy's salience equal to its popularity.
Second, the Euclidean projection dynamics \citep{NZ97} are obtained from the usual Euclidean metric, which fixes all saliences at one.
In both cases, similarities between pairs of different strategies are zero.
Other Riemannian metrics can be used in applications where different strategies have clear affinities, allowing the presence and performance of one strategy to positively influence the use of similar alternatives.

In Sections \ref{sec:Riemannian} and \ref{sec:dynamics}, we introduce the machinery from differential geometry needed to construct our dynamics, using the replicator and Euclidean projection dynamics as motivating examples.
This construction involves two core steps:
a \emph{dual-to-primal} conversion, and a \emph{projection} to an admissible direction of motion.
The key ideas behind each of these steps are explained below.

Regarding the first step, differential geometry makes a basic distinction between \emph{vectors}, representing displacements from a given point, and \emph{covectors}, which act as linear functionals on vectors.
Our starting point here is the observation that payoffs at a population state $x$ should be regarded as the components of a \emph{covector}, an observation we justify by examining the role of payoffs in a variety of basic game-theoretic definitions (Section \ref{sec:dual}).
As such, the first step in constructing our game dynamics is to convert payoff covectors to vectors \textendash\ that is, to an object representing a displacement from the current state.
Each Riemannian metric provides a canonical, state-dependent way of performing this conversion.

Second, the resulting vector must be projected to obtain the closest ``admissible'' direction of motion, where the notion of closeness is again provided by the underlying Riemannian metric.
In the interior of the simplex, all tangent directions are admissible;
at non-interior states however, the definition of ``admissible'' depends on the behavior of the Riemannian metric near the boundary of the simplex.

As we explain in Section \ref{sec:boundary}, the metric's boundary behavior is the source of a fundamental dichotomy that is best explained by looking at our two prototypical examples.
Under the replicator dynamics
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
the law of motion for every game is continuous;
\item
the support of the population state remains constant along every solution trajectory;
and
\item
the dynamics' rest points are the restricted equilibria of the game \textendash\ the states at which all strategies in use earn the same payoff.
\end{inparaenum}
In contrast, under the Euclidean projection dynamics
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
the law of motion is typically discontinuous at the boundary of the simplex;
\item
the support of the state may change repeatedly along a single solution trajectory;
and
\item
the dynamics' rest points are the Nash equilibria of the underlying game.
\end{inparaenum}
Based on this behavior, we obtain a natural classification between \emph{continuous} and \emph{discontinuous Riemannian dynamics}, each category sharing the boundary behavior of its prototype.\footnote{Discontinuous dynamics do not seem well-suited for describing behavior in naturally occurring environments.
However, given that they do not decelerate near the boundary of the state space, and in particular can converge to equilibrium in finite time, they seem particularly apt for applications to engineering and decentralized control, in which the revision protocols of the components of the controlled system are chosen by the system designer.
For a recent survey, see \cite{MS15}.}

A basic aim of our analysis is to demonstrate that many basic properties of the replicator and Euclidean projection dynamics extend to our substantially more general setting.
To start, we show that several microfoundations for these baseline dynamics can be adapted to more general Riemannian dynamics (Section \ref{sec:micro}).
These microfoundations are specified by means of \emph{revision protocols}, which describe the rates of switching between pairs of strategies, and yield the dynamics in question when aggregated over the population.\footnote{For microfoundations of the replicator dynamics, see \cite{BW96}, \cite{Wei95}, \cite{Hel92}, and \cite{Sch98};
for the projection dynamics, see \cite{LS08} and \cite{SDL08}.
For surveys, see \cite{San10,San15}.}
A key property of the revision protocols used here is that the assignment of revision opportunities and the selection of new strategies are influenced by the Riemannian metric in a symmetric way.
The form that this influence takes is determined by the choice of the metric, which specifies
the saliences of and similarities between strategies at each state.\footnote{\cite{Set98} is an early work that emphasizes how intrinsic relationships between strategies may influence evolutionary dynamics.}

In Section \ref{sec:analysis}, we show that Riemannian dynamics satisfy the basic desiderata for evolutionary game dynamics:
they heed a payoff monotonicity condition known as \emph{positive correlation}, and they converge to rest points in potential games.\footnote{Our convergence analysis requires a new technical result on Lyapunov functions for discontinuous dynamics;
for a precise statement, see App.~\ref{app:dynamics}.}
In the latter context, Riemannian dynamics also provide a broad generalization of \emph{Kimura's maximum principle} \citep{Kim58,Sha79}.
This principle states that, when agents are matched to play a common interest game, the replicator dynamics move in the direction of maximal increase in average payoffs, provided that lengths and angles of displacement vectors are evaluated using the Shahshahani metric.
Extending this principle in two directions, we observe that in potential games, Riemannian dynamics track the direction of maximum increase in potential, assuming that displacements from each state are evaluated using the specified Riemannian metric.

Obtaining further results on stability, convergence, and global behavior requires additional structure on our dynamics \textendash\ and hence on the underlying Riemannian metric.
This structure is provided by an \emph{integrability condition}.   
In prior work on game dynamics, such conditions have been imposed on the \emph{vector fields} used to convert the strategies' payoffs into vectors of choice probabilities.\footnote{See \cite{HMC01b}, \cite{HS07}, and \cite{San10c}.}
By contrast, the integrability condition employed here is imposed on the \emph{matrix field} that defines a Riemannian metric,\footnote{Since a Riemannian metric is a state-dependent inner product, it can be represented as a field of symmetric positive-definite matrices; for a detailed discussion, see Section \ref{sec:geometry}.}
requiring that it be expressible as the Hessian of a convex function.
We call this function the \emph{potential} of the metric, and we refer to the resulting dynamics as \emph{Hessian game dynamics}.\footnote{In the context of convex programming, gradient flows generated by \ac{HR} metrics of this sort have been explored at depth by \cite{BT03} and \cite{ABB04}.
\cite{LM15} examine the long-term rationality properties of a class of second-order, \emph{inertial} game dynamics derived from \ac{HR} metrics;
the Hessian dynamics considered here can be seen as a first-order analogue of their dynamics.}
Both the replicator dynamics and the Euclidean projection dynamics are members of this class.
Moreover, Hessian dynamics are continuous when their potential function becomes infinitely steep at the boundary of the simplex;
accordingly, a version of this criterion separates Hessian dynamics into a continuous and a discontinuous regime.

We analyze Hessian game dynamics in Section \ref{sec:HD}.
The key tool we employ is the \emph{Bregman divergence} of the metric's potential  \citep{Bre67}, an asymmetric measure of the ``remoteness'' of a given population state from any fixed target state.\footnote{In the case of the Shahshahani metric, the Bregman divergence corresponds to the \acl{KL} divergence, whose usefulness in the analysis of the replicator dynamics is well known \citep{Wei95,HS98}.}
By using the Bregman divergence as a Lyapunov function, we prove global convergence to Nash equilibrium in strictly contractive games and local stability of \aclp{ESS} under Hessian game dynamics.\footnote{For continuous Hessian dynamics
like the replicator dynamics,
``global'' should actually be read as ``almost global'';
see Section \ref{sec:contractive} for an exact statement.}
We also show that certain distinctive properties of the replicator dynamics in normal form games \textendash\ specifically, the convergence of the time averages of interior solutions to Nash equilibria, and the existence of simple sufficient conditions for permanence \textendash\ extend to all continuous Hessian dynamics.
Finally, we show that strictly dominated strategies are eliminated under continuous Hessian dynamics, a conclusion which does not extend to the discontinuous regime.\footnote{See \cite{SDL08} and cf.~Section \ref{sec:dominated}.}

\subsubsection*{Related work}
There are very close connections between the dynamics considered here and the dynamics studied by \cite{HS90}, \cite{Hop99b}, and \cite{Har11}.
In order to have the machinery in place to make these connections clear, we postpone this discussion until Section \ref{sec:previous}.

There is a more surprising connection between Hessian game dynamics and models of reinforcement learning in normal form games.
\cite{Rus99}, \cite{HSV09} and \cite{MM10} show that if players track the cumulative payoffs (or \emph{scores}) of their strategies, and they then choose mixed strategies at each instant by applying the logit choice rule to these scores, the evolution of mixed strategies is described by the replicator dynamics.\footnote{See \cite{BS97}, \cite{Pos97}, and \cite{Hop02} for related results.}
Combining our analysis here with that in a companion paper \citep{MS16}, we show that the Hessian dynamics derived from a steep potential function $h$ also describe the evolution of mixed strategies under reinforcement learning with penalty function $h$. 
In addition to substantially generalizing existing results, our analysis provides an intuitive explanation for the tight links between the two sorts of processes. 
Section \ref{sec:RL} describes these and other connections between Hessian dynamics and reinforcement learning in detail.

\section{Population games and evolutionary dynamics}
\label{sec:prelims}

\subsection*{Notation}
\label{sec:notation}

Let ${\mathcal{A}} = \{\alpha_{1},\dotsc,\alpha_{n}\}$ be a finite set.
The real space spanned by ${\mathcal{A}}$ will be denoted by ${\mathbb{R}}^{\mathcal{A}}$ and we will write $\delta_{\alpha\beta}$ for the Kronecker deltas on ${\mathcal{A}}$.
The nonnegative orthant of ${\mathbb{R}}^{\mathcal{A}}$ will be denoted by ${\mathcal{K}} \equiv {\mathbb{R}}_{+}^{\mathcal{A}}$ and the set of vectors whose components sum to zero will be written ${\mathbb{R}}_{0}^{\mathcal{A}} = \setdef{z\in{\mathbb{R}}^{\mathcal{A}}}{{\sum\nolimits}_{\alpha} z_{\alpha} = 0}$.
Finally, in a slight abuse of notation, we will write ${\mathbb{R}}^{\operatorname{supp}(x)}= \setdef{z\in{\mathbb{R}}^{\mathcal{A}}}{z_\alpha = 0 \text{ whenever } x_\alpha =0}$ for the set of vectors whose supports are contained in the support of $x\in{\mathbb{R}}^{\mathcal{A}}$.

\subsection{Population games}
\label{sec:games}

Throughout this paper we focus on games played by a population of nonatomic agents.\footnote{Except when noted otherwise, our analysis extends to the multi-population setting without significant effort.}
During play, each agent chooses an \emph{action} (or \emph{pure strategy}) from a finite set ${\mathcal{A}}$, and his payoff is determined by his choice of action and by the proportions $x_{\alpha}\in[0,1]$ of the population playing each action $\alpha\in{\mathcal{A}}$.
Collectively, these proportions define a \emph{population state} $x = (x_{\alpha})_{\alpha\in{\mathcal{A}}}\in{\mathbb{R}}^{\mathcal{A}}$;
hence, the set of population states (or \emph{state space}) is ${\mathcal{X}} = {\Delta}({\mathcal{A}})=\setdef{z\in{\mathbb{R}}^{\mathcal{A}}_+}{\sum_\alpha x_\alpha =1}$.
The payoff to an agent playing $\alpha\in{\mathcal{A}}$ when the population state is $x\in{\mathcal{X}}$ is then given by an associated \emph{payoff function} ${v}_{\alpha}{\colon} {\mathcal{X}}\to{\mathbb{R}}$ which we assume to be Lipschitz continuous.
Putting all this together, a \emph{population game} may be identified with a set of actions and their associated payoff functions, and will be denoted by ${\mathcal{G}} \equiv {\mathcal{G}}({\mathcal{A}},{v})$.

A population state ${x^{\ast}}\in{\mathcal{X}}$ is a \emph{Nash equilibrium} of a game ${\mathcal{G}}$ if
\begin{equation}
\label{eq:Nash}
\tag{NE}
{v}_{\alpha}({x^{\ast}})
	\geq {v}_{\beta}({x^{\ast}})
	\quad
	\text{for all $\alpha\in\operatorname{supp}({x^{\ast}})$ and for all $\beta\in{\mathcal{A}}$.}
\end{equation}
If ${x^{\ast}}$ is \emph{pure} (i.e. ${x^{\ast}} = {e}_{\alpha}$ for some $\alpha\in{\mathcal{A}}$) and satisfies \eqref{eq:Nash}, it is called a \emph{pure Nash equilibrium} of ${\mathcal{G}}$;
if, in addition, \eqref{eq:Nash} is strict for all $\beta\notin\operatorname{supp}({x^{\ast}})$, ${x^{\ast}}$ is said to be a \emph{strict equilibrium} of ${\mathcal{G}}$.

A \emph{restriction} of a game ${\mathcal{G}}$ is a population game ${\mathcal{G}}' \equiv {\mathcal{G}}'({\mathcal{A}}',{v}')$ that is defined by a subset ${\mathcal{A}}'\subseteq{\mathcal{A}}$ of the original game's action set and by payoff functions ${v}_{\alpha}$ obtained by restricting the original payoff functions to the reduced state space ${\mathcal{X}}' ={\Delta}({\mathcal{A}}')$ of ${\mathcal{G}}'$.
If $x\in{\mathcal{X}}$ is a Nash equilibrium of some restriction of ${\mathcal{G}}$, it will be called a \emph{restricted equilibrium};
put differently, $x \in {\mathcal{X}}$ is a restricted equilibrium of ${\mathcal{G}}$ if all strategies in the support of $x$ earn equal payoffs.

Below we introduce some classes of population games that have been studied extensively.
The first is the focus of the early literature on evolutionary game dynamics \citep{HS98,Wei95}:

\begin{example}
[Matching in normal form games]
\label{ex:matching}
The simplest example of a population game is obtained by uniformly matching a population of agents to play a two-player symmetric normal form game with payoff matrix $A = (A_{\alpha\beta})_{\alpha,\beta=1}^{n}$.
Aggregating over all matches, the payoff to an $\alpha$-strategist when the population is at state $x\in{\mathcal{X}}$ is ${v}_{\alpha}(x) = {\sum\nolimits}_{\beta\in{\mathcal{A}}} A_{\alpha\beta} x_{\beta}$.
Thus, uniform random matching generates payoff functions that are linear in the population state.
\end{example}

The next two classes of games have a wide variety of applications, and their Nash equilibria are the only stable limit points of a wide variety of evolutionary dynamics \citep{San10}.

\begin{example}
[Potential games]
\label{ex:potential}
A population game ${\mathcal{G}}$ is called a \emph{potential game} \citep{San01,San09} if there exists a \emph{potential function} ${f}$ defined on a neighborhood of ${\mathcal{X}}$ such that:\footnote{For most purposes, it suffices for ${f}$ to be defined on ${\mathcal{X}}$ and to ask that the directional derivative $D_{z} {f}(x)$ be equal ${\sum\nolimits}_{\alpha} {v}_{\alpha}(x) z_{\alpha}$ for all $z \in {\mathbb{R}}_{0}^{\mathcal{A}}$ and for all $x \in {\mathcal{X}}$ \textendash\ see \cite{San09}.
\label{fn:PG}}
\begin{equation}
\label{eq:potential}
\frac{{\partial}{f}}{{\partial} x_{\alpha}}
	={v}_{\alpha}(x)
	\quad
	\text{ for all $\alpha\in{\mathcal{A}}$ and for all $x\in{\mathcal{X}}$.}
\end{equation}
Potential games include congestion games,
games generated by variable pricing schemes,
and population games generated by random matching in common interest games ($A_{\alpha\beta} = A_{\beta\alpha}$).
\end{example}

\begin{example}
[Contractive games]
\label{ex:contract}
A population game ${\mathcal{G}}$ is called (\emph{weakly}) \emph{contractive} \citep{HS09} if
\begin{equation}
\label{eq:contract}
{\sum\nolimits}_{\alpha} ({v}_{\alpha}(x') - {v}_{\alpha}(x))  (x_{\alpha}' - x_{\alpha})
	\leq 0
	\quad
	\text{ for all $x, x' \in {\mathcal{X}}$.}
\end{equation}
If \eqref{eq:contract} binds only when $x = x'$, ${\mathcal{G}}$ is called \emph{strictly contractive};
at the other end of the spectrum, if \eqref{eq:contract} binds for all $x,x'\in{\mathcal{X}}$, ${\mathcal{G}}$ is called \emph{conservative}.
\footnote{\cite{HS09} use the name \emph{stable games}, but \cite{San15} proselytizes for the terms employed here.
In convex analysis, \eqref{eq:contract} is called \emph{monotonicity}.}
Contractive games include wars of attrition, nonatomic congestion games with increasing cost functions, and games that admit a concave potential.
Random matching in normal form games with an interior \acl{ESS} generates strictly contractive population games (see Section \ref{sec:contractive}), while
random matching in symmetric zero-sum games ($A_{\alpha\beta} = -A_{\beta\alpha}$) generates conservative population games.
\end{example}

\subsection{Evolutionary dynamics and their microfoundations}
\label{sec:ED}

\emph{Evolutionary dynamics} are rules that assign to each population game ${\mathcal{G}}$ a dynamical system on the state space ${\mathcal{X}}$ of ${\mathcal{G}}$.
This is usually done by mapping each game to a differential equation 
of the form:
\begin{equation}
\label{eq:ED}
\tag{D}
\dot x
	= {V}(x).
\end{equation}
In most cases, the vector field ${V}(x)$ is defined by introducing a map $(x,\pi)\mapsto {\widetilde}{V}(x,\pi)$ from state/payoff pairs to vectors, and then specifying that ${V}(x) \equiv {\widetilde}{V}(x,{v}(x))$.
In what follows, we will focus exclusively on this class of dynamics.

Now, to ensure that solutions to \eqref{eq:ED} remain in ${\mathcal{X}}$ for all $t\geq0$, ${V}(x)$ should not point outward from ${\mathcal{X}}$;
more formally, ${V}(x)$ should lie in the \emph{tangent cone} of ${\mathcal{X}}$ at $x$, defined as
\begin{equation}
\label{eq:tcone-simplex}
\operatorname{TC}_{\mathcal{X}}(x)
	= \setdef{z\in{\mathbb{R}}_{0}^{\mathcal{A}}}{z_\alpha \geq 0 \text{ whenever } x_{\alpha} = 0}.
\end{equation}
Under many evolutionary dynamics (including the replicator dynamics and other imitative dynamics), the support of $x(t)$ remains invariant under \eqref{eq:ED};
put differently, the (relative) interior of each face of ${\mathcal{X}}$ is invariant under \eqref{eq:ED}.
When this is the case, ${V}(x)$ actually lies in the \emph{tangent space} to ${\mathcal{X}}$ at $x$, defined as
\begin{equation}
\label{eq:tspace-simplex}
\operatorname{T}_{\mathcal{X}}(x)
	= \setdef{z\in{\mathbb{R}}^{\mathcal{A}}_0}{z_\alpha = 0\text{ whenever }x_\alpha=0}
	\subseteq \operatorname{TC}_{\mathcal{X}}(x).
\end{equation}
Clearly, for interior states $x\in{{\mathcal{X}}^{\circ}} \equiv \operatorname{rel\,int}({\mathcal{X}})$, we have $\operatorname{T}_{\mathcal{X}}(x) = \operatorname{TC}_{\mathcal{X}}(x) = {\mathbb{R}}_{0}^{\mathcal{A}}$.

A basic monotonicity criterion linking the dynamics $\dot x = {V}(x)$ with the underlying game
requires \emph{positive correlation} between the strategies' payoffs and growth rates;
formally, this means that
\begin{equation}
\label{eq:PC}
\tag{PC}
{\sum\nolimits}_{\alpha} {v}_\alpha(x) {V}_\alpha(x)
	\geq 0
	\quad
	\text{for all $x \in {\mathcal{X}}$,}
\end{equation}
with equality only if ${V}(x)=0$.
If \eqref{eq:ED} satisfies \eqref{eq:PC}, every Nash equilibrium of ${\mathcal{G}}$ is a rest point of \eqref{eq:ED}.
For a detailed discussion, see \cite{San10}.

To provide microfoundations for \eqref{eq:ED}, one specifies a revision process which induces \eqref{eq:ED} in the so-called ``mean field'' limit.
Formally, such processes can be described by means of a \emph{revision protocol} which specifies the \emph{unconditional switch rates} ${s}_{\alpha\beta}(x,\pi)$ at which $\alpha$-strategists switch to strategy $\beta$ given the current population state $x$ and payoffs $\pi$ ($={v}(x)$). 
It is sometimes useful to think of these switch rates as taking the product form
\begin{equation}
\label{eq:PProd}
{s}_{\alpha\beta}(x,\pi)
	= p_{\alpha}(x,\pi) \,r_{\alpha\beta}(x,\pi),
\end{equation}
where $p_{\alpha}(x,\pi)$ describes the rate at which revision opportunities are assigned to $\alpha$-strategists,
and $r_{\alpha\beta}(x,\pi)$ is the rate at which $\alpha$-strategists switch to $\beta$ when receiving a revision opportunity.\footnote{In the standard formulation of revision protocols \citep{San10,San10b}, agents are assigned revision opportunities uniformly at random, so an $\alpha$-strategist receives the next opportunity with probability $x_\alpha$,
and a revising $\alpha$-strategist switches to $\beta$ at \emph{conditional switch rate} $\rho_{\alpha\beta}(x,\pi)$.
The advantage of this way of presenting microfoundations is its focus on individual agents rather than on the groups of agents playing each strategy.
For present purposes, the advantage of the formulation used here is that it expresses the symmetries that define Riemannian dynamics most directly (Section \ref{sec:micro}).
One can convert this formulation into the standard one using the identity $\rho_{\alpha\beta}(x, \pi) \equiv {s}_{\alpha\beta}(x, \pi)/x_\alpha$.
(Note also that it is not necessary to define $\rho_{\alpha\beta}(x, \pi)$ when $x_\alpha=0$.)
\label{fn:Micro}}

Together, a population game ${\mathcal{G}}\equiv{\mathcal{G}}({\mathcal{A}},{v})$ and a revision protocol $s$ induce the following \emph{mean dynamics}
\begin{equation}
\label{eq:MD}
\tag{MD}
\dot x_{\alpha}
	= {\sum\nolimits}_{\beta}\left({s}_{\beta\alpha}(x, {v}(x))-{s}_{\alpha\beta}(x, {v}(x))\right),
\end{equation}
which describe the rate of change in the use of each strategy $\alpha$ as the difference between inflows into $\alpha$ from other strategies and outflows from $\alpha$ to other strategies.
For a fixed protocol $s$, \eqref{eq:MD} can be viewed as a map from population games to differential equations on ${\mathcal{X}}$, as discussed earlier in this section.\footnote{Solutions to \eqref{eq:MD} can be viewed as approximations to the sample paths of stochastic evolutionary models generated by the protocol $s$ and game ${\mathcal{G}}$: see \cite{Kur70}, \cite{Ben98}, \cite{BW03}, and \cite{RS13}.}

\begin{example}
[The replicator dynamics]
\label{ex:Rep}
The quintessential evolutionary game dynamics are the \emph{replicator dynamics} of \cite{TJ78}, defined as
\begin{equation}
\label{eq:RD}
\tag{RD}
\dot x_{\alpha}
	= x_{\alpha} \left[ {v}_{\alpha}(x) - {\sum\nolimits}_{\beta} x_{\beta} {v}_{\beta}(x) \right].
\end{equation}
Under \eqref{eq:RD}, the percentage growth rate of a strategy currently in use is equal to the difference between its payoff and the population's average payoff;
moreover, strategies that are initially unused remain so for all time.

There are three well-known protocols that provide microfoundations for the replicator dynamics:
\begin{subequations}
\label{eq:RDF}
\begin{align}
\label{eq:RDF1}
{s}_{\alpha\beta}(x,\pi)
	&=x_{\alpha} x_{\beta} \pi_{\beta},
	\\
\label{eq:RDF2}
{s}_{\alpha\beta}(x,\pi)
	&=-x_{\alpha} x_{\beta} \pi_{\alpha},
	\\
\label{eq:RDF3}
{s}_{\alpha\beta}(x,\pi)
	&=x_{\alpha} x_{\beta} \left[ \pi_{\beta} -\pi_{\alpha} \right]_{+},
\end{align}
\end{subequations}
with $\pi$ assumed nonnegative in \eqref{eq:RDF1} and nonpositive in \eqref{eq:RDF2}.\footnote{Protocols \eqref{eq:RDF1} and \eqref{eq:RDF2} are due to \cite{BW96} and \cite{Wei95}, and protocol \eqref{eq:RDF3} is due to \cite{Hel92} and \cite{Sch98}.}
Under all protocols \eqref{eq:RDF}, the group of $\alpha$-strategists receives revision opportunities at rate $x_{\alpha}$, and the rate at which strategy $\beta$ is chosen as the candidate replacement strategy is $x_{\beta}$.
A simple interpretation is that revising agents are selected uniformly at random from the population, and they likewise select a candidate strategy by choosing another agent at random from the population.
The protocols differ in how payoffs determine the rate at which these switches are consummated:
in \eqref{eq:RDF1}, this rate grows linearly in the payoff of the candidate strategy;
in \eqref{eq:RDF2} it decreases linearly in the payoff of the revising agent's original strategy;
and in \eqref{eq:RDF3} it grows linearly in the excess payoff of the sampled strategy versus that of the incumbent.
Substituting any of these protocols into \eqref{eq:MD} and rearranging the result yields the replicator dynamics \eqref{eq:RD}. 
\end{example}

\begin{example}
[The  Euclidean projection dynamics]
\label{ex:proj}
The other fundamental example we consider is the \emph{Euclidean projection dynamics} of \cite{NZ97} (see also \citealp{Fri91}, and \citealp{LS08}).
On the interior of ${\mathcal{X}}$, these dynamics are defined as
\begin{equation}
\label{eq:Friedman}
\dot x_{\alpha}
	= {v}_{\alpha}(x) -\frac{1}{\abs{\mathcal{A}}} \sum_{\beta\in{\mathcal{A}}} {v}_{\beta}(x),
\end{equation}
so a strategy's absolute growth rate equals the difference between its payoff and the unweighted average of all strategies' payoffs.  Geometrically, the right-hand side of \eqref{eq:Friedman} is the Euclidean projection of payoffs onto ${\mathbb{R}}_{0}^{\mathcal{A}}$, the tangent space to ${\mathcal{X}}$ at interior points.
On the other hand, on the boundary of the simplex, the dynamics are defined via closest point projections to the tangent \emph{cone} $\operatorname{TC}_{\mathcal{X}}(x)$ of ${\mathcal{X}}$;
we detail this construction in Section \ref{sec:Lexamples}.

\cite{SDL08} observed that, on the interior of ${\mathcal{X}}$, the following analogues of protocols \eqref{eq:RDF1}\textendash\eqref{eq:RDF3} provide microfoundations for the Euclidean projection dynamics:
\begin{subequations}
\label{eq:ProjF}
\begin{flalign}
\label{eq:ProjF1}
{s}_{\alpha\beta}(x,\pi)
	&= \tfrac{1}{\abs{\mathcal{A}}}\pi_{\beta},
	\\
\label{eq:ProjF2}
{s}_{\alpha\beta}(x,{v})
	&= -\tfrac{1}{\abs{\mathcal{A}}}\pi_{\alpha},
	\\
\label{eq:ProjF3}
{s}_{\alpha\beta}(x,\pi)
	&= \tfrac{1}{\abs{\mathcal{A}}}\left[ \pi_{\beta} -\pi_{\alpha} \right]_{+},
\end{flalign}
\end{subequations}
with $\pi$ assumed nonnegative in \eqref{eq:ProjF1} and nonpositive in \eqref{eq:ProjF2}.
The dependence on payoffs in these protocols is the same as in \eqref{eq:RDF}, but here neither the rate that a strategy is assigned a revision opportunity nor the rate at which a strategy becomes the candidate depends on the current state.
The former independence can be interpreted as ``insecurity'':
each $\alpha$-strategist reconsiders his strategy at a rate inversely proportional to the strategy's popularity $x_{\alpha}$ so the aggregate rate of revision by $\alpha$-strategists is constant.
The latter independence can be interpreted as agents randomly choosing a candidate strategy from a list of all strategies, without regard for the strategy's popularity.\footnote{At boundary states, the protocols must be altered to recover the projection onto the tangent cones $\operatorname{TC}_{\mathcal{X}}(x)$;
see \cite{SDL08} and Example \ref{ex:EuclideanAgain} below.}
\end{example}

\subsection{Antecedents}
\label{sec:previous}

The class of dynamics studied in this paper is a substantial generalization of both the replicator dynamics and of the Euclidean projection dynamics.
We now describe works from an assortment of fields that are antecedents of our approach.

The replicator equation \eqref{eq:RD} for common interest games
is a basic model from population genetics (see \citealp{SS83}).
The \emph{fundamental theorem of natural selection}, attributed to \cite{Fis30}, states that natural selection among genes increases overall population fitness.
\cite{Kim58} introduced a corresponding maximum principle, showing that population fitness increases at a maximum rate under \eqref{eq:RD}, provided that one imposes a certain nonlinear constraint on the set of feasible changes in population frequencies.
Later, \cite{Sha79} (see also \citealp{Aki79}) put Kimura's maximum principle on a firm mathematical footing using tools from differential geometry \textendash\ specifically, by introducing a suitable Riemannian metric.
The replicator dynamics and the Shahshahani metric both play a key role in what follows.

\cite{HS90} model natural selection in populations of animals whose traits are represented by elements of a continuous set.
They assume that all members of the population share the same trait $x$, except for an infinitesimal group of mutants whose traits differ infinitesimally from $x$. 
The evolution of the preponderant trait $x$ follows a gradient-like process, moving in the direction that agrees with the play of the most successful local mutants.
To obtain variations on this process, \cite{HS90} use Riemannian metrics to define the size and shape of the neighborhood of local mutants.
When the trait space is ${\mathcal{X}}$ and the fitness of mutant $y$ takes the linear form $\sum_{\alpha} y_{\alpha} {v}_{\alpha}(x)$, they showed that the evolution of $x$ on the interior of ${\mathcal{X}}$ is given by
\begin{equation}
\label{eq:Rie-coordsIntro}
\dot x_{\alpha}
	= \sum_{\beta\in{\mathcal{A}}} \left[
	g_{\alpha\beta}^{-1}(x)
	- \frac{\sum_{\gamma} g_{\alpha\gamma}^{-1}(x)  \sum_{\gamma} g_{\gamma\beta}^{-1}(x)}{\sum_{\gamma,\kappa} g_{\gamma\kappa}^{-1}(x)}
	\right]
	{v}_{\beta}(x),
\end{equation}
where $g(x)$ is a field of symmetric positive definite matrices that defines the Riemannian metric in question (see Section \ref{sec:geometry}).
\cite{HS90} then observed that under the Shahshahani metric, equation \eqref{eq:Rie-coordsIntro} becomes the replicator equation \eqref{eq:RD}.
As we shall see, \eqref{eq:Rie-coordsIntro} describes the dynamics studied in this paper at all interior states, and also at boundary states in what we will call the \emph{minimal-rank} case (cf. Section \ref{sec:boundary}).

In the course of analyzing perturbed best response dynamics \citep{FL98} and variants of fictitious play \citep{Bro49,Bro51}, \cite{Hop99b} introduced a class of game dynamics that are defined on the interior of ${\mathcal{X}}$ as
\begin{equation}
\label{eq:Hopkins}
\dot x_{\alpha}
	= \sum_{\beta\in{\mathcal{A}}} {M}_{\alpha\beta}(x){v}_{\beta}(x),
\end{equation}
where ${M}(x)$ is a smoothly-varying field of symmetric matrices that are positive definite on ${{\mathbb{R}}_{0}}^{\mathcal{A}}$ and map constant vectors to $0$.
\cite{Hop99b} showed that the linearization of these dynamics agrees with that of perturbed best response dynamics up to a positive affine transformation, implying that the local stability of rest points of \eqref{eq:Hopkins} agrees with that of the corresponding rest points of perturbed best response dynamics with sufficiently small noise levels. 
As we show in Appendix \ref{sec:Hopkins}, the dynamics \eqref{eq:Rie-coordsIntro} satisfy Hopkins' conditions, and, conversely, any dynamics satisfying Hopkins' conditions can be expressed in the form \eqref{eq:Rie-coordsIntro}.
Thus, on the interior of ${\mathcal{X}}$, the dynamics of \cite{Hop99b} are equivalent to the dynamics studied here (Proposition \ref{prop:HopDRD}).

Finally, \cite{Har11} uses ideas from information geometry to define generalizations of the replicator dynamics, and introduces Riemannian metrics to state and prove certain properties of the induced dynamics.
Ignoring boundary issues, Harper's dynamics are an important special case of ours \textendash\ specifically, the class of separable dynamics that we introduce in Example \ref{ex:separable}.

In this paper, we develop a unifying mathematical framework for all of the foregoing dynamics, provide them with microfoundations, and substantially generalize a variety of stability and convergence results known for various specific cases.

\section{Geometric preliminaries}
\label{sec:Riemannian}

The main goal of Riemannian geometry is to study the concepts of distance, length and curvature on ``manifolds'' \textendash\ that is, topological spaces that locally look like vector spaces.
Here we briefly introduce some basic ideas from Riemannian geometry needed to define the class of dynamics under study.
For a comprehensive introduction, we refer the reader to the masterful account of \cite{Lee97,Lee03}.

\subsection{Riemannian metrics and associated notions}
\label{sec:geometry}

The most basic notion in Riemannian geometry is that of a \emph{Riemannian metric}, a position-dependent variant of the ordinary (Euclidean) scalar product between vectors.
To define it, recall first that a \emph{scalar product} on a finite-dimensional real space ${{\mathbb{R}}^{n}}$ (henceforward referred to as the \emph{model space}) is a bilinear pairing $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}{\colon} {{\mathbb{R}}^{n}}\times {{\mathbb{R}}^{n}}\to {\mathbb{R}}$ which satisfies the following conditions for all $w,w'\in{{\mathbb{R}}^{n}}$:
\begin{enumerate}
\item
\emph{Symmetry:}
$\product{w}{w'} = \product{w'}{w}$.
\item
\emph{Positive-definiteness:}
$\product{w}{w} \geq 0$, with equality if and only if $w=0$.
\end{enumerate}
The \emph{norm} of a vector $w\in {{\mathbb{R}}^{n}}$ is then defined as
\begin{equation}
\label{eq:norm}
\norm{w}
	= \product{w}{w}^{1/2}.
\end{equation}

To express the above in components, take ${{\mathbb{R}}^{n}} = {\mathbb{R}}^{\mathcal{A}}$ as the model space and let $\{{e}_{\alpha}\}_{\alpha\in{\mathcal{A}}}$ be the canonical basis of ${{\mathbb{R}}^{n}}$.
Then, if we write $w = {\sum\nolimits}_{\alpha} w_{\alpha} {e}_{\alpha}$ and $w' = {\sum\nolimits} w_{\beta}' {e}_{\beta}$, the bilinearity of $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}$ implies that
\begin{equation}
\label{eq:scalar}
\product{w}{w'}
	= {\sum\nolimits}_{\alpha}{\sum\nolimits}_{\beta}w_{\alpha}  g_{\alpha\beta} w_{\beta}'
\end{equation}
for some symmetric positive-definite matrix $g = \left( g_{\alpha\beta} \right)_{\alpha,\beta\in{\mathcal{A}}}$.  This matrix is called the \emph{metric tensor} of $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}$, and its components (also known as the \emph{metric elements} of $g$) are given by
\begin{equation}
\label{eq:metric}
g_{\alpha\beta}
	= \product{{e}_{\alpha}}{{e}_{\beta}}.
\end{equation}
A scalar product is uniquely defined by its metric tensor and vice versa, so the two notions will be used interchangeably throughout.

Now, given an open subset ${U}$ of ${{\mathbb{R}}^{n}}$ and a base point $x\in{U}$, let ${{\mathbb{R}}^{n}}_{x} = \setdef{(x,w)}{w\in{{\mathbb{R}}^{n}}}$ denote the space of \emph{displacement} (or \emph{tangent}) \emph{vectors} ``attached'' to $x$;
more formally, we refer to the pair $(x,w) \in {{\mathbb{R}}^{n}}_{x}$ as \emph{$w$ based at $x$}.
A \emph{Riemannian metric} on ${U}$ is then defined as a $C^{1}$-smooth assignment of scalar products $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}_{x}$ (and hence norms $\norm{\mathopen{}\cdot\mathopen{}}_{x}$) to each ${{\mathbb{R}}^{n}}_{x}$ \textendash\ or, equivalently, as a $C^{1}$ field $g(x)$ of symmetric positive definite matrices on ${U}$.

In words, a Riemannian metric on ${U}$ prescribes a way of measuring lengths of tangent vectors at each point $x\in{U}$.\footnote{In the pair $(x,w)$, $x$ simply acts as a base point label for $w\in {{\mathbb{R}}^{n}}$;
apart from that, ${{\mathbb{R}}^{n}}_{x}$ inherits the vector space structure of the model space ${{\mathbb{R}}^{n}}$.
In view of the natural isomorphism  $(x,w)\mapsto w$ from ${{\mathbb{R}}^{n}}_{x}$ to ${{\mathbb{R}}^{n}}$, we will use the same notation for ${{\mathbb{R}}^{n}}$ and its ``tagged'' variant ${{\mathbb{R}}^{n}}_{x}$ unless there is danger of confusion.
In the same spirit, we will also write, e.g., $\norm{w}_{x}$ in place of $\norm{(x,w)}_{x}$.}
To build intuition, we proceed with two key examples, taking ${{\mathbb{R}}^{n}} = {\mathbb{R}}^{\mathcal{A}}$ as the model space and using its standard basis $\{{e}_{\alpha}\}_{\alpha\in{\mathcal{A}}}$ for coordinate calculations.

\begin{example}
\label{ex:Eucl}
The \emph{Euclidean metric} on ${U} = {\mathbb{R}}^{\mathcal{A}}$ is defined as
\begin{equation}
\label{eq:Eucl}
g_{\alpha\beta}(x)
	= \delta_{\alpha\beta}
	\quad
	\text{for all $x\in{U}$},
\end{equation}
so $g(x) = \delta$ is simply the identity matrix.
This definition yields the standard expressions $\product{w}{w'}_{x} = {\sum\nolimits}_{\alpha} w_{\alpha} w_{\alpha}'$ and $\norm{w}_{x} = \left({\sum\nolimits}_{\alpha} w_{\alpha}^{2}\right)^{1/2}$, both independent of $x$.
\end{example}

\begin{figure}[t]
\subfigure
[Euclidean unit balls]
{\label{fig:balls-orthant-Eucl}\includegraphics[width=.48\textwidth]{Figures/OrthantBalls-Eucl}} 
\hfill
\subfigure
[Shahshahani unit balls]
{\label{fig:balls-orthant-Shah}\includegraphics[width=.48\textwidth]{Figures/OrthantBalls-Shah}}
\caption{\small
Unit balls on the positive orthant of ${\mathbb{R}}^{2}$ under the Euclidean and Shahshahani metrics.
For each base point $x$ shown, the shaded regions comprise all tangent vectors $w$ based at $x$ that  satisfy $\norm{w}_{x} \leq 1$ (scaled down for illustration).
For large values of $x_{1}/x_{2}$, Shahshahani balls appear as ellipsoids that are elongated along ${e}_{1}$ (and vice versa for small values of $x_{1}/x_{2}$).
}
\label{fig:balls-orthant}
\end{figure}

\begin{example}
\label{ex:Shah}
For a less trivial example, let ${U} = {{\mathcal{K}}^{\circ}}$ be the open positive orthant of ${\mathbb{R}}^{\mathcal{A}}$.
Then the \emph{Shahshahani metric} on ${U}$ is defined as:
\begin{equation}
\label{eq:Shah}
g_{\alpha\beta}(x)
	= \delta_{\alpha\beta}/x_{\beta}
	\quad
	\text{for all $x\in{U}$}.
\end{equation}
Eq.~\eqref{eq:Shah} yields the Shahshahani inner product $\product{w}{w'}_{x} = {\sum\nolimits}_{\alpha} w_{\alpha} w_{\alpha}'/x_{\alpha}$, which, in contrast to the Euclidean product, is $x$-dependent.
For instance, since $\norm{{e}_{\alpha}}_{x} = x_{\alpha}^{-1/2}$, the sphere of vectors at $x$ with (Shahshahani) norm $1$ is squeezed toward the $x_{\alpha}$ axis as $x_{\alpha}\to 0$ (cf. Figs.~\ref{fig:balls-orthant} and \ref{fig:balls-simplex}).
\end{example}

The metrics in the previous examples were defined on open sets ${U}$ containing the positive orthant ${{\mathcal{K}}^{\circ}} \equiv {\mathbb{R}}_{++}^{\mathcal{A}}$;
the latter may be viewed as a set of generalized population states whose total masses may differ from $1$.
In order to define our class of game dynamics, we will extend these definitions and related constructions to the entire \emph{closed} orthant ${\mathcal{K}} = \operatorname{cl}({{\mathcal{K}}^{\circ}}) = {\mathbb{R}}_{+}^{\mathcal{A}}$ (Sec.~\ref{sec:boundary}), and then adapt the results to objects defined on the simplex ${\mathcal{X}} = {\Delta}({\mathcal{A}}) \subseteq {\mathcal{K}}$ (Sec.~\ref{sec:proj}). 

\begin{figure}[t]
\subfigure
[Euclidean unit balls]
{\label{fig:balls-simplex-Eucl}\includegraphics[width=.48\textwidth]{Figures/Balls-Eucl}} 
\hfill
\subfigure
[Shahshahani unit balls]
{\label{fig:balls-simplex-Shah}\includegraphics[width=.48\textwidth]{Figures/Balls-Shah}} 
\caption{\small
Unit balls on the $3$-simplex.
}
\label{fig:balls-simplex}
\end{figure}

\subsection{Duality: payoffs as covectors}
\label{sec:dual}

A linear functional $\omega{\colon}{{\mathbb{R}}^{n}}\to{\mathbb{R}}$ acting on vectors $w \in W$ is called a \emph{covector}, and the space ${({\mathbb{R}}^{n})^{\ast}}$ of such functionals is called the \emph{dual space} of ${{\mathbb{R}}^{n}}$.
We use the notation $\braket{\omega}{w}$ to denote the action of a covector $\omega \in {({\mathbb{R}}^{n})^{\ast}}$ on a vector $w \in {{\mathbb{R}}^{n}}$;
to emphasize this pairing, the elements of ${{\mathbb{R}}^{n}}$ and ${({\mathbb{R}}^{n})^{\ast}}$ are also referred to as \emph{primal} and \emph{dual} \emph{vectors} respectively.

Taking ${{\mathbb{R}}^{n}} = {\mathbb{R}}^{\mathcal{A}}$ as the model space, the standard (primal) basis $\{{e}_{\alpha}\}_{\alpha\in{\mathcal{A}}}$ of ${{\mathbb{R}}^{n}}$ induces a corresponding dual basis $\{{\varepsilon}_{\alpha}\}_{\alpha\in{\mathcal{A}}}$ via the identity $\braket{{\varepsilon}_{\alpha}}{{e}_{\beta}} = \delta_{\alpha\beta}$.
In this context, we can easily distinguish between vectors and covectors using matrix notation:
primal vectors $w = {\sum\nolimits}_{\alpha} w_{\alpha} {e}_{\alpha} \in {{\mathbb{R}}^{n}}$ correspond to column vectors, whereas dual vectors $\omega = {\sum\nolimits}_{\alpha} \omega_{\alpha} {\varepsilon}_{\alpha} \in {({\mathbb{R}}^{n})^{\ast}}$ correspond to row vectors.
From this point of view, the action $\braket{\omega}{w}$ of $\omega$ on $w$ is just the matrix product $\omega\,w = {\sum\nolimits}_{\alpha} \omega_{\alpha} w_{\alpha}$.

This brings us to the point of departure for our game-theoretic analysis:
in many contexts, payoff ``vectors'' naturally act as linear functionals on displacement vectors, and so should be regarded as \emph{covectors}.\footnote{Although this distinction is rarely made in evolutionary game theory, it is standard in learning and optimization theory; see \cite{NY83}, \cite{SS11}, and references therein.}
We illustrate this point through four examples below:

\begin{example}
\label{ex:Nash}
The definition \eqref{eq:Nash} of Nash equilibrium can be written equivalently as
\begin{equation}
{\sum\nolimits}_{\alpha} {v}_{\alpha}({x^{\ast}}) (x_{\alpha} - {x^{\ast}}_{\alpha})
	\leq 0
	\quad
	\text{for all $x\in{\mathcal{X}}$,}
\end{equation}
or, in primal-dual (vector-covector) notation,
\begin{equation}
\label{eq:Nash-variational}
\braket{{v}({x^{\ast}})}{x - {x^{\ast}}}
	\leq 0
	\quad
	\text{for all $x\in{\mathcal{X}}$.}
\end{equation}
Since the displacement vector $z = x - {x^{\ast}}$ is tangent to ${\mathcal{X}}$, \eqref{eq:Nash-variational} only restricts the action of 
payoff covectors on the tangent hyperplane ${{\mathbb{R}}_{0}}^{\mathcal{A}} = \setdef{z\in{\mathbb{R}}^{\mathcal{A}}}{\sum_{\alpha} z_{\alpha} = 0}$.
From a game-theoretic standpoint, this reflects the fact that Nash equilibria are not affected if payoffs to all strategies are shifted by the same amount.
\end{example}

\begin{example}
\label{ex:potential2}
The defining property \eqref{eq:potential} of potential games can be expressed as
\begin{equation}
\label{eq:potential2}
\braket{d{f}(x)}{z}
	= \braket{{v}(x)}{z}
	\quad
	\text{for all $z \in {\mathbb{R}}^{\mathcal{A}}$ and all $x\in{\mathcal{X}}$.}
\end{equation}
On the left hand side, $d{f}(x)$ denotes the  \emph{differential} of ${f}$ at $x$, a linear functional that acts on tangent vectors $z\in{\mathbb{R}}^{\mathcal{A}}$ to yield the \emph{directional derivative} $D_{z}{f}(x)$.
Thus, \eqref{eq:potential2} can be expressed as an equality between covectors:
${v}(x) = d{f}(x)$.\footnote{\label{fn:PG2}The equality in \eqref{eq:potential2} also characterizes potential games as defined in Footnote \ref{fn:PG}, with the domain of ${f}$ being the simplex ${\mathcal{X}}$.
In this case, the equality need only hold for vectors $z \in {{\mathbb{R}}_{0}}^{\mathcal{A}}$ that are tangent to ${\mathcal{X}}$ at $x$ \textendash\ that is, for vectors representing feasible changes in the population state.
Indeed, when ${f}$ is only defined on ${\mathcal{X}}$, the differential $d{f}(x)$ can only act on vectors in ${{\mathbb{R}}_{0}}^{\mathcal{A}}$.}
\end{example}

\begin{example}
\label{ex:contract2}
In defining contractive games, we can highlight the duality between payoffs and displacements in ${\mathcal{X}}$ by rewriting \eqref{eq:contract} as 
\begin{equation}
\label{eq:contract2}
\braket{{v}(x') - {v}(x)}{x' - x} 
	\leq 0
	\quad\text{for all $x,x'\in{\mathcal{X}}$.}
\end{equation}
Since the vector $x' - x$ is tangent to the simplex, this definition only restricts the action of ${v}(x')$ and ${v}(x)$ on ${{\mathbb{R}}_{0}}^{\mathcal{A}}$.
As in Example \ref{ex:Nash} above, this is an algebraic restatement of the fact that contractiveness is not affected by uniform payoff shifts.
\end{example}

\begin{example}
Finally, the positive correlation condition \eqref{eq:PC} can be written as: 
\begin{equation}
\label{eq:PC-2}
\braket{{v}(x)}{{V}(x)}
	\geq 0
	\quad
	\text{for all $x \in {\mathcal{X}}$},
\end{equation}
with equality only when ${V}(x)=0$.
Again, the vector of motion ${V}(x)$ must be tangent to ${\mathcal{X}}$ at $x$, so \eqref{eq:PC-2} only restricts the action of ${v}(x)$ on ${{\mathbb{R}}_{0}}^{\mathcal{A}}$. 
\end{example}

\subsection{Primal equivalents: representing payoffs as vectors}
\label{sec:sharp}

Our aim in this paper is to study evolutionary dynamics whose vectors of motion ${V}(x)$ respect payoffs ${v}(x)$ to the greatest possible extent.
This raises our next key point:
because payoffs ${v}(x)$ are covectors, the definition of such dynamics requires an operation that translates covectors into vectors that represent displacements from the current state.

To that end, let $g$ be a Riemannian metric on an open subset ${U}$ of ${{\mathbb{R}}^{n}}$, and fix a base point $x\in{U}$.
The \emph{primal equivalent} of a dual vector $\omega \in{({\mathbb{R}}^{n})^{\ast}}$ at $x\in{U}$ is the (unique) primal vector $\omega^{\sharp} \in {{\mathbb{R}}^{n}}$ (pronounced ``\emph{$\omega$-sharp}'') satisfying
\begin{equation}
\label{eq:sharp}
\braket{\omega}{w}
=\product{\omega^{\sharp}}{w}_x	
	\quad
	\text{for all $w\in{{\mathbb{R}}^{n}}$}.
\end{equation}
In other words, the action of $\omega\in{({\mathbb{R}}^{n})^{\ast}}$ on $w\in{{\mathbb{R}}^{n}}$ coincides with the scalar product between $\omega^{\sharp}$ and $w$ at $x$.
For concision, the notation $\omega^{\sharp}$ suppresses the dependence on $x$;
when we want to emphasize this dependence, we will write $g^{\sharp}(x){\colon} {({\mathbb{R}}^{n})^{\ast}}\to{{\mathbb{R}}^{n}}$ for the linear map $\omega\mapsto \omega^{\sharp}$ at $x$ (cf.~Eq.~\eqref{eq:sharp-coords} and Footnote \ref{fn:sharp} below).

To express the above in components, let $\{{e}_{\alpha}\}_{\alpha\in{\mathcal{A}}}$ denote the standard basis of the model space ${{\mathbb{R}}^{n}} = {\mathbb{R}}^{\mathcal{A}}$, and let $\omega_{\alpha} \equiv \braket{\omega}{{e}_{\alpha}}$ be the $\alpha$-th component of $\omega\in{({\mathbb{R}}^{n})^{\ast}}$.
Then, writing
\begin{equation}
\label{eq:sharp-coords}
\omega_{\alpha}^{\sharp}
	= {\sum\nolimits}_{\beta}   g_{\alpha\beta}^{\sharp}(x) \, \omega_{\beta}
\end{equation}
for the $\alpha$-th component of $\omega^{\sharp}$, the fundamental relation $\omega_{\alpha} = \braket{\omega}{{e}_{\alpha}} = \product{\omega^{\sharp}}{{e}_{\alpha}} = {\sum\nolimits}_{\beta}  g_{\alpha\beta}(x) \, \omega_{\beta}^{\sharp}$ readily yields
\begin{equation}
\label{eq:ginv}
g_{\alpha\beta}^{\sharp}(x)
	= g_{\alpha\beta}^{-1}(x),
\end{equation}
where $g_{\alpha\beta}^{-1}(x)$ are the components of the inverse matrix $g^{-1}(x)$ of $g(x)$. 
Thus, in matrix notation, $g^{\sharp}(x)$ is represented by $g^{-1}(x)$.\footnote{\label{fn:sharp}
Note though that the sharp operation incorporates a transposition, converting row vectors $\omega$ into column vectors $\omega^\sharp = (\omega\hspace{1pt} g^{\sharp}(x))^{\!\top}$.}

\begin{example}
Since the Euclidean metric tensor has $g_{\alpha\beta}(x) = \delta_{\alpha\beta} = g_{\alpha\beta}^{\sharp}(x)$ for all $x\in{\mathbb{R}}^{\mathcal{A}}$, we get
\begin{equation}
\label{eq:sharp-Eucl}
\omega_{\alpha}^{\sharp}
	= \omega_{\alpha}
	\quad
	\text{for all $\alpha\in{\mathcal{A}}$.}
\end{equation}
In other words, $\omega$ and $\omega^{\sharp}$ have the same coordinates
in the standard basis of ${\mathbb{R}}^{\mathcal{A}}$,
though $\omega$ should be treated as a row vector and $\omega^{\sharp}$ as a column vector.
\end{example}

\begin{example}
Since the Shahshahani metric tensor has $g_{\alpha\beta}(x) = \delta_{\alpha\beta}/x_{\beta}$ for all $x\in{\mathbb{R}}_{++}^{\mathcal{A}}$, we have $g_{\alpha\beta}^{\sharp}(x) = \delta_{\alpha\beta} x_{\beta}$ for its inverse;
hence:
\begin{equation}
\label{eq:sharp-Shah}
\omega_{\alpha}^{\sharp}
	= x_{\alpha} \omega_{\alpha}
	\quad
	\text{for all $\alpha\in{\mathcal{A}}$.}
\end{equation}
\end{example}

\subsection{Extensions to boundary points}
\label{sec:boundary}

We now extend the constructions of the previous sections to the orthant's boundary, and thus to population states where some actions are unused.
For that, recall that ${\mathcal{K}} = {\mathbb{R}}_{+}^{\mathcal{A}}$ denotes the non-negative orthant of ${\mathbb{R}}^{\mathcal{A}}$, and that ${{\mathcal{K}}^{\circ}} = {\mathbb{R}}_{++}^{\mathcal{A}}$ its interior.
The \emph{tangent space} $\operatorname{T}_{\mathcal{K}}(x)$ to ${\mathcal{K}}$ at $x\in{\mathcal{K}}$ is the linear subspace
\begin{equation}
\label{eq:tspace-orth}
\operatorname{T}_{\mathcal{K}}(x)
	
	= \setdef{z\in{\mathbb{R}}^{\mathcal{A}}}{z_\alpha = 0\text{ whenever }x_\alpha=0}
	= {\mathbb{R}}^{\operatorname{supp}(x)}.
\end{equation}

With this background at hand, we introduce a number of definitions below.
A Riemannian metric $g$ on ${{\mathcal{K}}^{\circ}}$ is said to be \emph{extendable} to ${\mathcal{K}}$ if the associated operator $g^{\sharp}$ admits a $C^1$ extension to ${\mathcal{K}}$ (also denoted by $g^{\sharp}$) such that
\begin{equation}
\label{eq:extension}
\operatorname{T}_{\mathcal{K}}(x)
	\subseteq \operatorname{im} g^{\sharp}(x)\;\text{ for all }x\in{\mathcal{K}}.
\end{equation}
In the above, $\operatorname{im} g^{\sharp}(x)$ is the image (column space) of $g^{\sharp}(x)$;
we henceforth call this set the \emph{domain} of $g$ at $x$ and denote it as ${\operatorname{dom} g}(x)$.
If ${\operatorname{dom} g}(x) = {\mathbb{R}}^{\mathcal{A}}$ for all $x \in {\mathcal{K}}$, we say that $g$ is \emph{full-rank extendable};
if instead ${\operatorname{dom} g}(x) = \operatorname{T}_{\mathcal{K}}(x)$ for all $x \in {\mathcal{K}}$, we say that $g$ is \emph{minimal-rank extendable}.

Given our game-theoretic motivation, it is natural to define extendability in terms of $g^{\sharp}$, the operator that converts payoffs from covector form into vector form.
The implications for the underlying metric $g$ are presented in the following proposition (which we prove in Appendix \ref{app:proofs}):

\begin{proposition}
\label{prop:extension}
Let $g$ be an extendable Riemannian metric on ${\mathcal{K}}$.
Then, for all $x\in{\mathcal{K}}$, there exists a unique scalar product $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}_{x}$ on ${\operatorname{dom} g}(x) $ such that $\product{w}{w'}_{x_{k}} \to \product{w}{w'}_{x}$ for all $w,w'\in{\operatorname{dom} g}(x)$ and for every interior sequence $x_{k}\to x$.
Moreover, if $g$ is minimal-rank extendable, we have $g_{\alpha\beta}^{\sharp}(x) = 0$ whenever $\alpha,\beta\notin\operatorname{supp}(x)$.
\end{proposition}

\begin{example}
\label{ex:Eucl-ext}
The Euclidean metric \eqref{eq:Eucl} is full-rank extendable by default:
we simply have $g_{\alpha\beta}^{\sharp}(x)=g_{\alpha\beta}(x) = \delta_{\alpha\beta}$ for all $x\in{\mathcal{K}}$.
\end{example}

\begin{example}
\label{ex:Shah-ext}
The Shahshahani metric \eqref{eq:Shah} has $g_{\alpha\beta}^{\sharp}(x) = x_{\alpha} \delta_{\alpha\beta}$, so ${\operatorname{dom} g}(x) =\operatorname{T}_{\mathcal{K}}(x) = {\mathbb{R}}^{\operatorname{supp}(x)}$.
Thus, the Shahshahani metric is minimal-rank extendable, and for each $x \in {\mathcal{X}}$, the induced scalar product on ${\operatorname{dom} g}(x)$ is
\begin{equation}
\product{w}{w'}_{x}
	= {\sum\nolimits}_{\alpha\in\operatorname{supp}(x)} w_{\alpha} w_{\alpha}' / x_{\alpha}
	\quad
	\text{for all $w,w' \in \operatorname{T}_{\mathcal{K}}(x)$.}
\end{equation}
\end{example}

\begin{remark}
\label{rem:gsharpBdMin}
In the minimal-rank case, the scalar product $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}_{x}$ on ${\operatorname{dom} g}(x)$ can be represented by the matrix $g(x) = (g^{\sharp}(x))^+$, the (Moore-Penrose) pseudoinverse of $g^{\sharp}(x)$ (see Appendix \ref{app:geometry}).
This means that the submatrix $(g_{\alpha\beta}(x))_{\alpha,\beta \in \operatorname{supp}(x)}$ is the inverse of the submatrix $(g_{\alpha\beta}^{\sharp}(x))_{\alpha,\beta \in \operatorname{supp}(x)}$, while the remaining components of $g(x)$ (which have no geometric significance) are set to $0$.
\end{remark}

\begin{remark}
The previous examples highlight an important dichotomy between full-rank and minimal-rank extendability.
In a certain sense, metrics that are full-rank extendable to the closed orthant ${\mathcal{K}}$ do not differentiate between its interior and its boundary.
By contrast, minimal-rank extendable metrics partition ${\mathcal{K}}$ into the relative interiors of each of its faces (including ${\mathcal{K}}$ itself), a distinction which plays a key role in what follows.
Specifically, when a minimal-rank extendable metric leads to well-posed dynamics, the dynamics' formula for the interior also works at the boundary.
This is not the case for full-rank extendable metrics.
\end{remark}

\begin{remark}
Of course, there are Riemannian metrics whose extensions are of neither full rank nor minimal rank.
Such metrics are of limited interest for our purposes so, in what follows, ``extendable'' will mean ``full-rank or minimal-rank extendable''.
\end{remark}

\subsection{Tangent projections}
\label{sec:proj}

The first step in obtaining a dynamical system on ${\mathcal{X}}$ from the game's payoff covectors ${v}(x)$ is to employ the sharp operation to turn them into vectors ${v}^{\sharp}(x)$.
Generally however, ${v}^{\sharp}(x)$ is not a feasible direction of motion on ${\mathcal{X}}$ at $x$:
its components may not sum to zero, and it may have negative components for strategies that are not used at $x$.
We thus need to project ${v}^{\sharp}(x)$ onto the tangent cone $\operatorname{TC}_{\mathcal{X}}(x)$ to ${\mathcal{X}}$ at $x$;
like the sharp operation, this projection depends on the Riemannian metric $g$ and, in particular, it may vary with $x$.

\begin{figure}
\footnotesize
\input{Figures/Cones}
\label{fig:cones}
\end{figure}

More precisely, for all $x \in {\mathcal{X}}$ and $w\in{\operatorname{dom} g}(x)$, the \emph{tangent projection} of $w$ at $x$ is defined as
\begin{equation}
\label{eq:tproj}
\operatorname{\Pi}_{x}(w)
	= \operatorname*{arg\,min} \setdef{\norm{w - z}_{x}}{z\in\operatorname{TC}_{\mathcal{X}}(x)\cap{\operatorname{dom} g}(x)}.
\end{equation}
The feasible set $\operatorname{TC}_{\mathcal{X}}(x)\cap{\operatorname{dom} g}(x)$ of \eqref{eq:tproj} consists of all tangent vectors $w\in\operatorname{TC}_{\mathcal{X}}(x)$ that also lie in the domain ${\operatorname{dom} g}(x)$ of $g$ at $x$.
We call such vectors \emph{$g$-admissible}, and we write
\begin{equation}
{\textrm{Adm}_{g}}(x)
	= \operatorname{TC}_{\mathcal{X}}(x)\cap{\operatorname{dom} g}(x)
\end{equation}
for the cone of $g$-admissible tangent vectors at $x$.

The restrictions on $w$ and $z$ above are required because ${\operatorname{dom} g}(x)$ is the set on which the norm $\norm{\cdot}_{x}$ is defined.
The only case in which  ${\operatorname{dom} g}(x)$ is not all of  ${\mathbb{R}}^{\mathcal{A}}$ occurs when $x \in \operatorname{bd}({\mathcal{X}})$ and $g$ is minimal-rank extendable;
in this case, ${\operatorname{dom} g}(x) = \operatorname{T}_{\mathcal{X}}(x) = {\mathbb{R}}^{\operatorname{supp}(x)}$, and ${v}^{\sharp}(x)$, the vector we will ultimately project, lies in this set (since ${\operatorname{dom} g}(x) = \operatorname{im} g^{\sharp}(x)$ by definition).  

Some instances of $g$-admissible cones are depicted in Figure~\ref{fig:cones}.
With this figure as a point of reference, we examine \eqref{eq:tproj} in detail in three exhaustive cases.

\subsubsection*{Interior points}
If $x \in {{\mathcal{X}}^{\circ}}$ is interior, we have $\operatorname{TC}_{\mathcal{X}}(x) = \operatorname{T}_{\mathcal{X}}(x) = {{\mathbb{R}}_{0}}^{\mathcal{A}}$ and ${\operatorname{dom} g}(x) = {\mathbb{R}}^{\mathcal{A}}$.
Thus, the $g$-admissible set is the hyperplane
\begin{equation}
\label{eq:ProjDomInt}
{\textrm{Adm}_{g}}(x)
	= {{\mathbb{R}}_{0}}^{\mathcal{A}} \cap {\mathbb{R}}^{\mathcal{A}}
	= {{\mathbb{R}}_{0}}^{\mathcal{A}},
\end{equation}
and the tangent projection \eqref{eq:tproj} is just the orthogonal projection of $w\in{\operatorname{dom} g}(x) = {\mathbb{R}}^{\mathcal{A}}$ onto ${\mathbb{R}}^{\mathcal{A}}_0$.
Accordingly, $\operatorname{\Pi}_{x}(w)$ can be computed by finding a normal vector to $\operatorname{TC}_{\mathcal{X}}(x)={{\mathbb{R}}_{0}}^{\mathcal{A}}$ and subtracting this vector's contribution to $w$ (as in the first step of the Gram-Schmidt orthonormalization process).

To carry this out, let ${\mathds{1}} = (1,\dotsc,1)$, so $\braket{\mathds{1}}{z} = {\sum\nolimits}_{\alpha} z_{\alpha} = 0$ for all $z\in{{\mathbb{R}}_{0}}^{\mathcal{A}}$.
If we then let
\begin{equation}
\label{eq:normal}
{n}(x)
	\equiv {\mathds{1}}^{\sharp}
	= \sum_{\alpha,\beta\in{\mathcal{A}}} g_{\alpha\beta}^{\sharp}(x) {e}_{\beta},
\end{equation}
we obtain
\begin{equation}
\product{{n}(x)}{z}_{x}
	= \braket{\mathds{1}}{z}
	= 0
	\quad
	\text{for all $z\in{{\mathbb{R}}_{0}}^{\mathcal{A}}$},
\end{equation}
i.e. ${n}(x)$ is a \emph{normal vector} to ${\mathbb{R}}^{\mathcal{A}}$ at $x$ with respect to $g$
(see Fig.~\ref{fig:normals}).
We may thus express the tangent projection $\operatorname{\Pi}_{x}(w)$ as
\begin{equation}
\label{eq:tproj-orth}
\operatorname{\Pi}_{x}(w)
	= w - \operatorname{proj}_{{n}(x)} w
	= w - \frac{\product{{n}(x)}{w}_{x}}{\norm{{n}(x)}_{x}^{2}} {n}(x).
\end{equation}
Using \eqref{eq:tproj-orth} and the definition of ${n}(x)$, we can write $\operatorname{\Pi}_{x}(w)$ in coordinate form as
\begin{equation}
\label{eq:tproj-coords}
\left( \operatorname{\Pi}_{x}(w) \right)_{\alpha}
	= w_{\alpha} - \frac{{\sum\nolimits}_{\beta} w_{\beta}}{{\sum\nolimits}_{\beta} {n}_{\beta}(x)} {n}_{\alpha}(x),
\end{equation}
where
${n}_{\alpha}(x)	
	=  {\sum\nolimits}_{\beta} g_{\alpha\beta}^{\sharp}(x)$
is the $\alpha$-th component of the normal vector 
to ${\mathcal{X}}$ at $x$.

\begin{figure}[t]
\subfigure[Euclidean unit normal vectors]{
\label{fig:proj-Eucl}
\includegraphics[width=.48\textwidth]{Figures/Normals-Eucl}}
\hfill
\subfigure[Shahshahani unit normal vectors]{
\label{fig:proj-Shah}
\includegraphics[width=.48\textwidth]{Figures/Normals-Shah}}\caption{\small
Unit normal vectors to the simplex ${\mathcal{X}}$ under the Euclidean and Shahshahani metrics.
Unit normals are obtained by dividing ${n}(x)={\mathds{1}}^{\sharp}$ by its norm $\norm{{n}(x)}_{x}$.
The shaded regions represent the metrics' unit balls at each base point (cf.~Fig.~\ref{fig:balls-orthant}).}
\label{fig:normals}
\end{figure}

\subsubsection*{Boundary points: the minimal-rank case}
Suppose that $x \in \operatorname{bd}({\mathcal{X}})$ and $g$ is minimal-rank extendable at $x$ (as with the Shahshahani metric).
Since $\operatorname{TC}_{\mathcal{X}}(x) = \setdef{z\in{{\mathbb{R}}_{0}}^{\mathcal{A}}}{z_\alpha \geq 0 \text{ if } x_{\alpha} = 0}$ and ${\operatorname{dom} g}(x) = \operatorname{T}_{\mathcal{K}}(x) = {\mathbb{R}}^{\operatorname{supp}(x)}$, the corresponding $g$-admissible set is
\begin{equation}
\label{eq:ProjDomBdMin}
{\textrm{Adm}_{g}}(x)
	= \operatorname{TC}_{\mathcal{X}}(x) \cap \operatorname{T}_{\mathcal{K}}(x)
	 = {{\mathbb{R}}_{0}}^{\mathcal{A}} \cap {\mathbb{R}}^{\operatorname{supp}(x)}= \operatorname{T}_{\mathcal{X}}(x).
	
\end{equation}
The difference with the previous case is that ${\operatorname{dom} g}(x)$ is now ${\mathbb{R}}^{\operatorname{supp}(x)}$ rather than the entire ambient space ${\mathbb{R}}^{\mathcal{A}}$.
Proceeding along the lines above, we observe that ${n}(x)$ lies in ${\operatorname{dom} g}(x)={\mathbb{R}}^{\operatorname{supp}(x)}$ and is normal to $\operatorname{T}_{\mathcal{X}}(x)={{\mathbb{R}}_{0}}^{\mathcal{A}} \cap {\mathbb{R}}^{\operatorname{supp}(x)}$.  
Thus the argument leading to Eqs.~\eqref{eq:tproj-orth} and \eqref{eq:tproj-coords} shows that the projection $\operatorname{\Pi}_{x}(w)$ is described by these same equations.
The main novelty is that since both $w$ and ${n}(x)$ lie in ${\operatorname{dom} g}(x) = {\mathbb{R}}^{\operatorname{supp}(x)}$,   $w_{\alpha}$ and ${n}_{\alpha}(x)$ both vanish in \eqref{eq:tproj-orth} and \eqref{eq:tproj-coords}
when $\alpha\notin\operatorname{supp}(x)$.
This property will lead to simple expressions for our game dynamics in minimal-rank cases.

\subsubsection*{Boundary points: the full-rank case}
Finally, if $x \in \operatorname{bd}({\mathcal{X}})$ and $g$ is full-rank extendable at $x$ (as in the Euclidean case), we have ${\operatorname{dom} g}(x) = {\mathbb{R}}^{\mathcal{A}}$, and hence:
\begin{equation}
\label{eq:ProjDomBdMax}
{\textrm{Adm}_{g}}(x)
	= \operatorname{TC}_{\mathcal{X}}(x) = \setdef{z\in{{\mathbb{R}}_{0}}^{\mathcal{A}}}{z_\alpha \geq 0 \text{ whenever } x_{\alpha} = 0}.
\end{equation}
Since this $g$-admissible set is a proper cone (and not a linear subspace of ${\mathbb{R}}^{\mathcal{A}}$), obtaining an explicit expression for \eqref{eq:tproj} requires solving a convex program whose inequality constraints may be active.

\medskip

\begin{example}
\label{ex:EuclideanAgain}
In the Euclidean case, we have $\operatorname{\Pi}_{x}(w) = \operatorname*{arg\,min} \setdef{\sum_{\alpha\in{\mathcal{A}}} \left(z_{\alpha} - w_{\alpha}\right)^{2}}{z \in\operatorname{TC}_{\mathcal{X}}(x) }$.
Since $g^\sharp_{\alpha\beta} = \delta_{\alpha\beta}$, it follows easily from \eqref{eq:tproj-coords} that  
\begin{equation}
\label{eq:tproj-Eucl-int}
\left(\operatorname{\Pi}_{x}(w)\right)_{\alpha}
	= w_{\alpha} - \abs{\mathcal{A}}^{-1} \sum\nolimits_{\beta\in{\mathcal{A}}} w_{\beta}
	\quad
	\text{for all $x\in{{\mathcal{X}}^{\circ}}$}.
\end{equation}

On the other hand, given that the Euclidean metric is full-rank extendable, deriving $\operatorname{\Pi}_{x}(w)$ for $x\in\operatorname{bd}({\mathcal{X}})$ requires solving a convex program.
Doing so, \cite{LS08} obtained the expression
\begin{equation}
\label{eq:tproj-Eucl}
\left( \operatorname{\Pi}_{x}(w)\right )_{\alpha}
	= \begin{cases}
	w_{\alpha} - \abs{{\mathcal{A}}(x)}^{-1} \sum_{\beta\in{\mathcal{A}}(x)} w_{\beta}
		&\quad
		\text{if $\alpha\in{\mathcal{A}}(x)$,}
		\\
	0
		&\quad
		\text{otherwise,}
	\end{cases}
\end{equation}
where ${\mathcal{A}}(x)$ is a subset of ${\mathcal{A}}$ that maximizes the average $\abs{{\mathcal{A}}'}^{-1} \sum_{\beta\in{\mathcal{A}}'} {v}_{\beta}(x)$ over all subsets ${\mathcal{A}}'\subset{\mathcal{A}}$ that contain $\operatorname{supp}(x)$ \textendash\ cf. \cite{LS08}.\end{example}

\begin{example}
Since the Shahshahani metric is minimal-rank extendable, tangent projections are readily obtained from \eqref{eq:tproj-coords} for all $x\in{\mathcal{X}}$.
Indeed, since $g_{\alpha\beta}^{\sharp}(x) = \delta_{\alpha\beta} x_{\beta}$, we get ${n}(x) = x$ (see Fig.~\ref{fig:normals}), and hence:
\begin{equation}
\label{eq:tproj-Shah}
\left( \operatorname{\Pi}_{x}(w) \right)_{\alpha}
	= w_{\alpha} -\left({\sum\nolimits}_{\beta} w_{\beta}\right) x_{\alpha}
	\quad
	\text{for all $w \in {\operatorname{dom} g}(x) = {\mathbb{R}}^{\operatorname{supp}(x)}$.}
\end{equation}
Note that $\left( \operatorname{\Pi}_{x}(w) \right)_{\alpha} = 0-0=0$ whenever $\alpha \notin \operatorname{supp}(x)$, as anticipated above.
\end{example}

\section{Riemannian game dynamics}
\label{sec:dynamics}

\subsection{Definition and coordinate formulas}
\label{sec:RieDDef}

Given the geometric background of the previous section, defining the class of game dynamics under study is straightforward.
Specifically, let ${\mathcal{G}}\equiv{{\mathcal{G}}({\mathcal{A}},{v})}$ be a population game with payoff functions ${v}_{\alpha}{\colon}{\mathcal{X}}\to{\mathbb{R}}$, $\alpha\in{\mathcal{A}}$, and let $g$ be an extendable Riemannian metric on the positive orthant ${{\mathcal{K}}^{\circ}}\equiv{\mathbb{R}}_{++}^{\mathcal{A}}$ of ${\mathbb{R}}^{\mathcal{A}}$.
Then, the \emph{Riemannian game dynamics} induced by $g$ are
\begin{equation}
\label{eq:RGD}
\tag{RmD}
\dot x
	= \operatorname{\Pi}_{x}({v}^{\sharp}(x)),
\end{equation}
where
${v}^{\sharp}(x)$ is the primal equivalent of the payoff covector ${v}(x)$
and $\operatorname{\Pi}_{x}({v}^{\sharp}(x))$ is the tangent projection of ${v}^{\sharp}(x)$ at $x$ \textendash\ cf.~Eqs.~\eqref{eq:sharp} and \eqref{eq:tproj} respectively.

On the interior of ${\mathcal{X}}$, a direct application of \eqref{eq:tproj-coords} yields the coordinate formula
\begin{subequations}
\label{eq:RGD-coords}
\begin{equation}
\label{eq:RGD-coords1}
\dot x_{\alpha}
	= {v}^{\sharp}_{\alpha}(x) - \frac{{\sum\nolimits}_{\gamma} {v}^{\sharp}_{\gamma}(x)}{{\sum\nolimits}_{\gamma} {n}_{\gamma}(x)} {n}_{\alpha}(x),
\end{equation}
where ${v}^{\sharp}_{\alpha}(x) = {\sum\nolimits}_{\beta} g_{\alpha\beta}^{\sharp}(x){v}_{\beta}(x)$ and ${n}_{\alpha}(x) = {\sum\nolimits}_{\beta} g_{\alpha\beta}^{\sharp}(x)$ is the $\alpha$-th component of the normal vector ${n}(x)$ to ${\mathcal{X}}$ at $x$.
Alternatively, rearranging \eqref{eq:RGD-coords1} lets us express the dynamics as a linear function of the payoff covector ${v}(x)$:
\begin{equation}
\label{eq:RGD-coords2}
\dot x_{\alpha}
	= \sum_{\beta\in{\mathcal{A}}} \left[ g^\sharp_{\alpha\beta}(x) - \frac{{n}_{\alpha}(x) {n}_{\beta}(x)}{{\sum\nolimits}_{\gamma} {n}_{\gamma}(x)} \right] {v}_{\beta}(x).
\end{equation}
\end{subequations}
Equation \ref{eq:RGD-Matrix} in Appendix \ref{app:geometry} provides a concise third expression for \eqref{eq:RGD} on ${{\mathcal{X}}^{\circ}}$ in terms of a pseudoinverse matrix. 

Two remarks are now in order.
First, note that a state-dependent change of speed converts the dynamics \eqref{eq:RGD-coords} into the pleasingly symmetric form
\begin{equation}\label{eq:RGDNC}
\dot x_{\alpha}
	= {v}^{\sharp}_{\alpha}(x){{\sum\nolimits}_{\gamma} {n}_{\gamma}(x)} - {n}_{\alpha}(x) {\sum\nolimits}_{\gamma} {v}^{\sharp}_{\gamma}(x).
\end{equation}
We will see that the microfoundations for \eqref{eq:RGDNC} are particularly simple.
Thus, given that changes of speed do not affect the qualitative behavior of the dynamics, we will move freely between \eqref{eq:RGD-coords} and \eqref{eq:RGDNC} in what follows.\footnote{\label{fn:Conformal}
Note that \eqref{eq:RGDNC} is a Riemannian system in its own right \textendash\ simply take the rescaled metric $\tilde g(x) = g(x) / \norm{{n}(x)}^{2}$ and apply \eqref{eq:RGD-coords} to $\tilde g$ instead of $g$.}

Secondly, if $g$ is minimal-rank extendable, \eqref{eq:RGD-coords} is still valid when $x\in\operatorname{bd}({\mathcal{X}})$ and one can equally well take all sums in \eqref{eq:RGD-coords} only over the strategies in the support of $x$.\footnote{This is so because $g_{\alpha\beta}^{\sharp}(x) = 0$ whenever $\alpha$ or $\beta$ is not in $\operatorname{supp}(x)$ \textendash\ see also Proposition \ref{prop:extension} and the discussion following Eq.~\eqref{eq:ProjDomBdMin}.}
If instead $g$ is full-rank extendable, computing \eqref{eq:tproj} requires solving a convex program \eqref{eq:tproj} whose inequality constraints may be active;
in that case, coordinate formulas for \eqref{eq:RGD} will depend on the support of $x$.

Because of this, \eqref{eq:RGD} may fail to be continuous at the boundary of ${\mathcal{X}}$.
It will thus be convenient to call those dynamics generated by minimal-rank extendable metrics \emph{continuous Riemannian dynamics}, and those generated by full-rank extendable metrics \emph{discontinuous Riemannian dynamics}.
Owing to their simplicity, we present continuous dynamics before discontinuous ones in what follows.

\subsection{Basic examples}
\label{sec:Lexamples}

We now verify that the two basic examples of dynamics from Section \ref{sec:prelims} \textendash\ the replicator dynamics and the projection dynamics \textendash\ are induced by the Riemannian metrics highlighted in Section \ref{sec:Riemannian}.
\begin{example}
[The replicator dynamics]
\label{ex:RDAgain}
Let $g$ be the Shahshahani metric, so $g^\sharp_{\alpha\beta}(x) = \delta_{\alpha\beta}x_{\beta}$, ${n}_{\alpha}(x) = x_{\alpha}$, and ${v}^{\sharp}_{\alpha}(x)=x_{\alpha} {v}_{\alpha}(x)$.
Then, \eqref{eq:RGD-coords} and \eqref{eq:RGDNC} both yield the continuous Riemannian dynamics
\begin{equation}
\tag{\ref*{eq:RD}}
\dot x_{\alpha}
	= x_{\alpha} \left[ {v}_{\alpha}(x) - {\sum\nolimits}_{\beta} x_{\beta} {v}_{\beta}(x) \right].
\end{equation}
\end{example}

\begin{example}
[The Euclidean projection dynamics]
\label{ex:PD}
Let $g$ be the Euclidean metric, so $g_{\alpha\beta}^{\sharp}(x) = \delta_{\alpha\beta}$, ${n}_{\alpha}(x) = 1$, and ${v}^{\sharp}_{\alpha}(x)={v}_{\alpha}(x)$.
Then, \eqref{eq:tproj-Eucl} and \eqref{eq:RGD} yield the (discontinuous) projection dynamics:
\begin{equation}
\label{eq:PD}
\tag{PD}
\dot x_{\alpha}
	= \begin{cases}
	{v}_{\alpha}(x) - \abs{{\mathcal{A}}(x)}^{-1} \sum_{\beta\in{\mathcal{A}}(x)} {v}_{\beta}(x)
		&\quad
		\text{if $\alpha\in{\mathcal{A}}(x)$,}
		\\
	0
		&\quad
		\text{otherwise,}
	\end{cases}
\end{equation}
where ${\mathcal{A}}(x)$ is defined as in Example \ref{ex:EuclideanAgain}.
These dynamics were introduced by \cite{NZ97} and examined further by \citealp{LS08} and \citealp{SDL08}.
The discontinuity of \eqref{eq:PD} is reflected in the appearance of the support of the state $x$ in \eqref{eq:PD} through the definition of ${\mathcal{A}}(x)$.\end{example}

The dynamics \eqref{eq:RD} and \eqref{eq:PD} highlight an important qualitative difference between Shahshahani and Euclidean projections \textendash\ and, more generally, between continuous and discontinuous Riemannian dynamics.
The replicator dynamics \eqref{eq:RD} comprise a Lipschitz continuous dynamical system on ${\mathcal{X}}$ which preserves the face structure of ${\mathcal{X}}$ (i.e. the relative interior of each face of ${\mathcal{X}}$ remains invariant).
By contrast, the projection dynamics \eqref{eq:PD} may fail to be continuous at the boundary of ${\mathcal{X}}$ so the relevant notion of a solution to \eqref{eq:PD} is that of a CarathÃ©odory solution (which allows for kinks at the boundary);
moreover, solutions of \eqref{eq:PD} may leave and re-enter the relative interior of any face of ${\mathcal{X}}$ in perpetuity.
Despite these complications, we will show in Proposition \ref{prop:wp} that \eqref{eq:RGD} admits unique forward solutions from every initial condition in ${\mathcal{X}}$.

\subsection{Invariances to payoff transformations}
\label{sec:shifts}

Riemannian game dynamics satisfy two basic invariances:
shifting all strategies' payoffs by the same amount does not affect the dynamics,
and rescaling all strategies' payoffs by the same factor only changes the speed at which solution paths are traversed. 

To state these invariances formally, fix a population game ${\mathcal{G}}\equiv{{\mathcal{G}}({\mathcal{A}},{v})}$ with payoff functions ${v}_{\alpha}{\colon}{\mathcal{X}}\to{\mathbb{R}}$ and consider the linearly transformed game $\tilde{\mathcal{G}} \equiv \tilde{\mathcal{G}}({\mathcal{A}},\tilde{v})$ with payoff functions $\tilde{v}_{\alpha}(x) = a(x) + b(x) {v}_{\alpha}(x)$ for a pair of scalar functions $a{\colon}{\mathcal{X}}\to{\mathbb{R}}$ and $b{\colon}{\mathcal{X}}\to(0,\infty)$ that represent a (state-dependent) shift and rescaling of the original payoffs of ${\mathcal{G}}$ respectively.
Then:

\begin{proposition}
\label{prop:shifts}
With notation as above, $\operatorname{\Pi}_{x}(\tilde{v}^{\sharp}(x)) = b(x) \operatorname{\Pi}_{x}({v}^{\sharp}(x))$.
Consequently, the solution orbits of \eqref{eq:RGD} for $\tilde{\mathcal{G}}$ coincide with those for ${\mathcal{G}}$.
\end{proposition}

\begin{proof}
By definition, we have $\tilde{v}^{\sharp}(x) = a(x) {n}(x) + b(x) {v}^{\sharp}(x)$.
However, since $\product{{n}(x)}{z} = 0$ for all $z\in{\textrm{Adm}_{g}}(x) = \operatorname{TC}_{\mathcal{X}}(x) \cap {\operatorname{dom} g}(x)$, we also have
\begin{multline}
\norm{a(x){n}(x) + b(x) {v}^{\sharp}(x) - z}^{2}
	\\
	= \norm{b(x){v}^{\sharp}(x) - z}^{2}
	+ \norm{a(x) {n}(x)}^{2}
	+ 2 a(x) b(x) \product{{n}(x)}{{v}^{\sharp}(x)},
\end{multline}
so $\operatorname{\Pi}_{x}( \tilde{v}^{\sharp}(x) ) = \operatorname{\Pi}_{x} (b(x) {v}^{\sharp}(x))$.
Using the definition of $\operatorname{\Pi}_{x}$ and the fact that $b(x)>0$,
we then get
\begin{align*}
\operatorname{\Pi}_{x}\left( \tilde{v}^{\sharp}(x) \right)
	&{\textstyle}
	=\operatorname*{arg\,min}_{z'} \norm{b(x) {v}^{\sharp}(x) - z'}_{x}
	= b(x) \, \operatorname*{arg\,min}_{z'} \norm{{v}^{\sharp}(x) - z'/b(x)}_{x}
	\notag\\
	&{\textstyle}
	= b(x) \, \operatorname*{arg\,min}_{z} \norm{{v}^{\sharp}(x) - z}_{x}
	=b(x) \operatorname{\Pi}_{x}\left( {v}^{\sharp}(x) \right).
	\qedhere
\end{align*}
\end{proof}

\begin{remark}
Continuous Riemannian dynamics satisfy a further invariance:
on the face of ${\mathcal{X}}$ that is spanned by a subset ${\mathcal{A}}'$ of ${\mathcal{A}}$, the dynamics are also invariant to changes in the payoffs of strategies outside of ${\mathcal{A}}'$.
This follows directly from \eqref{eq:RGD} and the fact that $\tilde {v}^{\sharp}(x) = {v}^{\sharp}(x)$ whenever $\operatorname{supp}(x) \subseteq{\mathcal{A}}'$ and $\tilde {v}_{\alpha}(x) = {v}_{\alpha}(x)$ for all $\alpha \in {\mathcal{A}}'$ (cf.~Proposition \ref{prop:extension}).
\end{remark}

\subsection{Microfoundations}
\label{sec:micro}

So far, the motivation behind \eqref{eq:RGD} has been chiefly geometric, inspired by certain fundamental properties of the replicator and projection dynamics.
We now proceed by describing a class of strategy revision processes that induce the Riemannian dynamics \eqref{eq:RGD} in the cases where the coordinate formulas \eqref{eq:RGD-coords} and \eqref{eq:RGDNC} hold.\footnote{Handling boundary states under full-rank extendable metrics requires modifications of the sort noted in \cite{LS08}.
We do not pursue this direction here.}
Our constructions apply to metrics with nonnegative $g^\sharp$ so as to ensure that all switch rates are positive;
also, as in Examples \ref{ex:Rep} and \ref{ex:proj}, we will sometimes assume that payoffs are nonnegative or nonpositive.
Since games that differ only by a payoff shift may be regarded as equivalent under \eqref{eq:RGD} by Proposition \ref{prop:shifts}, these assumptions are innocuous.
With this in mind, we have:

\begin{proposition}
\label{prop:RieFound}
Let $g$ be an extendable Riemannian metric such that $g^{\sharp}(x)$ is nonnegative for all $x\in{{\mathcal{X}}^{\circ}}$.
Then the following protocols generate \eqref{eq:RGDNC} as their mean dynamics on ${{\mathcal{X}}^{\circ}}$:
\begin{subequations}
\label{eq:RGDF}
\begin{flalign}
\label{eq:RGDF1}
{s}_{\alpha\beta}(x,\pi)
	&= ({\mathds{1}} g^{\sharp}(x) )_{\alpha} \, (\pi g^{\sharp}(x))_{\beta},
	\\
\label{eq:RGDF2}
{s}_{\alpha\beta}(x,\pi)
	&= -(\pi g^{\sharp}(x))_{\alpha} \, ({\mathds{1}} g^{\sharp}(x) )_{\beta},
\intertext{where $\pi$ is assumed nonnegative in \eqref{eq:RGDF1} and nonpositive in \eqref{eq:RGDF2}.
If in addition $g(x)$ is diagonal, \eqref{eq:RGDNC} is also generated by the protocol}
\label{eq:RGDF3}
{s}_{\alpha\beta}(x,\pi)
	&= g_{\alpha\alpha}^{\sharp}(x) g_{\beta\beta}^{\sharp}(x) [ \pi_{\beta} -\pi_{\alpha} ]_{+}.
\end{flalign}
\end{subequations}
\end{proposition}

\begin{proof}
Substitute \eqref{eq:RGDF} in \eqref{eq:MD} and recall that ${n}(x) \equiv {\mathds{1}}^{\sharp} = ({\mathds{1}} g^{\sharp}(x))^{\top}$ and that ${v}^{\sharp}(x) =  ({v}(x) g^{\sharp}(x))^{\top}$.
\end{proof}

As an example, the revision protocols \eqref{eq:RDF} that generate the replicator dynamics are recovered from \eqref{eq:RGDF} when $g$ is the Shahshahani metric;
likewise, the protocols \eqref{eq:ProjF} that generate the projection dynamics on ${{\mathcal{X}}^{\circ}}$ are recovered (up to renormalization by $\abs{\mathcal{A}}$) when $g$ is the Euclidean metric.

In view of Proposition \ref{prop:RieFound}, a Riemannian metric can be seen as a state-dependent (but payoff-\emph{independent}) measure of the salience of each strategy and the similarities between strategies.
Saliences are represented by the diagonal elements $g_{\alpha\alpha}^{\sharp}(x)$ of $g^\sharp(x)$ and similarities by its off-diagonal elements $g^\sharp_{\alpha\beta}(x)=g^\sharp_{\beta\alpha}(x)$.\footnote{It might be preferable to refer to the latter as ``cosaliences'', reserving the term ``similarities'' for the correlation coefficients $\rho_{\alpha\beta}(x) = g_{\alpha\beta}^{\sharp}(x)/(g_{\alpha\alpha}^{\sharp}(x) g_{\beta\beta}^{\sharp}(x))^{1/2} \in [-1,1]$.}
Under \eqref{eq:RGDF}, the salience of a strategy at a given state reflects the chance that it is considered for abandonment or adoption, while the similarity of a pair of strategies indicates how the presence of one spurs the abandonment or adoption of the other.

The crucial feature of the protocols \eqref{eq:RGDF} is the symmetric influence of the matrix $g^{\sharp}(x)$ on the assignment of revision opportunities to strategies and the selection of new strategies.
For instance, assignments of revision opportunities are determined under \eqref{eq:RGDF1} by applying $g^{\sharp}(x)$ to the equal-rate covector ${\mathds{1}}$, and selection of new strategies by applying $g^\sharp(x)$ to the payoff covector ${v}(x)$.
Protocol \eqref{eq:RGDF2} reverses the roles of the two covectors;
but since payoffs now determine assignments, ${v}(x)$ is replaced by $-{v}(x)$, so better-performing strategies receive revision opportunities less often.
Finally, when $g^\sharp(x)$ is diagonal (cf.~Example \ref{ex:separable} below), protocol \eqref{eq:RGDF3} provides foundations for \eqref{eq:RGD} based on pairwise comparisons of payoffs \textendash\ again, assignment and selection are influenced by $g^\sharp(x)$ symmetrically.

\subsection{Further examples} 
\label{sec:examples}

To illustrate the scope of the class of dynamics under study, and to interpret the microfoundations introduced in the previous section,
we provide some further examples below.

\begin{example}[Separable metrics]
\label{ex:separable}
A Riemannian metric $g$ on ${{\mathcal{K}}^{\circ}}$ is called \emph{separable} if its metric tensor is of the form

\begin{equation}
\label{eq:separable}
g_{\alpha\beta}(x)
	= \delta_{\alpha\beta}/{\phi}(x_{\beta}),
\end{equation}
for some continuous \emph{weighting function} ${\phi} {\colon} [0, \infty) \to [0, \infty)$ that is strictly positive on $(0, \infty)$.
We then get
\begin{equation}
\label{eq:separable2}
g_{\alpha\beta}^\sharp(x) = \delta_{\alpha\beta} {\phi}(x_{\beta}),
\end{equation}
so $g$ is minimal-rank extendable if $\lim_{z \to 0^{+}}\phi(z) = 0$ and full-rank extendable if not.

When the coordinate formulas \eqref{eq:RGD-coords} and \eqref{eq:RGDNC} apply, the normalized dynamics \eqref{eq:RGDNC} induced by $g$ take the simple form 
\begin{equation}
\label{eq:SepDyn2}
\dot x_{\alpha}
	= {\phi}(x_{\alpha})
	\left[
	{v}_{\alpha}(x) \sum_{\beta\in{\mathcal{A}}} {\phi}(x_{\beta}) -\sum_{\beta\in{\mathcal{A}}} {\phi}(x_{\beta}) {v}_{\beta}(x)
	\right].
\end{equation}
Taking ${\phi}(z) = z$ gives the Shahshahani metric and the replicator dynamics \eqref{eq:RD},
while setting 
${\phi}(z) = 1$ yields the Euclidean metric and the projection dynamics \eqref{eq:PD}.
Ignoring the dynamics' behavior at the boundary, the unnormalized version of \eqref{eq:SepDyn2} was studied by \cite{Har11} under the name \emph{escort replicator dynamics}, and was further examined by \cite{MS16} in the context of reinforcement learning in games (see Section \ref{sec:RL}).

In view of Proposition \ref{prop:RieFound}, Eq.~\eqref{eq:separable2} implies that switch rates to and from strategy $\alpha$ are affected only by the relevant population share $x_{\alpha}$; in the language of the previous section, all pairs of distinct strategies have similarity zero.  The degree of influence generated by population share $x_{\alpha}$ is described by the function ${\phi}$.
For instance, when ${\phi}(z) = z$ switch rates between strategies are proportional to popularity, while when ${\phi}(z) = 1$ they are independent of popularity.  
We discuss a range of other possibilities in Example \ref{ex:pReplicator} below.
\end{example}

\begin{example}
[\acl{HR} metrics and the induced dynamics]
\label{ex:HR}
A generalization of the above class of examples can be obtained by considering Riemannian metrics that are defined as the Hessians of convex functions.\footnote{For the origins of the idea in a differential-geometric setting, see \cite{Shi77}, \cite{Dui01}, and references therein;
for further applications to convex programming, see \cite{BT03} and \cite{ABB04}.}
To make this precise, let $h{\colon}{\mathcal{K}}\to{\mathbb{R}}$ be a continuous function on ${\mathcal{K}}$ such that
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
$h$ is $C^{3}$-smooth on every positive suborthant of ${\mathcal{K}}$;
and
\item
$\operatorname{Hess} h(x)$ is positive-definite for all $x\in{{\mathcal{K}}^{\circ}}$ (so, in particular, $h$ is strictly convex on ${{\mathcal{K}}^{\circ}}$).
\end{inparaenum}
Then, $h$ induces a natural Riemannian metric on ${{\mathcal{K}}^{\circ}}$ defined as
\begin{equation}
\label{eq:HR}
g = \operatorname{Hess} h ,
\end{equation}
or, in components:
\begin{equation}
g_{\alpha\beta}(x)
	= \frac{{\partial}^{2} h(x)}{{\partial} x_{\alpha} {\partial} x_{\beta}}.
\end{equation}
When this is the case, we will say that $g$ is a {\emph{\acl{{HR}}} \textup(\acs{{HR}}\textup)\acused{{HR}}} metric and will refer to $h$ as the \emph{potential} of $g$.
Clearly, every separable metric \eqref{eq:separable} is an \ac{HR} metric with potential function
\begin{equation}
\label{eq:decomposable}
h(x)
	= \sum_{\alpha\in{\mathcal{A}}} \theta(x_{\alpha})
\end{equation}
for some smooth function $\theta{\colon}[0,+\infty)\to{\mathbb{R}}$ with $1/\theta''(z) = {\phi}(z)$.

Definition \eqref{eq:HR} is an integrability condition on the matrix field $g$, thus justifying the terminology ``potential'' for $h$ (though, of course, $g$ is a matrix field, not a vector field).
As with vector fields on convex sets, integrability of matrix fields can be characterized by a symmetry condition on its derivatives, namely that\footnote{This characterization follows from the integrability condition for ordinary vector fields (i.e. symmetry of the Jacobian matrix), and the symmetry of $g(x)$ \textendash\ or alternatively, through an easy application of PoincarÃ©'s lemma (see \cite{ABB04}).} 
\begin{equation}
\label{eq:integrability}
\frac{\partial g_{\alpha\gamma}}{\partial x_\beta}
	=\frac{\partial g_{\beta\gamma}}{\partial x_\alpha}
	\quad
	\text{for all $\alpha,\beta,\gamma\in{\mathcal{A}}$}.
\end{equation}
Equation \eqref{eq:integrability} is an integrability condition on \emph{matrix fields}.  It thus differs fundamentally from integrability conditions appearing in previous work on game dynamics, which are imposed on \emph{vector fields} that are used to define the dynamics.\footnote{Specifically, these vector fields take the (normalized) current payoffs of all strategies as inputs, and return (unnormalized) vectors of choice probabilities as outputs---see \cite{HMC01b}, \cite{HS09}, and \cite{San10c, San14}.}

In Section \ref{sec:HD}, we will see that this integrability property provides important theoretical tools for the analysis of the resulting Riemannian dynamics
\begin{equation}
\label{eq:HD}
\tag{HD}
\dot x
	= \operatorname{\Pi}_{x}({v}^{\sharp}(x)),
	\quad
	g
	= \operatorname{Hess} h,
\end{equation}
which we call \emph{Hessian dynamics}.
\end{example}

\begin{example}
[The $p$-\emph{replicator dynamics}]
\label{ex:pReplicator}

Given $p\geq 0$, we define the \emph{$p$-replicator dynamics} as the Riemannian dynamics
induced by the separable metric $g_{\alpha\beta}(x) = \delta_{\alpha\beta}/x_{\beta}^{p}$ (corresponding to the monomial weighting function ${\phi}(z) = z^{p}$ in the parlance of Example \ref{ex:separable}).
For $p \neq1,2$, these dynamics are generated by the potential 
\begin{equation}
\label{eq:hp}
h_{p}(x) = \sum_{\alpha\in{\mathcal{A}}} \theta_{p}(x_{\alpha})
	\quad
	\text{with}
	\quad
	\theta_p(z)
= \tfrac{1}{(p-1)(p-2)}z^{2-p}.
\end{equation}
There are three values of $p$ worth highlighting, including $p\in\{1,2\}$:\footnote{It is possible to define the potential $h_{p}$ for all values of $p$ using a single formula.
Let $\theta_p(z) = (z^{2-p} + p(p-2)z - (p-1)^{2})/((p-1)(p-2))$ when $p \ne \{1, 2\}$, and define $\theta_1$ and $\theta_2$ by analytic continuation.  Linear and constant terms do not affect the resulting metric, and the explicit formulas for $\theta_1$ and $\theta_2$ follow from the fact that $\lim_{a \to 0} (z^{a} - 1)/a = \log z$.}

\begin{enumerate}
\item
For $p=0$, we get the quadratic potential $h_0(x)=\frac{1}{2} \sum_{\alpha} x_{\alpha}^{2}$, which generates the projection dynamics \eqref{eq:PD}.

\item
For $p=1$, a direct integration yields the entropic potential $h_1(x)=\sum_{\alpha} x_{\alpha} \log x_{\alpha}$, which generates the rep\-li\-ca\-tor dynamics \eqref{eq:RD}.

\item
For $p=2$, a direct integration yields the logarithmic potential $h_2(x)=- \sum_{\alpha} \log x_{\alpha}$, which generates the \emph{log-barrier dynamics}
\begin{equation}
\label{eq:LD}
\tag{LD}
\dot x_{\alpha}
	= x_{\alpha}^{2} \left( {v}_{\alpha}(x) - \frac{\sum_{\beta} x_{\beta}^{2} {v}_{\beta}(x)}{\sum_{\beta} x_{\beta}^{2}}\right),
\end{equation}
a dynamics first examined by \cite{BL89} in the context of convex programming.
\end{enumerate}
The special cases above induce a natural partition of the class in six cases, as summarized in Table \ref{tab:pReplicator}.\footnote{The (negative) \emph{Tsallis entropy} \citep{Tsa88} mentioned in Table \ref{tab:pReplicator} is defined as $S_{q}(x) = (q-1)^{-1} \sum_{\alpha} (x_{\alpha}^{q} - x_{\alpha})$ for some $q\in(0,1)$.
To agree with this definition, it is sometimes more convenient to use the parameter $q = 2-p$ instead of $p$;
for the benefits of this convention, see \cite{Har11} and \cite{MS16}.}
We should note that when $p\geq2$, $h_{p}$ becomes infinite on the boundary of ${\mathcal{K}}$, violating a standing assumption;
we will address this technicality in Section \ref{sec:HD}.

\begin{table}[tbp]
\centering

\small
\begin{tabular}{cccc}
\hline
\noalign{\vspace{1pt}}
$p $ 
	&\textsc{name}
	&\textsc{regularity}
	&\textsc{potential}
	\\
\hline
\hline
$0$ &projection
	&discontinuous
	&quadratic
	\\
\hline
$(0,1)$
	&------
	&not Lipschitz
	&power law
	\\
\hline
$1$	
	&replicator
	&smooth
	&Gibbs entropy
	\\
\hline
$(1,2)$
	&------
	&smooth
	&Tsallis entropy
	\\
	\hline
$2$
	&log-barrier	
	&smooth
	&logarithmic
	\\
\hline
$(2,\infty)$
	&------
	&smooth
	&inverse power law
	\\
\hline
\end{tabular}
\vspace{2ex}
\caption{\small
Regularity of the $p$-replicator dynamics and behavior of the generating potential function $h_p$.
}
\label{tab:pReplicator}
\end{table}

The value of $p$ describes the degree to which a strategy's popularity hastens switches to and from that strategy.
This is illustrated in Figure \ref{fig:portraits}, which presents a collection of $p$-replicator phase portraits for different choices of $p$ in a \acl{RPS} game with payoff matrix
\begin{equation}
\label{eq:RPS}
A =
	\left(
	\begin{array}{ccc}
	0	& -1	& 1
	\\
	1	& 0	& -1
	\\
	-1	& 1	& 0
	\end{array}
	\right).
\end{equation}
When $p = 0$, switch rates have no direct dependence on the current state, so the dynamics are determined solely by payoffs.
Thus the circular form of the payoffs \eqref{eq:RPS} generates circular closed orbits, subject to feasibility constraints (Fig.~\ref{fig:portraits-Eucl}).
As $p$ increases, the role of strategies' popularities in determining switching rates becomes more important relative to their payoffs (Figs.~\ref{fig:portraits-rep} and \ref{fig:portraits-qrep}).
When $p$ is large, popular strategies are far more likely to be considered for abandonment and selection than less popular ones.
Since switches involve a pair of distinct strategies, switches between the two most popular strategies are far more likely to occur than any others, with the direction of the switch determined by payoffs.
This leads the closed orbits to take a nearly triangular form (Fig.~\ref{fig:portraits-pprep}).\footnote{That all of these dynamics feature closed orbits is not coincidental \textendash\ see Proposition \ref{prop:conservative}.}
\end{example}

\begin{figure}[t]
\centering
\subfigure[$p=0$ (Euclidean projection).]
{\includegraphics[width=.45\textwidth]{Figures/RPS-Eucl2.pdf}
\label{fig:portraits-Eucl}} 
\hfill
\subfigure[$p=1$ (replicator).]
{\includegraphics[width=.45\textwidth]{Figures/RPS-Rep.pdf}
\label{fig:portraits-rep}}
\\
\subfigure[$p=3/2$.]
{\includegraphics[width=.45\textwidth]{Figures/RPS-qRep.pdf}
\label{fig:portraits-qrep}}
\hfill
\subfigure[$p=5$.]
{\includegraphics[width=.45\textwidth]{Figures/RPS-pRep5.pdf}
\label{fig:portraits-pprep}}\caption{\small
Phase portraits of the $p$-replicator dynamics in standard \acl{RPS}.
As $p \in [0, \infty)$ increases, the shape of the closed orbits changes from circular to triangular.
When $p = 0$, solutions enter and leave the boundary of the simplex, but forward solutions exist and are unique.
For $p \geq 1$, forward and backward solutions exist, are unique, and their support is constant.
}
\label{fig:portraits}
\end{figure}

Figure \ref{fig:portraits} also illustrates a basic dichotomy between continuous and discontinuous Riemannian dynamics.
In the discontinuous case $p=0$, there is a unique forward solution from every initial condition in ${\mathcal{X}}$;
however, solutions may enter and leave the boundary of ${\mathcal{X}}$, and solutions from different initial conditions can merge in finite time.
In the smooth regime ($p\geq1$), solutions exist and are unique in forward and backward time, and the support of the state remains fixed along each solution trajectory.
Existence and uniqueness of solutions is treated formally in Section \ref{sec:wp}.

\section{General properties}
\label{sec:analysis}

In the following sections we present our main results for \eqref{eq:RGD}.
In Section \ref{sec:wp} we state a basic but technically challenging result on the existence and uniqueness of solutions.
In Section \ref{sec:basic}, we show that the dynamics exhibit positive correlation with the game's payoffs, and we characterize the dynamics' rest points as either restricted equilibria or Nash equilibria.
Finally, in Section \ref{sec:GCPG} we study the global behavior of the dynamics in potential games.

\subsection{Existence and uniqueness of solutions}
\label{sec:wp}

To illustrate the possibilities for existence and uniqueness of solutions, it is useful to start with a simple example.

\begin{example}\label{ex:EU}
We consider the $p$-replicator dynamics of Example \ref{ex:pReplicator} for a $2$-strategy game with action set ${\mathcal{A}} = \{1,2\}$ and payoff functions ${v}_{1}(x) = 1$, ${v}_{2}(x) = 0$.

When $p=1$, we obtain the toy replicator equation
\begin{equation}
\dot x_{1}
	= x_{1}(1-x_{1}).
\end{equation}
Solutions to this equation exist and are unique for all $t \in (-\infty, \infty)$, and the support of $x(t)$ is invariant.
States $0$ and $1$ are rest points, and it is easy to check that the unique solution with initial condition $x_{1}(0) = a \in (0,1)$ is $x_{1}(t) = a / [a + (1 - a) e^{-t}]$.

When $p=0$, we obtain the Euclidean projection dynamics
\begin{equation}\label{eq:ToyEPD}
\dot x_{1}
	= \begin{cases}
	1/2
		&\quad
		\text{if $x_{1} < 1$}
		\\
	0
		&\quad
		\text{if $x_{1} = 1$}.
	\end{cases}
\end{equation}
For every initial condition $x_{1}(0) \in [0,1]$, this equation admits the unique forward solution $x_{1}(t) =x_{1}(0)+ t/2$ for $t \in [0, 2)$ and $x_{1}(t)=1$ thereafter.
Evidently, the support of $x(t)$ is not invariant;
also, backward solutions are not defined for all time, and solutions are not smooth in $t$ when $x_{1}=1$ is reached.

Finally, when $p=1/2$, we obtain the differential equation
\begin{equation}
\label{eq:non-Lipschitz}
\dot x_{1}
	= \frac{\sqrt{x_{1} (1-x_{1})}}{\sqrt{x_{1}} + \sqrt{1-x_{1}}}.
\end{equation}
Although this equation admits forward (and backward) solutions from every initial condition, these are no longer unique.
From the initial condition $x_{1}(0) = 0$, there is the stationary solution $x_{1}(t) = 0$ for $t \in[0, \infty)$.
However, one can verify by a direct \textendash\ if tedious \textendash\ calculation that there is another solution, namely $x_{1}(t) = \frac{1}{2} + \frac{t-2}{4} \sqrt{1 + t - t^{2}/4}$ for $t \in [0, 4)$ and $x_{1}(t) =4$ thereafter.
Additional solutions may linger at $x_{1} = 0$ before emulating the previous solution trajectory.
\end{example}

The differences in behavior in the three cases above can be traced back to the properties of the underlying Riemannian metrics.
First, the replicator dynamics are generated by the Shahshahani metric, which is minimal-rank extendable to all of ${\mathcal{X}}$.
In this case the induced dynamics \eqref{eq:RGD} are Lipschitz continuous, so existence and uniqueness of solutions is guaranteed by the Picard\textendash LindelÃ¶f theorem (along with an argument to account for ${\mathcal{X}}$ being closed).
Moreover, the support of $x(t)$ is constant, and solutions exist in both forward and backward time \cite[Theorems 4.A.5 and 5.4.7]{San10}.

On the other hand, the Euclidean projection dynamics \eqref{eq:PD} are generated by a full-rank extendable metric.
In such cases, the induced dynamics \eqref{eq:RGD} are typically discontinuous, so the relevant solution notion is that of a \emph{CarathÃ©odory solution}, an absolutely continuous trajectory that satisfies \eqref{eq:RGD} for almost all $t\geq0$.
In the case of \eqref{eq:PD}, \cite{LS08} show that every initial condition admits a unique CarathÃ©odory solution;
however, different solution orbits can merge in finite time, as illustrated in the previous example and in Figure \ref{fig:portraits-Eucl}.

The following proposition shows that the behavior of \eqref{eq:RD} and \eqref{eq:PD} is representative of the minimal-rank and full-rank extendable cases, respectively:

\begin{proposition}
\label{prop:wp}
Let $g$ be an extendable Riemannian metric.
\begin{enumerate}
[\textup(i\textup)]
\item
\label{itm:wp-min}
If $g$ is minimal-rank extendable, \eqref{eq:RGD} admits a unique global solution from every initial condition in ${\mathcal{X}}$;
moreover, each solution has constant support.

\item
\label{itm:wp-full}
If $g$ is full-rank extendable, \eqref{eq:RGD} admits a unique forward CarathÃ©odory solution from every initial condition in ${\mathcal{X}}$.
\end{enumerate}
\end{proposition}

Proposition \ref{prop:wp} justifies the terminology \emph{continuous} and \emph{discontinuous} that we introduced in Section \ref{sec:RieDDef} to refer to dynamics induced by minimal-rank and full-rank metrics.
The nontrivial part of Proposition \ref{prop:wp} is the proof of part \eqref{itm:wp-full}:
despite an apparent similarity, this result is considerably harder than the corresponding result of \cite{LS08} for \eqref{eq:PD}, so we relegate its proof to Appendix \ref{app:proofs}.
The main reason for this difficulty is that known 
uniqueness proofs for projected differential equations depend crucially on the Riemannian metric being constant throughout the dynamics' state space, an assumption that obviously fails here.

Finally, as can be seen from the continuous \textendash\ but not \emph{Lipschitz} continuous \textendash\ system \eqref{eq:non-Lipschitz}, \eqref{eq:RGD} may fail to admit unique solutions from initial conditions at the boundary of ${\mathcal{X}}$ if the underlying metric does not admit a Lipschitz continuous extension to the boundary of ${\mathcal{X}}$.
To avoid the resulting complications, we will not treat such pathologies in the rest of our paper.\footnote{While our definitions of Riemannian metrics and extendability require $g$ to be $C^1$, nearly all of our results can be proved under the weaker assumption that $g$ is Lipschitz continuous.
The exception to this is the uniqueness claim of Proposition \ref{prop:wp}\eqref{itm:wp-full}, the proof of which uses the smoothness of $g$ in an essential way.}

\subsection{Basic properties}
\label{sec:basic}

We now establish some basic relationships between \eqref{eq:RGD} and the payoffs of the underlying game.
We first show that \eqref{eq:RGD} respects the payoff monotonicity condition \eqref{eq:PC}:

\begin{proposition}
\label{prop:PC}
The dynamics \eqref{eq:RGD} satisfy positive correlation \eqref{eq:PC}.
\end{proposition}

\begin{proof}
Let ${V}(x) = \operatorname{\Pi}_{x}({v}^{\sharp}(x))$.
We then claim that
\begin{equation}
\label{eq:PC-derivation}
\braket{{v}(x)}{{V}(x)}
	= \product{{v}^{\sharp}(x)}{{V}(x)}_x
	\geq \product{\operatorname{\Pi}_{x}({v}^{\sharp}(x))}{{V}(x)}_x
	= \norm{{V}(x)}^{2}_x
	\geq0,
\end{equation}
with equality if and only if ${V}(x) = 0$.

The only step in \eqref{eq:PC-derivation} needing justification is the first inequality.
To that end, we split the analysis into three cases corresponding to those from the definition of $\operatorname{\Pi}_{x}$ (see Section \ref{sec:proj}).
First, if $x \in {{\mathcal{X}}^{\circ}}$, the inequality binds because $\operatorname{\Pi}_{x}$ projects $ {\mathbb{R}}^{\mathcal{A}}$ onto ${{\mathbb{R}}_{0}}^{\mathcal{A}} = {\textrm{Adm}_{g}}(x) $, which contains ${V}(x)$.
Secondly, if $x \in \operatorname{bd}({\mathcal{X}})$ and $g$ is minimal-rank extendable, then ${v}^{\sharp}(x) \in {\mathbb{R}}^{\operatorname{supp}(x)}$, so the inequality binds because $\operatorname{\Pi}_{x}$ projects ${\mathbb{R}}^{\operatorname{supp}(x)}$ orthogonally onto ${{\mathbb{R}}_{0}}^{\mathcal{A}} \cap {\mathbb{R}}^{\operatorname{supp}(x)} = {\textrm{Adm}_{g}}(x) $, which contains ${V}(x)$.
Finally, if $x \in \operatorname{bd}({\mathcal{X}})$ and $g$ is full-rank extendable, $\operatorname{\Pi}_{x}$ is the closest point projection of ${\mathbb{R}}^{\mathcal{A}}$ onto the tangent cone $\operatorname{TC}_{\mathcal{X}}(x)$.
Hence, Moreau's decomposition theorem \citep{HUL01} implies that ${v}^{\sharp}(x) - \operatorname{\Pi}_{x}({v}^{\sharp}(x))$ lies in the normal cone 
\begin{equation}
\label{eq:ncone}
\operatorname{NC}_{\mathcal{X}}(x)
	= \setdef{w\in{\mathbb{R}}^{\mathcal{A}}}{\product{w}{z}_{x}\leq 0 \text{ for all } z\in \operatorname{TC}_{\mathcal{X}}(x)}.
\end{equation} 
Since ${V}(x)\in{\textrm{Adm}_{g}}(x)= \operatorname{TC}_{\mathcal{X}}(x)$, the first inequality in \eqref{eq:PC-derivation} is immediate.
\end{proof}

Proposition \ref{prop:PC} is not particularly surprising:
after all, the basic postulate behind \eqref{eq:RGD} is that the dynamics' vector field of motion is the closest feasible approximation to the game's payoff field, with the notion of closeness determined by the underlying Riemannian metric.
As we show below, this alignment can be exploited further to characterize the dynamics' rest points.

To make this precise, recall first that one of the main attributes of the Euclidean projection dynamics \eqref{eq:PD} is \emph{Nash stationarity}:
\begin{equation}
\label{eq:NS}
\tag{NS}
\text{${x^{\ast}}\in{\mathcal{X}}$ is a rest point if and only if it is a Nash equilibrium.}
\end{equation}
This property does not hold under the replicator dynamics:
for instance, every pure state of ${\mathcal{X}}$ is stationary under \eqref{eq:RD}.
In this case, \eqref{eq:NS} is replaced by \emph{restricted stationarity}:\footnote{Recall here that $x^\ast$ is a \emph{restricted equilibrium} if all strategies in its support earn equal payoffs.}
\begin{equation}
\label{eq:RS}
\tag{RS}
\text{${x^{\ast}}\in{\mathcal{X}}$ is a rest point if and only if it is a restricted equilibrium.}
\end{equation}

Our next result shows that this difference between the projection and the replicator dynamics is representative of the discontinuous and continuous cases, and shows one advantage of the former over the latter:

\begin{proposition}
\label{prop:stationary}
\leavevmode
\begin{enumerate}
[\textup(\itshape i\textup)]
\item
\label{itm:stationary-min}
Continuous Riemannian dynamics satisfy \eqref{eq:RS}. 
\item
\label{itm:stationary-full}
Discontinuous Riemannian dynamics satisfy \eqref{eq:NS}. 
\end{enumerate}
\end{proposition}

\begin{proof}
For \eqref{itm:stationary-min}, recall that the coordinate expression \eqref{eq:RGD-coords} for \eqref{eq:RGD} always holds when $g$ is minimal-rank extendable, and $\dot x_{\alpha} = 0$ whenever $x_{\alpha} = 0$.
Therefore, it suffices to check that ${x^{\ast}}\in{{\mathcal{X}}^{\circ}}$ is a rest point if and only if ${v}({x^{\ast}}) \propto {\mathds{1}}$.
To that end, note first that ${x^{\ast}}$ is a rest point of \eqref{eq:RGD-coords} if and only if 
\begin{equation}
\label{eq:pfofrs}
{v}^{\sharp}({x^{\ast}})
	= \frac{{\sum\nolimits}_{\gamma} {v}^{\sharp}_{\gamma}({x^{\ast}})}{{\sum\nolimits}_{\gamma} {n}_{\gamma}({x^{\ast}})} {n}({x^{\ast}})
	\propto {n}({x^{\ast}}).
\end{equation}
In turn, this means that ${x^{\ast}}$ is a rest point of \eqref{eq:RGD} if and only if ${v}^{\sharp}({x^{\ast}}) \propto {n}({x^{\ast}}) = {\mathds{1}}^{\sharp}$;
our claim then follows from the fact that $g^{\sharp}({x^{\ast}})$ is invertible.

For \eqref{itm:stationary-full}, assume that $g$ if full-rank extendable and fix some ${x^{\ast}}\in{\mathcal{X}}$.
By the variational characterization \eqref{eq:Nash-variational} of Nash equilibria, ${x^{\ast}}$ is a Nash equilibrium if and only if
\begin{equation}
0	\leq
	\braket{{v}({x^{\ast}})}{x - {x^{\ast}}}
	= \product{{v}^{\sharp}({x^{\ast}})}{x-{x^{\ast}}}_{x^{\ast}}
	\quad
	\text{for all $x\in{\mathcal{X}}$},
\end{equation}
i.e. if and only if ${v}^{\sharp}({x^{\ast}})$ lies in the normal cone $\operatorname{NC}_{\mathcal{X}}({x^{\ast}})$ of ${\mathcal{X}}$ at ${x^{\ast}}$ (cf.~Eq.~\ref{eq:ncone} above).
Moreau's decomposition theorem then yields ${v}^{\sharp}({x^{\ast}}) \in \operatorname{NC}_{\mathcal{X}}({x^{\ast}})$ if and only if $\operatorname{\Pi}_{x^{\ast}}({v}^{\sharp}({x^{\ast}})) = 0$, so our assertion follows.
\end{proof}

\subsection{Global convergence in potential games}
\label{sec:GCPG}

Recall that a game ${\mathcal{G}}\equiv{{\mathcal{G}}({\mathcal{A}},{v})}$ is a potential game if ${v}_{\alpha}(x) = {\partial}_{\alpha}{f}(x)$ for some potential function ${f}$ on ${\mathcal{X}}$.
It then follows from Proposition \ref{prop:PC} that $f$ is a \emph{strict global Lyapunov function} for \eqref{eq:RGD}, meaning that its value increases along \eqref{eq:RGD} whenever the dynamics are not at rest.\footnote{Definitions concerning stability and convergence in dynamical systems are collected in Appendix \ref{app:dynamics}.}

For continuous Riemannian dynamics, a standard Lyapunov argument implies that all $\omega$-limit points of \eqref{eq:RGD} are rest points \textendash\ and hence, by Proposition \ref{prop:stationary}, restricted equilibria of ${\mathcal{G}}$.
However, this argument does not extend to discontinuous dynamics and Nash equilibria because it requires continuity of solutions with respect to initial conditions, a requirement which is difficult to establish in our case.
To circumvent this obstacle, we establish below a \ac{lsc} bound on the rate of change of the potential function.
In turn, this allows us to apply Proposition \ref{prop:omega} in Appendix \ref{app:dynamics}, which shows that, for dynamics on a compact set, such a bound on the rate of change of a Lyapunov function guarantees global convergence.  

\begin{proposition}
\label{prop:potential}
Let ${\mathcal{G}}$ be a potential game with potential function ${f}$;
then, ${f}$ is a strict Lyapunov function for \eqref{eq:RGD}, and every $\omega$-limit point of \eqref{eq:RGD} is a rest point of \eqref{eq:RGD}.
These are restricted equilibria if \eqref{eq:RGD} is continuous, and Nash equilibria if \eqref{eq:RGD} is discontinuous.
\end{proposition}

\begin{proof}
Let  ${V}(x) = \operatorname{\Pi}_{x}({v}^{\sharp}(x))$ and let $\{x(t)\}$ be a solution of \eqref{eq:RGD}.
Then, Proposition \ref{prop:PC} yields
\begin{equation}
\label{eq:Lyapunov-pot}
\frac{d}{dt}{f}(x(t))
	= \braket{d{f}(x(t))}{\dot x(t)}
	= \braket{{v}(x(t))}{{V}( x(t))}
	\geq 0,
\end{equation}
with equality if and only if ${V}(x(t)) = 0$, i.e. ${f}$ is a strict global Lyapunov function for \eqref{eq:RGD}.

When \eqref{eq:RGD} is (Lipschitz) continuous, a standard argument shows that every $\omega$-limit point of \eqref{eq:RGD} is a rest point thereof \cite[see e.g.][Theorem 7.B.3]{San10}.
The discontinuous case requires a different treatment.
To start, note that
\begin{equation}
\frac{d}{dt}{f}(x(t))
	= \braket{{v}(x(t))}{{V}( x(t))}
	= \product{{v}^{\sharp}(x)}{\operatorname{\Pi}_{x}({v}^{\sharp}(x))}_x
	\geq \norm{{V}( x(t))}^2_x
	\geq 0,
\end{equation}
where the first inequality follows from Moreau's decomposition theorem.
Both inequalities bind if and only if ${V}(x(t)) = 0$;
since the speed function $x \mapsto \norm{{V}(x)}_x$ is \acl{lsc} (cf. Lemma \ref{lem:RLSC}),
Proposition \ref{prop:omega} shows that every $\omega$-limit point of \eqref{eq:RGD} is a rest point.
\end{proof}

As we discussed in Section \ref{sec:previous}, the original analyses of \cite{Kim58} and \cite{Sha79} showed that, in common interest games, average payoffs are increased at a maximal rate under the replicator dynamics, provided that ``maximal'' is defined with respect to the Shahshahani metric.
We conclude this section by noting an analogous principle for all Riemannian game dynamics.

Doing so requires an additional definition from Riemannian geometry.
For a given Riemannian metric $g$ on the positive orthant ${{\mathcal{K}}^{\circ}}$ of ${\mathbb{R}}^{\mathcal{A}}$, the \emph{gradient} of a smooth function $f{\colon}{{\mathcal{K}}^{\circ}} \to {\mathbb{R}}$ is defined as
\begin{equation}
\label{eq:gradient}
\operatorname{grad\hspace{-1pt}} f(x)
	= df^{\sharp}(x),
\end{equation}
that is, as the (necessarily unique) vector satisfying
\begin{equation}
\braket{df(x)}{z}
	= \product{\operatorname{grad\hspace{-1pt}} f(x)}{z}_x 
	\quad
	\text{for all $z\in {\mathbb{R}}^{\mathcal{A}}$ and $x\in{{\mathcal{K}}^{\circ}}$}.
\end{equation}
Geometrically, the vector $\operatorname{grad\hspace{-1pt}} f(x)$ represents the direction of maximal increase of the function $f$ at $x$ with respect to the metric $g$.
\footnote{Specifically, this means that $\operatorname{grad\hspace{-1pt}} f(x) = \operatorname*{arg\,max}\setdef{D_{z}f(x)}{\norm{z}_{x} = 1}$;
that this is so follows from the definition of $\operatorname{grad\hspace{-1pt}} f(x)$ and the Cauchy-Schwarz inequality.}
We thus obtain the steepest ascent principle:

\begin{proposition}
Let ${\mathcal{G}}$ be a potential game with potential function ${f}$, and let $g$ be an extendable Riemannian metric.
Then, for all $x\in{{\mathcal{X}}^{\circ}}$, the vector field that defines \eqref{eq:RGD} is the projection of $\operatorname{grad\hspace{-1pt}}{f}$  onto $\operatorname{T}_{\mathcal{X}}(x)$ with respect to $g$.
\end{proposition}

\begin{proof}
Since ${v}(x) = df(x)$, we have $\operatorname{\Pi}_{x}({v}^{\sharp}(x)) = \operatorname{\Pi}_{x}(\operatorname{grad\hspace{-1pt}} f(x))$, as claimed.
\end{proof}

Hence, at interior states, the dynamics \eqref{eq:RGD} increase the value of potential at a maximal rate under the geometry defined by $g$, subject to feasibility.
For discontinuous dynamics, this conclusion remains true even at boundary states.
For continuous dynamics, the interior of each face of ${\mathcal{X}}$ is invariant under \eqref{eq:RGD}, so the conclusion holds provided that feasibility is understood to incorporate this additional constraint.

\section{Hessian game dynamics}
\label{sec:HD}

By virtue of the integrability property that defines them, potential games have desirable convergence properties under a wide range of dynamics.
By contrast, convergence results for other classes of games \textendash\ for instance, contractive games and games with an \ac{ESS} \textendash\ require additional structure, often taking the form of integrability properties built into the dynamics themselves.\footnote{See \cite{HS07} and \cite{San10c}.}
In this section, we show that the integrability of \acl{HR} metrics allows us to generalize several properties of the replicator dynamics and the Euclidean projection dynamics to the class of Hessian dynamics introduced in Section \ref{sec:dynamics}.
Conversely, this analysis pinpoints the fundamental integrability property that underlies a variety of convergence results for the replicator and the projection dynamics.

A key element of our analysis is the so-called \emph{Bregman divergence}, which we introduce in Section \ref{sec:Bregman}.
Section \ref{sec:contractive} establishes global convergence to equilibrium in contractive games and local stability of \acp{ESS}, while Section \ref{sec:averages} demonstrates the convergence of the time-averages of interior trajectories to Nash equilibrium and provides sufficient conditions for permanence.
Finally, Section \ref{sec:dominated} establishes the elimination of strictly dominated strategies under continuous Hessian dynamics.

\subsection{Bregman divergences}
\label{sec:Bregman}

When used as a tool for establishing convergence, Lyapunov functions typically measure some sort of ``distance'' between the current state and a target state ${x^{\ast}}$.
For Hessian dynamics, a natural point of departure is the potential function $h$ of the metric $g=\operatorname{Hess} h$ that defines the dynamics \eqref{eq:HD}.
However, since the (game-defined) target state ${x^{\ast}}$ is independent of $g$, there is no reason that $h$ itself should be able to serve as a Lyapunov function.
Instead, we take advantage of the convexity of $h$ and consider the difference between $h({x^{\ast}})$ and the best linear approximation of $h({x^{\ast}})$ from ${x}$.

Formally, the \emph{Bregman divergence} of $h$ \citep{Bre67} is defined as
\begin{equation}
\label{eq:Bregman}
{D_{h}}({x^{\ast}},{x})
	= h({x^{\ast}}) - h({x} ) - h'({x} ;{x^{\ast}} - {x}),
	\qquad
	{x^{\ast}}, {x}\in {\mathcal{X}},
\end{equation}
where $h'({x} ;{x^{\ast}} - {x} )$ is the one-sided derivative of $h$ at ${x} $ along ${x^{\ast}} - {x} $, i.e.
\begin{equation}
h'({x} ;{x^{\ast}} - {x} )
	= \lim_{t\to0^{+}} t^{-1} \left[ h({x} + t ({x^{\ast}} - {x} ) ) - h({x} ) \right].
\end{equation}
Since $h$ is convex, we have 
\begin{equation}\label{eq:BPD}
{D_{h}}({x^{\ast}},{x}) \geq 0,\:\text{ with equality if and only if }{x^{\ast}} = {x}.
\end{equation} 
On the other hand, ${D_{h}}$ is not symmetric in ${x^{\ast}}$ and ${x}$, so it is not a bona fide distance function on ${\mathcal{X}}$;
rather, ${D_{h}}({x^{\ast}},{x} )$ describes the remoteness of ${x}$ from the base point ${x^{\ast}}$, hence the name ``divergence''.

Revisiting our two archetypal examples, the Euclidean metric is generated by the quadratic potential $h(x) = \tfrac{1}{2} \sum_{\alpha} x_{\alpha}^{2}$.
\begin{subequations}
Definition \eqref{eq:Bregman} then yields the \emph{Euclidean divergence}
\label{eq:Bregs}
\begin{equation}
\label{eq:Breg-eucl}
{D_{\textup{Eucl}}}({x^{\ast}},{x} )
	= \frac{1}{2}{\sum\nolimits}_{\alpha} ({x}_{\alpha} - {x^{\ast}}_{\alpha})^{2},
\end{equation}
which is (uncharacteristically) symmetric in ${x^{\ast}}$ and ${x}$.
Analogously, the Shahshahani metric is generated by the (negative) entropy $h(x) = {\sum\nolimits}_{\alpha} x_{\alpha} \log x_{\alpha}$.
A short calculation then shows that the corresponding divergence function is the {\emph{\acl{{KL}}} \textup(\acs{{KL}}\textup)\acused{{KL}}} divergence
\begin{equation}
\label{eq:KL}
{D_{\textup{KL}}}({x^{\ast}},{x} )
	= {\sum\nolimits}_{\alpha:\,{x^{\ast}}_{\alpha}>0} {x^{\ast}}_{\alpha} \log( {x^{\ast}}_{\alpha}/{x} _{\alpha}),
\end{equation}
which has been used extensively in the analysis of the replicator dynamics \citep{Wei95,HS98}.
\end{subequations}

The key qualitative difference between the Euclidean divergence \eqref{eq:Breg-eucl} and the \ac{KL} divergence \eqref{eq:KL} is that the former is finite for all ${x},{x^{\ast}}\in{\mathcal{X}}$ whereas the latter blows up to $+\infty$ when $\operatorname{supp}({x^{\ast}}) \nsubseteq \operatorname{supp}({x})$.
The reason for this blow-up is that the entropy function $h(x) = \sum_{\alpha} x_{\alpha} \log x_{\alpha}$ becomes infinitely steep as any boundary point ${x}$ of ${\mathcal{X}}$ is approached from the interior of ${\mathcal{X}}$, i.e.
\begin{equation}
\label{eq:steep}
{\textstyle}
\sup_{\alpha\in{\mathcal{A}}} \abs{{\partial}_{\alpha} h({x}_{n})}
	\to \infty
	\quad
	\text{for every interior sequence ${x}_{n}$ converging to ${x}$.}
\end{equation}
When this is the case for all ${x}\in\operatorname{bd}({\mathcal{X}})$, we will say that $h$ is \emph{steep} \citep{HS02,ABB04,CGM15}.
At the opposite end of the spectrum, if $dh(x)$ exists for all $x\in{\mathcal{X}}$, we will say that $h$ is \emph{nonsteep}.

The link between steepness of $h$ and finiteness of the associated Bregman divergence is provided by the following lemma:

\begin{proposition}
\label{prop:Bregman}
Fix ${x^{\ast}}\in{\mathcal{X}}$ and let ${\mathcal{D}}({x^{\ast}})$ denote the union of the relative interiors of the faces of ${\mathcal{X}}$ that contain ${x^{\ast}}$, i.e.
\begin{equation}
\label{eq:domain}
{\mathcal{D}}({x^{\ast}})
	\equiv \setdef{{x}\in{\mathcal{X}}}{\operatorname{supp}({x^{\ast}}) \subseteq \operatorname{supp}({x})}.
\end{equation}
If $h$ is steep, we have ${D_{h}}({x^{\ast}},{x})<\infty$ for all $x\in{\mathcal{D}}({x^{\ast}})$;
otherwise, if $h$ is nonsteep, we have ${D_{h}}({x^{\ast}},{x}) < \infty$ for all $x\in{\mathcal{X}}$.
\end{proposition}

\begin{proof}
If $h$ is steep and ${x} \in {\mathcal{D}}({x^{\ast}})$, the smoothness of $h$ on the face of ${\mathcal{X}}$ spanned by $\operatorname{supp}(x)\supseteq\operatorname{supp}({x^{\ast}})$ implies that the directional derivative $h'({x};{x^{\ast}}-{x})$ exists and is finite, so ${D_{h}}({x^{\ast}},{x})$ is itself finite.
If instead $h$ is nonsteep, $h'({x};{x^{\ast}}-{x})$ exists and is finite for all ${x}\in{\mathcal{X}}$, so again ${D_{h}}({x^{\ast}},{x}) < \infty$.
\end{proof}

\begin{remark}
The implication of Proposition \ref{prop:Bregman} for steep $h$ can be turned into an equivalence (i.e. ${D_{h}}({x^{\ast}},{x}) < \infty$ if \emph{and only if} ${x}\in{\mathcal{D}}({x^{\ast}})$) by slightly strengthening the steepness requirement \eqref{eq:steep} to hold for every sequence ${x}_{n}$ with constant support $\operatorname{supp}({x}_{n}) \supsetneq \operatorname{supp}({x})$ and taking the supremum over $\operatorname{supp}({x}_{n})$ instead of ${\mathcal{A}}$.
Separable $h$ that satisfy \eqref{eq:steep} automatically satisfy this stronger requirement.
\end{remark}

Beyond the positive-definiteness property \eqref{eq:BPD}, the attribute of the Bregman divergence that recommends it as a Lyapunov function for \eqref{eq:HD} is that the level sets of ${D_{h}}({x^{\ast}},{\mathopen{}\cdot\mathopen{}})$ are \emph{radial} with respect to ${x^{\ast}}$, i.e. they are perpendicular to all rays emanating from ${x^{\ast}}$ under the \ac{HR} metric $g = \operatorname{Hess} h.$
Formally, we have:

\begin{proposition}
\label{prop:Bregman-grad}
Let $g = \operatorname{Hess} h $ be an extendable \ac{HR} metric and let ${x^{\ast}}\in{\mathcal{X}}$.
Then, for every smooth curve ${x}(t)$ with constant support containing that of ${x^{\ast}}$, we have:
\begin{equation}
\label{eq:Bregman-grad}
\frac{d}{dt} {D_{h}}({x^{\ast}},{x}(t))
	= \product{\dot {x}(t)}{{x}(t) - {x^{\ast}}}_{{x}(t)}.
\end{equation}
In particular, if ${D_{h}}({x^{\ast}},x(t))$ is constant, $\dot x(t)$ is perpendicular to $x(t) - {x^{\ast}}$.
Finally, if $h$ is nonsteep, the above conclusions hold for every smooth curve ${x}(t)$ on ${\mathcal{X}}$.
\end{proposition}

\begin{proof}
The proof is a direct application of the chain rule:
\begin{flalign}
\frac{d}{dt} {D_{h}}({x^{\ast}}, x)
	&= - {\sum\nolimits}_{\alpha}\left[
	\frac{{\partial} h}{{\partial} x_{\alpha}} \dot {x}_{\alpha}
	+ \frac{{\partial} h}{{\partial} x_{\alpha}} \frac{d}{dt}({x^{\ast}}_{\alpha} - {x}_{\alpha})
	+ {\sum\nolimits}_{\beta} \frac{{\partial}^{2}h}{{\partial} x_{\alpha} {\partial} x_{\beta}} ({x^{\ast}}_{\alpha} - {x}_{\alpha}) \dot {x}_{\beta}
	\right]
	\notag\\
	&= {\sum\nolimits}_{\alpha,\beta} ({x^{\ast}}_{\alpha} - x_{\alpha}) g_{\alpha\beta}(x) \dot x_{\beta}
	= \product{\dot x}{x-{x^{\ast}}}_{x},
\end{flalign}
where all summations are taken over the (constant) support ${\mathcal{A}}'\equiv\operatorname{supp}({x}(t))$ of ${x}(t)$ and we used the fact that $\dot {x}_{\alpha} = 0$ for $\alpha\notin{\mathcal{A}}'$.
Finally, in the nonsteep case, $h$ is smooth throughout ${\mathcal{X}}$, so the above holds for every smooth curve $x(t)$.
\end{proof}

Within the class of \acl{HR} metrics, steepness of the potential function $h$ roughly corresponds to minimal-rank extendability of the metric $g =\operatorname{Hess} h$, and nonsteepness of $h$ to full-rank extendability of $g$.
These analogies fail when the steepness of $h$ does not adequately control the regularity of $g$ near the boundary of ${\mathcal{X}}$, or when $g$ is minimal-rank extendable but generates non-Lipschitz dynamics.
Conversely, the separable metrics with weighting functions ${\phi}(z) = z^p$ for $p \in (0, 1)$, corresponding to the the non-Lipschitz cases of the $p$-replicator dynamics (Examples \ref{ex:pReplicator} and \ref{ex:EU}), are minimal-rank extendable but have nonsteep potential functions.}
Bearing this in mind, we use the term \emph{continuous Hessian dynamics} for Riemannian dynamics generated by a minimal-rank extendable metric $g = \operatorname{Hess} h $ with steep $h$, and the term \emph{discontinuous Hessian dynamics} for Riemannian dynamics generated by a full-rank extendable metric $g = \operatorname{Hess} h$ with nonsteep $h$.
In what follows, we will tacitly assume that the dynamics \eqref{eq:HD} are either continuous or discontinuous.

\subsection{Contractive games and \aclp{ESS}}
\label{sec:contractive}

Recall that a population game ${\mathcal{G}}\equiv{{\mathcal{G}}({\mathcal{A}},{v})}$ is called
\emph{contractive} if $\braket{{v}(x')-{v}(x)}{x' - x} \leq 0$ for all $x,x'\in{\mathcal{X}}$,
\emph{strictly contractive} if the inequality is strict whenever $x \neq x'$,
and \emph{conservative} if the inequality always binds (cf. Example \ref{ex:contract}).
As is well known, the set of Nash equilibria of any contractive game is convex and every strictly contractive game admits a unique Nash equilibrium \citep{HS09}.

Combining the defining inequality of strictly contractive games with the variational characterization of Nash equilibria \eqref{eq:Nash-variational},
it follows that the (necessarily unique) Nash equilibrium of a strictly contractive game satisfies the inequality
\begin{equation}
\label{eq:GESS}
\braket{{v}(x)}{x -{x^{\ast}} }
	\leq 0
	\quad
	\text{with equality if and only if $x={x^{\ast}}$}.
\end{equation}
\cite{HS09} call a state satisfying \eqref{eq:GESS} a {\emph{\acl{{GESS}}} \textup(\acs{{GESS}}\textup)\acused{{GESS}}}.
This is the global version of the seminal local solution concept of \cite{MSP73}: if \eqref{eq:GESS} holds for all $x \neq {x^{\ast}}$ in a neighborhood of ${x^{\ast}}$, then ${x^{\ast}}$ is called an {\emph{\acl{{ESS}}} \textup(\acs{{ESS}}\textup)\acused{{ESS}}}.\footnote{This concise characterization of evolutionary stability is due to \cite{HSS79}.}

It is well known that the \ac{GESS} ${x^{\ast}}$ of a strictly contractive game
is asymptotically stable under the replicator dynamics \eqref{eq:RD}, attracting all solutions whose initial support contains that of ${x^{\ast}}$;
by comparison, ${x^{\ast}}$ is globally asymptotically stable under the Euclidean projection dynamics \eqref{eq:PD}.
Theorem \ref{thm:contractive} extends these results to all Hessian dynamics \eqref{eq:HD}.

\begin{theorem}
\label{thm:contractive}
Let ${\mathcal{G}}$ be the \textup(necessarily unique\textup) Nash equilibrium of a strictly contractive game ${\mathcal{G}}$.
Then:
\begin{enumerate}
[\textup(i\textup)]

\item
For all continuous Hessian dynamics, ${D_{h}}({x^{\ast}},{\mathopen{}\cdot\mathopen{}})$ is a decreasing strict Lyapunov function on ${\mathcal{D}}({x^{\ast}})$, and ${x^{\ast}}$ is asymptotically stable with basin ${\mathcal{D}}({x^{\ast}})$.

\item
For all discontinuous Hessian dynamics, ${D_{h}}({x^{\ast}},{\mathopen{}\cdot\mathopen{}})$ is a decreasing strict global Lyapunov function, and ${x^{\ast}}$ is globally asymptotically stable.
\end{enumerate}
\end{theorem}

\begin{proof}
We begin with the continuous case.
By Proposition \ref{prop:wp}\eqref{itm:wp-min}, every solution $x(t)$ of \eqref{eq:HD} has constant support.
Hence, if $x(0) \in {\mathcal{D}}({x^{\ast}})$, Proposition \ref{prop:Bregman-grad} yields:
\begin{subequations}
\label{eq:Breg-calc}
\begin{flalign}
\frac{d}{dt} {D_{h}}({x^{\ast}},x)
	&
	= \product{\dot x}{x - {x^{\ast}}}_{x}
	= \product{\operatorname{\Pi}_{x}({v}^{\sharp}(x))}{x - {x^{\ast}}}_{x}
	\notag\\
	&\label{eq:Breg-calc2}
	= \product{{v}^{\sharp}(x)}{x - {x^{\ast}}}_{x}
	= \braket{{v}(x)}{x - {x^{\ast}}},
	\\
	&\label{eq:Breg-calc3}
	\leq 0,
\end{flalign}
\end{subequations}
where we used the definition of $\operatorname{\Pi}_{x}$ for minimal-rank extendable metrics (cf.~Section \ref{sec:proj}) to obtain \eqref{eq:Breg-calc2} and the definition \eqref{eq:GESS} of a \ac{GESS} for \eqref{eq:Breg-calc3}.
Since equality in \eqref{eq:Breg-calc} holds if and only if $x = {x^{\ast}}$, we conclude that ${D_{h}}({x^{\ast}},x)$ is a strict Lyapunov function on ${\mathcal{D}}({x^{\ast}})$.
If we can show in addition that $x(t)$ has no $\omega$-limit points in ${\mathcal{X}}\setminus{\mathcal{D}}({x^{\ast}})$, then asymptotic stability with basin ${\mathcal{D}}({x^{\ast}})$ follows from standard arguments (see e.g.~\cite{San10}, Theorem 7.B.3).

Assume therefore that $x(t)$ admits an $\omega$-limit point ${\hat x} \neq {x^{\ast}}$, so $x(t_{n})\to {\hat x}$ for some sequence of times $t_{n}\nearrow\infty$.
Since $\abs{\dot x_{\alpha}(t)}$ is bounded from above by ${V}_{\max} \equiv \sup_{x\in{\mathcal{X}}} \max_{\beta} \abs{{V}_{\beta}(x)} < \infty$, there exists an open neighborhood $U$ of ${\hat x}$ and positive $a,\delta,n_{0} > 0$ such that $x(t) \in U$ and $\braket{{v}(x(t))}{x(t) - {x^{\ast}}} \leq -a < 0$ for all $t\in[t_{n},t_{n}+\delta]$ and for all $n\geq n_{0}$.
Hence, by \eqref{eq:Breg-calc}, we get
\begin{equation}\label{eq:and}
{D_{h}}({x^{\ast}},x(t_{n}+\delta)) - {D_{h}}({x^{\ast}},x(0))
	\leq \int_{0}^{t_{n}+\delta} \braket{{v}(x(s))}{x(s) - {x^{\ast}}} {\:d} s
	\leq -a(n-n_0)\delta.
\end{equation}
Thus $\liminf_{t\to\infty} {D_{h}}({x^{\ast}},x(t)) = -\infty$, a contradiction.

For the discontinuous case, note first that since ${x^{\ast}} - x \in \operatorname{TC}_{\mathcal{X}}(x)$, Moreau's decomposition theorem implies that $\product{\operatorname{\Pi}_{x}({v}^{\sharp}(x))}{x - {x^{\ast}}}_{x} \leq \product{{v}^{\sharp}(x)}{x-{x^{\ast}}}_{x}$.
Thus, replacing the first equality in \eqref{eq:Breg-calc2} by an inequality, \eqref{eq:Breg-calc} shows that ${D_{h}}({x^{\ast}},{\mathopen{}\cdot\mathopen{}})$ is a strict global Lyapunov function for \eqref{eq:HD}.
Global asymptotic stability then follows from Proposition \ref{prop:omega}.
\end{proof}

The only implication of ${\mathcal{G}}$ being strictly contractive used in the previous proof is that its Nash equilibrium is a \ac{GESS}.
More generally, if a game admits an \ac{ESS} ${x^{\ast}}$, applying the above arguments in a neighborhood of ${x^{\ast}}$ defined by a level set of ${D_{h}}({x^{\ast}},\cdot)$ yields the following result:

\begin{theorem}
\label{thm:ESS}
Evolutionarily stable states are asymptotically stable under \eqref{eq:HD}.
\end{theorem}

\cite{HS90}, \cite{Hop99b}, and \cite{Har11} all offer results on the local stability of \emph{interior} evolutionarily stable states under Riemannian game dynamics.
\cite{HS90} show that under all Riemannian (not necessarily Hessian) dynamics \eqref{eq:RD}, the function ${L}(x) = \product{x-{x^{\ast}}}{x-{x^{\ast}}}_{x^{\ast}}$ is a local strict Lyapunov function for interior \acp{ESS} ${x^{\ast}}$, implying that ${x^{\ast}}$ is asymptotically stable.
Likewise, \cite{Hop99b} uses linearization to establish local stability of \emph{regular} interior \acp{ESS} \citep{TJ78} under Riemannian dynamics \eqref{eq:RD}.
Finally, \cite{Har11} employs a version of the argument above to prove asymptotic stability of \acp{ESS} for separable Hessian dynamics of the form \eqref{eq:separable}.

An important case of contractive games that do not admit an \ac{ESS} is the class of conservative games, which include population games generated by matching in symmetric zero-sum games.
Under the replicator dynamics, the \ac{KL} divergence does not provide a strict Lyapunov function for conservative games, but instead a constant of motion.
The following result extends this conclusion to all Hessian dynamics:

\begin{proposition}
\label{prop:conservative}
Let ${x^{\ast}}$ be a Nash equilibrium of a conservative game ${\mathcal{G}}$.
Then, ${D_{h}}({x^{\ast}},{\mathopen{}\cdot\mathopen{}})$ is a constant of motion along any interior solution segment of \eqref{eq:HD}.
\end{proposition}

\begin{proof}
Simply note that \eqref{eq:Breg-calc} binds if ${\mathcal{G}}$ is conservative and $x\in{{\mathcal{X}}^{\circ}}$.
\end{proof}

\begin{remark}
\label{rem:GenHess}
In the definition of \eqref{eq:HD}, we required that $h$ be finite throughout ${\mathcal{X}}$.
This requirement is unnecessary for the preceding results when ${x^{\ast}}$ is interior;
however, if ${x^{\ast}}$ lies on the boundary of ${\mathcal{X}}$, the proofs of Theorems \ref{thm:contractive} and \ref{thm:ESS} do not go through because ${D_{h}}({x^{\ast}}, {\mathopen{}\cdot\mathopen{}})$ is no longer well-defined throughout ${\mathcal{D}}({x^{\ast}})$.
Nevertheless, the results themselves remain true if $g = \operatorname{Hess} h$ is separable, allowing us to handle the $p$-replicator dynamics for $p \geq 2$ (Example \ref{ex:pReplicator}).
To prove this, it suffices to replace the summation in \eqref{eq:Bregman} with a sum over all strategies in the support of ${x^{\ast}}$;
after this patch, the analysis above goes through unchanged.
\end{remark}

\subsection{Convergence of time-averaged trajectories and permanence}
\label{sec:averages}

We now extend two classic results for the replicator dynamics in random matching games (Example \ref{ex:matching}) to Hessian dynamics.
The results for these games take advantage of the linearity of payoffs ${v}_{\alpha}(x) = {\sum\nolimits}_{\beta\in{\mathcal{A}}} A_{\alpha\beta} x_{\beta}$ in the population state.

The first such result states that if a solution $x(t)$ of the replicator dynamics stays a minimal positive distance away from the boundary of the simplex, the time-averaged orbit
\(
\bar x(t)
	= t^{-1} \int_{0}^{t} x(s) {\:d} s
\)
converges to the set of Nash equilibria of the underlying game \citep{SSHW81}.
The normal form games to which this result applies include zero-sum games (cf.~Proposition \ref{prop:conservative}) and games satisfying sufficient conditions for permanence (cf.~Proposition \ref{prop:perm} below).
The following proposition shows that this convergence property extends to all Hessian dynamics:

\begin{proposition}
\label{prop:averages}
Let ${\mathcal{G}}$ be a random matching game and let $x(t)$ be a solution orbit of \eqref{eq:HD}.
If $x(t)$ is contained in a compact subset of ${{\mathcal{X}}^{\circ}}$, the time-averaged orbit $\bar x(t) = t^{-1} \int_{0}^{t} x(s) {\:d} s$ converges to the Nash set of ${\mathcal{G}}$.
\end{proposition}

In the case of the replicator dynamics, this is proved by introducing the auxiliary variables ${y}_{\alpha} = \log x_{\alpha}$ and using the fact that $\dot{y} = \dot x/x$.
To extend this proof to \eqref{eq:HD}, we instead define ${y}$ via the Bregman divergence of $h$:

\begin{proof}[Proof of Proposition \ref{prop:averages}]
Let ${y}_{\alpha} = {D_{h}}({e}_{\alpha},x)$, so $\dot {y}_{\alpha} = \braket{{v}(x)}{x} - {v}_{\alpha}(x)$ by Proposition \ref{prop:Bregman-grad}.
Then, for all $\alpha,\beta\in{\mathcal{A}}$, we get
\begin{equation}
\label{eq:Legendre-diff}
{y}_{\alpha}(t) - {y}_{\beta}(t)
	= c_{\alpha\beta} + \int_{0}^{t} \left[ {v}_{\beta}(x(s)) - {v}_{\alpha}(x(s)) \right] {\:d} s,
\end{equation}
where $c_{\alpha\beta} = {y}_{\alpha}(0) - {y}_{\beta}(0)$.
Since $x(t)$ is contained in a compact subset of ${{\mathcal{X}}^{\circ}}$, Proposition \ref{prop:Bregman} implies that $\sup_{t} {y}_{\alpha}(t) < \infty$ for all $\alpha\in{\mathcal{A}}$.
Thus, dividing both sides of \eqref{eq:Legendre-diff} by $t$ and taking the limit $t\to\infty$, we obtain
\begin{equation}
\label{eq:av3}
\lim_{t\to\infty} \left[ {v}_{\alpha}({\bar} x(t)) - {v}_{\beta}({\bar} x(t)) \right]
	= 0,
\end{equation}
where we have used the linearity of ${v}_\alpha(x) = \sum_\beta A_{\alpha\beta}x_\beta$ in $x$ to bring the integral into the arguments of ${v}_\alpha$ and ${v}_\beta$.

Equation \eqref{eq:av3} implies that if ${\bar x^{\ast}}$ is an $\omega$-limit point of $\bar x(t)$, then ${v}_{\alpha}({\bar x^{\ast}}) = {v}_{\beta}({\bar x^{\ast}})$ for all $\alpha,\beta\in{\mathcal{A}}$, so ${\bar x^{\ast}}$ is a Nash equilibrium of ${\mathcal{G}}$.
Since ${\mathcal{X}}$ is compact, every solution of \eqref{eq:HD} converges to its $\omega$-limit set, and
our assertion follows.
\end{proof}

Proposition \ref{prop:averages} applies when the population share of each strategy remains bounded away from zero along all interior solution trajectories, a property known as \emph{permanence}. 
Formally, a dynamical system on ${\mathcal{X}}$ is called \emph{permanent} if there is a threshold $\delta>0$ such that every interior solution satisfies $\liminf_{t\to\infty} x_{\alpha}(t) \geq \delta$ for all $\alpha\in{\mathcal{A}}$.

\cite{HS98} establish a sufficient condition for permanence under the replicator dynamics.
Proposition \ref{prop:perm} extends this result to all continuous Hessian dynamics, providing a sufficient condition for Proposition \ref{prop:averages} to apply:

\begin{proposition}
\label{prop:perm}
Let ${\mathcal{G}}$ be a random matching game.
Assume that the dynamics \eqref{eq:HD} are continuous and there exists some $p\in{{\mathcal{X}}^{\circ}}$ such that
\begin{equation}
\label{eq:perm-cond}
\braket{{v}({x^{\ast}})}{p - {x^{\ast}}}
	> 0
	\quad
	\text{for all boundary rest points ${x^{\ast}}$ of \eqref{eq:HD}}.
\end{equation}
Then, the dynamics \eqref{eq:HD} are permanent.
\end{proposition}

The proof of Proposition \ref{prop:perm} follows the same lines as that of Theorem 13.6.1 of \cite{HS98}, and is presented in Appendix \ref{app:proofs}.

\subsection{Dominated strategies}
\label{sec:dominated}

We conclude our analysis by considering the elimination and survival of dominated strategies under \eqref{eq:HD}.
To that end, recall that $\alpha\in{\mathcal{A}}$ is \emph{strictly dominated} by $\beta\in{\mathcal{A}}$ if ${v}_{\alpha}(x) < {v}_{\beta}(x)$ for all $x\in{\mathcal{X}}$.
More generally, $p\in{\mathcal{X}}$ is \emph{strictly dominated} by $q\in{\mathcal{X}}$ if $\braket{{v}(x)}{p} < \braket{{v}(x)}{q}$ for all $x\in{\mathcal{X}}$, meaning that the average payoff of a small influx of mutants is always higher when the mutants are distributed according to $q$ rather than $p$.
We then say that $p\in{\mathcal{X}}$ \emph{becomes extinct} along trajectory $x(t)$ if $\min\{x_{\alpha}(t):\alpha\in\operatorname{supp}({x^{\ast}})\} \to 0$ as $t\to\infty$, or equivalently, if there are no $\omega$-limit points of $x(t)$ in ${\mathcal{D}}(p)$.

Under the replicator dynamics, it is well known that dominated strategies become extinct along every interior solution trajectory \citep{Aki80}.
As we show below, this elimination result extends to all continuous Hessian dynamics \eqref{eq:HD}:

\begin{proposition}
\label{prop:dominated}
Under all continuous Hessian dynamics \eqref{eq:HD}, strictly dominated strategies become extinct along all interior solution trajectories.
\end{proposition}

\begin{proof}
The proof follows a standard argument for the replicator dynamics, replacing the \ac{KL} divergence \eqref{eq:KL} with the Bregman divergence \eqref{eq:Bregman}.
Specifically,
Proposition \ref{prop:Bregman-grad} implies that along any interior solution $x(t)$,
\begin{multline}
\label{eq:DSProof}
\frac{d}{dt}\left({D_{h}}(p,x) - {D_{h}}(q,x)\right)
	= \product{\dot x}{x - p}_{x} - \product{\dot x}{x - q}_{x}
	\\
	= \product{\operatorname{\Pi}_{x}({v}^{\sharp}(x))}{q - p}_{x}
	= \product{{v}^{\sharp}(x)}{q-p}_x
	= \braket{{v}(x)}{q-p},
\end{multline}
where the penultimate equality uses the fact that $x\in{{\mathcal{X}}^{\circ}}$.
Since $q$ strictly dominates $p$ and ${v}$ is continuous, we have $\braket{{v}(x)}{q-p} \geq a$ for some positive constant $a>0$, implying that ${D_{h}}(p,x(t))\to\infty$.
Hence, by Proposition \ref{prop:Bregman}, we conclude that $\{x(t)\}$ has no $\omega$-limit points in ${\mathcal{D}}(p)$.
\end{proof}

The conclusion of Proposition \ref{prop:dominated} is false for discontinuous Hessian dynamics:
\cite{SDL08} construct a four-strategy game with a strictly dominated strategy that is played recurrently by a nonnegligible fraction of the population under the Euclidean projection dynamics \eqref{eq:PD}.
The argument above shows that this strategy must become less common when the state is in the interior of ${\mathcal{X}}$;
however, solutions to \eqref{eq:PD} are able to enter and leave the boundary of ${\mathcal{X}}$, and while there, dominated strategies may become more common.

\section{Hessian game dynamics and reinforcement learning}
\label{sec:RL}

Under a variety of reinforcement learning processes for normal form games, mixed strategies evolve according to the replicator dynamics \textendash\ see e.g. \cite{BS97}, \cite{Pos97}, \cite{Rus99}, \cite{Hop02} and \cite{HSV09}.
We conclude the paper by describing a broader connection between reinforcement learning and Hessian game dynamics.

Our starting point is a class of reinforcement learning dynamics for $N$-player normal form games introduced by \cite{CGM15} and \cite{MS16}.
Over the course of play, each player maintains a \emph{score vector} representing the cumulative payoffs of each of his actions;
then, at each moment in time, the player selects a mixed strategy by applying a \emph{choice map} to this score vector, similar in function to the perturbed best response maps used in stochastic fictitious play and perturbed best response dynamics \citep{FL98,HS02,HS07}.

Formally, let ${v}_{k\alpha}(x)$ denote the expected payoff of the $\alpha$-th action of player $k$ at mixed strategy profile $x = (x_{1},\dotsc, x_{N})$ in an $N$-player normal form game.
The choice map ${Q}_{k}$ of player $k$ is then defined as
\begin{equation}
\label{eq:choice}
{Q}_{k}(y_{k})
	= \operatorname*{arg\,max}_{x_k \in {\mathcal{X}}_k} \{\braket{y_{k}}{x_{k}} - h_k(x_{k}) \},
\end{equation}
where ${\mathcal{X}}_{k} \equiv {\Delta}({\mathcal{A}}_{k})$ denotes the mixed strategy space of player $k$ (${\mathcal{A}}_{k}$ being the corresponding action set), and $h_{k}{\colon}{\mathcal{X}}_k\to{\mathbb{R}}$ is a smooth, strongly convex \emph{penalty function}.
The reinforcement learning process described above can then be written as
\begin{equation}
\label{eq:RL}
\tag{RL}
\begin{aligned}
\dot y_{k}
	&= {v}_{k}(x)
	\\
x_{k}
	&= {Q}_{k}(y_{k}).
\end{aligned}
\end{equation}

Now, let $g_{k} = \operatorname{Hess} h_{k}$ and write ${n}_{k\alpha}(x) = \sum_{\beta\in{\mathcal{A}}_{k}} g_{k,\alpha\beta}^{-1}(x)$.
\cite{MS16} show that when the mixed strategy profile $x(t)$ is interior, its evolution under \eqref{eq:RL} is described by the dynamics
\begin{equation}
\label{eq:RLD}
\tag{RLD}
\dot x_{k\alpha}
	= \sum_{\beta\in{\mathcal{A}}_{k}} \left[
		g_{k,\alpha\beta}^{-1}(x) - \frac{{n}_{k\alpha}(x) {n}_{k\beta(x)}}{\sum_{\gamma} {n}_{k\gamma}(x)}
	\right]
	{v}_{k\beta}(x).
\end{equation}
Thus, writing $g_{k}^{\sharp}(x)$ for $g_{k}^{-1}(x)$ and comparing the result to  \eqref{eq:RGD-coords2} shows that the dynamics of mixed strategies under \eqref{eq:RL} agree with the Hessian dynamics \eqref{eq:HD} at interior states.\footnote{While we have defined Riemannian game dynamics for single population games and reinforcement learning for $N$-person normal form games, this difference is of no consequence.
One can similarly define Riemannian game dynamics for multipopulation games, or a symmetrized reinforcement learning process for symmetric two-player normal form games (cf.~ Appendix \ref{sec:HDandRL}).}

The derivation of \eqref{eq:HD} here and of \eqref{eq:RLD} in \cite{MS16} have very different starting points, leaving the reason behind their equivalence somewhat mysterious.   
In Appendix \ref{sec:HDandRL}, we derive both dynamics using a common set of tools \textendash\ convex conjugates and pseudoinverses \textendash\ making the reasons behind this equivalence clearer.\footnote{The usefulness of convex conjugates in analyzing maps of the form \eqref{eq:choice} is well known in learning and optimization \textendash\ see e.g. \cite{NY83}, \cite{HS02}, \cite{SS11}, and \cite{KM14}.}

Since continuous Hessian dynamics coincide with the reinforcement scheme \eqref{eq:RL} when the penalty functions $h_{k}$ are steep, certain results for interior trajectories \textendash\ Propositions \ref{prop:averages} and \ref{prop:dominated} in particular \textendash\ can be obtained directly from the analysis of \cite{MS16}.
On the other hand, in the nonsteep regime, \eqref{eq:RLD} and \eqref{eq:HD} still agree at interior states,
but their behavior at the boundary differs in a fundamental way.
Specifically, while  the boundary behavior of discontinuous Hessian dynamics is defined using closest point projections, the reinforcement learning process \eqref{eq:RL} for nonsteep $h$ can no longer be reduced to mixed strategy dynamics at all.
Instead one must work explicitly with the score variables $y_k$, which continue to aggregate payoff data of all strategies, even those that are not used.
Among other things, this means that a strong cumulative performance of an unused strategy will return it to use.  

This difference between how the processes are defined on the boundary has important consequences.
For instance, while \cite{SDL08} show that strictly dominated strategies may survive under the Euclidean projection dynamics, \cite{MS16} prove that the reinforcement learning process \eqref{eq:RL}  \emph{always} eliminates dominated strategies, whether the penalty functions are steep or not.
Under both processes, dominated strategies are initially eliminated along interior solution trajectories.
But while they may resurface under \eqref{eq:HD}, the score variables of \eqref{eq:RL} continue to register the poor performance of these strategies, ensuring that they remain extinct for all time.

\appendix

\section{Connections with other game dynamics}
\label{app:geometry}

Throughout this appendix, we write ${\mathbf{1}}\in{{\mathbb{R}}^{n}}$ for the $n$-dimensional column vector of ones, and ${\Phi} = I - \frac1n{\mathbf{1}}{\mathbf{1}}^{{\!\top}}$ for the Euclidean orthogonal projection of ${{\mathbb{R}}^{n}}$ onto ${{\mathbb{R}}_{0}^{n}} = \setdef{z\in{{\mathbb{R}}^{n}}}{{\mathbf{1}}^{{\!\top}} z = 0} = \operatorname{span}({\mathbf{1}})^{\bot}$.\footnote{In the above and what follows, $W^{\bot}$ denotes the orthogonal complement of a subspace $W$ of ${{\mathbb{R}}^{n}}$, defined with respect to the ordinary Euclidean metric.
Even though this might seem to suggest that the Euclidean metric plays a special role in what follows, it is just an artifact of writing everything in coordinates instead of abstractly;
for a detailed discussion, see \cite{Lan87}.}
Recall also that the (\emph{Moore-Penrose}) \emph{pseudoinverse} of a matrix $M \in {{\mathbb{R}}^{n\times n}}$ is the unique matrix $M^+\in {{\mathbb{R}}^{n\times n}}$ such that
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
$M^{+} y = 0$ whenever $y\in\operatorname{range}(M)^{\bot}$;
and
\item
$M^{+}y = x$ whenever $x\in \ker(M)^{\bot}$, $y\in\operatorname{range}(M)$, and $Mx = y$ \citep[Sec.~6.7]{FIS02}.
\end{inparaenum}
Since a symmetric matrix $M\in{{\mathbb{R}}^{n\times n}}$ satisfies $\operatorname{range}(M) = \ker(M)^{\bot}$, we have the following well-known characterization of pseudoinverses from linear algebra:

\begin{lemma}
\label{lem:pseudo}
Let $M\in {{\mathbb{R}}^{n\times n}}$ be a symmetric matrix.
Then, $M^{+}$ is the unique matrix that
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
inverts $M$ on $\ker(M)^{\bot} = \operatorname{range}(M)$;
and
\item
satisfies $\ker(M^{+}) = \ker(M)$.
\end{inparaenum}
\end{lemma}

\subsection{Interior equivalence of Riemannian dynamics and Hopkins' dynamics}
\label{sec:Hopkins}

We now derive the equivalence between \eqref{eq:RGD} and \citeauthor{Hop99b}' dynamics \eqref{eq:Hopkins} on ${{\mathcal{X}}^{\circ}}$, as noted in Section \ref{sec:previous}.
To begin with, we say that ${M}\in{{\mathbb{R}}^{n\times n}}$ is a \emph{Hopkins matrix} if it is positive-definite  with respect to ${{\mathbb{R}}_{0}^{n}}$ and maps ${\mathbf{1}}$ to $0$.
The following lemma establishes a basic characterization of Hopkins matrices:

\begin{lemma}
\label{lem:HBoth}
\leavevmode
\begin{enumerate}
[\textup(i\textup)]
\item
If ${H}\in{{\mathbb{R}}^{n\times n}}$ is symmetric positive-definite, then $({\Phi}{H}{\Phi})^{+}$ is a Hopkins matrix and
\begin{equation}
\label{eq:H1}
({\Phi}{H}{\Phi})^{+}
	= {H}^{-1} - \frac{{H}^{-1}{\mathbf{1}}{\mathbf{1}}^{{\!\top}}{H}^{-1}}{{\mathbf{1}}^{{\!\top}}{H}^{-1}{\mathbf{1}}}.
\end{equation}
\item 
Conversely, if  ${M}$ is a Hopkins matrix, then ${M} = ({\Phi}{H}{\Phi})^+$, where ${H} = {M} +{\mathbf{1}}{\mathbf{1}}^{{\!\top}}$ is symmetric positive definite.
\end{enumerate}
\end{lemma}

\begin{proof}
To prove part (i), let ${H}$ be symmetric positive definite. Then the symmetric matrix  $\Phi {H} \Phi $ is positive definite with respect to ${{\mathbb{R}}_{0}^{n}}$ and maps ${\mathbf{1}}$ to 0, so $\operatorname{range}(\Phi {H} \Phi)=\ker(\Phi {H} \Phi)^{{\!\top}}= {{\mathbb{R}}_{0}^{n}}$.
If we denote the right-hand side of \eqref{eq:H1} by $\bar{H}$,
a straightforward calculation shows that $\bar{H} \Phi {H} \Phi z = z$ for all $z \in {{\mathbb{R}}_{0}^{n}}$ and $\bar{H}{\mathbf{1}} = 0$.  Thus Lemma \ref{lem:pseudo} implies that $\bar{H}=({\Phi}{H}{\Phi})^+$.  That this is a Hopkins matrix is immediate from the fact that $\operatorname{range}(\Phi {H} \Phi)=\ker(\Phi {H} \Phi)^\bot= {{\mathbb{R}}_{0}^{n}}$ and Lemma \ref{lem:pseudo}.

To prove part (ii), let ${M}$ be a Hopkins matrix and let ${H} = {M} +{\mathbf{1}}{\mathbf{1}}^{{\!\top}}$.  Clearly ${H}$ is symmetric positive-definite.
Moreover, writing out $({\Phi}{H}{\Phi})^+$ using the right-hand side of  \eqref{eq:H1} and simplifying the result yields  $({\Phi}{H}{\Phi})^+={M}$.
\end{proof}

With Lemma \ref{lem:HBoth} at hand, Proposition \ref{prop:HopDRD} below establishes the equivalence between \eqref{eq:Hopkins} and \eqref{eq:RGD} on ${{\mathcal{X}}^{\circ}}$, and provides a concise third representation for both dynamics:

\begin{proposition}
\label{prop:HopDRD}
Let $\dot x = {V}(x)$ be a dynamical system on ${{\mathcal{X}}^{\circ}}$.
Then, the following are equivalent:
\begin{subequations}
\begin{enumerate}
[\textup(i\textup)]
\item
There is a smooth field of Hopkins matrices ${M}{\colon}{{\mathcal{X}}^{\circ}}\to{{\mathbb{R}}^{{\mathcal{A}}\times {\mathcal{A}}}}$ such that
\begin{alignat}{2}
\label{eq:HopkinsMat}
{V}(x)
	&= {M}(x)\,{v}(x)^{{\!\top}}
	&\quad
	&\text{for all $x\in{{\mathcal{X}}^{\circ}}$}.
\intertext{\item There is a smooth field of positive-definite matrices ${H}{\colon}{{\mathcal{X}}^{\circ}}\to{{\mathbb{R}}^{{\mathcal{A}}\times {\mathcal{A}}}}$ such that}
\label{eq:RGDMP}
{V}(x)
	&= ({\Phi} {H}(x) {\Phi})^{+} {v}(x)^{{\!\top}}
	&\quad
	&\text{for all $x\in{{\mathcal{X}}^{\circ}}$}.
\intertext{\item There is a smooth Riemannian metric $g$ on ${{\mathcal{K}}^{\circ}}$ such that}
{V}(x)
	&= \operatorname{\Pi}_{x}({v}^{\sharp}(x))
	&\quad
	&\text{for all $x\in{{\mathcal{X}}^{\circ}}$}.
\end{alignat}
\end{enumerate}
\end{subequations}
\end{proposition}

\begin{proof}
The equivalence of statements (i) and (ii) follows directly from Lemma \ref{lem:HBoth}.
Statements (ii) and (iii) are equivalent because 
\begin{equation}\label{eq:RGD-Matrix}
\operatorname{\Pi}_{x}({v}^{\sharp}(x))
	=\left(g^{\sharp}(x) - \frac{g^{\sharp}(x){\mathbf{1}}{\mathbf{1}}^{{\!\top}} g^{\sharp}(x)}{{\mathbf{1}}^{{\!\top}} g^{\sharp}(x) {\mathbf{1}}}\right){v}(x)^{{\!\top}}
	=  ({\Phi} g(x) {\Phi})^{+} {v}(x)^{{\!\top}}.
\end{equation}
The first equality in \eqref{eq:RGD-Matrix} follows from expression  \eqref{eq:RGD-coords2} for \eqref{eq:RGD} on ${{\mathcal{X}}^{\circ}}$, and the second follows from Lemma \ref{lem:HBoth}(i) and the fact that $g^{\sharp}(x) = g^{-1}(x)$;
the equivalence of (ii) and (iii) then follows by setting $g={H}$.\footnote{To formally complete the argument that (ii) implies (iii), we observe without proof that the field ${H}$ on ${{\mathcal{X}}^{\circ}}$ can be smoothly extended to ${{\mathcal{K}}^{\circ}}$.}
\end{proof}

\subsection{Equivalence of continuous Hessian dynamics and reinforcement learning}
\label{sec:HDandRL}

We now describe a common derivation of the reinforcement learning dynamics \eqref{eq:RLD} and \eqref{eq:HD} in the continuous regime.
Of course, the equivalence of \eqref{eq:HD} and \eqref{eq:RLD} for steep $h$  already follows from the derivation of \eqref{eq:RLD} in \cite{MS16};
the point of this appendix is to provide an alternative, more transparent view of this equivalence.

The starting point for both \eqref{eq:HD} and \eqref{eq:RLD} is the potential function $h$.
For convenience, we assume throughout this appendix that
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
$h$ is defined on the entire positive orthant ${{\mathcal{K}}^{\circ}}$;
\item
$h$ is smooth and strongly convex on ${{\mathcal{K}}^{\circ}}$;
and
\item
$h$ is steep at the boundary of ${\mathcal{K}}$ in the sense of \eqref{eq:steep}.
\end{inparaenum}

We now review some facts from convex analysis (for complete treatments, see \citealp{Roc70}, or \citealp{HUL01}).
First, the \emph{convex conjugate} of $h$ is defined as
\begin{equation}
\label{eq:conjugate}
h^\ast(y)
	= \sup_{x \in{{\mathcal{K}}^{\circ}}} \{\braket{y}{x} - h(x)\},
	\quad
	y\in{({\mathbb{R}}^{n})^{\ast}}.
\end{equation}
Since $h$ is steep and strongly convex, the supremum above is attained at a unique point ${Q_{\mathcal{K}}}(y) \in {{\mathcal{K}}^{\circ}}$ \cite[Theorem 26.5]{Roc70}.\footnote{Since $h$ is strongly convex, it is bounded below by a strictly convex quadratic function \citep[Theorem B.4.1.1]{HUL01}, which in turn implies that the domain of $h^\ast$ is ${({\mathbb{R}}^{n})^{\ast}}$ \cite[Corollary 13.1]{Roc70}.}
By the first-order optimality conditions for \eqref{eq:conjugate}, we then get
\begin{equation}
\label{eq:choice-app}
{Q_{\mathcal{K}}}(y)
	\equiv \operatorname*{arg\,max}_{x\in{{\mathcal{K}}^{\circ}}} \{\braket{y}{x} - h(x)\}
	= (dh)^{-1}(y),
\end{equation}
while applying the envelope theorem to \eqref{eq:conjugate} leads to \emph{Legendre's identity}:
\begin{equation}
\label{eq:Legendre}
dh^{\ast}(y)
	= {Q_{\mathcal{K}}}(y)
	= (dh)^{-1}(y).
\end{equation}

Convex conjugation is an \emph{involution} in that the  \emph{biconjugate} $h^{\ast\ast}{\colon}{{\mathcal{K}}^{\circ}}\to{\mathbb{R}}$ is equal to $h$, viz.
\begin{equation}
\label{eq:biconjugate}
h(x)
	= \sup \setdef{\braket{y}{x} - h^{\ast}(y)}{y\in{({\mathbb{R}}^{n})^{\ast}}}
	\quad
	\text{for all $x\in{{\mathcal{K}}^{\circ}}$}.
\end{equation}
The same reasoning as above then yields:
\begin{equation}
\label{eq:Legendre-conj}
dh(x)
	= (dh^{\ast})^{-1}(x)
	\quad
	\text{for all $x\in{{\mathcal{K}}^{\circ}}$}.
\end{equation}
Finally, by differentiating the identity $dh(dh^{\ast}(y)) = y$ and applying Legendre's identity, we get
\begin{equation}
\label{eq:Legendre-hess}
\operatorname{Hess} h^{\ast} (y)
	= \operatorname{Hess} h(x)^{-1},
\end{equation}
with the latter Hessian evaluated at $x={Q_{\mathcal{K}}}(y)$.
As the Hessians of $h$ and $h^{\ast}$ play a key role in what follows, we henceforth write ${H}(x)=\operatorname{Hess} h(x)$ and ${H}^{\ast}(y)=\operatorname{Hess} h^{\ast}(y)$.

With these tools at hand, we introduce full-dimensional analogues of continuous Hessian dynamics and (symmetric) reinforcement learning, taking ${{\mathcal{K}}^{\circ}}$ as the state space (and so defining payoffs ${v}$ on ${{\mathcal{K}}^{\circ}}$).
In the case of \eqref{eq:HD}, the full-dimensional domain makes projections redundant, so the induced dynamics take the form
\begin{equation}
\label{eq:HD+}
\tag{HD$_{\mathcal{K}}$}
\dot x
	={v}^{\sharp}(x)
	= g^{\sharp}(x) \, {v}(x)^{{\!\top}}
	=g^{-1}(x) \, {v}(x)^{{\!\top}}
	= {H}^{-1}(x) \, {v}(x)^{{\!\top}}.
\end{equation}
Likewise, the full-dimensional analogue of \eqref{eq:RL} is
\begin{equation}
\label{eq:RL+}
\tag{RL$_{\mathcal{K}}$}
\begin{aligned}
\dot y
	&= {v}(x),
	\\
x
	&= {Q_{\mathcal{K}}}(y),
\end{aligned}
\end{equation}
with ${Q_{\mathcal{K}}}$ defined as in \eqref{eq:choice-app} above.
Differentiating \eqref{eq:RL+} and applying \eqref{eq:Legendre} and \eqref{eq:Legendre-hess} then yields
\begin{equation}
\label{eq:RLD+}
\tag{RLD$_{\mathcal{K}}$}
\dot x
	= d{Q_{\mathcal{K}}}(y) {v}(x)^{{\!\top}}
	= {H}^{\ast}(y)  {v}(x)^{{\!\top}}
	= {H}^{-1}(x) {v}(x)^{{\!\top}},
\end{equation}
showing that the dynamics \eqref{eq:HD+} and \eqref{eq:RLD+} are equivalent.

Under the actual dynamics \eqref{eq:HD} and \eqref{eq:RLD}, the state is restricted to the simplex ${\mathcal{X}}$, and its interior ${{\mathcal{X}}^{\circ}}$ is an invariant set.
As \eqref{eq:HD+} and \eqref{eq:RLD+} are defined in terms of the Hessian ${H}(x)$, we can try to recover \eqref{eq:HD} and \eqref{eq:RLD} by restricting the action of ${H}(x)$ to the tangent space of ${{\mathcal{X}}^{\circ}}$.
To do so while preserving the symmetry of ${H}(x)$, we can replace ${H}(x)$ with the ``projected'' matrix ${\Phi} {H}(x) {\Phi}$.
Then, to mimic the derivation of \eqref{eq:HD+} and \eqref{eq:RLD+}, we must next ``invert'' ${\Phi} {H}(x) {\Phi}$.
In light of Lemma \ref{lem:pseudo}, the natural choice is to take the pseudoinverse of ${\Phi} {H}(x) {\Phi}$, leading to the dynamics 
\begin{equation}
\label{eq:NewDyn}
\dot x
	= ({\Phi} {H}(x) {\Phi})^{+} {v}(x)^{{\!\top}}.
\end{equation}
Proposition \ref{prop:HopDRD} shows that \eqref{eq:NewDyn} is equivalent to the continuous Hessian dynamics \eqref{eq:HD}.
With some additional work, one can then show directly that \eqref{eq:NewDyn} is equivalent to the reinforcement learning dynamics \eqref{eq:RLD}.

\section{Convergence and stability in dynamical systems}
\label{app:dynamics}

\subsection{Definitions}

Throughout this appendix, we focus on the general dynamics
\begin{equation}
\tag{\ref*{eq:ED}}
\dot x
	= {V}(x),
	\quad
	x\in{\mathcal{X}},
\end{equation}
and we assume that they admit unique solutions from every initial condition.
In this context, we say that ${\hat x}$ is an $\omega$-\emph{limit point} of the solution orbit $x(t)$ if there is an increasing sequence of times $t_{n}\nearrow\infty$ such that $\lim x(t_{n}) = {x^{\ast}}$.
We further say that ${x^{\ast}}$ is \emph{Lyapunov stable} if, for every neighborhood $U$ of ${x^{\ast}}$, there exists a neighborhood $U'$ of ${x^{\ast}}$ such that every solution orbit $x(t)$ that starts in $U'$ is contained in $U$ for all $t\geq0$.
By the same token, ${x^{\ast}}$ is \emph{attracting} if there is a neighborhood $U$ of ${x^{\ast}}$ such that every solution that starts
in $U$ converges to ${x^{\ast}}$;
finally, ${x^{\ast}}$ is called \emph{asymptotically stable} if it is Lyapunov stable and attracting.
In this case, the maximal (relatively) open set of states from which solutions converge to ${x^{\ast}}$ is called the \emph{basin} of ${x^{\ast}}$, and if the basin of ${x^{\ast}}$ is all of ${\mathcal{X}}$, we say that ${x^{\ast}}$ is \emph{globally asymptotically stable}.

\subsection{A global convergence result}

A standard result from dynamical systems states that if a smooth dynamical system on a compact set admits a strict global Lyapunov function, all $\omega$-limit points are rest points \citep[see e.g.][Theorem 7.B.3]{San10}.  
The proof of this result relies on the continuity of solutions on initial conditions, a property which is not easily established for discontinuous dynamics.
In Proposition \ref{prop:omega} below, we present a global convergence result that does not require continuity of solutions in initial conditions, but instead relies on a \ac{lsc} lower bound on the derivative of the Lyapunov function.
To state it, let
\begin{equation}
{\mathrm{RP}}	
	= \setdef{x\in{\mathcal{X}}}{{V}(x) = 0}
\end{equation}
denote the set of rest points of the dynamics \eqref{eq:ED}.
We then have:

\begin{proposition}
\label{prop:omega}
Let $x(t)$ be an absolutely continuous solution orbit of \eqref{eq:ED} and let $\Gamma_{+} = x({\mathbb{R}}_{+})$ denote the set of points visited by $x(t)$.
Assume further that ${\mathrm{RP}}$ is closed and there exist functions ${L}{\colon}{\mathcal{X}}\to{\mathbb{R}}$ and ${\phi}{\colon}{\mathcal{X}}\to{\mathbb{R}}_{+}$ such that:
\begin{enumerate}
[\textup(i\textup)]
\item
\label{itm:omega-smooth}
${L}$ is differentiable in a neighborhood of $\Gamma_{+}$.
\item
\label{itm:omega-rest}
${\phi}$ is \acl{lsc} and ${\phi}(x) = 0$ if and only if $x \in {\mathrm{RP}}$.
\item
\label{itm:omega-Lyap}
$\braket{d{L}(x)}{{V}(x)} \geq {\phi}(x)$ for all $x\in \Gamma_{+}$.

\end{enumerate}
Then, $x(t)$ converges to ${\mathrm{RP}}$.
\end{proposition}

\begin{proof}
By absolute continuity and Conditions \eqref{itm:omega-smooth} and \eqref{itm:omega-Lyap} above, we get
\begin{equation}
\label{eq:Lyapunov-int}
{L}(x(t)) - {L}(x(0))
	= \int_{0}^{t} \braket{d{L}(x(s))}{{V}(x(s))} {\:d} s
	\geq \int_{0}^{t} {\phi}(x(s)) {\:d} s
	\geq 0,
\end{equation}
i.e. ${L}$ is nondecreasing along $x(t)$.
Furthermore, since ${\mathcal{X}}$ is compact, $x(t)$ admits at least one $\omega$-limit point ${x^{\ast}}\in\operatorname{cl}(\Gamma_{+}) \subseteq {\mathcal{X}}$.
Assume now that $x(t)$ admits an $\omega$-limit point ${\hat x}$ such that ${V}({\hat x}) \neq 0$.
Since ${\mathrm{RP}}$ is closed, Condition \eqref{itm:omega-rest} implies that there is a compact neighborhood $K$ of ${\hat x}$ and some $a>0$ such that ${\phi}(x) \geq a > 0$ for all $x\in K$.
With this in mind, we consider two complementary cases below:

\smallskip
\paragraph{\emph{Case 1}}
Suppose that there exists some $T\geq 0$ such that $x(t) \in K$ for all $t \geq T$.
Then ${\phi}(x(t)) \geq a$ for all $t\geq T$, so \eqref{eq:Lyapunov-int} yields $\lim_{t\to\infty} {L}(x(t)) = \infty$, a contradiction.

\smallskip
\paragraph{\emph{Case 2}}
Assume instead that, for all $T \geq0$, we have $x(t) \notin K$ for some $t\geq T$.
In this case, there exist open neighborhoods $U$ and $U'$ of ${\hat x}$ with $\operatorname{cl}(U) \subseteq U' \subset K$,
and interlaced sequences $t_{n},t_{n}'\nearrow\infty$ such that, for all $n$:
\begin{inparaenum}
[\textup(\itshape i\textup)]
\item
$t_{n} < t_{n}' < t_{n+1}$;
\item
$x(t_{n}) \in U$, $x(t_{n}') \in K\setminus U'$;
and
\item
$x(t)\in K$ whenever $t\in[t_{n},t_{n}']$.
\end{inparaenum}
Then, since $\abs{\dot x_{\alpha}(t)}$ is bounded from above by ${V}_{\max} \equiv \sup_{x\in{\mathcal{X}}} \max_{\beta} \abs{{V}_{\beta}(x)} < \infty$, the time intervals $\delta_{n} \equiv t_{n}' - t_{n}$ will be bounded from below by $\delta_{\min} \equiv \operatorname{dist}(\operatorname{cl}(U),K\setminus U') / {V}_{\max} > 0$.
We thus get
\begin{flalign}
{L}(x(t_{n}')) - {L}(x(0))
	\geq \int_{0}^{t_{n}'} {\phi}(x(s)) {\:d} s
	\geq \sum_{j=1}^{n} \int_{t_{j}}^{t_{j}'} {\phi}(x(s)) {\:d} s
	\geq  an\delta_{\min},
\end{flalign}
i.e. ${L}(x(t_{n}')) \to \infty$, a contradiction.
\end{proof}

To apply Proposition \ref{prop:omega} to discontinuous Riemannian dynamics in potential games, we need the following result:

\begin{lemma}
\label{lem:RLSC}
The speed of motion $\norm{{V}(x)}_{x}$ of the dynamics \eqref{eq:RGD} is \ac{lsc} on ${\mathcal{X}}$.
\end{lemma}

\begin{proof}
If the dynamics \eqref{eq:RGD} are continuous, our claim follows immediately from the continuity of the underlying metric \textendash\ in fact, $\norm{{V}(x)}_{x}$ is continuous in this case.
Otherwise, if \eqref{eq:RGD} is discontinuous, recall that ${V}(x) \equiv \operatorname{\Pi}_{x}({v}^{\sharp}(x))$ is simply the projection of ${v}^{\sharp}(x)$ on the tangent cone $\operatorname{TC}_{\mathcal{X}}(x)$ to ${\mathcal{X}}$ at $x$ (because ${\textrm{Adm}_{g}}(x) = \operatorname{TC}_{\mathcal{X}}(x)$ in that case).
Therefore, if we write ${V}^{\perp}(x) = {v}^{\sharp}(x) - {V}(x)$ for the projection of ${v}^{\sharp}(x)$ on the normal cone $\operatorname{NC}_{\mathcal{X}}(x)$ to ${\mathcal{X}}$ at $x$, Moreau's decomposition theorem and the Cauchy-Schwarz inequality yield
\begin{equation}
\product{{v}^{\sharp}(x)}{z}_{x}
	= \product{{V}(x) + {V}^{\perp}(x)}{z}_{x}
	= \product{{V}(x)}{z}_{x}
	\leq \norm{{V}(x)}_{x} \norm{z}_{x},
\end{equation}
for all $z\in\operatorname{TC}_{\mathcal{X}}(x)$,
with the inequality binding if and only if $z\propto{V}(x)$.
We thus obtain the characterization
\begin{equation}
\label{eq:SpeedMin}
\norm{{V}(x)}_{x}
	= \max \setdef{\product{{v}^{\sharp}(x)}{z}_{x}}{z \in \operatorname{TC}_{\mathcal{X}}(x)\cap{B}(x)},
\end{equation}
where ${B}(x) = \setdef{z\in{\mathbb{R}}^{\mathcal{A}}}{\norm{z}_{x}\leq1}$.

Note now that the correspondence $x \mapsto \operatorname{TC}_{\mathcal{X}}(x)$ is \ac{lsc} because it is constant on the interior of each face of ${\mathcal{X}}$ and  $\operatorname{TC}_{\mathcal{X}}(x) \subseteq \operatorname{TC}_{\mathcal{X}}(y)$ whenever $\operatorname{supp}(x) \subseteq \operatorname{supp}(y)$.
This shows that the constraint correspondence $x \mapsto \operatorname{TC}_{\mathcal{X}}(x) \cap U(x)$ of \eqref{eq:SpeedMin} is \ac{lsc};
since the objective function $\product{{v}^{\sharp}(x)}{z}_{x}$ of \eqref{eq:SpeedMin} is jointly continuous in $x$ and $z$, a precursor to the maximum theorem \citep[Lemma 16.30]{AB99} implies that $x \mapsto \norm{{V}(x)}_{x}$ is itself \ac{lsc}, as claimed.
\end{proof}

\section{Additional proofs}
\label{app:proofs}

In this appendix, we collect certain proofs that would have otherwise disrupted the flow of the main text.
We begin with Proposition \ref{prop:extension} on the continuous extension of inner products to the boundary of ${\mathcal{X}}$:

\begin{proof}[Proof of Proposition \ref{prop:extension}]
Fix some $x\in{\mathcal{K}}$ and write $g^{\sharp}(x) = Q^{\top}\Lambda Q$ where the diagonal matrix $\Lambda$ consists of the eigenvalues of $g^{\sharp}(x)$ and $Q$ is an orthogonal matrix ($Q^{\top} = Q^{-1}$) whose columns are the eigenvectors of $g^{\sharp}(x)$.
Since $g^{\sharp}(x)$ is positive-semidefinite, its eigenvalues are non-negative.
Furthermore, since ${\operatorname{dom} g}(x) = \operatorname{im} g^{\sharp}(x)$, every eigenvector of a nonzero eigenvalue of $g^{\sharp}(x)$ must lie in ${\operatorname{dom} g}(x)$:
indeed, if $g^{\sharp}(x) z = \lambda z$ for some $\lambda>0$, we will also have $z = g^{\sharp}(x)z\lambda^{-1}$, i.e. $z\in\operatorname{im} g^{\sharp}(x) = {\operatorname{dom} g} (x)$.
As a result, we may write $g^{\sharp}(x) = \sum_{\lambda>0} \lambda\, {u}_{\lambda} {u}_{\lambda}^{\top}$ where the summation is taken over all positive eigenvalues $\lambda>0$ of $g^{\sharp}(x)$ (assumed for convenience to be distinct) and ${u}_{\lambda}$ is the corresponding column of $Q$.
The metric tensor of the induced scalar product $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}_{x}$ at $x$ is then defined as the pseudoinverse $( g^{\sharp}(x) )^{+} $ of $g(x)$ (cf. Appendix \ref{app:geometry}), given here by
\begin{equation}
\label{eq:product-eig}
( g^{\sharp}(x) )^{+} 
= {\sum\nolimits}_{\lambda>0} \lambda^{-1}  {u}_{\lambda} {u}_{\lambda}^{\top}.
\end{equation}
Our continuity and uniqueness claims are then immediate.

Finally, to show that $g_{\alpha\beta}^{\sharp}(x) = 0$ if $\alpha\notin\operatorname{supp}(x)$ and $g$ is minimal-rank extendable, simply note that $g_{\alpha\beta}^{\sharp}(x) = {e}_{\alpha}^{\top} g^{\sharp}(x) {e}_{\beta} = {\sum\nolimits}_{\lambda} \lambda\, {e}_{\alpha}^{\top} {u}_{\lambda} {u}_{\lambda}^{\top} {e}_{\beta} = 0$ because all eigenvectors of $g^{\sharp}(x)$ with positive eigenvalues lie in ${\mathbb{R}}^{\operatorname{supp}(x)} = {\operatorname{dom} g}(x)$.
\end{proof}

We now proceed with the proof of Proposition \ref{prop:wp} regarding the existence and uniqueness of solutions to \eqref{eq:RGD}.
As noted in the main text, we only need to prove part \eqref{itm:wp-full}, which concerns the case of full-rank extendable metrics.
Existence of forward solutions of \eqref{eq:RGD} on ${\mathcal{X}}$ follows from general results of \cite{AC84} on solutions to discontinuous differential equations;
see \cite{LS08} for a summary of their argument.  
It remains to show that forward solutions to \eqref{eq:RGD} from each initial condition in ${\mathcal{X}}$ are unique.
This conclusion follows from Lemma \ref{lem:Gronwall} below, where ${V}(x) = \operatorname{\Pi}_{x}({v}^{\sharp}(x))$ denotes the right-hand side of \eqref{eq:RGD}:

\begin{lemma}
\label{lem:Gronwall}
Let $x(t)$ and $x'(t)$ be solutions to \eqref{eq:RGD}, and let 
\begin{equation}
\label{eq:DefP}
P(t)
	= \norm{x'(t)-x(t)}_{x(t)}^{2} \, e^{-\lambda t}.
\end{equation}
If $\lambda>0$ is large enough, then $P(t)$ is nonincreasing. 
\end{lemma}

Given this lemma, we immediately obtain:
\begin{proof}[Proof of Proposition \ref{prop:wp}]
Let $x(t)$ and $x'(t)$ be solutions of \eqref{eq:RGD} with $x(0)= x'(0)$.
We then get $P(t) = P(0) = 0$ for all $t\geq 0$, so $x(t)= x'(t)$ for all $t \geq 0$.
\end{proof}

To prove Lemma \ref{lem:Gronwall}, we need one final auxiliary result, showing that ${V}$ satisfies a one-sided Lipschitz
condition with respect to the underlying metric:

\begin{lemma}
\label{lem:1SL}
There exists a $K_{V} >0$ such that 
\begin{equation}
\label{eq:1SL}
\product{{V}(x')-{V}(x)}{x'-x}_{x}
	\leq K_{V} \norm{x'-x}_{x}^{2}
	\quad
	\text{for all $x, x' \in {\mathcal{X}}$}.
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:1SL}]
Write $w(x)= {v}^\sharp(x)$ and $w^\perp(x) = w(x) - \operatorname{\Pi}_{x}(w(x))$, and recall that since $g$ is full-rank extendable, $\operatorname{\Pi}_{x}$ is the orthogonal projection onto $\operatorname{TC}_{\mathcal{X}}(x)$ with respect to $\product{\mathopen{}\cdot\mathopen{}}{\mathopen{}\cdot\mathopen{}}_{x}$.
We thus obtain
\begin{flalign}
\label{eq:1SLpf1}
\langle{V}(x')& - {V}(x),x'-x\rangle_{x}
	= \product{w(x') - w(x)}{x' - x}_{x} - \product{w^\perp(x') - w^\perp(x)}{x' - x}_{x}
	\notag\\
	&= \product{w(x') - w(x)}{x' - x}_{x}
	
	
	+ \product{w^{\perp}(x)}{x' - x}_{x}
	+ \product{w^{\perp}(x')}{x - x'}_{x'}
	\notag\\
	&\qquad\quad+ (w^{\perp}(x'))^{{\!\top}}(g(x') - g(x))(x' - x)\notag\\
	&\leq K_{w} \norm{x' - x}^2_{x} + (w^{\perp}(x'))^{{\!\top}}(g(x') - g(x))(x' - x),
\end{flalign}
where the bound for the first term in the last line from the Cauchy-Schwarz inequality and the Lipschitz continuity of $w$, and the rest follows from Moreau's decomposition theorem.
To bound the last term, write $g_{\alpha}(x)$ for the $\alpha$-th row of $g(x)$, let $W_{\max}^{\perp} = \max_{\alpha\in{\mathcal{A}}}\max_{x\in{\mathcal{X}}} w_{\alpha}^{\perp}(x)$, and let $\norm{\cdot}_{2}$ denote the standard Euclidean norm.
Then, if $C>0$ is chosen sufficiently large, we get
\begin{flalign}
\label{eq:1SLpf2}
(w^{\perp}(x'))^{{\!\top}}
	&(g(x') - g(x)) (x' - x)
	\notag\\
	&\leq {\sum\nolimits}_{\alpha\in{\mathcal{A}}} w_{\alpha}^{\perp}(x') \norm{g_\alpha(x') - g_\alpha(x)}_{2} \norm{x'-x}_{2}
	\notag\\
	&\leq W_{\max}^{\perp} \norm{x' - x}_{2} {\sum\nolimits}_{\alpha\in{\mathcal{A}}} \norm{g_\alpha(x') - g_\alpha(x)}_{2}
	\leq W_{\max}^{\perp} C \norm{x' - x}_{x}^{2}.
\end{flalign}
In the above, the first inequality is an immediate corollary of the Cauchy-Schwarz inequality;
the last one follows from the equivalence of norms on ${\mathbb{R}}^{\mathcal{A}}$ and the fact that $g$ is $C^{1}$ on ${\mathcal{X}}$;
finally, $C$ can be chosen independently of $x$ and $x'$ because ${\mathcal{X}}$ is compact.
Thus, combining \eqref{eq:1SLpf1} and \eqref{eq:1SLpf2} completes our proof.
\end{proof}

With Lemma \ref{lem:1SL}, we finally obtain:

\begin{proof}[Proof of Lemma \ref{lem:Gronwall}]
Define $\dot g(x) = (\dot g_{\alpha\beta}(x))_{\alpha,\beta\in{\mathcal{A}}}$ by $\dot g_{\alpha\beta}(x) = \braket{dg_{\alpha\beta}(x)}{{V}(x)} = {\sum\nolimits}_{\kappa\in{\mathcal{A}}} {V}_{\kappa}(x) \, {\partial}_{\kappa} g_{\alpha\beta}(x)$, and let $K_{g} = \max_{\alpha,\beta\in{\mathcal{A}}}\max_{x\in{\mathcal{X}}} \dot g_{\alpha\beta}(x) < \infty$ (recall that $g$ is $C^{1}$).
Then, for all $t \geq 0$ such that $x(t)$ and $x'(t)$ are differentiable, we have
\begin{flalign}
\dot P
	&= 2 \product{{V}(x') - {V}(x)}{x' - x}_{x} \,e^{-\lambda t}
	+(x' - x)^{{\!\top}} \dot g(x) (x' - x) \, e^{-\lambda t}
	- \lambda \norm{x' - x}_{x}^{2} \, e^{-\lambda t}
	\notag\\
	&\leq -(\lambda - 2 K_{V} - K_{g}) \,\norm{x'-x}^2_{x}\,\, e^{-\lambda t},
\end{flalign}
where we used Lemma \ref{lem:Gronwall} to bound the second term in the first line.
Taking $\lambda > 2K_{V} + K_{g}$ then yields $\dot P \leq 0$;
since $x(t)$ and $x'(t)$ are absolutely continuous, we conclude that $P(t)$ is nondecreasing.
\end{proof}

We close this appendix with the proof of our permanence criterion:

\begin{proof}[Proof of Proposition \ref{prop:perm}]
Define $P{\colon}{\mathcal{K}}\to{\mathbb{R}}$ as $P(x) = -\exp\left(\sum_{\alpha} p_{\alpha} {D_{h}}({e}_{\alpha}, x)\right)$ for $x\in{{\mathcal{K}}^{\circ}}$ and $P(x) = 0$ for $x\in\operatorname{bd}({\mathcal{K}})$.
The steepness of $h$ implies that $P$ is continuous, while Proposition \ref{prop:Bregman-grad} implies that $\frac{d}{dt}\log(P(x)) = \Psi(x) \equiv \braket{{v}(x)}{p - x}$ for $x \in {{\mathcal{X}}^{\circ}}$.
Hence, by Theorem 12.2.1 of \cite{HS98}, it suffices to show that the function $\Psi$ is an average Lyapunov function for \eqref{eq:HD}, meaning that, for every initial condition $x(0)\in\operatorname{bd}({\mathcal{X}})$, there is a $t>0$ such that
\begin{equation}
\label{eq:Lyap-avg}
\frac{1}{t}\int_{0}^{t} \Psi(x(s)) {\:d} s
	> 0.
\end{equation}

We proceed by induction on the cardinality of the support of the initial condition.
The claim is trivial if this cardinality is $1$.
For the inductive step, suppose that \eqref{eq:Lyap-avg} holds when the cardinality is $k \in \{1, \ldots , |{\mathcal{A}}|-2\}$, and consider an initial condition $x(0)$ whose support ${\mathcal{A}}'$ has cardinality $k +1$.
If $x(t)$ converges to the boundary of the face ${\mathcal{X}}'$ of ${\mathcal{X}}$ spanned by ${\mathcal{A}}'$, then our claim follows from the inductive hypothesis and the same arguments as in the proof of Theorem 12.2.2 in \cite{HS98}.
If instead $x(t)$ does not converge to the boundary of ${\mathcal{X}}'$, then there exists a $\delta>0$ and an increasing sequence of times $t_{n}\nearrow\infty$ with $x_{\alpha}(t_{n}) \geq \delta > 0$ for all $\alpha\in{\mathcal{A}}'$.
Then, letting $\bar x_{\alpha}(t) = t^{-1} \int_{0}^{t} x_{\alpha}(s) {\:d} s$ and $\bar u(t) = t^{-1} \int_{0}^{t} \braket{{v}(x(s))}{x(s)} {\:d} s$, we may assume (by descending to a subsequence of $t_{n}$ if necessary) that $\bar x(t_{n})$ and $\bar u(t_{n})$ converge to some ${\bar x^{\ast}}$ and $\bar u^{\ast}$ respectively as $n\to\infty$.

We now claim that ${v}_{\alpha}({\bar x^{\ast}}) = \bar u^{\ast}$ for all $\alpha\in{\mathcal{A}}'$, implying that ${\bar x^{\ast}}$ is a restricted equilibrium of ${\mathcal{G}}$.
Indeed, let ${y}_{\alpha}(t) = {D_{h}}({e}_{\alpha},x(t))$ for all $\alpha\in{\mathcal{A}}'$.
Then 
\(
\dot {y}_{\alpha}
	= \braket{{v}(x)}{x} - {v}_{\alpha}(x)
\)
by Proposition \ref{prop:Bregman-grad}, so the linearity of ${v}(x)$ in $x$ implies that
\begin{equation}
\label{eq:PermAvgs}
\frac{1}{t} \int_{0}^{t} \braket{{v}(x(s))}{x(s)} {\:d} s - {v}_{\alpha}(\bar x(t)) 
	= \frac{{y}_{\alpha}(t) - {y}_{\alpha}(0)}{t}.
\end{equation}
Given that $x(t_{n})$ remains a minimal positive distance away from $\operatorname{bd}({\mathcal{X}}')$, it follows that ${y}_{\alpha}(t_n)$ is bounded from above for all $\alpha\in{\mathcal{A}}'$.
Therefore, the right hand side of \eqref{eq:PermAvgs} vanishes as $t_{n}\to\infty$, implying in turn that ${v}_{\alpha}({\bar x^{\ast}}) = \bar u^{\ast}$ for all $\alpha\in{\mathcal{A}}'$, as claimed.

Now, since ${\bar x^{\ast}}$ is a restricted equilibrium of ${\mathcal{G}}$, Proposition \ref{prop:stationary} implies that it is a boundary rest point of \eqref{eq:HD}, so $\bar u^{\ast} = \braket{{v}({\bar x^{\ast}})}{\bar x^{\ast}} < \braket{{v}({\bar x^{\ast}})}{p}$ by \eqref{eq:perm-cond}.
Moreover, since $\Psi(x) = \braket{{v}(x)}{p - x}$, setting $t=t_{n}$ in \eqref{eq:Lyap-avg} yields
\begin{equation}
t_{n}^{-1}\int_{0}^{t_{n}} \Psi(x(s)) {\:d} s
	= t_{n}^{-1} \int_{0}^{t_{n}} \braket{{v}(x(s))}{p - x(s)} {\:d} s
	= \braket{{v}(\bar x(t_{n}))}{p} - \bar u(t_{n})
\end{equation}
so $\lim_{n\to\infty} t_{n}^{-1} \int_{0}^{t_{n}} \Psi(x(s)) {\:d} s = \braket{{v}({\bar x^{\ast}})}{p} - \bar u^{\ast} > 0$.
This establishes \eqref{eq:Lyap-avg} for large enough $t = t_{n}$, completing our proof.
\end{proof}

\bibliographystyle{ormsv080}
\bibliography{Bibliography-GD}

\end{document}
