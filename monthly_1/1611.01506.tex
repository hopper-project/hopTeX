\documentclass[11pt,reqno]{amsart}
\usepackage{a4wide,xcolor,eucal,enumerate,mathrsfs,graphics,caption,subfig,booktabs,verbatim}
\usepackage{pdfsync}
\usepackage{amsmath,amssymb,epsfig,amsthm,bm}
\usepackage[latin1]{inputenc}
\usepackage{dsfont}

\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm,amssymb,dsfont,url}
\usepackage[foot]{amsaddr}
\usepackage[babel]{csquotes}
\usepackage[style=numeric,backend=bibtex,doi=false,isbn=false,url=false]{biblatex}

\bibliography{shapeconstrained-estimation}

\renewbibmacro{in:}{  \ifentrytype{article}{}{\printtext{\bibstring{in}\intitlepunct}}}
  \DeclareFieldFormat[article]{pages}{#1}  \DeclareFieldFormat[article]{title}{#1}  \DeclareFieldFormat[book]{title}{#1}  \DeclareFieldFormat[incollection]{title}{#1}  \DeclareFieldFormat[thesis]{title}{#1}	
	
\theoremstyle{definition}
\newtheorem{de}{Definition}[section]
\newtheorem{ex}[de]{Example}
\theoremstyle{plain}
\newtheorem{theo}[de]{Theorem}
\newtheorem{lemma}[de]{Lemma}
\newtheorem{prop}[de]{Proposition}
\newtheorem{cor}[de]{Corollary}
\theoremstyle{remark}
\newtheorem{re}[de]{Remark}

\title[Isotonized smooth estimators of a monotone hazard in the Cox model]{Isotonized smooth estimators of a monotone baseline hazard in the Cox model}
\author[Hendrik P. Lopuha\"a and Eni Musta]{Hendrik P. Lopuha\"a \and Eni Musta \\ Delft University of Technology}
\address{Delft Institute of Applied Mathematics, Mekelweg 4, 2628 CD Delft, The Netherlands}
\email{H.P.Lopuhaa@tudelft.nl, E.Musta@tudelft.nl}
\keywords{isotonic estimation, hazard rate, kernel smoothing, asymptotic normality, Cox regression model,
isotonized smoothed Breslow estimator, maximum smoothed likelihood estimator.}
\begin{document}

\begin{abstract}
We consider two isotonic smooth estimators for a monotone baseline hazard in the Cox model,
a maximum smooth likelihood estimator and a Grenander-type estimator based on the smoothed Breslow estimator for the
cumulative baseline hazard.
We show that they are both asymptotically normal at rate $n^{m/(2m+1)}$, where $m\geq 2$ denotes the level of smoothness considered,
and we relate their limit behavior to kernel smoothed isotonic estimators studied in~\cite{LopuhaaMustaSI2016}.
It turns out that the Grenander-type estimator is asymptotically equivalent to the kernel smoothed isotonic estimators,
while the maximum smoothed likelihood estimator exhibits the same asymptotic variance but a different bias.
Finally, we present numerical results on pointwise confidence intervals that illustrate the comparable behavior of the two methods.
\end{abstract}

\maketitle

\section{Introduction}
For studying lifetime distributions in the presence of right censored survival data,
the Cox regression model is a very popular method that allows incorporation of covariates.
The fact that the regression coefficients (parametric component) can be estimated while leaving the baseline distribution (nonparametric component) unspecified,
together with its ease of interpretation, resulting from the formulation in terms of the hazard rate as well as the proportional effect of the covariates,
favor the wide use of this semi-parametric model, especially in medical applications.
Since its first introduction (see~\cite{Cox72}), much effort has been spent on giving a firm mathematical basis to this approach.
Initially, the attention was on the derivation of large sample properties of the maximum partial likelihood estimator of the regression coefficients and of the Breslow estimator for the cumulative baseline hazard (e.g., see~\cite{Efron72}, \cite{Cox75}, \cite{Tsiatis81}).
Although the most attractive property of this approach is that it does not assume any fixed shape on the hazard curve, there are several cases
where order restrictions better match the practical expectations
(e.g., see~\cite{Geloven13} for an example of a decreasing hazard in a large clinical trial for patients with acute coronary syndrome).
Estimation of the baseline hazard function under monotonicity constraints was first studied in~\cite{CC94} and
more recently by~\cite{LopuhaaNane2013}, who investigate the maximum likelihood estimator and a Grenander-type estimator
defined as the slope of the greatest convex minorant (or least concave majorant) of the Breslow estimator.

Traditional isotonic estimators, such as maximum likelihood estimators and Grenander-type estimators,
are step functions that exhibit a non normal limit distribution at rate~$n^{1/3}$.
On the other hand, a long stream of research has shown that,
if one is willing to assume more regularity on the function of interest,
smooth estimators can be used to achieve a faster rate of convergence to a Gaussian distributional law and to estimate derivatives.
Typically, these estimators are constructed by combining an isotonization step with a smoothing step.
Estimators constructed by smoothing followed by an isotonization step have been considered
in~\cite{chenglin1981}, \cite{wright1982}, \cite{friedmantibshirani1984}, and~\cite{ramsay1998}, for the regression setting,
and in~\cite{vdvaart-vdlaan2003} for estimating a monotone density.
Methods that interchange the smoothing step and the isotonization step,
can be found in~\cite{mukerjee1988}, \cite{DGL13}, and~\cite{LM15}, who study kernel smoothed isotonic estimators,
and in~\cite{eggermont-lariccia2000}, who consider maximum smoothed likelihood estimators for monotone densities.
Comparisons between isotonized smooth estimators and smoothed isotonic estimators are made in~\cite{mammen1991} for the regression setting,
in~\cite{GJW10} for the current status model,
and in~\cite{GJ13}, who investigate a smoothed maximum likelihood estimator and a penalized least squares estimator for a monotone hazard.

In~\cite{Nane}, several smooth monotone estimators for a monotone baseline hazard in the Cox model have been introduced, which were shown to be consistent.
Two of these methods are kernel smoothed versions of the maximum likelihood estimator and the Grenander-type estimator from~\cite{LopuhaaNane2013}.
Both methods have been studied by~\cite{LopuhaaMustaSI2016} and were shown to be asymptotically normal at rate $n^{m/(2m+1)}$,
where $m$ denotes the level of smoothness of the baseline hazard.

In this paper we investigate two other estimators,
for which the order of the smoothing step and the isotonization step is interchanged.
The first estimator that we consider is the maximum smoothed likelihood estimator.
This estimator is similar to the methods in~\cite{eggermont-lariccia2000} for monotone densities and in~\cite{GJW10} for the current status model.
The second estimator is a Grenander-type estimator based on the smoothed Breslow estimator.
This estimator is similar to the methods considered in~\cite{chenglin1981}, \cite{wright1982}, \cite{friedmantibshirani1984}, and~\cite{vdvaart-vdlaan2003},
and to one of the two methods studied in~\cite{mammen1991}.
Asymptotic normality at rate $n^{m/(2m+1)}$ is established for both estimators, for which we rely on techniques developed in~\cite{GJW10}.
The isotonized smoothed Breslow estimator is shown to be asymptotically equivalent to the smoothed Grenander-type estimator studied in~\cite{LopuhaaMustaSI2016}.
This means that the order of smoothing and isotonization is irrelevant, which is in line with the findings in~\cite{mammen1991}.
The maximum smoothed likelihood estimator exhibits the same variance as the previous ones but has a different asymptotic bias,
a phenomenon that was also encountered in~\cite{GJW10}.
A small simulation study shows that no method performs strictly better than the other.

The paper is organized as follows.
In Section~\ref{sec:model} we specify the Cox regression model and provide some background information that will be used  in the sequel.
The maximum smoothed likelihood estimator is considered in Section~\ref{sec:MSLE}
and the isotonized smoothed Breslow estimator in Section~\ref{sec:GS}.
We only consider the case of a non-decreasing baseline hazard.
The same results can be obtained similarly for a non-increasing hazard.
The results of a small simulation study are reported in Section~\ref{sec:conf-int}.

\section{The Cox regression model}\label{sec:model}
Let $X_1,\dots,X_n$ be an i.i.d.~sample representing the survival times of $n$ individuals, which can be observed only on time intervals $[0,C_i]$ for some i.i.d. censoring times $C_1,\dots,C_n$. The observations consists of  i.i.d. triplets $(T_1,\Delta_1,Z_1),\dots,(T_n,\Delta_n,Z_n)$, where $T_i=\min(X_i,C_i)$ denotes the follow up time,  $\Delta_i={\mathds{1}}_{\{X_i\leq C_i\}}$ is the censoring indicator and $Z_i\in{\mathbb{R}}^p$ is a time independent covariate vector. Given the covariate vector $Z,$ the event time $X$ and the censoring time $C$ are assumed to be independent. Furthermore, conditionally on $Z=z,$ the event time is assumed to be a nonnegative r.v. with an absolutely continuous distribution function $F(x|z)$ and density $f(x|z).$ Similarly the censoring time is assumed to be a nonnegative r.v. with an absolutely continuous distribution function $G(x|z)$ and density $g(x|z).$ The censoring mechanism is assumed to be non-informative, i.e. $F$ and $G$ share no parameters.

Let $H$ and $H^{uc}$ denote respectively the distribution function of the follow-up time and the sub-distribution function of the uncensored observations, i.e.,
\begin{equation}
\label{eq:def Huc}
H^{uc}(x)={\mathbb{P}}(T\leq x,\Delta=1)=\int \delta{\mathds{1}}_{\{t\leq x\}}\,\mathrm{d}\mathbb{P}(t,\delta,z),
\end{equation}
where ${\mathbb{P}}$ is the distribution of $(T,\Delta,Z)$.
We also require the following assumptions, some of which are common in large sample studies of the Cox model (e.g., see~\cite{LopuhaaNane2013}):
\begin{itemize}
\item[(A1)]
Let $\tau_F,\,\tau_G$ and $\tau_H$ be the end points of the support of $F,\,G$ and $H$. Then
\[
\tau_H=\tau_G<\tau_F\leq\infty.
\]
\item[(A2)]
There exists $\epsilon>0$ such that
\[
\sup_{|\beta-\beta_0|\leq\epsilon}{\mathbb{E}}\left[|Z|^2\,\mathrm{e}^{2\beta'Z}\right]<\infty.
\]
\item[(A3)]
There exists $\epsilon>0$ such that
\[
\sup_{|\beta-\beta_0|\leq\epsilon}{\mathbb{E}}\left[|Z|^2\,\mathrm{e}^{4\beta'Z}\right]<\infty.
\]
\end{itemize}
Let us briefly comment on these assumptions.
While the first one tells us that, at the end of the study, there is at least one subject alive, the other
two are somewhat hard to justify from a practical point of view.
One can think of (A2) and (A3) as conditions on the boundedness of the second moment of the covariates, for $\beta$ in a neighborhood of $\beta_0$.

Within the Cox model, the conditional hazard rate $\lambda(x|z)$ for a subject with covariate vector $z\in{\mathbb{R}}^p$, is related to the corresponding covariate by
\[
\lambda(x|z)=\lambda_0(x)\,\mathrm{e}^{\beta'_0z},\quad x\in{\mathbb{R}}^+,
\]
where $\lambda_0$ represents the baseline hazard function, corresponding to a subject with $z=0$, and $\beta_0\in{\mathbb{R}}^p$ is the vector of the regression coefficients.

By now, it seems to be rather a standard choice estimating $\beta_0$ by $\hat{\beta}_n$, the maximizer of the partial likelihood function, as proposed by~\cite{Cox72}.
The asymptotic behavior was first studied by~\cite{Tsiatis81}.
We aim at estimating $\lambda_0$, subject to the constraint that it is increasing (the case of a decreasing hazard is analogous), on the basis of $n$ observations $(T_1,\Delta_1,Z_1),\dots,(T_n,\Delta_n,Z_n)$.  We  refer to the quantity
\[
\Lambda_0(t)=\int_0^t\lambda_0(u)\,\mathrm{d}u,
\]
as the cumulative baseline hazard and, by introducing
\begin{equation}
\label{eq:def Phi}
\Phi(x;\beta)=\int {\mathds{1}}_{ \{t\geq x\}}\,\mathrm{e}^{\beta'z}\,\mathrm{d}{\mathbb{P}}(t,\delta,z),
\end{equation}
we have
\begin{equation}
\label{eqn:lambda0}
\lambda_0(x)
=
\frac{h(x)}{\Phi(x;\beta_0)},
\end{equation}
where $h(x)=\mathrm{d}H^{uc}(x)/\mathrm{d}x$
(e.g., see (9) in~\cite{LopuhaaNane2013}).
For $\beta\in{\mathbb{R}}^p$ and $x\in{\mathbb{R}}$, the function $\Phi(x;\beta)$ can be estimated by
\begin{equation}
\label{eq:def Phin}
\Phi_n(x;\beta)=\int {\mathds{1}}_{\{t\geq x\}} \mathrm{e}^{\beta'z}\,\mathrm{d}{\mathbb{P}}_n(t,\delta,z),
\end{equation}
where ${\mathbb{P}}_n$ is the empirical measure of the triplets $(T_i,\Delta_i,Z_i)$ with $i=1,\dots,n.$  Moreover, in Lemma 4 of~\cite{LopuhaaNane2013} it is shown that
\begin{equation}
\label{eqn:Phi}
\sup_{x\in{\mathbb{R}}}|\Phi_n(x;\beta_0)-\Phi(x;\beta_0)|=O_p(n^{-1/2}).
\end{equation}
It will be often used throughout the paper that a stochastic bound of the same order holds also for the distance between the Breslow estimator
\begin{equation}
\label{eq:Breslow}
\Lambda_n(x)=\int \frac{\delta{\mathds{1}}_{\{ t\leq x\}}}{\Phi_n(t;\hat{\beta}_n)}\,\mathrm{d}{\mathbb{P}}_n(t,\delta,z).
\end{equation}
of the cumulative hazard and $\Lambda_0$, but only on intervals staying away from the right boundary
\begin{equation}
\label{eqn:Breslow}
\sup_{x\in[0,M]}|\Lambda_n(x)-\Lambda_0(x)|=O_p(n^{-1/2}), \qquad\text{for all }0<M<\tau_H,
\end{equation}
(see Theorem 5 in~\cite{LopuhaaNane2013}).

Smoothing is done by means of kernel functions.
We will consider kernel functions $k$ that are $m$-orthogonal, for some $m\geq 1$,
which means that~$\int |k(u)||u|^m\,\mathrm{d}u<\infty$ and
$\int k(u)u^j\,\mathrm{d}u=0$, for $j=1,\ldots,m-1$, if $m\geq 2$.
We assume that
\begin{equation}
\label{def:kernel}
\text{$k$ has bounded support $[-1,1]$ and is such that $\int_{-1}^1 k(y)\,\mathrm{d}y=1$.}
\end{equation}
We denote by $k_b$ its scaled version $k_b(u)=b^{-1}k(u/b)$.
Here $b=b_n$ is a bandwidth that depends on the sample size, in such a way that
$0<b_n\to 0$ and $nb_n\to\infty$, as $n\to\infty$.
From now on, we will simply write $b$ instead of $b_n$.
Note that if $m>2$, the kernel function $k$ necessarily attains negative values and as a result also the smooth estimators of the baseline hazard defined in
Sections~\ref{sec:MSLE} and~\ref{sec:GS} may be negative.
To avoid this, one could restrict oneself to $m=2$.
In that case, the most common choice is to let $k$ be a symmetric probability density.

\section{Maximum smooth likelihood estimator}
\label{sec:MSLE}
Maximum smoothed likelihood estimation is studied in~\cite{eggermont-lariccia2000},
who obtain $L_1$-error bounds for the maximum smoothed likelihood estimator of a monotone density.
This method was also considered in~\cite{GJW10} for estimating the distribution function of interval censored observations.
The approach is to smooth the loglikelihood and then maximize the smoothed loglikelihood over all monotone functions of interest.
For a fixed $\beta$, the (pseudo) loglikelihood for the Cox model can be expressed as
\[
\int
\left(
\delta\log \lambda_0(t)-\mathrm{e}^{\beta'z}\int_0^t\lambda_0(u)\,\mathrm{d}u
\right)\,\mathrm{d}{\mathbb{P}}_n(t,\delta,z),
\]
(see (2) in~\cite{LopuhaaNane2013}).
To construct the maximum smoothed likelihood estimator (MSLE) we replace~${\mathbb{P}}_n$ in the previous expression with the smoothed empirical measure (in the time direction),
\[
\mathrm{d}\tilde{\mathbb{P}}_n(t,\delta,z)
=
\frac{1}{n}\sum_{i=1}^n {\mathds{1}}_{(\Delta_i,Z_i)}(\delta,z)\,k_b(t-T_i)\,\mathrm{d}t,
\]
and then maximize the smoothed (pseudo) loglikelihood
\begin{equation}
\label{def:smooth likelihood}
\ell^s_\beta(\lambda_0)
=
\int
\left(
\delta\log \lambda_0(t)-\mathrm{e}^{\beta'z}\int_0^t\lambda_0(u)\,\mathrm{d}u
\right)\,\mathrm{d}\tilde{\mathbb{P}}_n(t,\delta,z).
\end{equation}
The characterization of the MSLE is similar to that of the ordinary MLE
(see Lemma~1 in~\cite{LopuhaaNane2013}).
It involves the following processes.
Fix $\beta\in{\mathbb{R}}^p$ and let
\begin{equation}
\label{eqn:v_n w_n}
\begin{split}
w_n(t;\beta)
&=
\frac{1}{n}\sum_{i=1}^n
\mathrm{e}^{\beta'Z_i}\int_{t}^\infty k_b(u-T_i)\,\mathrm{d}u,\\
v_n(t)
&=
\frac{1}{n}
\sum_{i=1}^n \Delta_ik_b(t-T_i).
\end{split}
\end{equation}
The next lemma characterizes the maximizer of $\ell^s_\beta$.

\begin{lemma}
\label{lem:char MSLE}
Let $\ell^s_\beta$, $w_n$ and $v_n$ be defined by~\eqref{def:smooth likelihood} and~\eqref{eqn:v_n w_n}, respectively.
The unique maximizer of $\ell^s_\beta$ over all nondecreasing positive functions $\lambda_0$ can be described as the slope of the greatest convex minorant (GCM)
of the continuous cumulative sum diagram
\begin{equation}
\label{eqn:graph}
t\mapsto \left(\int_0^t w_n(x;\beta)\,\mathrm{d}x, \int_0^t v_n(x)\,\mathrm{d}x\right),
\qquad t\in[0,\tau_\beta],
\end{equation}
where $\tau_\beta
=
\sup\{t\geq 0:w_n(t;\beta)>0\}$.
\end{lemma}
\begin{proof}
We start by writing
\[
\begin{split}
\ell^s_\beta(\lambda_0)
&=
\frac{1}{n}\sum_{i=1}^n
\left\{
\Delta_i\int_0^\infty\log \lambda_0(t)k_b(t-T_i)\,\mathrm{d}t
-
\mathrm{e}^{\beta'Z_i}\int_0^\infty
\left(
\int_0^t \lambda_0(u)\,\mathrm{d}u
\right)k_b(t-T_i)
\,\mathrm{d}t\right\}\\
&=
\int_0^\infty\log \lambda_0(t)
\left( \frac{1}{n}\sum_{i=1}^n \Delta_i
k_b(t-T_i)\right)\,\mathrm{d}t\\
&\qquad\qquad\qquad\qquad\qquad\qquad-
\int_0^\infty
\lambda_0(u)
\left(\frac{1}{n}\sum_{i=1}^n\mathrm{e}^{\beta'Z_i}
\int_{u}^\infty k_b(t-T_i)\,\mathrm{d}t\right)\,\mathrm{d}u,
\end{split}
\]
which is equal to
\[
\begin{split}
\int_0^\infty
\Big\{
v_n(t)\log\lambda_0(t)-w_n(t;\beta)\lambda_0(t)
\Big\}\,\mathrm{d}t
=
\int_0^\infty
\left\{
\frac{v_n(t)}{w_n(t;\beta)}\log\lambda_0(t)-\lambda_0(t)
\right\}w_n(t;\beta)\,\mathrm{d}t,
\end{split}
\]
with $v_n$ and $w_n$ defined in~\eqref{eqn:v_n w_n}.
Applying Theorem~1 in~\cite{GJ10} with $\Phi(u)=u\log u$, we obtain
that the unique maximizer~$\hat{\lambda}^s_n(x;\beta)$ of $\ell^s_\beta$ is the solution of a generalized continuous isotonic regression problem, i.e.,
it is continuous and it is the minimizer of
\begin{equation}
\label{eqn:isot.regr}
\psi(\lambda)=\frac{1}{2}\int \left(\lambda(x)-\frac{v_n(x)}{w_n(x;\beta)}\right)^2
w_n(x;\beta)\,\mathrm{d}x,
\end{equation}
over all nondecreasing functions $\lambda$ and can be described as the slope of the GCM of the graph defined by~\eqref{eqn:graph}.
\end{proof}

For a fixed $\beta$, let $\hat{\lambda}^s_n(x;\beta)$ be the unique maximizer of $\ell^s_\beta(\lambda_0)$ over all nondecreasing positive functions $\lambda_0$.
We define the MSLE by
\begin{equation}
\label{def:lambdaMS}
\hat{\lambda}^{MS}_n(x)=\hat{\lambda}^s_n(x;\hat{\beta}_n),
\end{equation}
where $\hat{\beta}_n$ denotes the maximum partial likelihood estimator for $\beta_0$.
It can be seen that
\[
\begin{split}
\int_0^t w_n(x;\hat\beta_n)\,\mathrm{d}x
&=
\int
\hat W_n(s)k_b(t-s)\,\mathrm{d}s+O_p(b),\\
\int_0^t v_n(x)\,\mathrm{d}x
&=
\int V_n(s)k_b(t-s)\,\mathrm{d}s+O_p(b),
\end{split}
\]
where the processes $V_n$ and $\hat W_n$, as defined in~Lemma~1 in~\cite{LopuhaaNane2013}, determine the cumulative sum diagram corresponding
to the ordinary MLE.
This means that the cumulative sumdiagram that characterizes the MSLE,
is asymptotically equivalent to a kernel smoothed version of the cumulative sumdiagram that characterizes the ordinary MLE.

The representation in~\eqref{eqn:isot.regr} also suggests
\begin{equation}
\label{def:naive est MSLE}
\hat{\lambda}_n^{\mathrm{naive}}(x)=\frac{v_n(x)}{w_n(x;\hat{\beta}_n)}
\end{equation}
as a naive estimator for $\lambda_0$.
Figure~\ref{fig:MSLE}
\begin{figure}[t]
\includegraphics[bb=28 124 932 544,width=\textwidth,clip=]{MSLE1}
\caption{Left panel: The MSLE (solid) and the naive estimator (dotted) of the hazard function (dashed) using bandwidth $b_n=0.5n^{-1/5}$.
Right panel: The same but using bandwidth $b_n=n^{-1/5}$.}
\label{fig:MSLE}
\end{figure}
illustrates the MSLE and the naive estimator for
a sample of size $n=500$ from a Weibull baseline distribution with shape parameter $1.5$ and scale $1$.
For simplicity, we assume that the real valued covariate and the censoring times are uniformly $(0,1)$ distributed and we take $\beta_0=0.5$.
We used the triweight kernel function
$k(u)=(35/32)(1-u^2)^3{\mathds{1}}_{\{|u|\leq 1\}}$
and bandwidth $b=n^{-1/5}$.
Note that if we use bandwidth $b_n=0.5n^{-1/5}$, the naive estimator is not monotone,
but the distance to the MSLE (which is the isotonic version of $\hat{\lambda}^{\mathrm{naive}}_n$) is very small.
On the other hand, for bandwidth $b_n=n^{-1/5}$ isotonization is not needed and the two estimators coincide.

Following the reasoning in~\cite{GJW10}, the derivation of the asymptotic distribution of $\hat{\lambda}^{MS}_n$
is based on showing that with probability converging to one,
the naive estimator will be monotone and equal to $\hat{\lambda}^{MS}_n$ on large intervals.
Consequently, it will be sufficient to find the asymptotic distribution of the naive estimator.
The advantage of this approach is that in this way we basically have to deal with the naive estimator which is a more tractable process.
Note that~$\hat{\lambda}^{MS}_n$ and~$\hat{\lambda}_n^{\mathrm{naive}}$ are defined on $[0,\hat\tau_n]$, where
$\hat\tau_n=\sup\{t\geq 0:w_n(t;\hat{\beta}_n)>0\}$, and that~$\hat\tau_n\to \tau_H$ with probability one.
A first key result is the following lemma.
\begin{lemma}
\label{le:1}
Let $H^{uc}(t)$ be defined in~\eqref{eq:def Huc} and let $h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$.
Suppose that $h$ and $t\mapsto\Phi(t;\beta_0)$ are $m\geq 1$ times continuously differentiable.
Let $k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is differentiable with a bounded derivative.
Then, for each $0<\ell<M<\tau_H$, it holds
\begin{equation}
\label{eqn:lemma1-1}
\begin{split}
\sup_{t\in[\ell,M]}|v_n(t)-h(t)|
&=
O(b^m)+O_p(b^{-1}n^{-1/2}),\\
\sup_{t\in[\ell,M]}|w_n(t;\hat{\beta}_n)-\Phi(t;\beta_0)|
&=
O(b^m)+O_p(b^{-1}n^{-1/2}),
\end{split}
\end{equation}
where $v_n$, $w_n$ and $\Phi$ are defined in~\eqref{eqn:v_n w_n} and~\eqref{eq:def Phi}.
\end{lemma}

\begin{proof}
To obtain the first result in~\eqref{eqn:lemma1-1}, we write
\[
v_n(t)-h(t)=v_n(t)-h_s(t)+h_s(t)-h(t),
\]
where
\begin{equation}
\label{def:hs}
h_s(t)=\int k_b(t-u)\,h(u)\,\mathrm{d}u.
\end{equation}
By a change of variable and a Taylor expansion, using that $k$ is $m$-orthogonal,
we deduce that
\begin{equation}
\label{eq:taylor h}
\begin{split}
h_s(t)-h(t)
&=
\int k_b(t-u)h(u)\,\mathrm{d}u-h(t)
=
\int_{-1}^1 k(y)
\left\{
h(t-by)-h(t)
\right\}\,\mathrm{d}y\\
&=
\int_{-1}^1 k(y)
\left\{
-h'(t)by+\cdots+\frac{h^{(m-1)}(t)}{(m-1)!}(-by)^{m-1}+\frac{h^{(m)}(\xi_{ty})}{m!}(-by)^m
\right\}\,\mathrm{d}y\\
&=
\frac{(-b)^m}{m!}
\int_{-1}^1
h^{(m)}(\xi_{ty})k(y)y^m\,\mathrm{d}y,
\end{split}
\end{equation}
for some $|\xi_{ty}-t|<|by|$.
It follows that
\begin{equation}
\label{eq:h term}
\sup_{t\in[\ell,M]}|h_s(t)-h(t)|
\leq
\frac{b^m}{m!}
\sup_{t\in[0,\tau_H]}\left|h^{(m)}(t)\right|
\int_{-1}^1
|k(y)||y|^m\,\mathrm{d}y
=
O(b^m).
\end{equation}
Let $H^{uc}_n$ be the empirical sub-distribution function of the uncensored observations, defined by
\[
H_n^{uc}(x)
=
\int \delta {\mathds{1}}_{\{t\leq x\}}\,\mathrm{d}{\mathbb{P}}_n(t,\delta,z).
\]
Then integration by parts yields
\begin{equation}
\label{eq:v minus h}
\begin{split}
v_n(t)-h_s(t)&=\int k_b(t-u)\,\mathrm{d}(H^{uc}_n-H^{uc})(u)\\
&=
-\int \frac{\partial}{\partial u}k_b(t-u)\,(H^{uc}_n-H^{uc})(u)\,\mathrm{d}u\\
&=
\frac{1}{b}\int_{-1}^1 k'(y)\,(H^{uc}_n-H^{uc})(t-by)\,\mathrm{d}y.
\end{split}
\end{equation}
Since
$H^{uc}_n(x)-H^{uc}(x)=\int \delta{\mathds{1}}_{\{u\leq x\}}\,\mathrm{d}({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)$
and the class of indicator functions ${\mathcal{F}}=\left\{f(\cdot;x)\,:x\in[0,\tau_H]\right\}$,
with $f(u;x)={\mathds{1}}_{\{u\leq x\}}$,  is  Donsker, it follows by the continuous mapping theorem that
\begin{equation}
\label{eqn:H_uc}
\sqrt{n}\sup_{x\in[0,\tau_H]}|H^{uc}_n(x)-H^{uc}(x)|=O_p(1).
\end{equation}
Hence, we get
\begin{equation}
\label{eq:v minus h limit}
\sup_{t\in[\ell,M]}|v_n(t)-h_s(t)|
\leq
\frac{1}{b}
\sup_{x\in[\ell,M]}|H^{uc}_n(x)-H^{uc}(x)|
\int_{-1}^1 |k'(y)|\,\mathrm{d}y
=
O_p(b^{-1}n^{-1/2}).
\end{equation}
Together with~\eqref{eq:h term}, this proves the first result in~\eqref{eqn:lemma1-1}.

To prove the second result in~\eqref{eqn:lemma1-1}, note that from~\eqref{eqn:v_n w_n} and~\eqref{eq:def Phin} we have
\begin{equation}
\label{eqn:w}
w_n(t;\beta)
=
\frac{1}{n}\sum_{i=1}^n
\mathrm{e}^{\beta'Z_i}\int_{t}^\infty k_b(u-T_i)\,\mathrm{d}u
=
\int_{-1}^1 k(y)\Phi_n(t-by;\beta)\,\mathrm{d}y.
\end{equation}
Consequently, we can write
\[
\begin{split}
&
w_n(t;\hat{\beta}_n)-\Phi(t;\beta_0)\\
&=
\int_{-1}^1 k(y)\left\{\Phi_n(t-by;\hat{\beta}_n)-\Phi(t;\beta_0)\right\}\,\mathrm{d}y\\
&=
\int_{-1}^1 k(y)
\left\{
\Phi_n(t-by;\hat{\beta}_n)-\Phi(t-by;\beta_0)
\right\}
\,\mathrm{d}y
+
\int_{-1}^1 k(y)
\left\{
\Phi(t-by;\beta_0)-\Phi(t;\beta_0)
\right\}
\,\mathrm{d}y.
\end{split}
\]
Similar to~\eqref{eq:taylor h} and~\eqref{eq:h term}, for the second term on the right hand side, we obtain
\[
\sup_{t\in[\ell,M]}
\left|
\int_{-1}^1 k(y)
\left\{
\Phi(t-by;\beta_0)-\Phi(t;\beta_0)
\right\}
\,\mathrm{d}y
\right|
=
O(b^m).
\]
Hence, by means of the triangular inequality,
\[
\sup_{t\in[\ell,M]}
\left|w_n(t;\hat{\beta}_n)-\Phi(t;\beta_0)\right|
\leq
\sup_{x\in{\mathbb{R}}}
\left|\Phi_n(t;\hat{\beta}_n)-\Phi(t;\beta_0)\right|
+
O(b^m)
=
O_p(n^{-1/2})+O(b^m),
\]
according to Lemma~4 in~\cite{LopuhaaNane2013}.
\end{proof}
A direct consequence of Lemma~\ref{le:1} is the fact that the naive estimator converges to~$\lambda_0$ uniformly on
compact intervals within the support, as long as $b\to0$ and $1/b=o(n^{1/2})$.
\begin{lemma}
\label{lem:cons-naive}
Let $\hat{\lambda}_n^{\mathrm{naive}}$ be defined in~\eqref{def:naive est MSLE}.
Then, under the assumptions of Lemma~\ref{le:1}, for each $0<\ell<M<\tau_H$,
\[
\sup_{x\in[\ell,M]}|\hat{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)|
=
O(b^{m})+O_p(b^{-1}n^{-1/2}).
\]
\end{lemma}
\begin{proof}
By~\eqref{eqn:lambda0} and the definition of $\hat{\lambda}_n^{\mathrm{naive}}$, we have
\[
\begin{split}
\sup_{x\in[\ell,M]}|\hat{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)|
&=
\sup_{x\in[\ell,M]}\left|\frac{v_n(x)}{w_n(x;\hat{\beta}_n)}-\frac{h(x)}{\Phi(x;\beta_0)}\right|\\
&\leq
\frac{\sup_{x\in[\ell,M]}\left|v_n(x)\Phi(x;\beta_0)-h(x)w_n(x;\hat{\beta}_n)\right|}{|w_n(M;\hat{\beta}_n)|\Phi(M;\beta_0)}.
\end{split}
\]
The triangular inequality and Lemma~\ref{le:1} yield
\[
\sup_{x\in[\ell,M]}\left|v_n(x)\Phi(x;\beta_0)-h(x)w_n(x;\hat{\beta}_n)\right|=O(b^{m})+O_p(b^{-1}n^{-1/2}).
\]
and $w_n(M;\hat{\beta}_n)^{-1}=O_p(1).$
The statement follows immediately.
\end{proof}
A second key result is the following lemma, which will ensure that the naive estimator is increasing on large intervals
with probability tending to one.
\begin{lemma}
\label{le:1a}
Let $H^{uc}(t)$ be defined in~\eqref{eq:def Huc} and let $h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$.
Suppose that $h$ and $t\mapsto\Phi(t;\beta_0)$ are continuously differentiable.
Let $k$ satisfy~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
If $b\to0$ and $1/b=O(n^{\alpha})$, for some $\alpha\in(0,1/4)$, then
for each $0<\ell<M<\tau_H$, it holds
\begin{equation}
\label{eqn:lemma1-2}
\sup_{t\in[\ell,M]}|v'_n(t)-h'(t)|\xrightarrow{\mathbb{P}}0,
\qquad
\sup_{t\in[\ell,M]}|w'_n(t;\hat{\beta}_n)-\Phi'(t;\beta_0)|\xrightarrow{\mathbb{P}}0,
\end{equation}
where $v_n$, $w_n$ and $\Phi$ are defined in~\eqref{eqn:v_n w_n} and~\eqref{eq:def Phi}.
\end{lemma}
\begin{proof}
Let us consider the first statement  of~\eqref{eqn:lemma1-2}.
We write
\[
v'_n(t)-h'(t)
=
v'_n(t)-h'_s(t)+h'_s(t)-h'(t),
\]
where $h_s$ is defined in~\eqref{def:hs}.
For the second term we have
\[
\sup_{t\in[\ell,M]}
\left|h'_s(t)-h'(t)\right|
\leq
\sup_{t\in[\ell,M]}
\int_{-1}^1
|k(y)|\left|h'(t-by)-h'(t)\right|\,\mathrm{d}y\to0,
\]
by the uniform continuity of $h'$.
Moreover, similar to~\eqref{eq:v minus h} and~\eqref{eq:v minus h limit},
\[
\sup_{t\in[\ell,M]}
\left|v'_n(t)-h'_s(t)\right|
\leq
\frac{1}{b^2}
\sup_{x\in[\ell,M]}|H^{uc}_n(x)-H^{uc}(x)|
\int_{-1}^1 |k''(y)|\,\mathrm{d}y
=
O_p(n^{2\alpha-1/2}),
\]
which tends to zero in probability, as $\alpha<1/4$.
To obtain the second statement of~\eqref{eqn:lemma1-2}, first note that from~\eqref{eqn:v_n w_n},
\begin{equation}
\label{eqn:w'_n}
w'_n(t;\hat{\beta}_n)
=
\int k'_b(t-u)\Phi_n(u;\hat{\beta}_n)\,\mathrm{d}u,
\end{equation}
and write
\begin{equation}
\label{eq:decomp wn'}
w'_n(t;\hat{\beta}_n)-\Phi'(t;\beta_0)
=
w'_n(t;\hat{\beta}_n)-w'_s(t;\beta_0)
+
w'_s(t;\beta_0)-\Phi'(t;\beta_0),
\end{equation}
where
\[
w_s(t;\beta_0)=\int k_b(t-u)\Phi(u;\beta_0)\,\mathrm{d}u.
\]
For the second difference on the right hand side of~\eqref{eq:decomp wn'} we have
\begin{equation}
\label{eqn:w'}
\begin{split}
\sup_{t\in[\ell,M]}
\left|w'_s(t;\beta_0)-\Phi'(t;\beta_0)\right|
&=
\sup_{t\in[\ell,M]}
\left|\int_{-1}^1 k(y)\Phi'(t-by;\beta_0)\,\mathrm{d}y-\Phi'(t;\beta_0)\right|\\
&\leq
\sup_{t\in[\ell,M]}
\int_{-1}^1 |k(y)|
\left|
\Phi'(t-by;\beta_0)-\Phi'(t;\beta_0)
\right| \,\mathrm{d}y\to 0,
\end{split}
\end{equation}
by uniform continuity of $\Phi'$.
Furthermore, with~\eqref{eqn:w'_n}, we obtain
\[
\sup_{t\in[\ell,M]}
\left|w'_n(t;\hat{\beta}_n)-w'_s(t;\beta_0)\right|
\leq
\frac{1}{b}\sup_{x\in{\mathbb{R}}}|\Phi_n(x;\hat{\beta}_n)-\Phi(x;\beta_0)|
\int_{-1}^1|k'(y)|\,\mathrm{d}y\to 0,
\]
because
\begin{equation}
\label{eq:Phihat-Phi0}
\begin{split}
\sup_{x\in{\mathbb{R}}}|\Phi_n(x;\hat{\beta}_n)-\Phi(x;\beta_0)|
&\leq
\sup_{x\in{\mathbb{R}}}|\Phi_n(x;\hat{\beta}_n)-\Phi_n(x;\beta_0)|
+
\sup_{x\in{\mathbb{R}}}|\Phi_n(x;\beta_0)-\Phi(x;\beta_0)|\\
&\leq
\sup_{x\in{\mathbb{R}}}
\left|
\frac{\partial\Phi_n(x;\beta_n^*)}{\partial\beta}
\right|(\hat\beta_n-\beta_0)
+
O_p(n^{-1/2})
=
O_p(n^{-1/2}),
\end{split}
\end{equation}
due to Lemmas 3 and 4 in~\cite{LopuhaaNane2013}.
Together with~\eqref{eqn:w'} this proves the last result.
\end{proof}

Under similar smoothness conditions as in Lemma~\ref{le:1} one can obtain
\begin{equation}
\label{eqn:lemma1-2a}
\begin{split}
\sup_{t\in[\ell,M]}|v'_n(t)-h'(t)|
&=
O(b^{m-1})+O_p(b^{-2}n^{-1/2}),\\
\sup_{t\in[\ell,M]}|w'_n(t;\hat{\beta}_n)-\Phi'(t;\beta_0)|
&=
O(b^{m-1})+O_p(b^{-1}n^{-1/2}).
\end{split}
\end{equation}
In that case, using~\eqref{eqn:der.naive} and~\eqref{eqn:lambda0},
it can also be proved that
\[
\sup_{x\in[\ell,M]}\left|\frac{\mathrm{d}}{\mathrm{d}x}\hat{\lambda}_n^{\mathrm{naive}}(x)-\lambda'_0(x)\right|
=
O(b^{m-1})+O_p(b^{-2}n^{-1/2})=o_P(1),
\]
as long as $b\to0$ and $1/b^2=o(n^{1/2})$.
One would expect that if instead of a standard kernel we use a boundary corrected version,
then the results in Lemma~\ref{le:1a} would hold on the whole support~$[0,\tau_H]$
and consequently we would obtain that the naive estimator is monotone on $[0,\tau_H]$ with probability tending to one.
However, the use of boundary kernels makes the computations much more complicated.
Nevertheless, monotonicity on intervals $[\ell,M]$ is enough for our purposes,
because we aim at finding the pointwise asymptotic distribution at the interior of the support.

In order to show that the naive estimator is increasing on large intervals with probability tending to one,
we first establish the following intermediate result.
\begin{lemma}
\label{le:2}
Define
\begin{equation}
\label{eqn:W_n}
\tilde W_n(t)=\int_0^t w_n(x;\hat{\beta}_n)\,\mathrm{d}x,
\qquad
\tilde V_n(t)=\int_0^t v_n(x)\,\mathrm{d}x,
\qquad
W_0(t)=\int_0^t \Phi(x;\beta_0)\,\mathrm{d}x
\end{equation}
and let $H^{uc}$ be defined in~\eqref{eq:def Huc}.
If $b\to0$ and $1/b=O(n^{-1/2})$, then, under the assumptions of Lemma~\ref{le:1} with $m=1$, it holds
\begin{equation}
\label{eqn:lemma2}
\sup_{t\in[0,\tau_H]}
\left|H^{uc}(t)-\tilde V_n(t)\right|\xrightarrow{\mathbb{P}}0,
\qquad
\sup_{t\in[0,\tau_H]}
\left|\tilde W_n(t)-W_0(t)
\right|\xrightarrow{\mathbb{P}}0.
\end{equation}
\end{lemma}
\begin{proof}
To prove the first result in~\eqref{eqn:lemma2}, we take $0<\epsilon<\tau_H$ arbitrarily and write
\begin{equation}
\label{eq:bound Huc minus Vn}
\begin{split}
&\sup_{t\in[0,\tau_H]}|H^{uc}(t)-\tilde{V}_n(t)|
\leq
\int_0^{\tau_H} |v_n(u)-h(u)|\,\mathrm{d}u\\
&=\int_0^\epsilon |v_n(u)-h(u)|\,\mathrm{d}u+\int_\epsilon^{\tau_H-\epsilon} |v_n(u)-h(u)|\,\mathrm{d}u+\int_{\tau_H-\epsilon}^{\tau_H} |v_n(u)-h(u)|\,\mathrm{d}u\\
&\leq
2\epsilon\sup_{u\in[0,\tau_H]}|v_n(u)|
+
2\epsilon\sup_{u\in[0,\tau_H]}|h(u)|
+
(\tau_H-2\epsilon)\sup_{u\in[\epsilon,\tau_H-\epsilon]}|v_n(u)-h(u)|.
\end{split}
\end{equation}
Since $h$ is bounded and the last term tends to zero in probability, according to Lemma~\ref{le:1} with $m=1$,
it suffices to prove that $\sup_{u\in[0,\tau_H]}|v_n(u)|=O_p(1)$.
By definition and the triangular inequality we have
\[
\begin{split}
|v_n(t)|
&=
\left|
\int
k_b\left(t-u\right)\,\mathrm{d}H^{uc}_n(u)\right|\\
&\leq
b^{-1}\left|
H^{uc}_n((t+b)\wedge \tau_H)-H^{uc}_n((t-b)\vee 0)
\right|
\sup_{y\in[-1,1]}|k(y)|\\
&\leq
b^{-1}
\Bigg\{
\left|
H^{uc}_n((t+b)\wedge \tau_H)-H^{uc}((t+b)\wedge \tau_H)
-
H^{uc}_n((t-b)\vee 0)+H^{uc}((t-b)\vee 0)\right|\\
&\qquad\qquad\qquad\qquad+
H^{uc}((t+b)\wedge \tau_H)-H^{uc}((t-b)\vee 0)
\Bigg\}
\sup_{y\in[-1,1]}|k(y)|\\
&\leq
2
\left\{
b^{-1}
\sup_{y\in[0,\tau_H]}|H^{uc}_n(y)-H^{uc}(y)|
+
2
\sup_{u\in[0,\tau_H]}|h(u)|
\right\}
\sup_{y\in[-1,1]}|k(y)|.
\end{split}
\]
Using~\eqref{eqn:H_uc}, it follows that the right hand side of the previous inequality is bounded in probability.
For the second result in~\eqref{eqn:lemma2}, similar to~\eqref{eq:bound Huc minus Vn} we have
\[
\begin{split}
\sup_{t\in[0,\tau_H]}
\left|\tilde W_n(t)-W_0(t)
\right|
&\leq
\int_0^{\tau_H}
\left|w_n(u;\hat\beta_n)-\Phi(u;\beta_0)\right|
\,\mathrm{d}u\\
&\leq
2\epsilon\sup_{u\in[0,\tau_H]}|w_n(u;\hat\beta_n)|
+
2\epsilon\sup_{u\in[0,\tau_H]}|\Phi(u;\beta_0)|\\
&\qquad+
(\tau_H-2\epsilon)\sup_{u\in[\epsilon,\tau_H-\epsilon]}\left|w_n(u;\hat\beta_n)-\Phi(u;\beta_0)\right|.
\end{split}
\]
By using Lemma~\ref{le:1} with $m=1$ and the fact that $\Phi(u;\beta_0)$ is bounded, it remains to
handle the first term on right hand side.
Since
\begin{equation}
\label{eq:bound int kb}
\left|
\int_{t}^{\infty}k_b(s-u)\,\mathrm{d}s
\right|
=
\left|
\int_{(t-u)/b}^{\infty}k(y)\,\mathrm{d}y
\right|
\leq
2\sup_{y\in[-1,1]}|k(y)|,
\end{equation}
and $k_b(t-u)=0$, for $u<t-b$,
we have
\[
\begin{split}
|w_n(t;\hat\beta_n)|
&=
\left|
\int
\mathrm{e}^{\hat{\beta}'_nz}
\int_{t}^{\infty}k_b(s-u)\,\mathrm{d}s
\,\mathrm{d}{\mathbb{P}}_n(u,\delta,z)
\right|
\\
&\leq
2\sup_{y\in[-1,1]}|k(y)|
\int_{t-b}^{\infty}\mathrm{e}^{\hat{\beta}'_nz}\,\mathrm{d}{\mathbb{P}}_n(u,\delta,z)
=
2\Phi_n(t-b;\hat{\beta}_n)
\sup_{y\in[-1,1]}|k(y)|,
\end{split}
\]
whereas Lemma~3 in~\cite{LopuhaaNane2013} gives that $\sup_{x\in{\mathbb{R}}}\Phi_n(x;\hat{\beta}_n)=O_p(1)$.
This establishes the second result in~\eqref{eqn:lemma2}.
\end{proof}

\begin{lemma}
\label{lem:monotone}
Let $\lambda_0$ be differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Let $H^{uc}(t)$ be defined in~\eqref{eq:def Huc} and let $h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$.
Suppose that $h$ and $t\mapsto\Phi(t;\beta_0)$ are continuously differentiable.
Let $k$ satisfy~\eqref{def:kernel} and assume that it is twice differentiable with bounded second derivative.
Let $\hat{\lambda}_n^{\mathrm{naive}}$ be defined in~\eqref{def:naive est MSLE}.
If $b\to0$ and $1/b=O(n^{\alpha})$, for some $\alpha\in(0,1/4)$,
then for each $0<\ell<M<\tau_H$  it holds
\[
{\mathbb{P}}\big(\hat{\lambda}_n^{\mathrm{naive}}\text{ is increasing on } [\ell,M]\big)\to 1.
\]
\end{lemma}
\begin{proof}
Note that $w_n(x,\hat{\beta}_n)=0$ if and only if $T_i\leq x-b$,
for all $i=1,\ldots,n$, which happens with probability $H(x-b)^n\leq H(M)^n\to 0$.
This means that with probability tending to one, $w_n(x,\hat{\beta}_n)>0$ for all $x\in[\ell,M]$.
Thus with probability tending to one,
$\hat{\lambda}_n^{\mathrm{naive}}$ is well defined on $[\ell,M]$ and
\begin{equation}
\label{eqn:der.naive}
\frac{\mathrm{d}}{\mathrm{d}x}\hat{\lambda}_n^{\mathrm{naive}}(x)=\frac{v'_n(x)\,w_n(x;\hat{\beta}_n)-v_n(x)\,w'_n(x;\hat{\beta}_n)}{w_n(x;\hat{\beta}_n)^2}.
\end{equation}
In order to prove that $\hat{\lambda}_n^{\mathrm{naive}} $ is increasing on $[\ell,M]$ with probability tending to one, it suffices to show that
\begin{equation}
\label{eq:prob inf}
{\mathbb{P}}\left(
\inf_{x\in[\ell,M]}
\left\{
v'_n(x)w_n(x;\hat{\beta}_n)-v_n(x)w'_n(x;\hat{\beta}_n)
\right\}\leq 0
\right)\to 0.
\end{equation}
We can write
\[
\begin{split}
&
v'_n(x)w_n(x;\hat{\beta}_n)-v_n(x)w'_n(x;\hat{\beta}_n)\\
&=
w_n(x;\hat{\beta}_n)
\left(v'_n(x)-h'(x)\right)+v_n(x)\left(\Phi'(x;\beta_0)-w'_n(x;\hat{\beta}_n)\right)\\
&\qquad+
h'(x)\left(w_n(x;\hat{\beta}_n)-\Phi(x;\beta_0)\right)+\Phi'(x;\beta_0)\left(h(x)-v_n(x)\right)\\
&\qquad\qquad+
h'(x)\Phi(x;\beta_0)-\Phi'(x;\beta_0)h(x),
\end{split}
\]
where the right hand side can be bounded from below by
\[
\begin{split}
&
-\sup_{x\in[\ell,M]}|v'_n(x)-h'(x)|\sup_{x\in[\ell,M]}|w_n(x;\hat{\beta}_n)|\\
&\qquad-
\sup_{x\in[\ell,M]}|\Phi'(x;\beta_0)-w'_n(x;\hat{\beta}_n)|\sup_{x\in[\ell,M]}|v_n(x)|\\
&\qquad-
\sup_{x\in[\ell,M]}|w_n(x;\hat{\beta}_n)-\Phi(x;\beta_0)|\sup_{x\in[\ell,M]}|h'(x)|\\
&\qquad-
\sup_{x\in[\ell,M]}|h(x)-v_n(x)|\sup_{x\in[\ell,M]}|\Phi'(x;\beta_0)|
+
h'(x)\Phi(x;\beta_0)-\Phi'(x;\beta_0)h(x).
\end{split}
\]
From the proof of Lemma~\ref{le:2} we have that $\sup_{x\in[\ell,M]}|v_n(x)|$ and $\sup_{x\in[\ell,M]}w_n(x;\hat{\beta}_n)$ are~$O_p(1)$,
so that from Lemmas~\ref{le:1a} and~\ref{le:1} (with $m=1$), it follows that the first four terms on the right hand side tend to zero in probability.
Therefore, the probability in~\eqref{eq:prob inf} is bounded by
\[
{\mathbb{P}}
\left(
X_n\geq \inf_{x\in[\ell,M]}
\left(
h'(x)\Phi(x;\beta_0)-\Phi'(x;\beta_0)h(x)
\right)
\right),
\]
where $X_n=o_p(1)$.
This probability tends to zero, because with~\eqref{eqn:lambda0}, we have
\[
\begin{split}
\inf_{x\in[\ell,M]}
\left(
h'(x)\Phi(x;\beta_0)-\Phi'(x;\beta_0)h(x)
\right)
&=
\inf_{x\in[\ell,M]}\lambda'_0(x)\Phi(x;\beta_0)^2\\
&\geq
\Phi(M;\beta_0)^2
\inf_{x\in[0,\tau_H]}\lambda'_0(x)
>0.
\end{split}
\]
\end{proof}

\begin{cor}
\label{cor:1}
Under the assumptions of Lemma~\ref{lem:monotone}, it holds that, for each $0<\ell<M<\tau_H$,
\[
{\mathbb{P}}
\left(
\hat{\lambda}_n^{\mathrm{naive}}(x)=\hat{\lambda}_n^{MS}(x),\text{ for all } x\in[\ell,M]
\right)\to 1.
\]
Consequently, for all $x\in(0,\tau_H),$ the asymptotic distributions of $\hat{\lambda}_n^{\mathrm{naive}}(x)$ and $\hat{\lambda}_n^{MS}(x)$ are the same.
\end{cor}
\begin{proof}
It is enough to prove that for an arbitrarily fixed $\epsilon>0$ and for sufficiently large $n$
\[
{\mathbb{P}}
\left(
\hat{\lambda}_n^{\mathrm{naive}}(x)=\hat{\lambda}_n^{MS}(x),\text{ for all } x\in[\ell,M]
\right)
\geq 1-\epsilon.
\]
Recall that $\hat{\lambda}^{MS}_n(x)$ is defined as the slope of the greatest convex minorant $\big\{\big(\tilde W_n(x),C_n(x)\big), x\in[0,\tau_{\hat{\beta}_n}]\big\}$ of the graph
$\big\{\big(\tilde W_n(x),\tilde V_n(x)\big),x\in[0,\tau_{\hat{\beta}_n}]\big\}$,
where $\tilde W_n$ and $\tilde V_n$ are defined in~\eqref{eqn:W_n}.
We define the linearly extended  version of $\tilde V_n$ by
\[
C^*_n(x)=
\begin{cases}
\tilde V_n(\ell)+\big(\tilde W_n(x)-\tilde W_n(\ell)\big)\hat{\lambda}_n^{\mathrm{naive}}(\ell), &\text{ for } x\in[0,\ell),\\
\tilde V_n(x), &\text{ for } x\in[\ell,M],\\
\tilde V_n(M)+\big(\tilde W_n(x)-\tilde W_n(M)\big)\hat{\lambda}_n^{\mathrm{naive}}(M), &\text{ for } x\in(M,\tau_H].
\end{cases}
\]
It suffices to prove that, for sufficiently large $n$,
\begin{equation}
\label{eq:cor prop1}
{\mathbb{P}}
\left(
\left\{
\big(\tilde W_n(x),C^*_n(x)\big):x\in[0,\tau_H]
\right\} \text{ is convex }
\right)\geq 1-\epsilon/2,
\end{equation}
and
\begin{equation}
\label{eq:cor prop2}
{\mathbb{P}}
\left(
C^*_n(x)\leq \tilde V_n(x),
\text{ for all }x\in [0,\tau_H]
\right)\geq 1-\epsilon/2.
\end{equation}
Indeed, if~\eqref{eq:cor prop1} and~\eqref{eq:cor prop2} hold,
then with probability greater than or equal to $1-\epsilon$,
the curve $\big\{\big(\tilde W_n(x),C^*_n(x)\big):x\in[0,\tau_H]\big\}$ is a convex curve lying below the graph
$\big\{\big(\tilde W_n(x),\tilde V_n(x)\big):x\in[0,\tau_H]\big\}$, with $C^*_n(x)=\tilde V_n(x)$ for all $x\in[\ell,M]$.
Hence, $\tilde V_n(x)=C^*_n(x)\leq C_n(x)\leq \tilde V_n(x)$,
for all $x\in[\ell,M]$.
It follows that, for sufficiently large $n$,
\[
{\mathbb{P}}\left(
\hat{\lambda}_n^{\mathrm{naive}}(x)=\frac{\mathrm{d}\tilde V_n(x)}{\mathrm{d}\tilde W_n(x)}=\frac{\mathrm{d}C_n(x)}{\mathrm{d}\tilde W_n(x)}=\hat{\lambda}_n^{MS}(x),
\text{ for all }x\in [\ell,M]
\right)\geq 1-\epsilon.
\]

To prove~\eqref{eq:cor prop1}, define the event
\[
A_n=\left\{\hat{\lambda}_n^{\mathrm{naive}} \text{ is increasing on } [\ell-\eta_1,M+\eta_2]\right\},
\]
for $\eta_1\in(0,\ell)$ and $\eta_2\in(0,\tau_H-M)$.
Note that for the intervals $[0,\ell)$ and $(M,\tau_H]$,
the curve $\big\{\big(\tilde W_n(x),C^*_n(x)\big):x\in[0,\tau_H]\big\}$ is the tangent line of the graph $\big\{\big(\tilde W_n(x),\tilde V_n(x)\big):x\in[0,\tau_H]\big\}$
at the points $\big(\tilde W_n(\ell),\tilde V_n(\ell)\big)$ and $\big(\tilde W_n(M),\tilde V_n(M)\big)$.
As a result, on the event $A_n$ the curve is convex, so that together with Lemma~\ref{lem:monotone},
for sufficiently large $n$
\[
{\mathbb{P}}
\left(
\left\{
\big(\tilde W_n(x),C^*_n(x)\big):x\in[0,\tau_H]
\right\} \text{ is convex }
\right)
\geq
{\mathbb{P}}(A_n)
\geq
1-\epsilon/2.
\]
To prove~\eqref{eq:cor prop2}, we split the interval $[0,\tau_H]$ in five different intervals
$I_1=[0,\ell-\eta_1),$ $I_2=[\ell-\eta_1,\ell),$ $I_3=[\ell,M],$ $I_4=(M,M+\eta_2]$, and $I_5=(M+\eta_2,\tau_H]$,
and show that
\begin{equation}
\label{eqn:C}
{\mathbb{P}}
\left(
C^*_n(x)\leq \tilde V_n(x),\text{ for all }x\in I_i
\right)\geq 1-\epsilon/10,
\quad
i=1,\ldots,5.
\end{equation}
For $x\in I_3,$ $C^*_n(x)=\tilde V_n(x)$ and thus~\eqref{eqn:C} is trivial.
For $x\in I_2$, by the mean value theorem,
\[
\tilde V_n(x)-\tilde V_n(\ell)=\big(\tilde W_n(x)-\tilde W_n(\ell)\big)\,\hat{\lambda}_n^{\mathrm{naive}}(\xi_x)
\]
for some $\xi_x\in[x,\ell]$.
Thus, since $\tilde W_n(x)\leq\tilde W_n(\ell)$,
\begin{equation}
\label{eq:argument I2}
\begin{split}
&
{\mathbb{P}}
\left(
C^*_n(x)\leq \tilde V_n(x),\text{ for all }x\in I_2
\right)
\\
&=
{\mathbb{P}}
\left(
\big(\tilde W_n(x)-\tilde W_n(\ell)\big)
\big(\hat{\lambda}_n^{\mathrm{naive}}(\xi_x)-\hat{\lambda}_n^{\mathrm{naive}}(\ell)\big)\geq 0,
\text{ for all }x\in I_2
\right)\\
&=
{\mathbb{P}}
\left(
\big(\hat{\lambda}_n^{\mathrm{naive}}(\xi_x)-\hat{\lambda}_n^{\mathrm{naive}}(\ell)\big)\leq 0,
\text{ for all }x\in I_2\right)\\
&\geq
{\mathbb{P}}(A_n)\geq 1-\epsilon/10,
\end{split}
\end{equation}
for $n$ sufficiently large, according to Lemma~\ref{lem:monotone}.
The argument for $x\in I_4$ is exactly the same.
Furthermore, making use of the relation~\eqref{eqn:lambda0}, for each $x\in I_1,$ we obtain
\[
\begin{split}
H^{uc}(x)-H^{uc}(\ell)-\lambda_0(\ell)\big(W_0(x)-W_0(\ell)\big)
&=
\int_x^{\ell} \big(\lambda_0(\ell)-\lambda_0(u)\big)\,\mathrm{d}W_0(u)\\
&\geq
\int_{\ell-\eta_1}^{\ell}
\big(\lambda_0(\ell)-\lambda_0(u)\big)\,\mathrm{d}W_0(u).
\end{split}
\]
This implies that
\[
\begin{split}
\tilde V_n(x)-C^*_n(x)
&=
\tilde V_n(x)-
\tilde V_n(\ell)-\big(\tilde W_n(x)-\tilde W_n(\ell)\big)\hat{\lambda}_n^{\mathrm{naive}}(\ell)\\
&\geq
\tilde V_n(x)-H^{uc}(x)+H^{uc}(\ell)-\tilde V_n(\ell)+\lambda_0(\ell)\big(W_0(x)-\tilde W_n(x)\big)\\
&\quad+
\lambda_0(\ell)\big(\tilde W_n(\ell)-W_0(\ell)\big)
+
\big(\hat{\lambda}_n^{\mathrm{naive}}(\ell)-\lambda_0(\ell)\big)\big(\tilde W_n(\ell)-\tilde W_n(x)\big)\\
&\qquad+
\int_{\ell-\eta_1}^{\ell}\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}W_0(u)\\
&\geq
-2\sup_{x\in[0,\ell]}|\tilde V_n(x)-H^{uc}(x)|
-2\lambda_0(\ell)\sup_{x\in[0,\ell]}|\tilde W_n(x)-W_0(x)|\\
&\qquad-
2|\hat{\lambda}_n^{\mathrm{naive}}(\ell)-\lambda_0(\ell)|\sup_{x\in[0,\ell]}|\tilde W_n(x)|
+
\int_{\ell-\eta_1}^{\ell}\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}W_0(u).
\end{split}
\]
According to Lemmas~\ref{le:2} and~\ref{lem:cons-naive}, with $m=1$, the first three terms on the right hand side tend to zero in probability.
This means that the probability on the left hand side of~\eqref{eqn:C} for $i=1$, is bounded
from below by
\[
{\mathbb{P}}
\left(
X_n\leq \int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}W_0(u)
\right),
\]
where $X_n=o_p(1)$.
This probability is greater than $1-\epsilon/10$ for $n$ sufficiently large, since
\[
\int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\,\mathrm{d}W_0(u)
\geq
\Phi_0(\ell;\beta_0)
\int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\,\mathrm{d}u
>0,
\]
using that $\lambda_0$ is strictly increasing.
For $I_5$ we can argue exactly in the same way.
\end{proof}

\begin{cor}
\label{cor:cons-MS}
Under the assumptions of Lemmas~\ref{le:1} and~\ref{lem:monotone},
the maximum smooth likelihood estimator is uniformly consistent on compact intervals $[\ell,M]\subset [0,\tau_H]$:
\[
\sup_{x\in[\ell,M]}\left|\hat{\lambda}^{MS}_n(x)-\lambda_0(x)\right|
=
O(b^{m})+O_p(b^{-1}n^{-1/2}).
\]
\end{cor}
\begin{proof}
The result follows immediately from Corollary~\ref{cor:1} and Lemma~\ref{lem:cons-naive}.
\end{proof}
To obtain the asymptotic distribution of $\hat{\lambda}^{MS}_n(x)$, we first obtain the asymptotic distribution of $\hat{\lambda}_n^{\mathrm{naive}}(x)$.
To this end we establish the joined asymptotic distribution of the vector~$(w_n(x;\hat\beta_n),v_n(x))$.
\begin{lemma}
\label{lemma:distr}
Fix $x\in(0,\tau_H)$.
Let $\lambda_0$ be differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Let $H^{uc}(t)$ be defined in~\eqref{eq:def Huc} and let $h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$.
Suppose that $h$ and $t\mapsto\Phi(t;\beta_0)$ are $m\geq 2$ times continuously differentiable.
Let $k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is twice differentiable with bounded second derivative.
Let~$v_n$ and~$w_n$ be defined in~\eqref{eqn:v_n w_n} and suppose that $n^{1/(2m+1)}b\to c>0$.
Then
\[
n^{m/(2m+1)}
\left(
\begin{bmatrix}
w_n(x;\hat\beta_n)\\
v_n(x)
\end{bmatrix}
-
\begin{bmatrix}
\Phi(x;\beta_0)\\
h(x)
\end{bmatrix}
\right)
\to
N\left(\begin{bmatrix}
             \mu_1 \\
             \mu_2 \\
           \end{bmatrix},\begin{bmatrix}
0 & 0\\
0 & \sigma^2
\end{bmatrix}\right)
\]
where
\[
\begin{bmatrix}
             \mu_1 \\
             \mu_2 \\
           \end{bmatrix}
=
\frac{(-c)^m}{m!}\int_{-1}^1 k(y)y^m\,\mathrm{d}y
\begin{bmatrix}
\Phi^{(m)}(x;\beta_0)\\
h^{(m)}(x)
\end{bmatrix},
\qquad
\sigma^2=\frac{h(x)}c
\int_{-1}^1 k^2(y)\,\mathrm{d}y.
\]
\end{lemma}
\begin{proof}
First we show that $n^{m/(2m+1)}(w_n(x;\hat{\beta}_n)-w_n(x;\beta_0))\to 0$ in probability,
which enables us to replace $w_n(x;\hat{\beta}_n)$ with $w_n(x;\beta_0)$ in the statement.
From~\eqref{eqn:v_n w_n}, together with~\eqref{eq:bound int kb}, we find
\[
\begin{split}
\left|w_n(x;\hat{\beta}_n)-w_n(x;\beta_0)\right|
&\leq
\frac1n
\sum_{i=1}^n \left|\mathrm{e}^{\hat{\beta}'_nZ_i}-\mathrm{e}^{\beta'_0Z_i}\right|
\left|
\int_x^{\infty}k_b(u-T_i)\,\mathrm{d}u
\right|\\
&\leq
2\sup_{y\in[-1,1]}|k(y)|
\frac1n
\sum_{i=1}^n
|Z_i|\mathrm{e}^{\tilde{\beta}'_{n,i}Z_i}\left|\hat{\beta}_n-\beta_0\right|,
\end{split}
\]
for some $|\tilde{\beta}_{n,i}-\beta_0|\leq|\hat{\beta}_n-\beta_0|=O_p(n^{-1/2})$.
Furthermore, for all $M>0$,
\[
{\mathbb{P}}\left(
\frac{1}{n}\sum_{i=1}^n
|Z_i|\mathrm{e}^{\tilde{\beta}'_{n,i}Z_i}\geq M
\right)
\leq
\frac{1}{nM}\sum_{i=1}^n
{\mathbb{E}}
\left[|Z_i|\mathrm{e}^{\tilde{\beta}'_{n,i}Z_i} \right]
\leq
\frac{1}{M}\sup_{|\beta-\beta_0|\leq \epsilon}{\mathbb{E}}\left[|Z|\mathrm{e}^{\beta'Z} \right],
\]
where $\sup_{|\beta-\beta_0|\leq \epsilon}{\mathbb{E}}[|Z|\mathrm{e}^{\beta'Z}]<\infty$ according to assumption~(A2).
It follows that
\[
n^{m/(2m+1)}
\left(
w_n(x;\hat{\beta}_n)-w_n(x;\beta_0)
\right)
=
O_p(n^{-1/(4m+2)}).
\]

Now, define
\[
Y_{ni}=
\begin{bmatrix}
Y_{ni,1}\\
Y_{ni,2}
\end{bmatrix}
=
n^{-(m+1)/(2m+1)}
\begin{bmatrix}
\mathrm{e}^{\beta'_0Z_i}\int_x^{\infty}k_b(s-T_i)\,\mathrm{d}s
\\
k_b(x-T_i)\Delta_i
\end{bmatrix}.
\]
By a Taylor expansion, using that
$h$ is $m$ times continuously differentiable and that $k$ is $m$-orthogonal, as in~\eqref{eq:taylor h} we obtain
\begin{equation}
\label{eq:expec Yni2}
\begin{split}
{\mathbb{E}}\left[Y_{ni,2}\right]
&=
n^{-(m+1)/(2m+1)}
\int_{-1}^1 k(y)h(x-by)\,\mathrm{d}y\\
&=
n^{-(m+1)/(2m+1)}
\left(h(x)+\frac{(-b)^m}{m!}h^{(m)}(x)\int_{-1}^1k(y)y^m\,\mathrm{d}y + o(b^m)\right).
\end{split}
\end{equation}
Similarly, with Fubini we get
\begin{equation}
\label{eq:expec Yni1}
\begin{split}
{\mathbb{E}}\left[Y_{ni,1}\right]
&=
n^{-(m+1)/(2m+1)}
\int \mathrm{e}^{\beta'_0z}
\int_x^{\infty}k_b(s-u)\,\mathrm{d}s\,\mathrm{d}{\mathbb{P}}(u,\delta,z)\\
&=
n^{-(m+1)/(2m+1)}
\int_{-1}^1
\left(\int
\mathrm{e}^{\beta'_0z}{\mathds{1}}_{\{u\geq x-by\}}\,\mathrm{d}{\mathbb{P}}(u,\delta,z)
\right)
k(y)\,\mathrm{d}y\\
&=
n^{-(m+1)/(2m+1)}
\int_{-1}^1 k(y)\Phi(x-by;\beta_0)\,\mathrm{d}y\\
&=
n^{-(m+1)/(2m+1)}
\left(
\Phi(x;\beta_0)+\frac{(-b)^m}{m!}\Phi^{(m)}(x;\beta_0)\int_{-1}^1 k(y)y^m\,\mathrm{d}y + o(b^m)
\right).
\end{split}
\end{equation}
Hence, we have
\[
{\mathbb{E}}\left[Y_{ni}\right]
=
n^{-(m+1)/(2m+1)}
\begin{bmatrix}
\Phi(x;\beta_0)\\
h(x)
\end{bmatrix}
+
n^{-1}
\begin{bmatrix}
\mu_1 \\
\mu_2 \\
\end{bmatrix}
+
o(n^{-1}),
\]
and we can write
\[
n^{-(m+1)/(2m+1)}
\left(
\begin{bmatrix}
w_n(x;\hat\beta_n)\\
v_n(x)
\end{bmatrix}
-
\begin{bmatrix}
\Phi(x;\beta_0)\\
h(x)
\end{bmatrix}
\right)
=
\begin{bmatrix}
\mu_1 \\
\mu_2 \\
\end{bmatrix}
+
\sum_{i=1}^n
\big(
Y_{ni}-{\mathbb{E}}\left[Y_{ni}\right]
\big)
+
o(1).
\]
It remains to show that $\sum_{i=1}^n \big(Y_{ni}-{\mathbb{E}}\left[Y_{ni}\right]\big)$ converges in distribution to a bivariate normal distribution with mean zero.
From~\eqref{eq:expec Yni1} we have,
\begin{equation}
\label{eq:EYn1^2}
\begin{split}
&
\mathrm{Var}(Y_{ni,1})
=
{\mathbb{E}}\left[Y_{ni,1}^2\right]+O(n^{-2(m+1)/(2m+1)})\\
&=
n^{-2(m+1)/(2m+1)}
\int \mathrm{e}^{2\beta'_0z}
\left(\int_x^{\infty}k_b(s-u)\,\mathrm{d}s\right)^2\,\mathrm{d}{\mathbb{P}}(u,\delta,z)+O(n^{-2(m+1)/(2m+1)})\\
&=
O(n^{-2(m+1)/(2m+1)}),
\end{split}
\end{equation}
using that, with~\eqref{eq:bound int kb},
\[
\begin{split}
\int
\mathrm{e}^{2\beta'_0z}\left(\int_x^{\infty}k_b(s-u)\,\mathrm{d}s\right)^2\,\mathrm{d}{\mathbb{P}}(u,\delta,z)
&\leq
\left(
2\sup_{y\in[-1,1]}|k(y)|
\right)^2
\int \mathrm{e}^{2\beta'_0z}\,\mathrm{d}{\mathbb{P}}(u,\delta,z)\\
&=
\left(
2\sup_{y\in[-1,1]}|k(y)|
\right)^2
\Phi(0;2\beta_0)<\infty.
\end{split}
\]
Moreover,
\[
\begin{split}
&
\mathrm{Cov}(Y_{ni,1},Y_{ni,2})
=
{\mathbb{E}}\left[Y_{ni,1}Y_{ni,2}\right]+O(n^{-2(m+1)/(2m+1)})\\
&=
n^{-2(m+1)/(2m+1)}
\int \delta\mathrm{e}^{\beta'_0z}
\left(\int_x^{\infty}k_b(s-u)\,\mathrm{d}s\right)
k_b(x-u)\,\mathrm{d}{\mathbb{P}}(u,\delta,z)+O_p(n^{-2(m+1)/(2m+1)})\\
&=
o(n^{-1})+O(n^{-2(m+1)/(2m+1)}),
\end{split}
\]
because, with~\eqref{eq:bound int kb},
\[
\begin{split}
&
\left|
b\int \delta\mathrm{e}^{\beta'_0z}
\left(\int_x^{\infty}k_b(s-u)\,\mathrm{d}s\right)
k_b(x-u)\,\mathrm{d}{\mathbb{P}}(u,\delta,z)
\right|
\\
&\leq
2\sup_{y\in[-1,1]}|k(y)|
\int {\mathds{1}}_{\{x-b\leq u\leq x+b\}}
\mathrm{e}^{\beta'_0z}
\left|
k\left(\frac{x-u}{b}\right)
\right|\,\mathrm{d}{\mathbb{P}}(u,\delta,z)\\
&\leq
2\left(\sup_{y\in[-1,1]}|k(y)|\right)^2
\big( \Phi(x-b;\beta_0)-\Phi(x+b;\beta_0)\big)
\to 0.
\end{split}
\]
Once again, by a Taylor expansion, from~\eqref{eq:expec Yni2}, we obtain
\begin{equation}
\label{eq:EYn2^2}
\begin{split}
\text{Var}(Y_{ni,2})
&=
{\mathbb{E}}
\left[Y_{ni,2}^2\right]+O(n^{-2(m+1)/(2m+1)})\\
&=
n^{-2(m+1)/(2m+1)}b^{-1}
\int_{-1}^1 k^2(y)h(x-by)\,\mathrm{d}y+O(n^{-2(m+1)/(2m+1)})\\
&=
n^{-1}\sigma^2+o(n^{-1}).
\end{split}
\end{equation}
It follows that
\[
\sum_{i=1}^n
\mathrm{Cov}( Y_{ni})
=
\begin{bmatrix}
 0 & 0\\
 0 & \sigma^2
\end{bmatrix}+o(1).
\]
Furthermore, since
\[
|Y_{ni}|^2
=
n^{-2(m+1)/(2m+1)}\left(
\mathrm{e}^{2\beta'_0Z_i}
\left(\int_x^{\infty}k_b(s-T_i)\,\mathrm{d}s\right)^2
+
k^2_b(x-T_i)\Delta_i
\right),
\]
with~\eqref{eq:bound int kb}, we obtain
\[
\begin{split}
\sum_{i=1}^n
{\mathbb{E}}\left[|Y_{ni}|^2{\mathds{1}}_{\{|Y_{ni}|>\epsilon \}} \right]
&\leq
\left(2\sup_{y\in[-1,1]}|k(y)|\right)^2
n^{-1/(2m+1)}{\mathbb{E}}\left[\mathrm{e}^{2\beta'_0Z}\right]\\
&\qquad+
n^{-2(m+1)/(2m+1)}b^{-2}\sup_{y\in[-1,1]}|k(y)|
\sum_{i=1}^n{\mathbb{P}}\left(|Y_{ni}|>\epsilon\right),
\end{split}
\]
where the right hand side tends to zero, because ${\mathbb{E}}[\mathrm{e}^{2\beta'_0Z}]=\Phi(0;2\beta_0)<\infty$ and,  with~\eqref{eq:EYn1^2} and~\eqref{eq:EYn2^2},
we have
\[
\sum_{i=1}^n
{\mathbb{P}}\left(|Y_{ni}|>\epsilon\right)
\leq
\epsilon^{-2}
\sum_{i=1}^n
{\mathbb{E}}|Y_{ni}|^2
=
O(1).
\]
By the multivariate Lindeberg-Feller central limit theorem, we get
\[
\sum_{i=1}^n \big(Y_{ni}-{\mathbb{E}}\left[Y_{ni}\right]\big)
\stackrel{d}{\to}
N\left(\begin{bmatrix}
             \mu_1 \\
             \mu_2 \\
           \end{bmatrix},\begin{bmatrix}
0 & 0\\
0 & \sigma^2
\end{bmatrix}\right),
\]
which finishes the proof.
\end{proof}

\begin{theo}
\label{theo:distrMS}
Fix $x\in(0,\tau_H)$.
Let $\lambda_0$ be differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Let $H^{uc}(t)$ be defined in~\eqref{eq:def Huc} and let $h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$.
Suppose that $h$ and $t\mapsto\Phi(t;\beta_0)$ are $m\geq 2$ times continuously differentiable.
Let $k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
Let~$\hat{\lambda}^{MS}_n(x)$ be defined in~\eqref{def:lambdaMS} and assume that $n^{1/(2m+1)}b\to c>0$.
Then, for each $x\in(0,\tau_H)$, the following holds
\[
n^{m/(2m+1)}
\left(\hat{\lambda}^{MS}_n(x)-\lambda_0(x)\right)\xrightarrow{d} N(\mu,\sigma^2),
\]
where
\begin{equation}
\label{def:mu sigma}
\begin{split}
\mu
&=
\frac{(-c)^m}{m!}
\frac{h^{(m)}(x)-\lambda_0(x)\Phi^{(m)}(x;\beta_0)}{\Phi(x;\beta_0)}
\int_{-1}^1 k(y)y^m\,\mathrm{d}y\\
\sigma^2
&=
\frac{\lambda_0(x)}{c\Phi(x;\beta_0)}\int_{-1}^1 k^2(y)\,\mathrm{d}y.
\end{split}
\end{equation}
This also holds if we replace $\hat{\lambda}^{MS}_n(x)$ with $\hat{\lambda}_n^{\mathrm{naive}}(x)$, as defined in~\eqref{def:naive est MSLE}.
\end{theo}
\begin{proof}
By definition of $\hat{\lambda}_n^{\mathrm{naive}}(x)$ in~\eqref{def:naive est MSLE} together with~\eqref{eqn:lambda0},
we can write
\[
\hat{\lambda}_n^{naive}(x)-\lambda_0(x)
=
\phi\Big(w_n(x;\hat\beta_n),v_n(x)\Big)
-
\phi\Big(\Phi(x;\beta_0),\lambda_0(x)\Phi(x;\beta_0)\Big)
\]
with $\varphi(w,v)=v/w$.
The asymptotic distribution of $\hat{\lambda}_n^{\mathrm{naive}}(x)$ then follows from an application of the delta method to the result in Lemma~\ref{lemma:distr}.
Then, by Corollary~\ref{cor:1}, this also gives the asymptotic distribution of $\hat{\lambda}^{MS}_n(x)$.
\end{proof}

Theorem~\ref{theo:distrMS} is comparable to Theorem~3.5 in~\cite{LopuhaaMustaSI2016},
where the limiting normal distribution of the smoothed maximum likelihood estimator $\hat{\lambda}^{SM}_n(x)$
and the smoothed Grenander-type estimator $\tilde\lambda_n^{SG}(x)$ is established, i.e.,
\begin{equation}
\label{eq:asymp norm SM SG}
\begin{split}
n^{m/(2m+1)}
\left(\hat{\lambda}^{SM}_n(x)-\lambda_0(x)\right)
&\xrightarrow{d}
N(\widetilde{\mu},\sigma^2),\\
n^{m/(2m+1)}
\left(
\tilde\lambda_n^{SG}(x)-\hat{\lambda}^{SM}_n(x)
\right)
&\xrightarrow{\mathbb{P}} 0,
\end{split}
\end{equation}
where
\begin{equation}
\label{def:mutilde}
\widetilde{\mu}
=\frac{(-c)^m}{m!}
\lambda_0^{(m)}(x)
\int_{-1}^1 k(y)y^m\,\mathrm{d}y.
\end{equation}The limiting variance is the same, but the asymptotic mean is shifted.
A natural question is whether $\hat{\lambda}^{MS}_n(x)$ is asymptotically equivalent to these estimators,
if we correct for the difference in the asymptotic mean.
The next theorem shows that this is indeed the case.
\begin{theo}
\label{theo:MSLE asymptotic equivalence}
Fix $x\in(0,\tau_H)$.
Suppose that $\lambda_0$ and $t\mapsto\Phi(t;\beta_0)$ are $m\geq 2$ times continuously differentiable,
with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Let $k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
Let~$\hat{\lambda}^{MS}_n(x)$ be the maximum smoothed likelihood estimator and let
$\hat{\lambda}^{SM}(x)$ be the smoothed maximum likelihood estimator, defined in~\cite{LopuhaaMustaSI2016}.
Let $\mu$ and $\widetilde{\mu}$ be defined in~\eqref{def:mu sigma} and~\eqref{def:mutilde}, respectively.
Then, for each $x\in(0,\tau_H)$, the following holds
\[
n^{m/(2m+1)}
\left(
\hat\lambda_n^{MS}(x)
-
\hat\lambda_n^{SM}(x)
\right)
-
\left(
\mu-\widetilde{\mu}
\right)
\to
0
\]
in probability, and similarly if we replace~$\hat\lambda_n^{SM}(x)$
by the smoothed Grenander-type estimator~$\tilde\lambda_n^{SG}(x)$, defined in~\cite{LopuhaaMustaSI2016}.
\end{theo}
\begin{proof}
First note that by means of~\eqref{eqn:lambda0}, it follows from the assumptions of the theorem that~$h(t)=\mathrm{d}H^{uc}(t)/\mathrm{d}t$ is $m\geq 2$ times
continuously differentiable.
We write
\[
\begin{split}
&
n^{m/(2m+1)}
\left(
\hat{\lambda}_n^{MS}(x)-\tilde{\lambda}_n^{SM}(x)
\right)\\
&=
n^{m/(2m+1)}
\left(
\hat{\lambda}_n^{naive}(x)-\tilde{\lambda}_n^{SM}(x)
\right)
+
n^{m/(2m+1)}
\left(
\hat{\lambda}_n^{MS}(x)-\hat{\lambda}_n^{naive}(x)
\right).
\end{split}
\]
By Corollary~\ref{cor:1}, the second term on the right hand side converges to zero in probability.
Furthermore, as can be seen from the proof of Theorem~3.5 in~\cite{LopuhaaMustaSI2016},
\[
n^{m/(2m+1)}
\left(
\tilde{\lambda}^{SM}_n(x)-\lambda_0(x)
\right)
=
\widetilde{\mu}
+
n^{m/(2m+1)}
\int
\frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,\mathrm{d}({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)+o_p(1),
\]
with $\widetilde{\mu}$ from~\eqref{def:mutilde}.
From the proof of Lemma~\ref{lemma:distr}, we have
\[
\hat{\lambda}_n^{naive}(x)-\lambda_0(x)
=
\phi\Big(w_n(x;\hat\beta_n),v_n(x)\Big)
-
\phi\Big(\Phi(x;\beta_0),\lambda_0(x)\Phi(x;\beta_0)\Big),
\]
where $\phi(w,v)=v/w$ and
\[
n^{m/(2m+1)}
\left(
\begin{bmatrix}
w_n(x;\hat\beta_n)\\
v_n(x)
\end{bmatrix}
-
\begin{bmatrix}
\Phi(x;\beta_0)\\
h(x)
\end{bmatrix}
\right)
=
\begin{bmatrix}
\mu_1\\
\mu_2
\end{bmatrix}
+
\begin{bmatrix}
Z_{n1}\\
Z_{n2}
\end{bmatrix}
+
o(1),
\]
with $Z_{n1}=o_P(1)$ and
\[
\begin{bmatrix}
\mu_1\\
\mu_2
\end{bmatrix}
=
\frac{(-c)^m}{m!}\int_{-1}^1 k(y)y^m\,\mathrm{d}y
\begin{bmatrix}
\Phi^{(m)}(x;\beta_0)\\
h^{(m)}(x)
\end{bmatrix}.
\]
Then with a Taylor expansion it follows that
\[
\begin{split}
n^{m/(2m+1)}
\left(
\hat{\lambda}_n^{naive}(x)-\lambda_0(x)
\right)
&=
\begin{bmatrix}
  -\dfrac{\lambda_0(x)}{\Phi(x;\beta_0)} & \dfrac1{\Phi(x;\beta_0)}
\end{bmatrix}
\left(
\begin{bmatrix}
\mu_1\\
\mu_2
\end{bmatrix}
+
\begin{bmatrix}
Z_{n1}\\
Z_{n2}
\end{bmatrix}
\right)
+
o_p(1)\\
&=
\mu
+
\frac{Z_{n2}}{\Phi(x;\beta_0)}
+
o_p(1),
\end{split}
\]
where $\mu$ is from Theorem~\ref{theo:distrMS}.
Moreover, from the proof of Lemma~\ref{lemma:distr} it can be seen that
\[
\begin{split}
\frac{Z_{n2}}{\Phi(x;\beta_0)}
&=
\frac1{\Phi(x;\beta_0)}
n^{m/(2m+1)}
\int \delta k_b(x-u)\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)+o_P(1)\\
&=
n^{m/(2m+1)}
\int \frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)\\
&\quad
+
n^{m/(2m+1)}
\int \delta k_b(x-u)
\left(
\frac{1}{\Phi(x;\beta_0)}
-
\frac{1}{\Phi(u;\beta_0)}
\right)
\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)+o_P(1)\\
&=
n^{m/(2m+1)}
\int \frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)
+o_P(1),
\end{split}
\]
because
\[
n^{m/(2m+1)}
\int \delta k_b(x-u)
\left(
\frac{1}{\Phi(x;\beta_0)}
-
\frac{1}{\Phi(u;\beta_0)}
\right)
\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)
=
\sum_{i=1}^n
\left(
X_{ni}-{\mathbb{E}}\left[X_{ni}\right]
\right)
\]
with
\[
X_{ni}
=
n^{-(m+1)/(2m+1)}
\Delta_i k_b(x-T_i)
\left(
\frac{1}{\Phi(x;\beta_0)}
-
\frac{1}{\Phi(T_i;\beta_0)}
\right),
\]
where similar to the proof of Lemma~\ref{lemma:distr},
\[
\begin{split}
{\mathbb{E}}\left[X_{ni}^2\right]
&=
n^{-2(m+1)/(2m+1)}
\int k_b^2(x-u)
\left(
\frac{1}{\Phi(x;\beta_0)}
-
\frac{1}{\Phi(u;\beta_0)}
\right)^2 h(u)\,\mathrm{d}u\\
&=
n^{-2(m+1)/(2m+1)}b^{-1}
\int k^2(y)
\left(
\frac{1}{\Phi(x;\beta_0)}
-
\frac{1}{\Phi(x-by;\beta_0)}
\right)^2 h(x-by)\,\mathrm{d}y\\
&=
o(n^{-1}).
\end{split}
\]
We conclude that
\[
\begin{split}
n^{m/(2m+1)}
\left(
\hat{\lambda}_n^{naive}(x)-\lambda_0(x)
\right)
&=
\mu
+
n^{m/(2m+1)}
\int \frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)
+o_P(1)\\
&=
\mu-\widetilde{\mu}
+
n^{m/(2m+1)}
\left(
\tilde{\lambda}^{SM}_n(x)-\lambda_0(x)
\right)
+
o_p(1)
\end{split}
\]
which proves the first statement in the theorem.
The second statement is immediate using the asymptotic equivalence in~\eqref{eq:asymp norm SM SG}.
\end{proof}

\section{Isotonized smoothed Breslow estimator}
\label{sec:GS}

The second method that we consider is an isotonized version of the smoothed Breslow estimator, defined by
\begin{equation}
\label{def:smoothed Breslow}
\Lambda_n^s(x)
=
\int k_b(x-u)\Lambda_n(u)\,\mathrm{d}u.
\end{equation}
In order to avoid problems at the right end of the support, we fix $0<\tau^*<\tau_H$ and consider estimation only on $[0,\tau^*]$.
A similar approach was considered in~\cite{GJ13}, when estimating a monotone hazard of uncensored observations.
The main reason in our setup is that in order to exploit the representation in~\eqref{eqn:lambda0},
we must have $x<\tau_H$, because $\Phi(x;\beta_0)=0$ otherwise.
The isotonized smoothed Breslow estimator of a nondecreasing baseline hazard is a Grenander-type estimator,
as being defined as the left derivative of the greatest convex minorant of $\Lambda_n^s$ on $[0,\tau^*]$.
We denote this estimator by $\tilde{\lambda}_n^{GS}$.
Note that this type of estimator was defined also in~\cite{Nane} without the restriction on $[0,\tau^*]$.
Strong pointwise consistency was proved  and uniform consistency on intervals $[\epsilon, \tau_H-\epsilon]\subset[0,\tau_H]$ follows immediately from the monotonicity and the continuity of $\lambda_0$.
These results also illustrate that there are consistency problems at the end point of the support.
Since in practice we do not even know $\tau_H$, the choice of $\tau^*$ might be an issue.
Since one wants $\tau^*$ to be close to $\tau_H$, one reasonable choice would be to take as $\tau^*$ the $95\%$-empirical quantile of the follow-up times, because this converges to the theoretical $95\%$-quantile,
which is strictly smaller than $\tau_H$.
Note that we cannot choose $T_{(n)}$, because it converges to $\tau_H$,
i.e., for large $n$, it will be greater than any fixed $\tau^*<\tau_H$.

Figure~\ref{fig:GS} shows the smoothed Breslow estimator and the isotonized smoothed Breslow estimator for the same sample as in Figure~\ref{fig:MSLE}.
To avoid problems at the boundary we use the boundary corrected version of the kernel function and consider the data up to the $95\%$-empirical quantile of the follow-up times.
The bandwidth is $b_n=n^{-1/5}$.
\begin{figure}[t]
\includegraphics[bb=28 124 932 544,width=\textwidth,clip=]{SB-GS1}
\caption{Left panel:
The smoothed version (solid) of the Breslow estimator (solid-step function) for the cumulative baseline hazard (dotted)
and the greatest convex minorant (dashed).
Right panel: The Grenander-type smoothed estimator (solid) of the baseline hazard (dotted).}
\label{fig:GS}
\end{figure}
Similar to Lemma~\ref{lem:char MSLE}, it follows from Lemma 1 in~\cite{GJ10},
that $\tilde{\lambda}_n^{GS}$ is continuous and is the unique maximizer of
\[
\psi(\lambda)=\frac{1}{2}
\int_0^{\tau^*}
\left(
\lambda(x)-\lambda_n^s(x)
\right)^2\,\mathrm{d}x
\]
over all nondecreasing functions $\lambda$, where
\begin{equation}
\label{def:vn GS}
\lambda_n^s(x)=\frac{\mathrm{d}}{\mathrm{d}x}\Lambda_n^s(x)=\int k'_b(x-u)\Lambda_n(u)\,\mathrm{d}u.
\end{equation}
This suggests
\begin{equation}
\label{def:naive est GS}
\tilde{\lambda}_n^{\mathrm{naive}}(x)=\lambda_n^s(x)
\end{equation}
as another naive estimator for $\lambda_0(x)$.

Similar to the approach used in Section~\ref{sec:MSLE}, the derivation of the asymptotic distribution
of~$\tilde{\lambda}^{GS}_n$ is based on showing that it is equal to the naive estimator in~\eqref{def:naive est GS}
on large intervals with probability converging to one.
We first obtain the analogue of Lemma~\ref{lem:monotone}.
\begin{lemma}
\label{lem:monotone2}
Let $\lambda_0$ be continuously differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Let $k$ satisfy~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
If $b\to0$ and $1/b=O(n^{\alpha})$, for some $\alpha\in(0,1/4)$, then for each $0<\ell<M<\tau^*$,  it holds
\[
{\mathbb{P}}
\left(
\tilde{\lambda}_n^{\mathrm{naive}}\text{ is increasing on } [\ell,M]
\right)\to 1.
\]
\end{lemma}
\begin{proof}
From~\eqref{def:naive est GS}, it follows with integration by parts that
\begin{equation}
\label{eq:decomp naive GS}
\begin{split}
\tilde{\lambda}_n^{\mathrm{naive}}(x)
&=
\int k_b'(x-u)\Lambda_0(u)\,\mathrm{d}u
+
\int k_b'(x-u)
\left(
\Lambda_n(u)-\Lambda_0(u)
\right)\,\mathrm{d}u\\
&=
\lambda_0(x)+\int k_b(x-u)
\big\{
\lambda_0(u)-\lambda_0(x)
\big\}\,\mathrm{d}u
+
\int k_b(x-u)
\,\mathrm{d}\left(\Lambda_n-\Lambda_0\right)(u),\\
\end{split}
\end{equation}
so that
\begin{equation}
\label{eqn:der.naive2}
\begin{split}
\frac{\mathrm{d}}{\mathrm{d}x}\tilde{\lambda}_n^{\mathrm{naive}}(x)
=
\lambda'_0(x)
&+
\int_{-1}^1 k(y)
\big\{
\lambda'_0(x-by)-\lambda'_0(x)
\big\}\,\mathrm{d}y\\
&+
\frac{1}{b^2}\int k'\left(\frac{x-u}{b}\right)\,\mathrm{d}\left(\Lambda_n-\Lambda_0\right)(u).
\end{split}
\end{equation}
By assumption, the first term on the right hand side of~\eqref{eqn:der.naive2} is
bounded from below by a strictly positive constant and the second term converges to zero because of the continuity of~$\lambda'_0$.
Moreover, let $0<M<M'<\tau_H$, so that for $n$ sufficiently large $M+b<M'$.
Then, the second term on the right hand side of~\eqref{eqn:der.naive2} is bounded from above in absolute value by
\[
\frac{1}{b^2}\sup_{x\in[0,M']}\left|\Lambda_n(x)-\Lambda_0(x)\right|
\sup_{y\in[-1,1]}|k''(y)|
=
O_p(n^{2\alpha-1/2})=o_p(1),
\]
according to~\eqref{eqn:Breslow} and the fact that $\alpha<1/4$.
We conclude that $\tilde{\lambda}_n^{\mathrm{naive}}$ is increasing on $[\ell,M]$ with probability tending to one.
\end{proof}
\begin{re}
Note that in case the kernel function is strictly positive on $(-1,1)$ and the baseline hazard is strictly increasing, one can easily check  that
\[
x\mapsto\int_{-1}^{x/b} k(y) \lambda_0(x-by)\,\mathrm{d}y
\]
is a continuously differentiable, strictly increasing function on $[0,M]$ and as a result we obtain that
\begin{equation}
\begin{split}
\frac{\mathrm{d}}{\mathrm{d}x}\tilde{\lambda}_n^{\mathrm{naive}}(x)
&=
\frac{\mathrm{d}}{\mathrm{d}x}\left(\int_{-1}^{x/b} k(y) \lambda_0(x-by)\,\mathrm{d}y  \right)
+
\frac{1}{b^2}\int k'\left(\frac{x-u}{b}\right)\,\mathrm{d}\left(\Lambda_n-\Lambda_0\right)(u)\\
&\geq C+o_P(1).
\end{split}
\end{equation}
This implies that $\tilde{\lambda}_n^{\mathrm{naive}}$ is increasing on $[0,M]$.
\end{re}
The next corollary is the analogue of Corollary~\ref{cor:1}.
Because in this case the naive estimator in~\eqref{def:naive est GS} is much simpler than
the naive estimator in~\eqref{def:naive est MSLE} corresponding to the MLSE,
the proof does not require preparatory results such as Lemmas~\ref{le:1}-\ref{le:2}.
These are obtained along the way, see~\eqref{eq:Lemma54 GS} and~\eqref{eq:Lemma53 GS}.
\begin{cor}
\label{cor:1-2}
Under the assumptions of Lemma~\ref{lem:monotone2}, it holds that, for each $0<\ell<M<\tau^*$,
\[
{\mathbb{P}}
\left(
\tilde{\lambda}_n^{\mathrm{naive}}(x)=\tilde{\lambda}_n^{GS}(x)\text{ for all } x\in[\ell,M]
\right)\to 1.
\]
Consequently, for all $x\in(0,\tau^*)$, the asymptotic distributions of $\tilde{\lambda}_n^{\mathrm{naive}}(x)$
and $\tilde{\lambda}_n^{GS}(x)$ are the same.
\end{cor}
\begin{proof}
The proof is completely similar to that of Corollary~\ref{cor:1}.
We recall that~$\tilde{\lambda}^{GS}_n(x)$ is defined as the slope of the greatest convex minorant $\hat\Lambda_n^s$
on $[0,\tau^*]$ of the smoothed Breslow estimator.
Define the linearly extended  version of $\hat\Lambda_n^s$ by
\[
\hat\Lambda_n^*(x)
=
\begin{cases}
\hat\Lambda_n^s(\ell)+\big(x-\ell\big)\tilde{\lambda}_n^{\mathrm{naive}}(\ell), &\text{ for } x\in[0,\ell),\\
\hat\Lambda_n^s(x), &\text{ for } x\in[\ell,M],\\
\hat\Lambda_n^s(M)+\big(x-M\big)\tilde{\lambda}_n^{\mathrm{naive}}(M), &\text{ for } x\in(M,\tau^*].
\end{cases}
\]
It suffices to prove that, for sufficiently large $n$,
\begin{equation}
\label{eq:cor prop1 GS}
{\mathbb{P}}
\left(
\hat\Lambda_n^*\text{ is convex on }[0,\tau^*]
\right)\geq 1-\epsilon/2,
\end{equation}
and
\begin{equation}
\label{eq:cor prop2 GS}
{\mathbb{P}}
\left(
\hat\Lambda_n^*(x)\leq \hat\Lambda_n^s(x),
\text{ for all }x\in [0,\tau^*]
\right)\geq 1-\epsilon/2.
\end{equation}
To prove~\eqref{eq:cor prop1 GS}, define the event
\[
A_n=\left\{\tilde{\lambda}_n^{\mathrm{naive}} \text{ is increasing on } [\ell-\eta_1,M+\eta_2]\right\},
\]
for $\eta_1\in(0,\ell)$ and $\eta_2\in(0,\tau^*-M)$.
Note that for the intervals $[0,\ell)$ and $(M,\tau^*]$,
the graph of $\hat\Lambda_n^*$ is the tangent line of the graph of $\hat\Lambda_n^s$
at the points $\big(\ell,\hat\Lambda_n^s(\ell)\big)$ and $\big(M,\hat\Lambda_n^s(M)\big)$.
As a result, on the event $A_n$ the curve is convex, so that together with Lemma~\ref{lem:monotone2},
for sufficiently large $n$
\[
{\mathbb{P}}
\left(
\hat\Lambda_n^*\text{ is convex on }[0,\tau^*]
\right)
\geq
{\mathbb{P}}(A_n)
\geq
1-\epsilon/2.
\]
To prove~\eqref{eq:cor prop2 GS}, we split the interval $[0,\tau^*]$ in five different intervals
$I_1=[0,\ell-\eta_1),$ $I_2=[\ell-\eta_1,\ell),$ $I_3=[\ell,M],$ $I_4=(M,M+\eta_2]$, and $I_5=(M+\eta_2,\tau^*]$,
and show that
\begin{equation}
\label{eqn:C GS}
{\mathbb{P}}
\left(
\hat\Lambda_n^*(x)\leq \hat\Lambda_n^s(x),\text{ for all }x\in I_i
\right)\geq 1-\epsilon/10,
\quad
i=1,\ldots,5.
\end{equation}
For $x\in I_3,$ $\hat\Lambda_n^*(x)=\hat\Lambda_n^s(x)$ and thus~\eqref{eqn:C GS} is trivial.
For $x\in I_2$, by the mean value theorem,
\[
\hat\Lambda_n^s(x)-\hat\Lambda_n^s(\ell)
=
\tilde{\lambda}_n^{\mathrm{naive}}(\xi_x)(x-\ell)
\]
for some $\xi_x\in[x,\ell]$.
Thus, since $x\leq \ell$, similar to~\eqref{eq:argument I2},
\[
{\mathbb{P}}
\left(
\hat\Lambda_n^*(x)\leq \hat\Lambda_n^s(x),\text{ for all }x\in I_2
\right)
\geq
{\mathbb{P}}(A_n)\geq 1-\epsilon/10,
\]
for $n$ sufficiently large, according to Lemma~\ref{lem:monotone2}.
The argument for $x\in I_4$ is exactly the same.

For intervals $I_1$ and $I_5$,
\[
\begin{split}
&
\left|\hat\Lambda_n^s(x)-\Lambda_0(x)\right|\\
&=
\left|\int k_b(x-u)
\left(\Lambda_n(u)-\Lambda_0(u)\right)\,\mathrm{d}u
+
\int k_b(x-u)\Lambda_0(u)\,\mathrm{d}u-\Lambda_0(x)\right|\\
&\leq \left|
\int_{-1}^{1} k(y)
\left(\Lambda_n(x-by)-\Lambda_0(x-by)\right)\,\mathrm{d}y\right|
+
\left|\int_{-1}^{1} k(y)\Lambda_0(x-by)\,\mathrm{d}y-\Lambda_0(x)\right|\\
&\leq
\sup_{x\in[0,\tau^*]}\left|\Lambda_n(x)-\Lambda_0(x)\right|\int_{-1}^1 |k(y)|\,\mathrm{d}y
+
\left|\int_{-1}^{x/b} k(y)\Lambda_0(x-by)\,\mathrm{d}y-\Lambda_0(x)\right|.
\end{split}
\]
According to~\eqref{eqn:Breslow}, the first term on the right hand side is of the order $O_p(n^{-1/2})$.
For the second term we distinguish between $x\geq b$ and $x<b$.
When $x\geq b$, then with~\eqref{def:kernel},
\[
\int_{-1}^{x/b} k(y)\Lambda_0(x-by)\,\mathrm{d}y-\Lambda_0(x)
=
\int_{-1}^1 k(y)
\Big\{
\Lambda_0(x-by)-\Lambda_0(x)
\Big\}\,\mathrm{d}y
\to0.
\]
When $x<b$, then again with~\eqref{def:kernel}, we can write
\[
\int_{-1}^{x/b} k(y)\Lambda_0(x-by)\,\mathrm{d}y-\Lambda_0(x)
=
\int_{-1}^{x/b} k(y)
\Big\{
\Lambda_0(x-by)-\Lambda_0(x)
\Big\}\,\mathrm{d}y
+
\Lambda_0(x)\int_{x/b}^1 k(y)\,\mathrm{d}y,
\]
which also tends to zero, because for $x\in[0,b]$, one has $\Lambda_0(x)\leq b\lambda_0(b)\to 0$.
It follows that for $x$ in $I_1$ and $I_5$
\begin{equation}
\label{eq:Lemma54 GS}
\left|\hat\Lambda_n^s(x)-\Lambda_0(x)\right|=o_P(1).
\end{equation}
Moreover, for $x=\ell,M$, it holds
\begin{equation}
\label{eq:Lemma53 GS}
\begin{split}
&
\tilde{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)\\
&=
\int k_b(x-u)\lambda_0(u)\,\mathrm{d}u-\lambda_0(x)
+
\int k_b(x-u)\,\mathrm{d}\left(\Lambda_n(u)-\Lambda_0(u)\right)\\
&=
\int_{-1}^1 k(y)
\left\{
\lambda_0(x-by)-\lambda_0(x)
\right\}\,\mathrm{d}y
+
b^{-1}
\int_{-1}^1
\left(\Lambda_n(x-by)-\Lambda_0(x-by)\right)
k'(y)\,\mathrm{d}y\\
&=
o_p(1).
\end{split}
\end{equation}
It now follows for $x\in I_1$,
\[
\Lambda_0(x)-\Lambda_0(\ell)
-
\lambda_0(\ell)(x-\ell)
=
\int_x^\ell\left(\lambda_0(\ell)-\lambda_0(u)\right)\,\mathrm{d}u
\geq
\int_{\ell-\eta_1}^\ell\left(\lambda_0(\ell)-\lambda_0(u)\right)\,\mathrm{d}u.
\]
This implies that
\[
\begin{split}
\hat\Lambda_n^s(x)-\hat\Lambda_n^*(x)
&=
\hat\Lambda_n^s(x)-
\hat\Lambda_n^s(\ell)-\big(x-\ell\big)\tilde{\lambda}_n^{\mathrm{naive}}(\ell)\\
&\geq
\hat\Lambda_n^s(x)-\Lambda_0(x)+\Lambda_0(\ell)-\hat\Lambda_n^s(\ell)+
\big(\tilde{\lambda}_n^{\mathrm{naive}}(\ell)-\lambda_0(\ell)\big)\big(\ell-x\big)\\
&\qquad+
\int_{\ell-\eta_1}^{\ell}\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}u\\
&\geq
-2\sup_{x\in[0,\ell]}|\hat\Lambda_n^s(x)-\Lambda_0(x)|
-
\ell|\tilde{\lambda}_n^{\mathrm{naive}}(\ell)-\lambda_0(\ell)|\\
&\qquad+
\int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}u.
\end{split}
\]
According to~\eqref{eq:Lemma54 GS} and~\eqref{eq:Lemma53 GS} the first two terms on the right hand side tend to zero in probability.
This means that the probability on the left hand side of~\eqref{eqn:C GS} for $i=1$, is bounded
from below by
\[
{\mathbb{P}}
\left(
X_n\leq \int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\mathrm{d}u
\right)
\]
where $X_n$ tends to zero in probability.
This probability is greater than $1-\epsilon/10$ for $n$ sufficiently large, since
\[
\int_{\ell-\eta_1}^\ell\big(\lambda_0(\ell)-\lambda_0(u)\big)\,\mathrm{d}u
>0
\]
using that $\lambda_0$ is strictly increasing.
For $I_5$ we can argue exactly in the same way.
\end{proof}

Finally, consistency and the asymptotic distribution of $\tilde{\lambda}_n^{GS}(x)$ is provided by the next theorem.
\begin{theo}
\label{theo:as.distrGS}
Fix $x\in(0,\tau_H)$ and $\tau^*\in(x,\tau_H)$.
Assume that $\lambda_0$ is $m\geq2$ times continuously differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Assume that $t\mapsto \Phi(t;\beta_0)$ is continuous in a neighborhood of $x$.
Let~$k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
Let~$\tilde{\lambda}_n^{GS}$ be the left derivative of the greatest convex minorant on $[0,\tau^*]$ 
of $\Lambda_n^s$ defined in~\eqref{def:smoothed Breslow}
and suppose that $n^{1/(2m+1)}b\to c>0$.
Then, for all $0<\ell<M<\tau^*$,
\[
\sup_{x\in[\ell,M]}
\left|
\tilde{\lambda}_n^{GS}(x)-\lambda_0(x)
\right|
=
O(b^m)+O_p(b^{-1}n^{-1/2}),
\]
in probability, and it holds that
\[
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{GS}(x)-\lambda_0(x)
\right)
\xrightarrow{d}N(\mu,\sigma^2),
\]
where
\[
\mu
=
\frac{(-c)^m}{m!}
\lambda_0^{(m)}(x)
\int_{-1}^1 k(y)y^m\,\mathrm{d}y
\quad\text{ and }\quad
\sigma^2
=
\frac{\lambda_0(x)}{c\Phi(x;\beta_0)}
\int_{-1}^1 k(y)^2\,\mathrm{d}y.
\]
\end{theo}
\begin{proof}
From~\eqref{eq:Lemma53 GS}, similar to~\eqref{eq:taylor h},
we find
\[
\begin{split}
&
\tilde{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)\\
&=
\int_{-1}^1 k(y)
\left\{
\lambda_0(x-by)-\lambda_0(x)
\right\}\,\mathrm{d}y
+
b^{-1}
\int_{-1}^1
\left(\Lambda_n(x-by)-\Lambda_0(x-by)\right)
k'(y)\,\mathrm{d}y\\
&=
\frac{(-b)^m}{m!}
\int_{-1}^1
\lambda_0^{(m)}(\xi_{xy})k(y)y^m\,\mathrm{d}y
+
b^{-1}
\int_{-1}^1
\left(\Lambda_n(x-by)-\Lambda_0(x-by)\right)
k'(y)\,\mathrm{d}y,
\end{split}
\]
for some $|\xi_{xy}-x|\leq |by|$.
It follows that
\[
\begin{split}
&
\sup_{x\in[\ell,M]}
\left|
\tilde{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)
\right|\\
&\leq
\frac{b^m}{m!}
\sup_{t\in[0,\tau_H]}\left|\lambda_0^{(m)}(t)\right|
\int_{-1}^1
|k(y)||y|^m\,\mathrm{d}y
+
b^{-1}
\sup_{x\in[\ell,M]}
\left|
\Lambda_n(x)-\Lambda_0(x)
\right|
\int_{-1}^1
|k'(y)|\,\mathrm{d}y
=
o_p(1).
\end{split}
\]
Similar to~\eqref{eq:h term}, the first term on the right hand side is of the order $O(b^m)$,
and according to~\eqref{eqn:Breslow} the second term is of the order $O_p(b^{-1}n^{-1/2})$.
The first statement now follows directly from Corollary~\ref{cor:1-2}.

To obtain the asymptotic distribution, note that from~\eqref{eq:decomp naive GS}, \eqref{eqn:lambda0} and~\eqref{eqn:Breslow}, we have
\begin{equation}
\label{eqn:asymptotic-mean2}
\begin{split}
&n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{\mathrm{naive}}(x)-\lambda_0(x)
\right)\\
&=
n^{m/(2m+1)}
\left(
\int k_b(x-u)\,\lambda_0(u)\,\mathrm{d}u-\lambda_0(x)
\right)\\
&\quad+
n^{m/(2m+1)}
\int k_b(x-u)\frac{\delta}{\Phi(u;\beta_0)}\mathrm{d}({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)\\
&\qquad+
n^{-(m+1)/(2m+1)}
\sum_{i=1}^n k_b(x-T_i)\Delta_i
\left(
\frac{1}{\Phi_n(T_i;\hat{\beta}_n)}-\frac{1}{\Phi(T_i;\beta_0)}
\right).
\end{split}
\end{equation}
We find that,
the first term in the right hand side of~\eqref{eqn:asymptotic-mean2} converges to $\mu$,
since
\[
\begin{split}
&
n^{m/(2m+1)}
\left(
\int k_b(x-u)\,\lambda_0(u)\,\mathrm{d}u-\lambda_0(x)
\right)\\
&=
n^{m/(2m+1)}
\int_{-1}^1 k(y)
\left\{
\lambda_0(x-by)-\lambda_0(x)
\right\}\,\mathrm{d}y\\
&=
n^{m/(2m+1)}
\frac{(-b)^m}{m!}
\int_{-1}^1
\lambda_0^{(m)}(\xi_{xy})k(y)y^m\,\mathrm{d}y
\to
\frac{(-c)^m}{m!}
\lambda_0^{(m)}(x)
\int_{-1}^1 k(y)y^m\,\mathrm{d}y,
\end{split}
\]
for some $|\xi_{xy}-x|\leq |by|$.
Let $0<M<M'<\tau_H$, so that $x+b\leq M'$ for sufficiently large $n$.
Because $1/\Phi_n(M';\hat{\beta}_n)=O_p(1)$, similar to~\eqref{eq:Phihat-Phi0}
\[
\sup_{u\in[0,M']}
\left|
\frac{1}{\Phi_n(u;\hat{\beta}_n)}-\frac{1}{\Phi(u;\beta_0)}
\right|
\leq
\sup_{u\in[0,M']}
\left|
\frac{\Phi_n(u;\hat{\beta}_n)-\Phi(u;\beta_0)}{\Phi_n(M';\hat{\beta}_n)\Phi(M';\beta_0)}
\right|
=
O_p(n^{-1/2}),
\]
and similar to~\eqref{eq:EYn2^2}
\[
\text{Var}
\left(
n^{-(m+1)/(2m+1)}
\sum_{i=1}^n
\left|
k_b(x-T_i)
\right|\Delta_i\right)
=
O(n^{-1}),
\]
so that the last term on the right hand side of~\eqref{eqn:asymptotic-mean2} converges to zero in probability.
The second term on the right hand side of~\eqref{eqn:asymptotic-mean2} can be written as
\[
\sum_{i=1}^n
\left(
Y_{ni}-{\mathbb{E}}\left[Y_{ni}\right]
\right),
\quad
Y_{ni}
=
n^{-(m+1)/(2m+1)}
k_b(x-T_i)\frac{\Delta_i}{\Phi(T_i;\beta_0)}.
\]
where similar to~\eqref{eq:EYn2^2},
\begin{equation}
\label{eq:EYn2^2 GS}
\begin{split}
\text{Var}(Y_{ni})
&=
{\mathbb{E}}\left[Y_{ni}^2\right]+O(n^{-2(m+1)/(2m+1)})\\
&=
n^{-2(m+1)/(2m+1)}b^{-1}
\int_{-1}^1 \frac{k^2(y)h(x-by)}{\Phi^2(x-by;\beta_0)}\,\mathrm{d}y
+
O\left(n^{-2(m+1)/(2m+1)}\right)\\
&=
n^{-1}\sigma^2+o(n^{-1}).
\end{split}
\end{equation}
Moreover,
\[
\sum_{i=1}^n
{\mathbb{E}}\left[|Y_{ni}|^2{\mathds{1}}_{\{|Y_{ni}|>\epsilon \}} \right]
\leq
n^{-2(m+1)/(2m+1)}b^{-2}
\sup_{y\in[-1,1]}|k(y)|
\sum_{i=1}^n{\mathbb{P}}\left(|Y_{ni}|>\epsilon\right),
\]
where the right hand side tends to zero, because with~\eqref{eq:EYn2^2 GS},
\[
\sum_{i=1}^n
{\mathbb{P}}\left(|Y_{ni}|>\epsilon\right)
\leq
\sum_{i=1}^n
\frac{{\mathbb{E}}|Y_{ni}|^2}{\epsilon^2}
=
O(1).
\]By Lindeberg-Feller central limit theorem, we obtain
\[
\sum_{i=1}^n \left(Y_{ni}-{\mathbb{E}}\left[Y_{ni}\right]\right)\xrightarrow{d}N(0,\sigma^2),
\]
which determines the asymptotic distribution of $\tilde{\lambda}_n^{\mathrm{naive}}(x)$.
Then, by Corollary~\ref{cor:1-2}, this also gives the asymptotic distribution of $\tilde{\lambda}^{GS}_n(x)$.
\end{proof}
As can be seen from~\eqref{eq:asymp norm SM SG} and~\eqref{def:mutilde}, the limiting distribution of the
isotonized smooth Breslow estimator in Theorem~\ref{theo:as.distrGS} is completely the same the one for the smoothed MLE and smoothed
Grenander-type estimator, as provided by Theorem~3.5 in~\cite{LopuhaaMustaSI2016}.
The following theorem shows that $\tilde{\lambda}_n^{GS}(x)$ is in fact asymptotically equivalent
to both these estimators.
In particular, this means that the order of smoothing and isotonization for the Grenander-type estimator
yields exactly the same limit behavior.
This is in line with results in ~\cite{mammen1991} and~\cite{vdvaart-vdlaan2003}.
\begin{theo}
Fix $x\in(0,\tau_h)$ and $\tau^*\in(x,\tau_H)$.
Assume that $\lambda_0$ is $m\geq2$ times continuously differentiable, with $\lambda'_0$ uniformly bounded from below by a strictly positive constant.
Assume that $t\mapsto \Phi(t;\beta_0)$ is differentiable with a bounded derivative in a neighborhood of $x$.
Let~$k$ be $m$-orthogonal satisfying~\eqref{def:kernel} and assume that it is twice differentiable with a bounded second derivative.
Let~$\tilde{\lambda}_n^{GS}$ be the left derivative of the greatest convex minorant on $[0,\tau^*]$ of $\Lambda_n^s$ defined in~\eqref{def:smoothed Breslow}
and suppose that $n^{1/(2m+1)}b\to c>0$.
Let $\tilde\lambda_n^{SG}$ be the smoothed Grenander-type estimator defined in~\cite{LopuhaaMustaSI2016}.
Then
\[
n^{m/(2m+1)}
\left(
\hat\lambda_n^{GS}(x)-\tilde\lambda_n^{SG}(x)
\right)
\to 0,
\]
in probability, and similarly if we replace $\tilde\lambda_n^{SG}(x)$
by the smoothed maximum likelihood estimator $\hat\lambda_n^{SM}(x)$, defined in~\cite{LopuhaaMustaSI2016}.
This also holds if we replace~$\tilde{\lambda}^{GS}_n(x)$ with $\tilde{\lambda}_n^{\mathrm{naive}}(x)$, defined in~\eqref{def:naive est GS}.
\end{theo}
\begin{proof}
We write
\[
\begin{split}
&
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{SG}(x)-\tilde{\lambda}_n^{GS}(x)
\right)\\
&=
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{SG}(x)-\tilde{\lambda}_n^{naive}(x)
\right)
+
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{naive}(x)-\tilde{\lambda}_n^{GS}(x)
\right).
\end{split}
\]
By Corollary~\ref{cor:1-2}, the second term on the right hand side converges to zero in probability.
Furthermore, as can be seen from the proof of Theorem~3.5 in~\cite{LopuhaaMustaSI2016},
\[
\begin{split}
&
n^{m/(2m+1)}
\tilde{\lambda}^{SG}_n(x)\\
&=
n^{m/(2m+1)}
\int k_b(x-u)\,\mathrm{d}\Lambda_0(u)
+
n^{m/(2m+1)}
\int
\frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,\mathrm{d}({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)+o_p(1).
\end{split}
\]
Similarly, from the proof of Theorem~\ref{theo:as.distrGS}, we have
\[
\begin{split}
&
n^{m/(2m+1)}
\tilde{\lambda}_n^{naive}(x)
=
n^{m/(2m+1)}
\int k_b'(x-u)\Lambda_n(u)\,\mathrm{d}u\\
&=
n^{m/(2m+1)}
\int k_b(x-u)\,\mathrm{d}\Lambda_n(u)\\
&=
n^{m/(2m+1)}
\int k_b(x-u)\,\mathrm{d}\Lambda_0(u)
+
n^{m/(2m+1)}
\int k_b(x-u)\,\mathrm{d}(\Lambda_n(u)-\Lambda_0(u))\\
&=
n^{m/(2m+1)}
\int k_b(x-u)\,\mathrm{d}\Lambda_0(u)
+
n^{m/(2m+1)}
\int
\frac{\delta k_b(x-u)}{\Phi(u;\beta_0)}\,\mathrm{d}({\mathbb{P}}_n-{\mathbb{P}})(u,\delta,z)+o_p(1).
\end{split}
\]
From this it immediately follows that
\[
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{SG}(x)-\tilde{\lambda}_n^{naive}(x)
\right)
=
o_p(1),
\]
and hence, with Corollary~\ref{cor:1-2}, also
\[
n^{m/(2m+1)}
\left(
\tilde{\lambda}_n^{SG}(x)-\tilde{\lambda}_n^{GS}(x)
\right)
=
o_p(1).
\]
The second statement about $\hat{\lambda}_n^{SM}(x)$, is immediate using the asymptotic equivalence in~\eqref{eq:asymp norm SM SG}.
\end{proof}

\section{Numerical results for pointwise confidence intervals}
\label{sec:conf-int}

In this section we illustrate the finite sample performance of the two estimators considered
in Sections~\ref{sec:MSLE} and~\ref{sec:GS} by constructing pointwise confidence intervals for the baseline hazard rate.
From Theorems~\ref{theo:distrMS} and~\ref{theo:as.distrGS},
it can be seen that the asymptotic $100(1-\alpha)\%$-confidence intervals at the point $x_0\in(0,\tau_H)$
are of the form
\begin{equation}
\label{def:confint}
\widehat\lambda_n^{IS}(x_0)
\pm n^{-2/5}
\left\{
\widehat{\sigma}_n(x_0)q_{1-\alpha/2}+\widehat{\mu}_n(x_0)
\right\},
\end{equation}
where $q_{1-\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution,
$\widehat\lambda_n^{IS}(x_0)$ is the isotonized smooth estimator at hand (MSLE or GS),
and $\widehat{\sigma}_n(x_0)$, $\widehat{\mu}_n(x_0)$ are corresponding plug-in estimators of the asymptotic mean and standard deviation, respectively.
In order to avoid estimation of the bias, we use undersmoothing (see also~\cite{Hall92}, \cite{GJ15}, \cite{CHT06}).

In our simulations, the event times are generated from a Weibull baseline distribution with shape parameter $1.5$ and scale
parameter $1$.
The real valued covariate and the censoring time are chosen to be uniformly distributed on the interval $(0,1)$ and we take $\beta_0=0.5$.
We note that this setup corresponds to around $35\%$ uncensored observations.
Confidence intervals are calculated at the point $x_0=0.5$ using 1000 sets of data and
we take the bandwidth $b=n^{-1/4}$.

Table~\ref{tab:1}
\begin{table}[h]
\begin{tabular}{ccccc}
\toprule
      &    \multicolumn{2}{c}{MSLE}&   \multicolumn{2}{c}{GS}  \\
$n$     & AL    & CP & AL    & CP    \\
50    &  1.456 & 0.727 & 1.392 & 0.721 \\
100   &  1.035 & 0.779 & 1.007 & 0.740  \\
500   &  0.562 & 0.840 & 0.554 & 0.841 \\
1000  &  0.424 & 0.846 & 0.428 & 0.843 \\
5000  &  0.234 & 0.892  & 0.234 & 0.888 \\
\bottomrule
\end{tabular}
\caption{The average length (AL) and the coverage probabilities (CP) for $95\%$ pointwise confidence intervals of the baseline hazard rate at the point $x_0=0.5$ based on the asymptotic distribution. }
\label{tab:1}
\end{table}
shows the performance, for various sample sizes, of the confidence intervals given in~\eqref{def:confint}.
Even though the coverage probabilities are far from the nominal level of $95\%$, smoothing leads to significantly better results in comparison with the non-smoothed estimators
(e.g., the traditional Grenander-type estimator considered in~\cite{LopuhaaNane2013} has coverage probabilities $0.468$, $0.479$, $0.593$, $0.642$ and $0.783$).
The performance depends strongly on the choice of the constant $c$, because the asymptotic length is inversely proportional to $c$.
This means that, by choosing a larger $c$, one will get wider confidence intervals and higher coverage probabilities.
Unfortunately, it is not clear what would be the optimal choice of such a constant.
This is actually a common problem in the literature (see for example~\cite{CHT06} and \cite{GCM96}),
but it is beyond the scope of this paper to investigate methods of bandwidth selection.
We notice that the two estimators are comparable with each other.
\begin{figure}[t]
\centering
\subfloat[][MSLE]
{\includegraphics[bb=189 44 770 625,width=0.45\textwidth,clip=]{conf-int-MSLE1}} \quad
\subfloat[][GS]
{\includegraphics[bb=189 44 770 625,width=0.45\textwidth,clip=]{conf-int-GS1}}
\caption{$95\%$ confidence intervals based on the asymptotic distribution for the baseline hazard rate using undersmoothing.}
\label{fig:subfig4}
\end{figure}
The behavior of the two methods for a fixed sample size~$n=500$ at different points of the support is illustrated in Figure~\ref{fig:subfig4}.
The results are again comparable and the common feature is that the length increases as we move to the left boundary.
This is due to the fact that the length is proportional to the asymptotic standard deviation, which in this case turns out to be increasing.

\printbibliography

\end{document}

